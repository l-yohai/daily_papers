## Daily Papers (2025-01-13)

### [VideoRAG: Retrieval-Augmented Generation over Video Corpus](https://arxiv.org/abs/2501.05874)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.05874.png)

Vote: 40

Authors: Sung Ju Hwang, Soyeong Jeong, Kangsan Kim, Jinheon Baek

- ***What's New***: VideoRAG는 기존의 Retrieval-Augmented Generation(RAG) 접근 방식에 새로운 방향을 제시하며, 쿼리와 관련성이 높은 비디오를 동적으로 검색하고 그 시각적 및 텍스트적 요소를 종합적으로 활용하여 응답을 생성하는 혁신적인 프레임워크입니다. 이는 텍스트나 이미지만을 주로 사용했던 기존 RAG 방식을 비디오를 포함한 다중 모달 지식을 활용하도록 확장합니다.
- ***Technical Details***: VideoRAG의 주요 구성 요소는 대형 비디오 언어 모델(Large Video Language Models; LVLMs)을 사용하여 비디오 내용을 직접 처리하고 검색 및 생성 과정에서 시각적 텍스트 정보를 통합합니다. 검색 단계에서는 쿼리와 비디오 간의 표현적 유사성을 계산하여 관련성 높은 비디오를 식별하며, 생성 단계에서는 이러한 비디오의 프레임과 텍스트 데이터를 함께 활용하여 응답을 생성합니다. 일부 비디오에 텍스트가 없을 경우, 자동 음성 인식(Automatic Speech Recognition; ASR) 기법을 활용하여 텍스트 전사를 생성하는 전략도 제시합니다.
- ***Performance Highlights***: VideoRAG는 WikiHowQA와 HowTo100M 데이터셋을 활용한 실험에서 기존 RAG 베이스라인을 뛰어넘는 성능을 보였습니다. 특히, 시각적 정보가 포함된 응답 생성에서 의미 있는 성능 향상이 있었으며, 시각적 및 텍스트적 특징을 결합함으로써 검색 및 생성 성능이 증가하였습니다. 이는 비디오가 가진 다중 모달 리치니스를 RAG 시스템에 효과적으로 통합할 수 있음을 보여줍니다.

### [OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints](https://arxiv.org/abs/2501.03841)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03841.png)

Vote: 38

Authors: Mingjie Pan, Tianshu Wu, Wenlong Gao, Yinghao Zhao, Jiyao Zhang, Hao Dong

- ***What's New***: OmniManip은 객체 중심의 상호작용 프리미티브(Object-Centric Interaction Primitives)를 공간적 제약(Spatial Constraints)으로 변환하여, 비정형 환경에서 로봇의 일반적인 조작을 가능케 하는 새로운 접근 방식을 제시합니다. 이 모델은 비전-언어 모델(Vision-Language Model; VLM)의 고수준 추론과 세밀한 3D 공간 이해 사이의 간격을 메우며, VLM의 세부 튜닝 없이도 실행할 수 있는 이중 폐쇄 루프(Open-Loop) 시스템을 도입했습니다.
- ***Technical Details***: OmniManip은 객체의 정준 공간(Canonical Space) 내에서 상호작용 프리미티브를 사용하여 조작 작업을 구성하고 설명합니다. 이는 개체의 주요 축을 따라 상호작용 방향을 샘플링하여 저수준의 실행을 지원하는 방식입니다. 또한, 상호작용 렌더링과 프리미티브 리샘플링(Primitive Resampling)을 통한 자가 수정 메커니즘(Self-Correction Mechanism)을 통해 VLM의 오류를 줄이고, 명확한 작업 실행을 보장합니다.
- ***Performance Highlights***: OmniManip은 다양한 로봇 조작 작업에서 강력한 제로샷(Zero-Shot) 범용성을 보이며, 개별적인 작업 훈련 없이 우수한 성능을 기록했습니다. 특히 계획 단계에서의 폐쇄 루프 계획(Closed-Loop Planning)은 기존 방법에 비해 15% 이상의 성능 향상을 보여주며, 시스템의 신뢰성을 높입니다.

### [LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs](https://arxiv.org/abs/2501.06186)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06186.png)

Vote: 32

Authors: Ritesh Thawkar, Ahmed Heakl, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Ketan More, Fahad Shahbaz Khan, Mubarak Shah, Noor Ahsan, Yuhao Li, Ivan Laptev, Jean Lahoud, Dinura Dissanayake, Salman Khan, Mohammed Zumri

- ***What's New***: LlamaV-o1는 대형 언어 모델(Large Language Models; LLMs)의 단계별 시각적 추론을 재고하기 위한 새로운 프레임워크를 제안합니다. 이 연구는 시각적 추론 체인 벤치마크(Visual Reasoning Chain Benchmark; VRC-Bench)를 소개하여 다단계 추론을 평가하며, 각 단계를 세부적으로 평가하는 새로운 메트릭을 제안합니다. 또한, 여러 유형의 문제를 다룰 수 있도록 단계별 커리큘럼 학습을 활용한 새로운 멀티모달 시각적 추론 모델 LlamaV-o1을 도입했습니다.
- ***Technical Details***: VRC-Bench는 8개의 범주로 나뉜 1000개 이상의 샘플을 포함하며, 4000개 이상의 논리적 추론 단계를 평가합니다. 새로운 평가 메트릭은 단계별로 시각적 추론의 정확성과 논리적 일관성을 강조합니다. LlamaV-o1은 커리큘럼 학습(curriculum learning) 방식으로 훈련되며, Beam Search를 활용하여 효율적인 추론을 가능케 했습니다.
- ***Performance Highlights***: LlamaV-o1은 기존 오픈 소스 모델보다는 뛰어나고, 닫힌 소스 독점 모델들과 비교해 경쟁력 있는 성능을 발휘합니다. 최근 Llava-CoT와 비교하여 LlamaV-o1은 평균 점수 면에서 3.8%의 절대적 향상을 이루고, 추론 확장(추론 처리 속도) 시 5배 빠른 성능을 나타냅니다.

### [OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?](https://arxiv.org/abs/2501.05510)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.05510.png)

Vote: 27

Authors: Shuangrui Ding, Yuhang Zang, Ziyang Miao, Pan Zhang, Yuanhang Zhou, Yifei Li, Rui Qian, Yuhang Cao, Conghui He, Jiaqi Wang, Xiaoyi Dong, Qihao He, Haodong Duan, Junbo Niu, Chunjiang Ge

- ***What's New***: OVO-Bench는 온라인 비디오 이해를 위한 새로운 벤치마크로, 비디오 LLM의 시간 인식 능력을 세 가지 시나리오에서 평가합니다: 과거 추적(Backward Tracing), 실시간 이해(Real-Time Understanding), 미래 능동적 응답(Forward Active Responding)입니다. 이 벤치마크는 구체적인 타임스탬프가 있는 약 2,800개의 메타 주석을 포함한 644개의.unique 비디오로 구성됩니다.
- ***Technical Details***: OVO-Bench는 다양한 소스로부터 수집된 비디오와 인간의 큐레이션을 통해 생성된 샘플을 사용합니다. 비디오 스트림에 따라 비디오 LLMs를 체계적으로 쿼리하는 평가 파이프라인을 개발했습니다. 가정된 12개 과제는 체계적으로 나뉜 세 가지 범주에 걸쳐 모델의 문제 해결 능력을 평가합니다. 다양한 길이의 프레임과 클립을 기반으로 평가가 진행됩니다.
- ***Performance Highlights***: 비디오 LLMs는 전통적인 벤치마크에서는 높은 성과를 보였으나, 온라인 비디오 이해에서는 인간 에이전트와의 큰 차이를 보여주었습니다. 기존 모델들은 다중 오답 상황에서 실시간 인식이 부족함을 나타내며, Gemini 1.5 Pro와 같은 모델도 실시간 인식에서 54.49%에서 66.97% 사이의 성능을 기록하여 인간의 92.81% 대비 큰 차이를 보였습니다.

### [Enabling Scalable Oversight via Self-Evolving Critic](https://arxiv.org/abs/2501.05727)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.05727.png)

Vote: 18

Authors: Benyou Wang, Dayiheng Liu, Zhengyang Tang, Fei Huang, Zhenyang Xiao, Tian Ding, Bowen Yu, Junyang Lin, Ruoyu Sun, Tianyu Liu, Ziniu Li

- ***What's New***: SCRIT(Self-Evolving CRITic)은 대형 언어 모델(LLMs)이 외부 감독 없이 자체 평가 능력을 발전시킬 수 있는 프레임워크입니다. 이 접근법은 기존의 인간 주석 또는 더 강력한 모델에 의존하는 방식의 한계를 극복하며, 스스로 비평 데이터를 생성하고 검증하여 critique 능력을 향상시킵니다.
- ***Technical Details***: SCRIT는 대조 기반(self-contrastive) 비평 기법과 self-validation 메커니즘을 사용하여 합성 데이터를 기반으로 교육합니다. Qwen2.5-72B-Instruct 모델로 구현되어 있으며, 모델이 스스로 훈련 데이터를 생성하고 비평 능력을 지속적으로 강화할 수 있도록 설계되었습니다. 이 과정은 수학적 문제에 대한 엄격한 해답 및 해결 전략을 참조 솔루션을 통해 이해하여, 학생의 솔루션을 단계별로 비평하게 돕습니다.
- ***Performance Highlights***: SCRIT는 비평-수정(critique-correction) 및 오류 식별 벤치마크에서 최대 10.3%의 성능 향상을 달성했습니다. LLMs의 self-evolving 비평 능력을 활성화하며, 데이터 및 모델의 크기가 증가함에 따라 성능이 강화된다는 것을 실험을 통해 보여줍니다. 이를 통해 외부 감독의 필요성을 줄이고, 자체적이고 확장 가능한 감시 체계를 구현합니다.

### [ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding](https://arxiv.org/abs/2501.05452)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.05452.png)

Vote: 8

Authors: Minqian Liu, Jianwei Yang, Xingyu Fu, Dinei Florencio, Yijuan Lu, Dan Roth, Zhengyuan Yang, Cha Zhang, John Corring

- ***What's New***: REFOCUS는 멀티모달 대형 언어 모델(Multimodal Large Language Models; LLMs)에 '시각적 사고'(Visual Thoughts) 기능을 추가하여 구조적 이미지 이해를 촉진하는 새로운 프레임워크입니다. 입력 이미지를 코드로 수정함으로써 모델의 시각적 초점을 변경하고 정밀화하여 표와 차트를 보다 효과적으로 이해할 수 있게 합니다.
- ***Technical Details***: REFOCUS는 파이썬 코드를 사용하여 입력 이미지를 수정하는 인터페이스를 제공합니다. 이 코드들은 이미지를 수정하여 특정 영역을 강조하거나, 필요 없는 부분을 가리거나, 특정 대상에 초점을 맞추는 등의 작업을 수행합니다. 이러한 시각적 편집 기술은 모델이 선택적 주의를 할 수 있도록 도움을 줍니다.
- ***Performance Highlights***: REFOCUS는 무시했던 GPT-4o와 비교하여, 표 문제에서 평균 11.0%, 차트 문제에서 6.8%의 성능 향상을 달성했습니다. 또한, 차트와 표 데이터를 포함한 14k의 훈련 세트를 사용하여 표준 VQA 데이터보다 더 나은 감독 효과를 제공함으로써 모델의 성능을 평균 8.0% 향상시켰습니다.

### [ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning](https://arxiv.org/abs/2501.04698)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04698.png)

Vote: 7

Authors: Di Zhang, Yuzhou Huang, Pengfei Wan, Ruimao Zhang, Ziyang Yuan, Kun Gai, Qiulin Wang, Quande Liu, Xintao Wang

- ***What's New***: ConceptMaster는 멀티 개념 영상 맞춤화(Multi-Concept Video Customization; MCVC) 작업을 수행하는 혁신적인 프레임워크로, 테스트 시간 튜닝(Test-Time Tuning) 없이 다중 참조 이미지를 기반으로 고품질의 맞춤형 동영상을 생성할 수 있습니다. 주요 혁신점으로는 정체성 분리(identity decoupling) 문제를 효과적으로 해결하면서 개념 충실도를 유지하기 위한 독립적인 멀티 개념 임베딩을 학습하고 이를 디퓨전 트랜스포머 모델에 주입하는 전략을 제안합니다.
- ***Technical Details***: ConceptMaster는 CLIP 이미지 인코더를 사용하여 참조 이미지로부터 시각적 특징을 추출하고, Q-포머(Q-Former) 아키텍처를 통해 이러한 특징을 종합적인 시각적 임베딩으로 변환합니다. 이후 디퓨전 트랜스포머 모델의 개념 충실도를 위해 디커플 어텐션 모듈(Decouple Attention Module; DAM)을 사용해 각 개념의 시각적 및 텍스트 특징을 통합합니다. 마지막으로, 멀티 개념 인젝터(Multi-Concept Injector; MC-Injector)를 사용하여 이러한 복합 임베딩을 독립적으로 주입합니다.
- ***Performance Highlights***: ConceptMaster는 MC-Bench 벤치마크를 통해 기존의 최첨단 방식보다 월등한 성능을 보이며 다중 개념 시나리오에서 개념 충실도, 정체성 분리 역량, 그리고 비디오 생성 품질에서 우수한 평가를 받았습니다. 종합적인 평가에서 CLIP-T와 DINO-I 점수 등 여러 지표에서 최고 혹은 준 최고의 결과를 달성해 맞춤형 비디오 생성의 새로운 가능성을 제시했습니다.

### [Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains](https://arxiv.org/abs/2501.05707)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.05707.png)

Vote: 6

Authors: Antonio Torralba, Vighnesh Subramaniam, Joshua B. Tenenbaum, Yilun Du, Shuang Li, Igor Mordatch

- ***What's New***: 이 논문에서는 다중 에이전트(finetuning)를 제안하며, 다양한 추론 체인을 통해 자가 개선(self-improvement)을 시도합니다. 여러 언어 모델이 동일한 기반 모델에서 출발하여 상호작용을 통해 생성된 데이터를 사용하여 독립적으로 전문화되고 다양화됩니다. 이를 통해 단일 에이전트 자가 개선 방식과 비교하여 더 많은 라운드의 미세 조정을 통해 더 나은 성능을 유지하게 됩니다.
- ***Technical Details***: 이 방법에서는 다중 에이전트로 구성된 언어 모델이 기초 모델에서 출발하여 독립적인 데이터 세트로 각 모델을 미세 조정해 특화된 기능을 구축합니다. 각 모델은 생성 에이전트(generation agents)와 비평 에이전트(critic agents)로 구분되어, 생성 에이전트는 초기에 반응을 생성하고, 비평 에이전트는 그 반응을 평가하고 개선합니다. 이 접근법은 다중 에이전트 토론(multiagent debate) 과정을 통해 최종 응답을 생성하는 강력한 피드백 루프를 만듭니다.
- ***Performance Highlights***: 이 방법의 효과는 Phi-3, Mistral, LLaMA-3 등 다양한 개방형 및 독점형 LLM(open-source and proprietary LLMs)에 직접 적용하여 성능을 크게 향상시켰습니다. 예를 들어, MATH 데이터셋 실험에서 Phi-3와 Mistral의 정확도는 각각 58.8%에서 66.0%, 22.5%에서 28.2%로 개선되었습니다. 다중 에이전트 미세 조정의 다섯 라운드는 단일 에이전트 미세 조정보다 더 우수한 성능을 보였습니다.

### [Infecting Generative AI With Viruses](https://arxiv.org/abs/2501.05542)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.05542.png)

Vote: 5

Authors: Forrest McKee, David Noever

- ***What's New***: 이번 연구에서는 JPEG 이미지에 EICAR 테스트 파일을 포함하여 Vision-Large Language Model(VLM/LLM)의 보안 경계를 시험하는 새로운 방법을 제시하였습니다. 본 연구는 OpenAI GPT-4o, Microsoft Copilot, Google Gemini 1.5 Pro 및 Anthropic Claude 3.5 Sonnet(2025) 등 여러 LLM 플랫폼에서 네 가지 프로토콜을 성공적으로 실행하는 것을 보여줍니다.
- ***Technical Details***: 이 연구는 JPEG 이미지의 메타데이터에 EICAR 문자열을 은닉하여 탐지되지 않을 수 있는지를 테스트하고, Python을 사용하여 LLM 환경 내에서 테스트 파일을 성공적으로 추출하는 방법을 설명합니다. 또한, base64 인코딩 및 문자열 뒤집기와 같은 다양한 난독화 기법을 시연합니다. Microsoft Research의 'Penetration Testing Rules of Engagement' 프레임워크를 확장하여 클라우드 기반 생성 AI 및 LLM의 보안 경계를 평가합니다.
- ***Performance Highlights***: 연구 결과, JPEG 포맷에 탑재된 EICAR 서명은 대다수의 데스크탑 애플리케이션에서 원활히 표시 및 로드되며, LLM의 가상 워크스페이스 내에서 수정된 파일이 전송, 조작, 잠재적으로 실행될 수 있음을 입증하였습니다. 이는 LLM의 파일 처리 시스템에 잠재적인 취약점이 존재한다는 것을 시사합니다. LLM이 다단계 잠재적인 악성 페이로드 조작을 지원할 수 있는 능력은 'uplift' 취약점으로 간주됩니다.

### [Multi-subject Open-set Personalization in Video Generation](https://arxiv.org/abs/2501.06187)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06187.png)

Vote: 5

Authors: Jun-Yan Zhu, Ivan Skorokhodov, Kwot Sin Lee, Tsai-Shien Chen, Kfir Aberman, Aliaksandr Siarohin, Willi Menapace, Ming-Hsuan Yang, Sergey Tulyakov, Yuwei Fang

- ***What's New***: 이번 연구는 여러 개체(Subjects)에 대한 비정형 개인화(Multi-subject Open-set Personalization)가 가능한 비디오 생성 모델, 'Video Alchemist'를 소개합니다. 이는 테스트 시간 최적화(Test-time Optimization) 없이 포그라운드 객체(Foreground Objects)와 배경(Background)을 포함한 개인화를 지원합니다.
- ***Technical Details***: Video Alchemist는 새로운 Diffusion Transformer 모듈을 기반으로 하며, 각 조건부 참조 이미지와 그에 상응하는 텍스트 프롬프트를 교차 주의(Cross-attention) 계층을 통해 융합합니다. 데이터셋 구축을 위해 이미지 증강(Image Augmentation)을 사용하고, 오버피팅을 줄이기 위해 자동화된 데이터 구축 파이프라인을 설계했습니다. 또한, 다양한 개인화 시나리오를 지원하는 평가 벤치마크인 MSRVTT-Personalization을 도입하였습니다.
- ***Performance Highlights***: Video Alchemist는 MSRVTT-Personalization 벤치마크에서 기존의 개인화 방법들에 비해 정량적, 정성적으로 우수한 성능을 보여주며, 텍스트와의 정렬 및 주제 충실도(Subject Fidelity)에서 뛰어난 결과를 보였습니다. 실험결과에 따르면, 주제 유사도가 기존의 비슷한 연구보다 최대 23.2% 높았습니다.

### [Demystifying Domain-adaptive Post-training for Financial LLMs](https://arxiv.org/abs/2501.04961)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04961.png)

Vote: 5

Authors: Shafiq Joty, Xuan-Phi Nguyen, Yifei Ming, Zixuan Ke, Caiming Xiong

- ***What's New***: 이 연구는 금융 분야에 특화된 대형 언어 모델(LLM; Large Language Models)의 도메인 적응 후 훈련(domain-adaptive post-training)에 관한 새로운 접근법인 FINDAP를 제안합니다. 이를 통해 금융 LLM, Llama-Fin을 개발하며, 이는 다양한 금융 과제에서 최첨단 성능(state-of-the-art performance)을 달성합니다.
- ***Technical Details***: FINDAP는 목표 도메인에 필요한 핵심 기능을 정의하고 이를 충족하는 평가 세트를 설계합니다. 주요 훈련 단계로 지속적 사전 훈련(continual pre-training; CPT), 지시 조율(instruction tuning; IT), 선호도 정렬(preference alignment; PA)을 포함합니다. 또한, 생성적 보상 모델(generative reward model)에서 프로세스 신호를 활용한 새로운 선호 데이터 증류 방법(preference data distillation method)을 제안합니다.
- ***Performance Highlights***: Llama-Fin은 훈련되지 않은 유사 및 새로운 과제에서도 다양한 기준을 뛰어넘으며, 특히 감성 분석(Sentiment Analysis), 요약(Abstract Summarization), 수리적 추론(Math Reasoning)에서 뛰어난 성능을 보여줍니다. 이 모델은 공개 금융 LLM인 Palmyra-Fin-32k뿐만 아니라 독점 모델인 GPT-4o도 능가하며, 금융 과제에서 최고의 성과를 보입니다.

