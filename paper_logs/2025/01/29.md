## Daily Papers (2025-01-29)

### [SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training](https://arxiv.org/abs/2501.17161)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17161.png)

Vote: 28

Authors: Yi Ma, Tianzhe Chu, Dale Schuurmans, Jihan Yang, Yuexiang Zhai, Sergey Levine, Quoc V. Le, Shengbang Tong, Saining Xie

- ***What's New***: 이 연구는 기본 모델 포스트 트레이닝 방법인 SFT(지도 학습 기반 미세 조정)와 RL(강화 학습)의 일반화와 암기 효과를 비교 분석한 것입니다. 텍스트 기반과 시각적 환경 모두에 대한 실험을 통해 RL은 규칙 기반 텍스트와 시각적 환경 모두에서 일반화할 수 있는 반면 SFT는 학습 데이터를 주로 암기하고 분포 밖(out-of-distribution)에서 일반화하기 어려움을 겪고 있음을 보였습니다.
- ***Technical Details***: 연구는 GeneralPoints라는 산술 추론 카드 게임과 실제 내비게이션 환경인 V-IRL을 사용해 SFT와 RL로 훈련된 모델이 새로운 변형에 어떻게 일반화하는지를 평가합니다. RL은 특히 결과 기반 보상으로 훈련될 때 규칙 기반 텍스트와 시각 환경 모두에서 일반화할 수 있으며, SFT는 훈련 데이터를 주로 암기하는 경향이 있습니다. RL의 경우 다단계 학습 프레임워크를 사용하여 시각 인식 능력의 향상을 구현했습니다.
- ***Performance Highlights***: RL을 사용한 다단계 학습 접근 방식은 시각적 OOD(Out-Of-Distribution) 작업에서 +33.8% 성능을 향상시키며, V-IRL mini benchmark에서 44.0%에서 77.8%로 성능을 크게 향상시켰습니다. 이 실험 결과는 RL이 복잡한 다중 모달 작업에서 일반화 가능한 지식을 획득하는 데 유리함을 입증하며, SFT는 RL 학습을 안정화하는 데 여전히 도움이 될 수 있음을 보여줍니다.

### [Optimizing Large Language Model Training Using FP4 Quantization](https://arxiv.org/abs/2501.17116)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17116.png)

Vote: 13

Authors: Peng Cheng, Yeyun Gong, Ruizhe Wang, Baining Guo, Zhengjun Zha, Guoshuai Zhao, Xiao Liu, Ziyue Yang

- ***What's New***: FP4 양자화(FP4 Quantization)를 이용한 대형 언어 모델(Large Language Model; LLM)의 훈련 최적화에 관한 최초의 프레임워크가 도입되었습니다. 이는 저비트 연산을 활용하여 훈련 비용을 줄일 수 있는 가능성을 제시합니다. FP8 정밀도가 이미 가능성을 입증했지만, FP4는 그동안 큰 양자화 오류와 제한된 표현 능력으로 어려움이 있었습니다. 이번 연구는 FP4 정밀도의 제한을 해결하기 위한 두 가지 주요 혁신을 소개합니다: 정밀한 가중치 업데이트를 위한 미분 가능한 양자화 추정기 및 활성화 붕괴를 방지하기 위한 이상치 클램핑과 보상 전략.
- ***Technical Details***: 이 프레임워크는 안정성을 보장하기 위해 혼합 정밀도 훈련 체계와 벡터 단위 양자화를 통합합니다. 가중치를 위한 미분 가능한 양자화 추정기를 제안하여 FP4 계산에서의 그라디언트 업데이트를 개선하고, LLM 훈련 중 자주 관찰되는 이상치 값을 처리하기 위한 활성화 클램핑 및 보상 전략을 개발했습니다. 모든 구성 요소는 FP8의 성능을 모방하여 NVIDIA H100 GPU의 FP8 텐서 코어를 활용하여 시뮬레이션되었습니다.
- ***Performance Highlights***: 실험 결과, 제안된 FP4 프레임워크는 FP8 및 BF16과 유사한 정확성을 보여주었으며, 13B 매개변수의 LLM까지 확장성 있게 작동하였습니다. BP16을 기반으로 한 훈련 방법과 대비하여 FP4 훈련은 훈련 손실 및 다운스트림 작업 정확도 면에서 거의 동등한 성능을 달성하여 대형 언어 모델 훈련의 효율적인 방법으로서의 가능성을 입증했습니다.

### [Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling](https://arxiv.org/abs/2501.16975)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16975.png)

Vote: 10

Authors: Defa Zhu, Xun Zhou, Banggu Wu, Hongzhi Huang, Qiyang Min, Yutao Zeng, Ya Wang

- ***What's New***: 이 논문에서는 입력과 출력 어휘력을 분리하여 언어 모델링 성능을 개선하는 새로운 프레임워크인 Over-Tokenized Transformer를 소개합니다. 특히, 입력 어휘력을 확장하여 멀티그램 토큰(multi-gram tokens)을 활용하고, 입력 어휘력 크기와 학습 손실 간에 로그-선형 관계를 발견하였습니다.
- ***Technical Details***: Over-Encoding(OE)는 대형 계층적 n-그램 입력 어휘를 사용하며, 입력 어휘력 크기를 128배로 늘려도 추가 비용 없이 학습 손실을 유지합니다. Over-Decoding(OD)은 출력 어휘력을 확장하여 세밀한 감독을 제공합니다. Over-Tokenized Transformer는 OE와 OD를 결합하여 더 큰 잠재력을 보여주었습니다.
- ***Performance Highlights***: 400M 매개변수의 OE-12.8M 모델이 1B baseline 모델과 같은 성능을 달성하면서도 추가 비용이 들지 않는 것으로 나타났습니다. 또한, 입력 어휘력 크기의 지수적 증가가 손실의 선형 감소로 일관되게 이어졌습니다. 실험 결과 OE 모델은 baseline 모델에 비해 학습 손실이 0.12 감소하고 다운스트림 평가에서 3배 이상의 가속을 달성하였습니다.

### [DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation](https://arxiv.org/abs/2501.16764)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16764.png)

Vote: 8

Authors: Bangbang Yang, Panwang Pan, Zeming Li, Chenguo Lin, Yadong Mu

- ***What's New***: DIFFSPLAT은 텍스트나 단일 이미지로부터 3D 콘텐츠를 생성하는 새로운 프레임워크로, 대규모 텍스트-이미지 변환(diffusion) 모델을 활용하여 3D Gaussian splats를 생성합니다. 이 모델은 2D 웹 규모의 priors를 효과적으로 활용하면서도 3D 일관성을 유지하는 통합 모델을 제공합니다.
- ***Technical Details***: DIFFSPLAT은 3D Gaussian splats의 생성에 있어 3D Gaussian Splatting(3DGS)을 사용하며, 간편한 렌더링과 고품질의 균형을 제공합니다. 훈련을 위해 다수의 2D 이미지와 쉽게 결합할 수 있으며, 0.1초 이내에 다중 보기(multiview) Gaussian splat grid를 생성합니다. 이 gride는 객체의 질감과 구조를 암시하는 속성을 포함하고 있으며, 이미지 diffusion 모델에 의해 처리됩니다.
- ***Performance Highlights***: DIFFSPLAT은 텍스트 및 이미지 기반의 생성 작업과 관련된 다양한 실험에서 뛰어난 성능을 보였습니다. 텍스트 기반 3D 생성에서는 최신 모델들과의 비교에서 가장 우수한 시각적 품질과 텍스트 일치를 보여주었으며, 이미지 기반 생성에서도 높은 지리적 신뢰성을 유지하면서 입력 이미지와 잘 맞아 떨어지는 3D 콘텐츠를 생성할 수 있는 능력을 입증했습니다.

### [Open Problems in Mechanistic Interpretability](https://arxiv.org/abs/2501.16496)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16496.png)

Vote: 7

Authors: Nandi Schoots, Jeff Wu, Jack Lindsey, William Saunders, Mor Geva, Eric J. Michaud, Tom McGrath, Lee Sharkey, Lucius Bushnaq, Arthur Conmy, Joshua Batson, Neel Nanda, Jessica Rumbelow, Stephen Casper, Max Tegmark, Joseph Miller, Atticus Geiger, Martin Wattenberg, Bilal Chughtai, Daniel Murfet, Nicholas Goldowsky-Dill, Stefan Heimersheim, Stella Biderman, Joseph Bloom, Jesse Hoogland, Alejandro Ortega, Eric Todd, Adria Garriga-Alonso, David Bau

- ***What's New***: 이 논문은 인공지능 시스템의 메커니즘 해석 가능성(Mechanistic Interpretability)의 현재 경계와 미래 우선순위에 대해 논의합니다. 메커니즘 해석 가능성은 신경망의 계산 메커니즘을 이해하여 AI 시스템의 행동에 대한 신뢰성을 높이고, 정보의 본질에 대한 과학적 질문에 답하기 위한 목표를 가지고 있습니다.
- ***Technical Details***: 메커니즘 해석 가능성 연구의 주요 과제는 신경망의 내부 작용 원리를 역엔지니어링하여 이해하는 것입니다. 논문은 네트워크의 구성 요소를 식별하고 해석하는 '역설계(Reverse Engineering)' 및 '개념 기반 해석(Concept-based Interpretability)' 접근법을 검토합니다. 이 연구는 모델의 행동 예측을 위한 특성 설명과 단위 해석 방법의 검증을 포함합니다.
- ***Performance Highlights***: 논문은 메커니즘 해석 가능성이 AI 모델의 예측 능력 향상과 같은 실질적인 목표를 달성하는 데 어떻게 기여할 수 있는지 제시합니다. 구체적으로, 모델의 비정상적 행동을 식별하고, 위험한 지식을 제거하며, 모델 편향을 수정하는 데 도움이 될 수 있습니다.

### [Low-Rank Adapters Meet Neural Architecture Search for LLM Compression](https://arxiv.org/abs/2501.16372)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16372.png)

Vote: 5

Authors: J. Pablo Muñoz, Jinjie Yuan, Nilesh Jain

- ***What's New***: 이 연구는 대형 언어 모델(LLM)의 압축과 미세조정을 위한 혁신적인 접근 방식을 제안합니다. 여기에서는 저순위 어댑터(Low-Rank Adapters)와 신경 아키텍처 검색(Neural Architecture Search; NAS)을 결합하여 미세조정 효율성을 높이고 메모리 사용량을 줄이는 방법을 탐구합니다.
- ***Technical Details***: 저순위 어댑터는 파라미터 효율성 미세조정(PEFT)에서 선호되는 방법으로, NAS 기법과 결합하여 서브네트웍 공유 및 저순위 표현의 가이드 하에 구조를 활성화합니다. 특히 Elastic LoRA Adapters는 조정 가능한 구조로, 모드 A에서는 LoRA의 계급만을 조정 가능하게 하고, 모드 B에서는 입력 및 출력 채널까지 조정 가능하게 합니다.
- ***Performance Highlights***: 실험 결과, LoNAS는 모델 파라미터 수를 약 80%까지 줄이고 추론 속도를 최대 1.4배까지 향상시킬 수 있음을 보여주었습니다. Shears와 SQFT는 더 높은 정확도 개선을 제공하며, 특정 다운스트림 작업에 효과적으로 적응할 수 있도록 미세조정의 효율성을 증대합니다.

### [IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding](https://arxiv.org/abs/2501.15747)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15747.png)

Vote: 4

Authors: Laxmaan Balaji, Aman Chadha, Ashutosh Kumar, Nikunj Kotecha, Sreyoshi Bhaduri, Vinija Jain, Sankalp KJ

- ***What's New***: IndicMMLU-Pro는 인도 서브대륙의 주요 언어를 대상으로 하는 인디 언어 대형 언어 모델(Indic Large Language Models)의 멀티태스크 언어 이해 능력을 평가하기 위한 새로운 벤치마크입니다. 힌디, 벵갈리, 구자라트어 등 다양한 언어를 포함하여 인디 언어의 복잡한 언어적 특성과 문화적 특수성을 반영하도록 설계되었습니다.
- ***Technical Details***: 우리는 IndicTrans2를 사용하여 MMLU-Pro(영어) 데이터셋을 힌디, 벵갈리, 타밀 등 9개 인디 언어로 번역하였습니다. 번역된 데이터는 역번역 및 다양한 품질 지표(BLEU, chrF++, METEOR 등)를 활용하여 품질 검증을 거쳤습니다. 이 데이터셋은 각각의 언어마다 별도로 제공되며 각종 시험엔 인적 리뷰어와 테스트 케이스 검증을 포함합니다.
- ***Performance Highlights***: GPT-4o 모델이 9개 언어에서 38.46%에서 44.80% 사이의 정확도로 가장 높은 성능을 보였습니다. XLM-RoBERTa와 Navarasa와 같은 다른 모델은, 특히 드라비다어군 언어에서 뛰어난 성능을 보였습니다. 그러나, 전체적으로 큰 모델들이 인디 언어의 복잡성을 처리하는 데 있어 장점을 가지고 있습니다.

### [Histoires Morales: A French Dataset for Assessing Moral Alignment](https://arxiv.org/abs/2501.17117)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17117.png)

Vote: 2

Authors: Irina Proskurina, Antoine Gourru, Charlotte Laclau, Thibaud Leteno, Christophe Gravier, Julien Velcin, Guillaume Metzler

- ***What's New***: Histoires Morales는 프랑스어로 된 인간 가치 정렬(Linguistic Moral Alignment)을 평가하기 위한 최초의 데이터셋입니다. MoralStories에서 유래된 이 데이터셋은 프랑스의 문화적 맥락에 맞도록 번역 및 수정을 거쳤습니다.
- ***Technical Details***: Histoires Morales는 12,000개의 스토리를 포함하며 도덕적 규범, 상황, 의도, 결과 등을 다룹니다. gpt-3.5-turbo-16k 모델을 사용하여 번역했고, 프랑스 원어민의 피드백으로 번역 품질을 개선했습니다. Direct Preference Optimization(DPO)을 통해 모델이 도덕적 또는 비도덕적 행동을 선호하도록 훈련할 수 있음을 보였습니다.
- ***Performance Highlights***: 모델은 영어에서 더 높은 도덕적 설정 정렬 점수를 보였으며, 대체로 도덕적 행동을 선택하는 경향이 있습니다. 그러나 번역 품질 연구에서는 프랑스어 쪽에서도 상당히 높은 정확도를 보여주었습니다. 약 84개의 예시만으로도 모델의 정렬이 변화될 수 있음을 확인했습니다.

