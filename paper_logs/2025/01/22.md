## Daily Papers (2025-01-22)

### [Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training](https://arxiv.org/abs/2501.11425)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11425.png)

Vote: 54

Authors: Zehui Chen, Zhiheng Xi, Siyu Yuan, Junjie Ye, Zhengyin Du, Jiecao Chen

- ***What's New***: Agent-R은 대형 언어 모델 에이전트를 위해 실시간 반영 및 자기 개선을 가능하게 하는 새로운 프레임워크입니다. 기존의 보상 기반 방식과 달리 Agent-R은 오류 궤적에서 올바른 궤적으로의 실시간 회복을 가능하게 하여, 독립적으로 행동을 수정하고 학습할 수 있는 능력을 제공합니다.
- ***Technical Details***: Agent-R은 몬테카를로 트리 서치(Monte Carlo Tree Search; MCTS)를 활용하여 동적으로 수정 궤적을 생성합니다. 잘못된 궤적 내 첫 번째 오류를 찾아, 이를 올바른 궤적의 인접 경로와 연결하여, 각 행동 단계에서 오류를 교정합니다. 이 접근법은 모델의 현재 정책에 기반한 반영 학습을 가능하게 하며, 더욱 효율적인 학습을 제공합니다. 더불어, 에러 교정 능력과 데이터셋 구축 과정을 반복적인 자기 훈련을 통해 정교화합니다.
- ***Performance Highlights***: Agent-R은 세 가지 대표적인 상호작용적 환경(WebShop, ScienceWorld, TextCraft)에서 기존의 베이스라인 모델들보다 우수한 성능을 보여줍니다. 제안된 프레임워크는 언어 에이전트가 오류 작업을 식별하고 바로잡을 수 있도록 하며, 루프를 회피하게 하여 성능을 5.59% 향상시킵니다. Agent-R은 특히 나선을 피하고 장기 과제를 더 견고하게 처리하는 데 효과적입니다.

### [Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models](https://arxiv.org/abs/2501.11873)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11873.png)

Vote: 49

Authors: Bo Zheng, Dayiheng Liu, Jingren Zhou, Rui Men, Zeyu Huang, Ivan Titov, Kaiyue Wen, Zekun Wang, Zihan Qiu, Junyang Lin

- ***What's New***: 이 연구는 전문화된 전문가 혼합 모델(Mixture-of-Expert Models; MoEs) 훈련을 위한 부하 균형 손실(Load-balancing Loss; LBL)의 구현을 재탐구하였습니다. 글로벌 배치(Global-Batch)를 사용하여 LBL을 계산함으로써 전문가의 선택 빈도를 더욱 균형 있게 유지하는 새로운 방법을 제안합니다.
- ***Technical Details***: 일반적으로 MoE 훈련 프레임워크에서는 미니배치(Micro-Batch) 수준에서 LBL을 계산하지만, 이는 전문가의 전문화를 저해할 수 있습니다. 제안된 기법은 각 병렬 그룹 내에서 전문가 선택 빈도를 동기화하여 글로벌 배치 LBL로 연산함으로써 이를 해결합니다. 이를 통해 미니배치에서 도메인 특수 시퀀스의 균등 분배 문제를 피하고, 더욱 다양한 시퀀스를 포함하는 글로벌 배치에서 부하 균형을 촉진합니다.
- ***Performance Highlights***: 글로벌 배치 LBL을 이용한 결과, 사전 훈련에서 혼란도(perplexity)와 다운스트림 작업에서 성능이 크게 향상됨을 발견했습니다. 모델 성능이 글로벌 배치 크기에 따라 효과적으로 증가하며, 실험에서는 최대 42.8B 파라미터와 400B 토큰으로 구성된 MoE 모델에서 3% 미만의 지연을 도입하면서도 성능과 해석 가능성을 향상시켰음을 시사합니다.

### [MMVU: Measuring Expert-Level Multi-Discipline Video Understanding](https://arxiv.org/abs/2501.12380)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12380.png)

Vote: 48

Authors: Junyang Song, Arman Cohan, Weiyuan Chen, Haowei Zhang, Yilun Zhao, Chengye Wang, Lujing Xie, Yitao Long, Tongyan Hu, Zhijian Xu, Chuhan Li, Ziyao Shangguan, Guo Gan, Zhenwen Liang, Yixin Liu, Zhiyuan Hu, Chen Zhao, Xiangru Tang, Weifeng Pan

- ***What's New***: MMVU는 영상 이해를 위한 멀티모달 페이지 모델(multimodal foundation models)의 전문 지식 레벨 평가를 위해 설계된 종합적인 벤치마크입니다. 이는 총 4개의 핵심 분야, 즉 과학, 건강관리, 인문사회 및 공학 전반에 걸쳐 27개의 주제를 아우르는 3,000개의 전문가 주석 질문을 포함하고 있습니다. 현재 비디오 벤치마크에서는 주로 기본적인 시각적 인식에 집중하는 반면, MMVU는 전문적인 지식과 논리적 추론을 필요로 하는 특화된 도메인 비디오 분석을 요구하여 모델들에게 더 높은 수준의 도전과제를 부여합니다.
- ***Technical Details***: MMVU는 분야별 전문가들이 처음부터 주석을 달아 제작되었으며, 텍스트북을 기반으로 한 데이터 주석 과정을 채택하여 각 분야 내 지식의 넓은 범위와 심층적 추론을 보장합니다. 영상자료는 주로 YouTube에서 크리에이티브 커먼즈 라이선스로 제공되는 자료를 사용하여 구성되며, 전문적, 도메인 특정 지식의 활용을 평가하기 위해 생성되었습니다. 예제마다 명시적으로 주석된 논리적 근거와 관련 지식이 포함되어 있어 모델 성능의 정밀한 평가를 지원합니다.
- ***Performance Highlights***: 32개의 첨단 멀티모달 기반 모델을 테스트한 결과, 가장 우수한 성능을 기록한 모델은 최신 o1과 Gemini 2.0 플래시 씽킹 모델로 나타났으나 여전히 인간 전문가 수준에는 미치지 못했습니다. 예를 들어, GPT-4o는 66.7%의 정확도를 기록했는데 이는 인간 전문가가 열린 책(open-book) 조건에서 달성한 86.8%보다 크게 낮습니다. 이러한 결과는 MMVU가 다루는 지식 집중적이고 전문가 수준의 비디오 이해 분야에서 기존의 멀티모달 기반 모델의 발전 가능성을 잘 보여줍니다.

### [TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space](https://arxiv.org/abs/2501.12224)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12224.png)

Vote: 31

Authors: Inbar Mosseri, Daniel Garibi, Omer Tov, Ariel Ephrat, Shahar Yadin, Tomer Michaeli, Shiran Zada, Roni Paiss, Tali Dekel

- ***What's New***: TokenVerse는 다중 개념 개인화를 위한 새로운 방법을 제안하며, 사전 학습된 텍스트-이미지 확산 모델(Diffusion Model)을 활용하여 개념의 조합을 자유롭게 생성할 수 있게 합니다. 각각의 개념 이미지를 독립적으로 처리하며, 부가적인 감독 없이 첨부된 캡션만으로 개념을 분리하고 학습합니다.
- ***Technical Details***: 이 방법은 Diffusion Transformer(디퓨전 변환기; DiT) 기반의 모델을 이용하여 입력 텍스트가 주입과 변조(shift 및 scale)를 통해 이미지 생성에 영향을 미치도록 합니다. 주어진 이미지와 텍스트 설명을 입력으로 받아, 각 단어의 변조 공간에서 별개의 방향을 찾는 최적화 기반 프레임워크를 개발하여 새로운 이미지를 생성할 수 있도록 합니다. 변조 공간 M+를 활용하여 비지도 학습의 개인화 및 구성 작업이 이루어지며, 텍스트 토큰별로 학습된 개념을 사용할 수 있습니다.
- ***Performance Highlights***: TokenVerse는 기존 방법들보다 더 뛰어난 다중 개념 추출과 통합 성능을 보여줍니다. 사용자가 요구하는 설정에 따라 복합적인 개념을 표현한 이미지를 생성하는 데 있어 유연성을 제공하며, 기존 접근법에 비해 개념 보존과 프롬프트 일치를 더 잘 유지합니다.

### [UI-TARS: Pioneering Automated GUI Interaction with Native Agents](https://arxiv.org/abs/2501.12326)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12326.png)

Vote: 28

Authors: Haihua Yang, Junda Zhang, Zhaojian Li, Feng Lin, Xiaojun Xiao, Kuanye Li, Xin Liu, Haoming Wang, Junjie Fang, Longxiang Liu, Yunxin Li, Tao Peng, Yu Miao, Chen Li, Chuang Li, Wanjun Zhong, Jiale Yang, Shijue Huang, Shihao Liang, Qianli Ma, Yining Ye, Haoli Chen, Jingyu Li, Xiao Zhou, Jiahao Li, Shizuo Tian, Woyu Lin, Xu Jiang, Yaowei Zheng, Haifeng Liu, Yujia Qin, Kai Cai, Guang Shi, Minchao Wang, Chaolin Jin

- ***What's New***: UI-TARS는 스크린샷만을 입력으로 받아 인간처럼 그래픽 사용자 인터페이스(GUI)와 상호작용하는 네이티브 GUI 에이전트 모델입니다. 이 모델은 복잡한 프롬프트나 모듈화를 필요로 하지 않는 종단간(end-to-end) 설계로, 기존의 상용 에이전트 프레임워크를 능가하는 성능을 보입니다.
- ***Technical Details***: UI-TARS는 GUI 인식을 향상시키기 위해 대규모 데이터셋을 사용해 UI 요소를 인식하고 정확한 캡션을 생성합니다. 'Unified Action Modeling'을 통해 다양한 플랫폼에서 통합된 액션 공간을 표준화하고, 시스템 2 추론(System-2 Reasoning)을 접목해 복잡한 의사결정 과정을 지원합니다. 또한 수백 대의 가상 머신에서 자동으로 상호작용 데이터를 수집하고 미세 조정하며, 최소한의 인간 개입으로 학습을 반복합니다.
- ***Performance Highlights***: UI-TARS는 10개 이상의 GUI 에이전트 벤치마크에서 최첨단(SOTA) 성능을 달성했습니다. 특히 OSWorld에서는 50 스텝에서 24.6, 15 스텝에서 22.7이라는 점수를 기록하여 다른 모델인 Claude와 GPT-4o를 크게 상회했습니다. 이러한 결과는 UI-TARS의 강력한 추론 및 상호작용 능력을 대변합니다.

### [InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model](https://arxiv.org/abs/2501.12368)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12368.png)

Vote: 20

Authors: Jiaqi Wang, Wenwei Zhang, Shengyuan Ding, Ziyu Liu, Pan Zhang, Kai Chen, Yuhang Zang, Dahua Lin, Xiaoyi Dong, Shenxi Wu, Haodong Duan, Yuhang Cao, Yubo Ma

- ***What's New***: InternLM-XComposer2.5-Reward (IXC-2.5-Reward)는 대형 시각-언어 모델(LVLMs)을 인간의 선호와 정렬시키는 간단하면서도 효과적인 멀티 모달 보상 모델(Reward Model; RM)입니다. 텍스트, 이미지, 비디오 입력을 아우르는 고품질의 선호 데이터에 기반한 보상 모델을 제안하였습니다.
- ***Technical Details***: IXC-2.5-Reward는 다양한 도메인의 멀티 모달 선호 데이터셋을 구축하며, 평가 프롬프트와 GPT-4o 또는 검증자를 통해 선호 판단을 수행합니다. 멀티 모달 데이터에서 시각적인 데이터와 텍스트 데이터의 모달리티 정렬 없이 보상 점수를 예측할 수 있도록 설계되었습니다. PPO 알고리즘을 사용하여 IXC-2.5-Chat을 강화 학습(REINFORCEMENT LEARNING)으로 최적화하여 사용자의 대화 경험을 향상시켰습니다.
- ***Performance Highlights***: IXC-2.5-Reward는 VL-RewardBench에서 이전의 생성 기반 보상 모델인 Gemini-1.5-Pro와 GPT-4o를 제치고 최고 성능을 기록했습니다. 단일 모달 텍스트 RM 벤치마크에서도 평균 88.6%와 68.8%의 높은 점수를 기록하며 우수한 성능을 입증했습니다. 특히 다양한 태스크에 대해 다루는 성능이 뛰어난 것으로 나타났습니다.

### [Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks](https://arxiv.org/abs/2501.11733)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11733.png)

Vote: 17

Authors: Junyang Wang, Fei Huang, Zhenhailong Wang, Ji Zhang, Xi Zhang, Heng Ji, Ming Yan, Haiyang Xu

- ***What's New***: Mobile-Agent-E는 기존의 최첨단 접근 방법을 능가하는 새로운 계층적 다중 에이전트 모바일 비서를 제안합니다. 고수준의 계획과 저수준 행동 결정을 전담 에이전트로 분리하여 복잡한 실세계 작업에서 성능과 효율성을 향상시켰습니다. 새로운 자기 진화 모듈(Self-Evolution Module)을 도입하여 과거 경험에서 일반적인 팁(Tips)과 재사용 가능한 지름길(Shortcuts)을 학습하여 지속적인 성능 개선을 이루었습니다.
- ***Technical Details***: Mobile-Agent-E는 관리자가 복잡한 작업을 하위 목표로 분해하여 전반적인 계획을 수립하고, 지각자(Perceptor), 운영자(Operator), 행동 반사기(Action Reflector), 기록자(Notetaker) 네 개의 하위 에이전트가 세밀화된 시각적 인식, 즉각적인 행동 실행, 오류 검증, 정보 집계를 처리하는 계층적 구조로 구성되어 있습니다. 또한, 팁(Tips)과 지름길(Shortcuts)을 포함하는 지속적인 장기 메모리와 두 개의 경험 반사기(Experience Reflectors)를 포함한 자기 진화 모듈이 특징입니다.
- ***Performance Highlights***: Mobile-Agent-E는 Mobile-Eval-E 벤치마크에서 이전 최첨단 접근 방식보다 절대적으로 22% 향상된 성능을 달성했습니다. 자기 진화 메커니즘의 도입은 또한 성능과 효율성 모두에서 긍정적인 진화를 보여주며, 특정 백본 모델에서는 6.5% 향상을 기록했습니다. 이와 함께 여러 대규모 멀티모달 모델(LMM) 백본을 활용하여 변형된 성능 비교에서도 일관된 개선이 관찰되었습니다.

### [Reasoning Language Models: A Blueprint](https://arxiv.org/abs/2501.11223)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11223.png)

Vote: 16

Authors: Marcin Copik, Torsten Hoefler, Patrick Iff, Hannes Eberhard, Tomasz Sternal, Robert Gerstenberger, Hubert Niewiadomski, Afonso Catarino, Julia Barth, Maciej Besta, Yueling Li, Sam Houliston, Grzegorz Kwaśniewski, Eric Schreiber, Ales Kubicek, Piotr Nyczyk, Jürgen Müller, Łukasz Flis

- ***What's New***: 이 논문은 새로운 추리 언어 모델(RLM; Reasoning Language Model)의 청사진을 제시하여 기존 대형 언어 모델(LLM; Large Language Model)의 한계를 넘어서고자 합니다.저자들은 강화 학습(Reinforcement Learning), 검색 휴리스틱, LLM을 결합한 고도의 추리 메커니즘으로 RLM을 구성하는 모듈형 프레임워크를 제안합니다. 다양한 추리 구조(체인, 트리, 그래프 등)와 강화 학습 개념을 통합하여 기존 모델보다 저비용으로 확장 가능한 RLM 구축을 목표로 합니다.
- ***Technical Details***: 논문은 RLM의 구성 요소를 체계적으로 조직하여, 사용자 맞춤형 RLM을 구축할 수 있는 유연하고 모듈화된 프레임워크를 제시합니다. 특히 x1이라는 모듈형 구현을 통해 RLM 프로토타입을 신속하게 개발하고 실험할 수 있도록 지원합니다. 이 프레임워크는 다양한 최적화 및 설계 결정을 수용하고, 실험 인프라의 기초가 되어 연구자들이 새로운 추리 패러다임을 탐구하고 성능을 최적화할 수 있도록 설계되었습니다.
- ***Performance Highlights***: 논문은 여러 RLM의 성능을 분석하고, RLM이 수학적 추론과 같이 복잡한 문제를 해결하는 능력을 강화하기 위해 특정 설계가 필요함을 지적합니다. 또, RLM의 추리 능력을 평가하기 위한 여러 벤치마크를 설정하고, 다양한 성과 지표를 통해 모델의 효과를 검토합니다. 연구 결과, 기존의 LLM이 다룰 수 없던 고차원 추리 작업에서도 RLM의 성능이 두드러짐을 보여줍니다.

### [Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation](https://arxiv.org/abs/2501.12202)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12202.png)

Vote: 14

Authors: Runzhou Wu, Haohan Weng, Di Wang, Xinming Wu, Zhongyi Fan, Shuhui Yang, Changrong Hu, Zhichao Hu, Yiwen Jia, Jiaao Yu, Kai Liu, Chunchao Guo, Xinhai Liu, Lixin Xu, Chao Zhang, Chongqing Zhao, Zhan Li, Jie Jiang, Xianghui Yang, Sicong Liu, Zhuo Chen, Jianchen Zhu, Minghui Chen, Haozhao Kuang, YingPing He, Jingwei Huang, Jinbao Xue, Mingxin Yang, Zebin He, Lifu Wang, Lin Niu, Jihong Zhang, Ruining Tang, Yong Yang, Weihao Zhuang, Qingxiang Lin, Peng He, Yingkai Wang, Yifei Feng, Tianyu Huang, Meng Chen, Xu Zheng, Yuhong Liu, Yixuan Tang, Yihang Lian, Zibo Zhao, Xuhui Zuo, Biwen Lei, Zheng Ye, Sheng Zhang, Yonghao Tan, Fan Yang, Zeqiang Lai, Haolin Liu, Xipeng Zhang, Jian Liu, Paige Wang, Lei Qin, Jianbing Peng, Tian Liu, Liang Dong, Jie Xiao, Jing Xu, Yiling Zhu, Hao Zhang, Yunfei Zhao, Huiwen Shi, Yangyu Tao, Yulin Cai, Junta Wu, Xinzhou Wang

- ***What's New***: Hunyuan3D 2.0은 고해상도 질감 3D 자산 생성 시스템으로, 두 가지 주요 모델을 포함합니다: Hunyuan3D-DiT는 싱글플로우 기반 확산 변환기(Flow-based Diffusion Transformer)를 통해 3D 형태를 생성하고, Hunyuan3D-Paint는 고해상도 질감 맵을 생성하여 생성된 메시나 수작업 메시에 생생한 질감을 부여합니다. Hunyuan3D-Studio 플랫폼은 전문가와 아마추어 모두가 3D 자산 제작을 보다 손쉽게 할 수 있도록 지원합니다.
- ***Technical Details***: Hunyuan3D-DiT는 대형 싱글플로우 기반 확산 모델로, 자동 인코더인 Hunyuan3D-ShapeVAE를 활용하여 메쉬의 미세한 세부사항을 캡처합니다. Hunyuan3D-Paint는 강력한 기하학적 프라이어와 다중 이미지 뷰 매핑을 통해 고해상도 질감 맵을 생성합니다. 질감 생성에서 참고 이미지를 기반으로 각 뷰를 일관되게 전체 생성 프로세스를 관리합니다.
- ***Performance Highlights***: 실험 결과, Hunyuan3D 2.0은 기존의 상업용 비공개 모델뿐만 아니라 트렐리스(Trellis) 같은 오픈소스 모델보다 3D 모양의 세부사항, 조건 정렬 및 질감 품질에서 우수하다는 것을 보여줍니다. 사용자가 텍스트 및 이미지 프롬프트로 가이드를 주면 모든 지정된 기하학에 텍스처를 줄 수 있어 사용자의 창의성을 극대화합니다.

### [Video Depth Anything: Consistent Depth Estimation for Super-Long Videos](https://arxiv.org/abs/2501.12375)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12375.png)

Vote: 13

Authors: Hengkai Guo, Feihu Zhang, Zilong Huang, Sili Chen, Jiashi Feng, Shengnan Zhu, Bingyi Kang

- ***What's New***: 비디오 Depth Anything은 초장시간 비디오에서 효율성을 희생하지 않으면서도 높은 품질과 일관된 depth 추정을 가능하게 하는 새로운 방법을 제안합니다. 이는 Depth Anything V2를 기반으로 한 것으로, 효율적인 공간-시간 head(spatial-temporal head)로 대체됩니다. 이 혁신적인 접근은 여러 비디오 벤치마크에서 제로-샷(zero-shot) 비디오 depth 추정에서 새로운 상태를 설정합니다.
- ***Technical Details***: 이 모델은 Depth Anything V2에 기반하여, 효율적인 공간-시간 head를 사용하여 각 공간 위치에 대한 temporal 정보 상호 작용을 가능하게 합니다. 새로운 temporal 일관성 손실 함수는 추가적인 기하학적 priors 없이 temporal depth gradient를 제어함으로써 설계되었습니다. 모델은 비디오 depth 데이터셋과 비표시된 이미지 데이터셋에서 훈련됩니다. 또한, 장시간 비디오 추론을 지원하기 위해 참조프레임 중심 전략이 개발되었습니다.
- ***Performance Highlights***: 우리의 모델은 다섯 개의 데이터셋에 걸친 제로-샷 비디오 depth 추정에서 공간적 정확성(spatial accuracy)과 temporal 일관성(temporal consistency) 측면에서 SOTA 성과를 달성했으며, 모든 데이터셋에서 기존 모델을 능가했습니다. 모델은 약 30 FPS의 실시간 성능을 지원할 수 있는 다양한 규모로 제공됩니다.

### [Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments](https://arxiv.org/abs/2501.10893)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10893.png)

Vote: 13

Authors: Sercan Ö. Arık, Tao Yu, Ruoxi Sun, Jinsung Yoon, Hongjin Su, Pengcheng Yin

- ***What's New***: Learn-by-interact는 자가 적응형 에이전트(Self-Adaptive Agents)를 위한 데이터 중심 프레임워크로, 인간의 주석 없이도 LLM(대형 언어 모델) 에이전트가 새로운 환경에 적응할 수 있도록 합니다. 문서화된 내용을 기반으로 에이전트와 환경의 상호작용 경로(trajectories)를 생성하고, 이를 요약 및 추상화하여 역구성(backward construction) 과정을 통해 지침을 작성합니다.
- ***Technical Details***: Learn-by-interact 프레임워크는 주어진 환경에서 다양한 작업 지침을 생성하기 위해 self-instruct 방식과 같은 접근을 사용합니다. 생성된 지침에 따라 LLM이 주어진 과제를 수행하고, 이로 인해 에이전트-환경 상호작용의 긴 경로가 생성됩니다. 각 하위 경로에 대해 LLM의 요약 기능을 활용하여 역구성을 통해 새로운 지침을 구성합니다. 이렇게 생성된 데이터는 필터링 과정을 거쳐 훈련 기반 시나리오와 훈련이 필요 없는 인컨텍스트 학습(ICL)에 사용되어 에이전트에게 최적화된 정보를 제공합니다.
- ***Performance Highlights***: SWE-bench, WebArena, OSWorld, Spider2-V와 같은 다양한 벤치마크에서 Learn-by-interact의 효과가 입증되었습니다. ICL을 적용할 경우, Claude-3.5와 같은 모델에서 최대 12.2% 성능 향상을 보였고, Codestral-22B 훈련 에는 19.5% 향상을 이뤄냈습니다. 역구성 과정은 데이터의 양과 질을 각각 최대 14.0% 향상시켰습니다. 실험 결과, Learn-by-interact는 기존의 데이터 생성 및 검색 방법을 능가하며 다양한 환경에서의 에이전트 성능 향상에 기여합니다.

### [Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement](https://arxiv.org/abs/2501.12273)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12273.png)

Vote: 11

Authors: Mo Li, Songyang Zhang, Kai Chen, Taolin Zhang, Yunxin Liu, Haodong Duan, Maosong Cao, Chuyu Zhang

- ***What's New***: Condor는 세계 지식 트리(World Knowledge Tree)와 자가 반성 정제(Self-Reflection Refinement)를 통합하여 대규모로 고품질의 SFT 데이터를 생성하는 혁신적인 이단계 합성 데이터 생성 프레임워크입니다. 이러한 접근 방식은 LLM의 대화 능력을 향상시키며, Condor로 생성된 샘플만으로도 우수한 성능을 달성할 수 있음을 입증합니다.
- ***Technical Details***: Condor는 태그 기반의 세계 지식 트리를 이용하여 합성 데이터를 생성하며, LLM에게 태그를 사용하여 다양한 질문과 응답을 생성하도록 하고, 자가 반성 정제 단계에서 응답의 품질을 더욱 높일 수 있도록 설계되었습니다. 이렇게 생성된 SFT 데이터는 최신 세계 지식 태그를 통합하여 광범위하고 높은 품질의 데이터를 생산합니다.
- ***Performance Highlights***: Condor 방법론으로 벤치마크 테스트에서 모델의 주관적 대화 능력이 크게 향상되었으며, 제안된 Condor에 의해 생성된 데이터는 기존의 방법과 비교해 높은 성능을 나타냈습니다. 실험 결과 7B와 72B 크기의 모델은 RLHF를 포함한 공식 모델을 능가하는 성능 향상을 보였습니다.

### [Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise](https://arxiv.org/abs/2501.08331)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08331.png)

Vote: 11

Authors: Li Ma, Michael Ryoo, Oliver Pilarski, Mingming He, Mohsen Mousavi, Paul Debevec, Yitong Deng, Yuancheng Xu, Ning Yu, Wenqi Xian, Lingxiao Li, Pascal Clausen, Ryan Burgert

- ***What's New***: Go-with-the-Flow는 비디오 확산 모델(Video Diffusion Models)에 모션 제어 기능을 추가한 새로운 기법으로 실시간 변형된 노이즈(Real-Time Warped Noise)를 활용하여 모델을 미세 조정합니다. 이는 다양한 사용자 친화적 모션 제어를 위한 원스탑 솔루션을 제공하며, 비디오 확산 모델의 설계나 훈련 파이프라인 변경없이 기존 모델에 모션 제어를 통합할 수 있습니다.
- ***Technical Details***: Go-with-the-Flow는 발표된 새로운 노이즈 변형 알고리즘(noise warping algorithm)을 통해 공간 가우시안성을 유지하면서 시간적 움직임 추적이 가능하도록 합니다. 이 알고리즘은 각 프레임의 이전 상태만 추적하여 차세대 노이즈를 계산하며, 효율성 면에서 실시간으로 동작할 수 있도록 설계되었습니다. 이는 비디오 확산 모델의 미세 조정에 있어 최소한의 오버헤드를 발생시킵니다.
- ***Performance Highlights***: Go-with-the-Flow의 실험 결과, 이미지 기반 확산 모델에 비디오 편집 태스크를 수행할 때 공간적 가우시안성과 시간적 일관성이 유지되는 것을 확인했습니다. 또한 사용자 연구와 정량적 실험을 통해 픽셀 품질, 모션 제어 및 시간적 일관성에서의 전반적인 우수함을 입증하였습니다.

### [EMO2: End-Effector Guided Audio-Driven Avatar Video Generation](https://arxiv.org/abs/2501.10687)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10687.png)

Vote: 9

Authors: Qi Wang, Siqi Hu, Liefeng Bo, Bang Zhang, Linrui Tian

- ***What's New***: 이 논문에서는 오디오 기반 아바타 영상 생성 방법인 EMO2를 제안합니다. 이 방법은 손동작을 오디오 입력으로부터 생성하고, 표정과 신체 움직임을 합성하여 자연스러운 말하는 얼굴 비디오를 생성할 수 있습니다. 기존의 전신 또는 반신 자세 생성에 집중했던 방식과는 달리, 손동작과 오디오와의 상관관계를 강화하여 새로운 방법론을 제시합니다.
- ***Technical Details***: EMO2는 두 단계로 나누어져 있습니다. 첫 번째 단계에서는 오디오 입력을 통해 손동작을 생성하며, 이 과정에서 디퓨전 모델(Diffusion Model)을 사용하여, 음성과 손의 동작을 효율적으로 맵핑합니다. 두 번째 단계에서는 첫 단계에서 생성된 손동작을 바탕으로, 표정과 신체의 움직임을 합성하여 비디오 프레임을 생성합니다. 이 방법은 음성과 입술 움직임의 동기화를 유지하면서 자연스럽고 연속적인 코합주(Cospeech) 비디오를 생성할 수 있도록 설계되었습니다.
- ***Performance Highlights***: 실험 결과, EMO2는 CyberHost와 Vlogger 등 최신 기술 대비 시각적 품질과 동기화 정확성에서 우수한 성능을 보였습니다. 이는 주어진 프레임 내에서 다양한 움직임을 생성해내어, 기존 방법론보다 더 다양한 모션과 감정 표현을 가능하게 함을 보여줍니다.

### [GPS as a Control Signal for Image Generation](https://arxiv.org/abs/2501.12390)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12390.png)

Vote: 9

Authors: Andrew Owens, Aleksander Holynski, Ziyang Chen, Chao Feng, Alexei A. Efros

- ***What's New***: 이 논문에서는 사진 메타데이터에 포함된 GPS 태그가 이미지 생성에 유용한 제어 신호로 사용될 수 있음을 보여줍니다. GPS와 텍스트를 조건으로 하는 확산 모델(diffusion model)을 학습하여 도시 내 위치에 따라 이미지가 세밀하게 변하는 방법을 이해하는 과제를 수행합니다. 이러한 모델은 도시의 다양한 지역, 공원, 랜드마크의 독특한 외관을 포착하는 이미지를 생성할 수 있습니다.
- ***Technical Details***: GPS를 조건으로 하는 이미지 생성은 GPS 좌표와 텍스트에 조건화된 확산 모델을 학습하여 이루어집니다. GPS를 사용해 각각의 관점에서 이미지를 생성하도록 학습하며, 이 모델에서 3D 구조를 추출하기 위해 스코어 증류 샘플링(score distillation sampling)을 사용합니다. 이러한 접근 방식은 명시적인 카메라 포즈 추정 없이 3D 모델을 형성합니다.
- ***Performance Highlights***: 실험 결과, GPS 기반 조건 모델은 도시 내의 위치 변화를 기반으로 이미지를 성공적으로 생성할 수 있음을 보여줍니다. GPS 조건화는 3D 구조 추정의 개선에도 기여하며, 기존의 2D 이미지-텍스트 기반 모델보다 높은 품질의 3D 랜드마크 재구성을 가능하게 합니다.

### [MSTS: A Multimodal Safety Test Suite for Vision-Language Models](https://arxiv.org/abs/2501.10057)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10057.png)

Vote: 7

Authors: Bertie Vidgen, Sujata Goswami, Xudong Shen, Chiara Di Bonaventura, Seogyeong Jeong, Flor Miriam Plaza-del-Arco, Felix Friedrich, Andrea Zugarini, Donya Rooein, Jieun Han, Roman Eng, Gaia El Khoury Geagea, Alicia Parrish, Paul Röttger, Patrick Schramowski, Giuseppe Attanasio, Anastassia Shaitarova, Richard Willats, Dirk Hovy, Paloma Jeretič, Rishabh Bhardwaj, Janis Goldzycher

- ***What's New***: MSTS는 VLM(Vision-Language Models)의 안전성을 평가하기 위해 40개 세부 위험 범주로 구성된 400개의 테스트 프롬프트를 제시하는 다중 모달 안전 테스트 스위트를 최초로 도입했습니다. 각각의 프롬프트는 텍스트와 이미지를 결합해야 완전한 의미를 드러내며, 멀티모달 입력에 의해 발생하는 새로운 안전 위험에 초점을 맞추고 있습니다.
- ***Technical Details***: MSTS는 LLM(Large Language Models)의 벤치마킹을 위해 개발된 MLCommons의 위험 분류법을 바탕으로 VLM 환경에 맞게 조정한 세부적인 위험 분류에 따라 구성됩니다. 각 테스트 케이스는 독특한 이미지와 문장 조각으로 이루어져 있으며, 이는 다양한 언어로 번역되어 멀티링구얼한 VLM 안전 평가를 가능하게 합니다. MSTS는 400개의 불안전한 영어 다중 모달 프롬프트와 10개의 언어로 번역된 4,000개의 프롬프트를 포함하고 있습니다. 또한, MSTS는 8개의 VLM 시스템을 이용한 자동화된 안전 평가를 탐색했습니다.
- ***Performance Highlights***: 상업용 VLM은 대체로 안전한 반면, 오픈 VLM은 명확한 안전 문제를 드러냈습니다. 특히 MiniCPM-2.6는 힌디어 프롬프트에 대해 36.5%의 불안전한 응답을 보여 영어보다 불안전한 비율이 높았습니다. GPT-4o는 모든 언어에서 단 하나의 불안전한 응답도 하지 않았으며, 멀티모달 프롬프트보다 텍스트 전용 프롬프트에서 더 안전한 반응을 보였습니다. 자동화된 안전 평가의 경우, 가장 우수한 모델조차도 불안전한 반응을 정확히 분류하는 데 있어 53%의 정확도를 기록하며 과제의 어려움을 드러냈습니다.

### [The Geometry of Tokens in Internal Representations of Large Language Models](https://arxiv.org/abs/2501.10573)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10573.png)

Vote: 5

Authors: Matteo Biagetti, Giada Panerai, Alberto Cazzaniga, Yuri Gardinazzi, Karthik Viswanathan

- ***What's New***: 이 연구는 대형 언어 모델(Large Language Models; LLMs)에서 내부 표현의 기하학적 구조와 다음 토큰 예측의 상호작용에 대한 새로운 통찰을 제공합니다. 토큰 임베딩의 기하학적 속성과 암시적 차원(intrinsic dimension), 코사인 유사성(cosine similarity) 등의 지표를 활용하여 토큰 표현을 분석하였습니다.
- ***Technical Details***: 내부 레이어의 경험적 측정을 관찰적으로 조사하기 위해 암시적 차원, 코사인 유사성, 및 이웃 중첩(neighborhood overlap) 지표를 사용하였습니다. 이러한 지표를 사용하여 각 레이어를 통과하는 동안 토큰 표현의 진화를 추적하고, 체계적인 토큰 셔플링을 통해 구문 및 의미 구조가 어떻게 변화하는지를 분석하였습니다.
- ***Performance Highlights***: 조사 결과, 암시적 차원이 초기에서 중간 레이어에서 피크를 나타내며, 셔플링 정도가 증가할수록 이 피크의 높이가 증가함을 발견했습니다. 이는 구조화된 데이터(구문, 의미)에 비해 셔플링된 데이터에서 발생하는 기하학적 변화가 더욱 큼을 시사합니다. 또한, 토큰 표현의 기하학적 특성과 모델의 평균 크로스 엔트로피 손실(cross-entropy loss) 사이에 통계적 상관관계가 있음을 확인하였습니다.

### [Panoramic Interests: Stylistic-Content Aware Personalized Headline Generation](https://arxiv.org/abs/2501.11900)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11900.png)

Vote: 3

Authors: Xiang Ao, Qing He, Junhong Lian, Yang Liu, Xinyu Liu

- ***What's New***: SCAPE(Stylistic-Content Aware Personalized Headline Generation)은 사용자의 콘텐츠 및 스타일 선호도를 모두 통합하여 개인화된 뉴스 헤드라인을 생성하는 새로운 프레임워크입니다. 이 연구는 사용자 프로파일에서 콘텐츠와 스타일 선호도를 함께 활용한 개인화 뉴스 헤드라인 생성의 첫 시도입니다.
- ***Technical Details***: SCAPE 프레임워크는 대형 언어 모델(LLM)의 협업을 통해 헤드라인에서 콘텐츠와 스타일 속성을 추출하는 헤드라인 추론 모듈을 설계했습니다. 이렇게 추출된 스타일 및 콘텐츠 속성은 계층적 게이티드 융합 네트워크에 의해 사용자 장·단기 관심사와 결합됩니다. 또한, 셀프 슈퍼바이즈드 전략을 통해 사용자의 관심사를 병합하여 개인화된 헤드라인 생성기를 통해 최종 결과를 산출합니다.
- ***Performance Highlights***: PENS 데이터셋을 사용한 실험 결과, SCAPE는 ROUGE 및 Fact Scores 측면에서 기존 방법들을 상회하는 성능을 보여주었으며, 개인화가 잘 반영된 헤드라인을 생성함으로써 새로운 기록을 세웠습니다.

### [Taming Teacher Forcing for Masked Autoregressive Video Generation](https://arxiv.org/abs/2501.12389)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12389.png)

Vote: 1

Authors: Heung-Yeung Shum, Zheng Ge, Quan Sun, Xiangyu Zhang, Kun Yan, Yuang Peng, Duomin Wang, Lionel M. Ni, Nan Duan, Deyu Zhou, Runpei Dong

- ***What's New***: 이 논문은 MAGI라는 하이브리드 비디오 생성 프레임워크를 소개하였습니다. 이 프레임워크는 프레임 내(masked modeling)와 프레임 간(causal modeling) 종속성을 결합한 새로운 방식입니다. 주요 혁신으로는 Complete Teacher Forcing(CTF)라는 방법이 소개되어, 마스킹된 프레임이 아닌 완전한 관찰 프레임에 기반하여 다음 프레임 생성을 조건지우는 것이 가능해졌습니다.
- ***Technical Details***: MAGI는 Autoregressive Video Generation을 위해 두 가지 모델링 체계를 융합한 프레임워크입니다. Masked Teacher Forcing(MTF)의 한계를 극복하고자 CTF 기법을 도입하면서, 모델이 완전한 프레임 관찰을 통해 다음 프레임을 생성할 수 있도록 했습니다. 이는 학습과 추론 간 일관성을 유지하여 성능을 향상시켰습니다. 또한, 노이즈 주입과 동적 간격 훈련(dyanmic interval training)을 통해 노출 편향(exposure bias)을 해결하고 모델의 일반화 능력을 강화했습니다.
- ***Performance Highlights***: 새로운 CTF 기법은 기존의 MTF 대비 첫 프레임 조건부 비디오 예측에서 FVD 점수가 23% 개선되었습니다. 또한 MAGI는 Kinetics-600 데이터셋의 비디오 예측에서 뛰어난 성능을 보여주며, 100프레임 이상의 긴 비디오 시퀀스를 생성할 수 있는 확장 가능한 가능성을 입증했습니다.

### [Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model](https://arxiv.org/abs/2501.12206)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12206.png)

Vote: 0

Authors: Sajib Acharjee Dip, Lang Zhang, Chris Thomas, Khizar Hussain, Kazi Hasan Ibn Arif

- ***What's New***: 이 연구는 대형 시각 언어 모델(Large Vision Language Models; LVLMs)의 맥락상의 환각 문제를 해결하기 위한 새로운 어텐션 수정 방법을 제안합니다. 주로 시각적 토큰에 집중하여 환각률을 최대 62.3%까지 감소시키면서도 유사한 작업 성능을 유지하는 방법론을 설명합니다.
- ***Technical Details***: 제안된 방법은 선택적 토큰 강조와 헤드 고유의 변조를 결합하여 생성 과정 전반에 걸쳐 시각적 그라운딩을 유지합니다. 이 접근법은 두 가지 주요 요소로 구성됩니다: (1) 지역 정보와 공간적으로 중요한 시각적 토큰을 식별하고 우선 순위를 매기는 이중 스트림 토큰 선택 메커니즘, (2) 개별 어텐션 헤드의 시각적 민감도를 측정하여 시각 정보 처리를 차별적으로 증폭하는 어텐션 헤드 특유의 변조 전략입니다.
- ***Performance Highlights***: MSCOCO 데이터셋에서의 실험 결과, 제안된 방법은 기존 모델에 비해 환각률을 최대 62.3%까지 감소시켰으며, 성능 면에서는 기존 방식과 유사한 결과를 보였습니다. 제안된 방법은 모델 재교육 없이 다양한 시각적 민감도를 가진 어텐션 헤드를 선택적으로 조절하여 시각적 상세 정보의 정확도를 향상시켰습니다.

