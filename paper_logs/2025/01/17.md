## Daily Papers (2025-01-17)

### [Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps](https://arxiv.org/abs/2501.09732)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09732.png)

Vote: 30

Authors: Tommi Jaakkola, Nanye Ma, Yu-Chuan Su, Mingda Zhang, Yandong Li, Saining Xie, Shangyuan Tong, Haolin Jia, Xuhui Jia, Xuan Yang, Hexiang Hu

- ***What's New***: 이번 연구에서는 덴조잉 스텝(Denoising Steps)을 증가시키는 것 이상의 추론 시간에서의 스케일링(inference-time scaling) 방법을 탐구하여, 확산 모델(Diffusion Models)의 생성 성능을 더욱 향상시킬 수 있는 방안을 제시합니다. 특히, 확산 샘플링 과정에서 더 나은 노이즈를 찾는 탐색 문제를 고려합니다.
- ***Technical Details***: 제안된 검색 프레임워크는 피드백을 제공하는 검증자(verifiers)와 더 나은 노이즈 후보를 찾기 위한 알고리즘을 두 축으로 설계되어 있습니다. 클래스 기반 및 텍스트 기반 이미지 생성 벤치마크에서의 광범위한 실험을 통해, 추론 시간에 계산 능력을 증가시키면 확산 모델의 샘플 품질이 상당히 개선되는 것을 발견했습니다.
- ***Performance Highlights***: 실험 결과 확산 모델의 추론 시간에 계산량을 증가시키는 것이 생성한 샘플의 품질 향상으로 이어졌으며, 이미지의 복잡한 특성 때문에 프레임워크 내의 구성 요소 조합이 다양한 응용 시나리오에 맞게 특별히 선택될 수 있음을 보여줍니다.

### [OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking](https://arxiv.org/abs/2501.09751)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09751.png)

Vote: 28

Authors: Jialong Wu, Runnan Fang, Jiang Yong, Pengjun Xie, Huajun Chen, Wenbiao Yin, Zekun Xi, Ningyu Zhang, Jizhan Fang, Fei Huang

- ***What's New***: OmniThink는 기계 글쓰기에 있어 인간과 유사한 사고 과정인 반복 확장과 반사를 모방한 새로운 콘텐츠 생성 프레임워크입니다. 이는 복잡한 주제에 대한 이해를 지속적으로 심화시키면서 정보의 경계를 확장하는 것을 목표로 하고 있습니다.
- ***Technical Details***: OmniThink의 구조는 정보 획득, 개요 구성, 그리고 기사 작성의 세 가지 단계로 나뉩니다. 정보 획득 단계에서는 반복적인 확장 및 반사의 과정을 통해 정보 트리 및 개념 풀을 구성하여 주제에 대한 심층적인 이해를 돕습니다. 그런 다음, 개념 풀을 기반으로 자세한 개요를 작성하고, 이를 기반으로 세부 섹션을 생성하여 콘텐츠를 작성합니다. 이 과정에서는 확대 과정과 반사 과정이 중요한 역할을 하며, 정보의 깊이와 다양성을 높입니다.
- ***Performance Highlights***: OmniThink는 WildSeek 기준 데이터셋에서 다양한 평가 기준에 따른 자동 및 인간 평가에서 기존 방법보다 우수한 결과를 나타냈습니다. 특히, 수도 밀도(knowledge density)와 정보 다양성에서 GPT-4o를 사용하는 다른 최첨단 방법을 능가하는 성과를 보여주었습니다. 이러한 다차원적 평가에서 주목할만한 것은 Omnithink의 반사적 능력으로 인해 발생하는 기사 참신성의 두드러진 증가입니다.

### [Learnings from Scaling Visual Tokenizers for Reconstruction and Generation](https://arxiv.org/abs/2501.09755)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09755.png)

Vote: 16

Authors: Philippe Hansen-Estruch, Orr Zohar, Peter Vajda, Xinlei Chen, Sriram Vishwanath, Jialiang Wang, Tingbo Hou, Tao Xu, Ching-Yao Chung, David Yan

- ***What's New***: 이 논문은 Visual Tokenizer를 확장하여 Reconstruction 및 Generation 작업을 수행하면서 얻은 학습 내용을 다룬다. 기존의 Convolutional 기반 방식을 Transformer 기반으로 변경하여 데이터를 압축하는 개선된 아키텍처인 ViTok(Vision Transformer Tokenizer)을 도입하였다. 이 기법을 통해 ImageNet-1K를 넘는 대규모 이미지와 비디오 데이터셋을 학습하여 기존 데이터 제약을 극복하고자 한다.
- ***Technical Details***: ViTok 아키텍처는 Convolutional 백본을 Transformer 기반 Auto-Encoder로 대체하여 Vision Transformer 아키텍처를 채택하였다. 이 Auto-Encoder는 Llama(Transformer 아키텍처의 한 변형)를 사용하여 튜브렛과 패치를 통한 Latent 공간 코드화를 수행한다. Bottleneck의 크기 확장을 통해 Reconstruction에서의 상관관계를 밝혔지만 Generative Task에서는 혼합된 결과를 보였다. 또한, 비디오에서 동일한 압축률로 더 나은 복원 지표를 달성할 수 있었다.
- ***Performance Highlights***: ViTok은 COCO 및 UCF-101 데이터셋의 이미지 및 비디오 Reconstruction 작업에서 기존 최첨단 Auto-Encoder를 능가하는 성능을 보여주었다. 특히, 128p 비디오 복원에서는 2-5배 적은 FLOPs로 성능을 유지하며, Generative Task에서도 강력한 성능을 발휘하였다. 이는 Video 데이터의 내재적 중복성을 활용함으로써 가능했다.

### [Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators](https://arxiv.org/abs/2501.09484)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09484.png)

Vote: 14

Authors: Wen Ye, Zhishou Zhang, Qiang Ju, Yu Xiao, Hengfu Cui, Jian Xie, Zhaocheng Liu, Yalun Zhu, Shizheng Li, Quan Tu

- ***What's New***: 이 논문은 대화형 의사-환자 시뮬레이터를 통해 '질문'과 '진단' 사이의 관계를 탐구하는 데 초점을 맞추고 있습니다. 실제 의사-환자 대화에서 대화 전략을 추출하여 환자 시뮬레이터를 만들고, 이를 사용하여 온라인 의료 상담 과정에서 질문과 진단의 관계를 실험적으로 분석했습니다.
- ***Technical Details***: 이 연구에서는 실제 의사-환자 대화를 바탕으로 대화 전략 흐름을 추출했습니다. 그런 다음 해당 전략을 선택하여 인공적으로 대화 데이터를 생성하고, 이를 통해 환자 시뮬레이터를 학습시켰습니다. 이 시뮬레이터는 GPT-4o를 활용하여 여러 유형의 질의 패턴을 분류하였고, 이를 통해 다른 의사 모델들이 생성한 질의와 진단 간의 상관관계를 분석했습니다.
- ***Performance Highlights***: 실험 결과, 서로 다른 의사 모델들 간에는 질문과 진단 능력에서 큰 차이가 있다는 점이 드러났습니다. 특히 '질문'의 질이 낮으면, '진단'의 강력한 능력에도 불구하고 좋은 결과를 얻기 어렵다는 점을 발견했습니다. 이는 '질문'과 '진단'이 Liebig의 법칙에 따름을 나타내는 결과로, 일반적으로 의사 모델 개발 시에는 이 두 가지 능력을 어떻게 조화시킬지에 대한 깊이 있는 탐구가 필요하다는 것을 시사합니다.

### [Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models](https://arxiv.org/abs/2501.09686)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09686.png)

Vote: 11

Authors: Fanjin Meng, Sijian Ren, Yunke Zhang, Yuwei Yan, Tianjian Ouyang, Jie Feng, Jiahui Gong, Chen Gao, Xinyuan Hu, Zefang Zong, Jingwei Wang, Fengli Xu, Qianyue Hao, Jingyi Wang, Chenyang Shao, Xiaochong Lan, Yiwen Song, Yu Li, Yong Li, Qinglong Yang

- ***What's New***: 대형 언어 모델(LLM)에서 강화 학습을 활용한 이유 추론의 발전을 체계적으로 정리한 논문입니다. 이 논문은 주로 OpenAI의 o1 시리즈와 같은 최신 모델의 발전을 통해 대형 이유 모델의 방향성을 제시합니다.
- ***Technical Details***: 이 논문에서는 LLM의 강력한 이유 능력을 강화하기 위해 자동화된 데이터 생성, 학습-추론 기법, 그리고 테스트 시간 스케일링의 핵심 기술 구성 요소를 탐구합니다. 특히 PRM(Process Reward Model)을 사용하여 중간 단계에서의 이유를 평가하고, RL(Reinforcement Learning)을 활용해 LLM의 이유 능력을 향상시키는 방법을 소개합니다.
- ***Performance Highlights***: OpenAI o1 시리즈는 수학적 문제 해결에서 국제 수학 올림피아드 수준의 성과를 보이며, 물리학, 화학, 생물학 등 여러 분야에서도 박사 수준의 성능을 보여줍니다. 이러한 모델은 트레이닝과 인퍼런스 단계에서 새로운 스케일링 법칙을 통해 성능을 지속적으로 향상시킬 수 있음을 보여줍니다.

### [FAST: Efficient Action Tokenization for Vision-Language-Action Models](https://arxiv.org/abs/2501.09747)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09747.png)

Vote: 11

Authors: Quan Vuong, Karl Pertsch, Chelsea Finn, Kyle Stachowicz, Danny Driess, Oier Mees, Suraj Nair, Sergey Levine, Brian Ichter

- ***What's New***: FAST(주파수 공간 행동 시퀀스 토큰화)은 로봇의 고주파 컨트롤과 고난이도 기술 학습을 가능하게 하는 새로운 토큰화 방식입니다. 이 방식은 기존의 단순한 시차 당 차원 별 비닝(binning) 방식에 비해 향상된 성능을 보여 주며, 1M 개의 실제 로봇 행동 데이터를 학습하여 다양한 로봇 액션 시퀀스에 블랙박스 토큰화기로 사용할 수 있는 FAST+을 출시했습니다.
- ***Technical Details***: FAST는 데이터 압축 방법에서 영감을 받아, 영상을 압축하는 데 사용되는 이산 코사인 변환(DCT)을 기반으로 한 새로운 행동 시퀀스 토큰화 방법을 제안합니다. 데이터셋의 각 행동 차원에 대해 DCT를 사용하여 시퀀스를 주파수 공간으로 변환한 후, 중요하지 않은 주파수 성분을 생략하여 높은 정보의 토큰으로 변환합니다. 그런 다음 Byte Pair Encoding(BPE)을 활용해 압축하여 VLA 모델의 기존 어휘에 통합할 수 있는 고정 크기의 출력 어휘를 생성합니다.
- ***Performance Highlights***: FAST를 사용한 새로운 방법은 다양한 작업에서 기존의 방법보다 최대 5배 빠른 시간 내에 학습 가능한 성능을 제공합니다. 특히 DROID 데이터셋을 사용한 평가에서 처음으로 새로운 환경에서 제로-샷 평가가 가능하다는 점을 보여, 기존의 DROID 및 OpenVLA 모델보다 뛰어난 일반화를 보여줍니다.

### [SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces](https://arxiv.org/abs/2501.09756)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09756.png)

Vote: 11

Authors: Mengwei Ren, Zhixin Shu, Julie Dorsey, Yannick Hold-Geoffroy, Sumit Chaturvedi, Jingyuan Liu

- ***What's New***: SynthLight는 인공지능 디퓨전 모델(Diffusion Model)을 활용하여 초상화의 조명을 새롭게 조정하는 혁신적인 방식을 제안합니다. 이 모델은 조명 변화에 따라 픽셀을 변환하는 재렌더링(re-rendering) 문제로 초상화 조명을 다루며, 3D 인물 모형과 물리적 기반의 렌더링 엔진을 활용하여 조명 조건에 따라 변환할 데이터셋을 생성합니다. 이를 통해 실제 초상화 사진에 자연스러운 조명 효과를 제공할 수 있습니다.
- ***Technical Details***: SynthLight는 Blender(Cycles)를 사용하여 다양한 조명 조건에서 3D 인물 이미지를 렌더링하고, 이러한 이미지 쌍을 바탕으로 디퓨전 모델을 학습시켜 재렌더링 기능을 직접 학습합니다. 모델 학습 시, 실제 인물 사진과의 도메인 격차를 좁히기 위해 멀티태스크 학습과 classifier-free guidance를 사용하여, 세밀한 텍스처 보존과 조명 효과의 균형을 맞춥니다.
- ***Performance Highlights***: Light Stage 데이터를 사용하지 않고도 최첨단 초상화 리라이팅(relighting) 방법과 비슷하거나 더 우수한 성능을 보이며, 사용자 연구에서 조명의 정확성, 정체성 보전 및 이미지 품질 등 모든 평가 항목에서 높은 선호도를 기록했습니다. 또한, 다양한 실제 상황에서의 평가를 통해 모형이 과거 방식으로는 생성할 수 없던 복잡하고 사실적인 조명 효과를 잘 구현하는 것을 확인했습니다.

### [The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models](https://arxiv.org/abs/2501.09653)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09653.png)

Vote: 8

Authors: Arie van Deursen, Jonathan Katzy, Maliheh Izadi, Razvan Mihai Popescu

- ***What's New***: The Heap은 대규모 언어 모델(Large Language Models; LLMs)을 위한 멀티언어 코드 데이터셋으로, 다른 공개 코드 데이터셋과의 중복을 제거하여 데이터 오염(Data Contamination)을 방지하며 공정한 평가를 가능하게 합니다. 이는 오염되지 않은 데이터를 통해 LLMs의 downstream task를 평가할 수 있는 새로운 방법을 제안한 것입니다.
- ***Technical Details***: The Heap 데이터셋은 57개의 프로그래밍 언어를 다루며, GitHub의 코드 저장소에서 검색 API를 통해 수집되었습니다. 데이터셋은 비허가 라이센스 비율(non-permissive licenses)을 주요 필터로 사용하여 저작권 문제를 피하고, 유사중복 및 정확한 중복 제거를 위해 SHA-256 해시와 MinHash Locality-Sensitive Hashing(LSH)을 활용합니다. 모든 파일은 주석과 공백을 제거하여 중복을 평가했습니다.
- ***Performance Highlights***: The Heap은 데이터셋 내에서 96,990,250개의 raw 파일 중 38,681,609개의 고유 파일을 유지하며, 중복제거를 통해 데이터 오염 가능성을 낮추는 데 성공했습니다. 또한, 데이터셋은 후속 연구에서 더 높은 품질의 평가를 지원하기 위해 향후 자연 언어 주석 및 주제 기반 정제를 추가하는 계획을 가지고 있습니다.

### [CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation](https://arxiv.org/abs/2501.09433)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09433.png)

Vote: 7

Authors: Hwan Heo, Jeong A Wi, Seongyeong Lee, Junyoung Choi, Sangjun Ahn, Jangyeong Kim

- ***What's New***: CaPa는 고품질의 3D 에셋을 빠르고 효율적으로 생성하는 새로운 프레임워크입니다. 이 접근 방식은 기하 구조 생성과 질감 합성을 분리한 두 단계의 프로세스를 통해 텍스처 품질과 기하학적 안정성을 모두 개선하며, 상업적 응용에 즉시 사용 가능한 3D 에셋을 30초 이내에 제공합니다.
- ***Technical Details***: CaPa는 3D 잠재 확산 모델을 사용하여 다중 시점 입력에 기반한 명확한 기하 구조를 생성하며, 'Spatially Decoupled Attention'을 활용하여 4K 해상도의 고품질 텍스처를 합성합니다. 또한, 3D 인지 폐색 보정(3D-aware occlusion inpainting)을 통해 텍스처가 없는 영역을 메꿔 전체 모델에 일관성을 부여합니다.
- ***Performance Highlights***: CaPa는 기존의 방법들에 비해 텍스처 충실도와 기하학적 안정성에서 뛰어난 성능을 보이며, 실용적이고 확장 가능한 3D 에셋 생성의 새로운 기준을 확립하였습니다. 또한, 이 시스템은 고충실도의 폴리곤 메시를 기존 방법에 비해 훨씬 빠르게 생성합니다.

### [RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation](https://arxiv.org/abs/2501.08617)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08617.png)

Vote: 7

Authors: Kaiqu Liang, Jaime Fernández Fisac, Haimin Hu, Ryan Liu, Thomas L. Griffiths

- ***What's New***: RLHS(Reinforcement Learning from Hindsight Simulation)는 RLHF(Reinforcement Learning from Human Feedback)의 정렬(misalignment) 문제를 완화하기 위해 과거 결과를 시뮬레이션하여 피드백을 제공하는 새로운 알고리즘적 접근법입니다. 이는 시뮬레이션된 과거 피드백을 기반으로 학습하여 사용자 만족도 및 유틸리티를 개선합니다.
- ***Technical Details***: RLHS는 AI 시스템이 시뮬레이션을 통해 가능한 결과를 예측한 후, 실제로 도움이 된 행동에 대해 피드백을 요청하는 방식으로 작동합니다. Proximal Policy Optimization(PPO) 및 Direct Preference Optimization(DPO)와 같은 온라인 및 오프라인 선호 최적화 방법에 적용되며, 시뮬레이션된 과거 피드백을 통합함으로써 모델의 정렬 문제를 해결합니다.
- ***Performance Highlights***: 실험 결과 RLHS는 온라인 인간 사용자 테스트에서도 사용자의 목표 달성 및 만족도에서 RLHF를 능가했습니다. RLHS를 통해 오픈 소스 및 독점 모델 모두에서 정렬 오류가 크게 감소했으며, 이는 실제 인간 평가자가 아닌 AI 모델이 시뮬레이션된 피드백을 활용하여도 효과적임을 보여줍니다.

### [AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation](https://arxiv.org/abs/2501.09503)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09503.png)

Vote: 6

Authors: Yifeng Geng, Binghui Chen, Yuxiang Tuo, Junjie He, Liefeng Bo, Chongyang Zhong

- ***What's New***: AnyStory는 텍스트-이미지 생성에서 단일 및 다중 주제 개인화를 통합하는 새로운 접근법을 선보입니다. 이 방법은 단일 주제뿐만 아니라 다중 주제를 고화질로 개인화하면서도 주제의 충실도를 유지하여 다양한 배경과 자세, 뷰를 함께 표현할 수 있는 복합적인 내러티브를 생성할 수 있습니다.
- ***Technical Details***: AnyStory는 UniPortrait의 'encode-then-route' 설계를 개선하여 RefereneNet과 CLIP vision encoder를 사용해 주제 특징을 고화질로 인코딩합니다. 주제 라우터(subject router)로는 디커플드 인스턴스-인식 라우팅(decoupled instance-aware subject routing)을 사용하여 잠재 공간 상의 주제의 잠재적 위치를 정확하게 예측하고 조건 주입을 안내합니다. 라우터는 경량 이미지 분할 디코더로 모델링되어 인스턴스 세그멘테이션과 유사한 방식으로 동작하여 주제가 제대로 분할될 수 있습니다.
- ***Performance Highlights***: AnyStory는 주제의 세부 사항을 충실하게 유지하고, 텍스트 설명을 일치시키며, 다중 주제 개인화를 지원하는 데 있어 우수한 성능을 입증하였습니다. 특히 다중 주제 생성을 수행할 때 주제 혼합을 방지하며 독특한 스타일을 유지할 수 있는 능력을 보여줍니다.

### [Do generative video models learn physical principles from watching videos?](https://arxiv.org/abs/2501.09038)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09038.png)

Vote: 4

Authors: Kevin Swersky, Priyank Jaini, Robert Geirhos, Saman Motamed, Laura Culp

- ***What's New***: 이 논문은 영상 생성 AI 비디오 모델이 물리적 원리를 학습할 수 있는지를 평가하기 위해 Physics-IQ라는 포괄적이고 심층적인 벤치마크(benchmark)를 개발했습니다. 이 벤치마크는 유체역학(fluid dynamics), 광학(optics), 고체역학(solid mechanics), 자력(magnetism), 열역학(thermodynamics) 등 다양한 물리적 원리를 이해해야만 해결할 수 있는 데이터셋을 포함하고 있습니다.
- ***Technical Details***: Physics-IQ 데이터셋은 66개의 다양한 물리적 시나리오에 기반한 396개의 8초짜리 동영상으로 구성되어 있습니다. 각 시나리오는 특정 물리 법칙에 초점을 두고 AI 비디오 생성 모델의 물리적 이해 능력을 테스트하며, 이는 객체의 연속성, 가려짐, 객체의 지속성 등을 포함합니다. 비디오 모델들은 주어진 초기 3초 영상(또는 마지막 프레임)을 기반으로 5초간의 비디오를 생성해야 하며, 예측의 물리적 타당성을 평가하기 위해 특정 기준에 따라 점수를 매깁니다.
- ***Performance Highlights***: 테스트 결과, VideoPoet (multiframe) 모델이 가장 높은 Physics-IQ 점수인 24.1%를 기록했지만, 이는 여전히 최대 성능(100%)에 크게 미치지 못합니다. 이는 현재 비디오 생성 모델이 물리적 이해가 부족하며, 각 모델은 물리적 이해 능력이 시각적 실사와 관련이 없음을 보여줍니다. 시각적 리얼리즘은 높은 수준이나 물리적 이해를 나타내지는 못합니다.

