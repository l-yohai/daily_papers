## Daily Papers (2025-01-16)

### [MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents](https://arxiv.org/abs/2501.08828)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08828.png)

Vote: 17

Authors: Kuicai Dong, Yong Liu, Ruiming Tang, Yujing Chang, Dexun Li, Xin Deik Goh

- ***What's New***: MMDocIR 벤치마크는 멀티모달 문서 검색(Multi-modal Document Retrieval)을 위한 새로운 벤치마크로서, 페이지 수준과 레이아웃 수준의 두 가지 중요한 작업을 포함합니다. 페이지 수준 검색은 긴 문서 내에서 가장 관련성이 높은 페이지를 찾아내는 것을 목표로 하고, 레이아웃 수준 검색은 특정 레이아웃을 찾아내어, 전체 페이지 분석보다 더 세밀한 수준의 요소를 감지하는 데 집중합니다.
- ***Technical Details***: MMDocIR 벤치마크는 여러가지 도메인에서 313개의 긴 문서와 1,685개의 질문을 제공하며, 각 질문에 대해 페이지와 레이아웃 수준의 주석을 제공합니다. 주요 실험 결과로는 시각적 리트리버(Visual Retriever)가 텍스트 리트리버보다 뛰어난 성능을 보인다는 점과 MMDocIR의 훈련 데이터셋이 멀티모달 문서 검색 학습에 효율적이라는 점이 밝혀졌습니다. 또한 GPT-4o를 사용하는 VLM 텍스트가 전통적인 OCR 텍스트보다 훨씬 더 나은 성능을 보임을 확인했습니다.
- ***Performance Highlights***: 시각적 리트리버는 탁월한 성능을 보여줬으며 특히 DPR-Phi3와 Col-Phi3 모델은 다양한 도메인에서 지속적으로 높은 검색 정확도를 보였습니다. 페이지 검색에서는 상위 5위까지의 검색으로 상당한 커버리지를 제공하여 관련 정보를 효과적으로 포착할 수 있게 되었고, 레이아웃 검색에서는 시각적 리트리버가 OCR 기반 텍스트 리트리버보다 우세한 성능을 발휘했습니다.

### [Towards Best Practices for Open Datasets for LLM Training](https://arxiv.org/abs/2501.08365)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08365.png)

Vote: 16

Authors: Rebecca Weiss, Greg Leppert, Jennifer Wang, Paul Keller, Greg Lindahl, Kasia Chmielinski, Thomas Wolf, Cullen Miller, Max Ryabinin, Solana Larsen, Kasia Odrozek, Nik Marda, Pierre-Carl Langlais, Shayne Longpre, Maarten Van Segbroeck, Guilherme Penedo, Stella Biderman, Hynek Kydlíček, Jennifer Ding, Leandro von Werra, Marzieh Fadaee, Sebastian Majstorovic, Anna Tumadóttir, Mitchell Baker, Andrew Strait, Maurice Weber, Aviya Skowron, Mark Surman, Maximilian Gahntz, Stefan Baack, Jillian Bommarito, Angela Oduor Lungati, EM Lewis-Jong, Kathleen Siminyu, Ayah Bdeir, Lisa Gutermuth, Victor Miller, Julie Belião, Lee White

- ***What's New***: 이 논문은 LLM(대형 언어 모델) 학습을 위한 공개 데이터셋(Open Datasets)의 모범 사례를 제시합니다. Mozilla와 EleutherAI는 2024년 6월에 연구자와 전문가 30명을 모아 공개 라이선스 데이터셋을 만드는 데 필요한 규범적 원칙과 기술적 최선 사례를 논의했으며 이 논문은 그 논의의 결과를 바탕으로 작성되었습니다.
- ***Technical Details***: 데이터셋 제작에서 발생하는 법적 및 기술적 어려움을 해결하기 위해 공개 라이선스 데이터셋의 생성, 처리, 관리, 배포에 대한 실용적인 권장 사항이 제시됩니다. 또한 데이터셋 투명성 및 평가 가능성을 높이기 위해 재현 가능성을 강화하는 방식으로 데이터를 제공하는 것이 중요하다고 강조합니다.
- ***Performance Highlights***: 기업과 연구자가 LLM 개발에 있어 데이터를 투명하고 책임 있게 사용할 수 있도록 돕기 위해 일곱 가지 원칙이 제안되었습니다. 이는 LLM 생태계의 경쟁 촉진, 데이터의 책무성과 투명성 강화, 다양한 언어와 문화의 포용성 향상 등을 포함합니다.

### [CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities](https://arxiv.org/abs/2501.08983)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08983.png)

Vote: 11

Authors: Zhaoxi Chen, Ziwei Liu, Fangzhou Hong, Haozhe Xie

- ***What's New***: CityDreamer4D는 무한한 4D 도시를 생성하기 위한 조합적 생성 모델입니다. 이 연구에서는 특히 동적 객체(예: 차량)와 정적 장면(예: 건물과 도로)을 분리하여 생성하며, 서로 다른 유형의 신경장을 통해 도시 내 모든 객체를 생성합니다. 또한 OSM, Google Earth, CityTopia 등의 다양한 도시 데이터셋을 제공하여 실제적인 도시 배치와 고품질 이미지를 사용, 사실적인 4D 도시 생성을 지원합니다.
- ***Technical Details***: CityDreamer4D 모델은 Unbounded Layout Generator와 Traffic Scenario Generator를 통해 동적 트래픽 시나리오와 정적 도시 배치를 생성하며, Bird’s Eye View (BEV) 표현을 사용하여 효율성과 확장성을 보장합니다. 건물과 차량의 경우, 특정 인스턴스 중심의 신경장과 주기적 위치 임베딩을 사용하여 객체의 다양성을 효과적으로 표현합니다. 또한, 각 객체는 객체 중심 좌표 공간에 배치하여 사실적인 형태를 캡처합니다.
- ***Performance Highlights***: CityDreamer4D는 다양한 4D 도시를 현실감 있게 생성하며 인스턴스 수준의 편집을 가능케 합니다. 주요 성능 지표로는 FID, KID 및 VBench에서 기존 방법보다 뛰어난 성과를 나타냈으며, 이는 3D 지오메트리 및 보기 일관성에서 우수성을 보여 줍니다.

### [RepVideo: Rethinking Cross-Layer Representation for Video Generation](https://arxiv.org/abs/2501.08994)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08994.png)

Vote: 10

Authors: Zhengyao Lv, Weichen Fan, Yu Qiao, Ziwei Liu, Ziqi Huang, Chenyang Si

- ***What's New***: RepVideo는 비디오 생성 (Video Generation)을 위한 텍스트-비디오 확산 모델 (text-to-video diffusion models)에 대한 새로운 표현 프레임워크를 도입하였습니다. 이는 인접 레이어의 특징을 축적하여 풍부한 표현을 형성하여 더 안정적인 의미 정보를 포착하며, 이러한 개선된 표현을 주의를 끌어내기 위한 입력으로 사용해 의미 표현성을 향상시키고 인접 프레임 간의 특징 일관성을 보장합니다.
- ***Technical Details***: RepVideo는 각 트랜스포머 레이어의 주의 모듈이 캡처한 다양한 로컬 정보를 활용하여 생성된 비디오의 의미적 일관성과 품질을 향상시키려고 시도합니다. 이를 위해 특징 캐시 모듈 (Feature Cache Module)을 도입하여 여러 인접 트랜스포머 레이어의 특징을 모으고, 모은 특징들에 대한 평균 집계를 통해 안정적인 의미 표현을 이루어 냅니다. 그런 다음, 합쳐진 표현은 원본 트랜스포머 입력과 게이트 메커니즘을 통해 조합되어 각 트랜스포머 레이어에 대한 강화된 특징 입력을 생성합니다.
- ***Performance Highlights***: RepVideo는 인접 프레임 간에 더 높은 유사성을 유지하여 임시적 일관성을 향상시키면서도, 생성된 비디오에서 복잡한 공간 관계를 정확하게 캡처하는 능력을 향상시킵니다. VBench 벤치마크에서 CogVideoX-2B와 비교하여 Motion Smoothness와 Object Class에서 각각 0.4%와 4.46%가 향상된 성능을 나타냈고, Spatial Relationship와 Multiple Objects 항목에서도 각각 4.84%, 8.55% 향상된 결과를 보였습니다.

### [Multimodal LLMs Can Reason about Aesthetics in Zero-Shot](https://arxiv.org/abs/2501.09012)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09012.png)

Vote: 5

Authors: Changwen Chen, Ruixiang Jiang

- ***What's New***: 이 논문은 다중 모달 대형 언어 모델(Multimodal LLMs; MLLMs)이 예술 작품의 미학적 평가를 할 수 있는 능력을 제로샷(Zero-Shot) 방식으로 논의한 최초의 연구입니다. 이를 위해 MLLMs의 예술 스타일화 성능을 평가할 수 있는 새로운 고품질 데이터셋인 MM-StyleBench를 구축했습니다.
- ***Technical Details***: MM-StyleBench는 예술 스타일화를 벤치마킹하기 위한 대규모 데이터셋입니다. 사용자가 예술 스타일을 평가할 때의 선호도를 모델링하기 위해 체계적인 접근 방식을 개발하였으며, MLLMs의 응답과 인간의 선호도 간의 상관 관계 분석을 수행했습니다. 또한, 환각(hallucination)을 줄이고 MLLM의 미학적 추론 능력을 향상시키기 위해 ArtCoT라는 새롭게 도출한 작업 분해 및 구체적인 언어 사용을 제안합니다.
- ***Performance Highlights***: ArtCoT 방법을 통해 MLLMs가 인간의 미학적 선호도에 맞춰진 결과를 보였으며, 기존의 제로샷 연쇄 추론(Zero-shot Chain-of-Thought) 프롬프트를 사용할 때보다 미학적 정렬이 평균 56% 향상되었습니다. 이는 다중 모달 추론 능력을 향상시켜 시각적 예술 평가에 높은 상관 관계를 나타낸다는 것을 보여줍니다.

### [Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding](https://arxiv.org/abs/2501.07783)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.07783.png)

Vote: 5

Authors: Gen Luo, Hao Li, Zhaokai Wang, Yu Qiao, Junqi Ge, Wenhan Dou, Jifeng Dai, Lewei Lu, Changyao Tian, Xue Yang, Xizhou Zhu

- ***What's New***: 새로운 네트워크 아키텍처인 Parameter-Inverted Image Pyramid Networks(PIIP)를 제안했습니다. 이 아키텍처는 높은 해상도의 이미지를 작은 네트워크로 처리하고 낮은 해상도의 이미지를 큰 모델로 처리하여 계산 비용과 성능 간의 균형을 맞춥니다. 또한, 여러 공간 스케일의 정보를 통합하는 새로운 cross-branch feature interaction 기법을 제안합니다.
- ***Technical Details***: PIIP는 ViT(Vision Transformers) 또는 CNN(Convolutional Neural Networks)과 같은 사전 학습된 모델을 사용하여 다양한 해상도의 이미지를 처리합니다. 고해상도의 이미지는 작은 네트워크로, 저해상도의 이미지는 큰 모델로 처리하여 효율성을 확보하며, 다양한 스케일의 특징을 결합하는 cross-branch interaction 메커니즘을 도입했습니다. 이 메커니즘은 서로 다른 해상도의 특징들이 서로 보완할 수 있도록 돕습니다.
- ***Performance Highlights***: 다양한 객체 탐지, 분할, 이미지 분류, 멀티모달 이해 작업에서 기존의 단일 또는 다중 해상도 접근법보다 우수한 성능을 보였습니다. PIIP는 기존의 InternViT-6B를 기반으로 MS COCO에서 1.9% 및 ADE20K에서 1.3%의 성능 향상을 이루었습니다. 또한, 멀티모달 이해 작업에서 PIIP-LLaVA는 TextVQA에서 73.0%의 정확도, MMBench에서 74.5%의 정확도를 얻었으며, 학습 데이터 2.8M만을 사용해 뛰어난 성능을 입증했습니다.

### [XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework](https://arxiv.org/abs/2501.08809)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08809.png)

Vote: 5

Authors: Sida Tian, Can Zhang, Wei Yuan, Wei Tan, Wenjie Zhu

- ***What's New***: XMusic는 다목적 상징적 음악 생성 프레임워크로, 이미지, 비디오, 텍스트, 태그, 허밍과 같은 다중 입력을 활용하여 감정적으로 조절 가능한 고품질의 상징적 음악을 생성할 수 있습니다. 이 프레임워크는 XProjector와 XComposer라는 두 가지 핵심 구성 요소로 이루어져 있으며, 다양한 모달리티의 프롬프트를 상징적 음악 요소로 변환하여 생성 과정을 안내합니다.
- ***Technical Details***: XProjector는 멀티모달 프롬프트를 감정, 장르, 리듬과 같은 상징적 음악 요소로 분석하고 매핑하여 상징적 음악을 생성하는 과정을 돕습니다. XComposer는 생성자(Generator)와 선택자(Selector)로 구성되어, 전자는 감정 조절이 가능한 음악을 생성하고, 후자는 감정 인식 및 장르 인식 등의 다중 작업 학습을 통해 고품질의 상징적 음악을 식별합니다. 이를 가능케 하기 위해 XMIDI라는 대규모 상징적 음악 데이터셋을 구축하였습니다.
- ***Performance Highlights***: XMusic는 현재 최첨단 방법들보다 우수한 성능을 보이며, WAIC 2023에서 '컬렉션의 하이라이트' 중 하나로 선정되었습니다. 객관적 및 주관적 평가 모두에서 높은 음악 품질을 나타내었으며, 다양한 프롬프트 형식에 대한 효과적인 음악 생성을 보여주었습니다. 이를 통해 감정 제어 능력이 확실히 강화되었음을 입증하였습니다.

### [Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion](https://arxiv.org/abs/2501.09019)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09019.png)

Vote: 4

Authors: Zhaofan Qiu, Fuchen Long, Jiebo Luo, Jie An, Tao Mei, Ting Yao, Jingyuan Chen

- ***What's New***: Ouroboros-Diffusion은 프리-트레이닝된 텍스트-비디오 확산 모델을 기반으로 한 조정 없이도 일관성 있는 긴 비디오 생성을 가능하게 하는 새로운 비디오 디노이징 프레임워크입니다. 이 프레임워크는 구조적 일관성과 주제 일관성을 강화하여 임의의 길이의 일관된 비디오를 생성할 수 있도록 설계되었습니다.
- ***Technical Details***: 이 연구에서는 구조적 일관성을 개선하기 위해 큐의 꼬리에서 새로운 잠재 샘플링 기법을 도입하였으며, 프레임 간의 지각적 매끄러운 전환을 보장합니다. 주제 일관성을 강화하기 위해, 주제-인지 교차 프레임 주의 메커니즘(Subject-Aware Cross-Frame Attention; SACFA)을 고안하여 짧은 구간 내에서 프레임 간 주제를 정렬하여 더 나은 시각적 일관성을 달성합니다. 또한, 모든 앞의 클린 상태의 프레임에서 정보를 활용하여 노이즈가 많은 프레임을 안내하는 자체 반복적 가이던스를 소개하여 풍부하고 맥락적인 글로벌 정보 상호작용을 촉진합니다.
- ***Performance Highlights***: VBench 벤치마크에서의 성능을 통해 Ouroboros-Diffusion의 우수성을 입증하였으며, 특히 주제 일관성, 모션의 매끄러움 및 시간적 일관성 측면에서 뛰어난 결과를 보였습니다. 주제 일관성과 배경 일관성, 모션의 매끄러움, 시간적 깜박임 감소 등에 있어 다른 최신 기법을 능가하는 성능을 보여주었습니다.

### [Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography](https://arxiv.org/abs/2501.08970)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08970.png)

Vote: 3

Authors: Borja Balle, Florian Hartmann, Sarah Meiklejohn, Ilia Shumailov, Peter Kairouz, Daniel Ramage, Eugene Bagdasarian

- ***What's New***: 이 논문은 기계 학습 모델(Machine Learning Models; MLMs)을 활용해 기존 암호화 기술로는 해결하기 어려운 문제들을 비공개 추론(Private Inference)으로 해결할 수 있는 새로운 패러다임을 제안합니다. 신뢰할 수 있는 환경(Trusted Capable Model Environments; TCMEs)을 통해 입력/출력 제약 조건 하에 안전한 컴퓨팅을 가능하게 하여, 개인 데이터를 제3자에게 노출시키지 않고도 문제 해결이 가능합니다.
- ***Technical Details***: 기술적으로 TCMEs는 기계 학습 모델을 신뢰할 수 있는 제3자로 사용하여, 모델이 상태를 저장하지 않고 명확한 정보 흐름 통제(Information Flow Control)를 준수할 수 있도록 합니다. 이를 통해, 각 참가자가 제공한 개인 데이터를 노출하지 않으면서 상호작용하며 출력 결과에 합의할 수 있습니다. 또한, 실험 시나리오로 경제 문제와 같은 다양한 사용 예를 통해 이것의 실제 구성 및 가능성을 설명합니다.
- ***Performance Highlights***: 비록 현재 TCMEs의 보장 수준이 암호학적 기반의 이론적 증명보다는 더 약하지만, 기계 학습 모델을 활용한 TCME가 더 복잡하고 비정형적인 계산 문제에 있어 효율적으로 적용될 수 있음을 보였습니다. 특히 반복적인 크기 또는 복잡성 증가에 대한 비용 부담이 적다는 점에서 시각적 데이터나 비정형 데이터의 처리에 있어 장점을 발휘할 수 있습니다.

### [Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding](https://arxiv.org/abs/2501.04693)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04693.png)

Vote: 0

Authors: Kyle Stachowicz, Pieter Abbeel, Joshua Jones, Carmelo Sferrazza, Sergey Levine, Oier Mees

- ***What's New***: FuSe 기법은 다양한 센서 모달리티(vision, touch, audio)를 사용하는 일반화 로봇 정책을 미세조정(finetuning)하여 복잡한 작업 수행 능력을 향상시킵니다. 자연어를 공통의 크로스모달 접점으로 활용하여, 대규모 데이터셋 없이도 여러 모달리티에서의 추론을 가능하게 합니다.
- ***Technical Details***: FuSe는 시각, 촉각, 청각 모달리티를 사용하여 pre-trained한 generalist 로봇 정책을 미세조정하며, 이를 위해 멀티모달 대조 손실(multimodal contrastive loss)과 감각 기반 언어 생성 손실(sensory-grounded language generation loss)을 결합합니다. 이 방법론으로 인해 다양한 관찰 데이터를 자연어로 표현할 수 있으며, Zero-shot 설정에서 복합적인 크로스모달 추론을 수행할 수 있습니다.
- ***Performance Highlights***: FuSe는 기준 모델 대비 성공률을 20% 이상 향상시켰으며, 특히 복잡한 부분 관찰 상황에서의 작업 수행 능력이 뛰어나다는 점을 실험을 통해 증명했습니다. 다양한 정책 아키텍처(Oxto, PaliGemma)에도 적용 가능하며, 이종 센서 입력을 활용한 최초의 오픈소스 VLA 모델입니다.

### [MINIMA: Modality Invariant Image Matching](https://arxiv.org/abs/2412.19412)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.19412.png)

Vote: 0

Authors: Dingkang Liang, Xiang Bai, Xin Zhou, Xingyu Jiang, Zizhuo Li, Jiangwei Ren

- ***What's New***: MINIMA는 여러 크로스 모달(Cross-modal) 케이스에 대해 이미지 매칭을 해결하는 통합 프레임워크로, 데이터 확장을 통해 범용적인 성능 향상을 목표로 하고 있습니다. 이를 위해 다양한 모달리티를 포함한 'MD-syn'이라는 대규모 합성 데이터를 생성하여 데이터 격차를 해소하고 있습니다.
- ***Technical Details***: MINIMA는 RGB 데이터만을 사용한 매칭 데이터를 확대하여 여러 모달리티를 포함한 대규모 합성 데이터셋을 생성합니다. 데이터 엔진을 활용하여 세밀한 레이블과 다양한 장면의 합성 데이터를 생성하며, 이를 통해 정확한 매칭 성능을 지원합니다. 생성된 'MD-syn' 데이터셋은 다양한 크로스 모달 이미지 매칭 작업을 검증하는 데 사용됩니다.
- ***Performance Highlights***: MINIMA는 19가지의 크로스 모달 매칭 실험에서 기존 모델을 크게 능가하는 성능을 보여줍니다. 특히, 절대 학습 없이 새로운 모달리티에 대한 매칭 작업에서 뛰어난 일반화 능력을 보여주었습니다. 또한, 효율성 측면에서는 기존 모델들과 유사한 수준의 빠른 처리 속도를 유지합니다.

