## Daily Papers (2025-01-28)

### [Baichuan-Omni-1.5 Technical Report](https://arxiv.org/abs/2501.15368)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15368.png)

Vote: 35

Authors: Haoze Sun, Chenglin Zhu, Youquan Li, Dian Wang, Wei Song, Yijie Zhou, Wentao Zhang, Zenan Zhou, Yaqi Zhao, Chenzheng Zhu, Bowen Li, Ping Zhang, Da Pan, Lingfeng Ming, Lei Su, Xu Jia, Zheng Liang, Ran Xiao, Yanzhao Qin, Fuzhong Chen, Xin Wu, Kun Li, Tao Zhang, Yijia Sun, Mengyu Ai, Wenhao Lu, Jun Liu, Aiyuan Yang, Jinjie Yang, Youwei Zhang, Ting Li, Bin Xiao, Mang Wang, Guangwei Ai, Tianyu Zhang, Kegeng Wu, Chong Li, Yanjun Shen, Xuezhen Dong, Shunya Dang, Fan Yang, Yicong Chen, Shusen Zhang, Jianqiang Zhang, Jiahui Ye, Lijun Liu, Yozhen Wu, Zhiying Wu, Yaqi Zhou, Yan Zhang, Shuai Zhao, Xiaoxi Chen, Zhi Ma, Zhe Su, Mingan Lin, Hui Liu, Guosheng Dong, Lei Zhang, Zehuan Li, Xionghai Lin, Bowen Ding, Jianhua Xu, Yuran Wang, Keer Lu, Yadong Li, Fei Li, Yuanbo Fang, Dongdong Kuang, Xin Chen, Lingling Zhu, Weipeng Chen, Mingrui Wang, Hongda Zhang, Yujing Qiao, Hongyu Guo, Linzhuang Sun, Mingyang Chen, Song Chen, Wenjing Luo, Xiaoqin Huang, Jiani Pu, Jincheng Wu, Tianpeng Li, Linchu Xiong, Na Nie, Jia li, Yifei Duan, Yuqi Huo, Hao Liang, Fengyu Zhang, Xu Li, Miao Zhen

- ***What's New***: Baichuan-Omni-1.5는 멀티모달 데이터를 활용한 학습을 통해 사람의 모든 감각을 포괄하는 종합 프레임워크로 개발된 새로운 오므니모달(omni-modal) 모델입니다. 텍스트, 이미지, 비디오, 오디오 이해에서 뛰어난 성능을 보여 차세대 인공지능 개발에 기여하고 있습니다.
- ***Technical Details***: Baichuan-Omni-1.5는 텍스트, 오디오, 시각적 입력을 동시에 처리할 수 있는 통합 모델입니다. MLLM 코어를 중심으로 시각 인코더와 오디오 인코더가 결합되었으며, Residual Vector Quantization 기술을 활용한 Baichuan-Audio-Tokenizer를 사용해 오디오 처리를 세밀하게 수행합니다. 멀티스테이지 훈련 전략을 통해 이미지-텍스트, 이미지-오디오-텍스트 단계별로 다양한 모달리티 통합을 최적화합니다.
- ***Performance Highlights***: 이 모델은 GPT-4o-mini나 MiniCPM-o 2.6 같은 다른 최신 멀티모달 모델들보다 다양한 벤치마크에서 뛰어난 성능을 보여주었습니다. 한국어 및 중국어를 포함한 다양한 언어로 텍스트 이해 능력을 유지하면서 이미지 및 비디오 이해에서도 최상의 성능을 발휘합니다. 특히, OpenMM-Medical 평가에서는 83.8%의 점수를 기록, Qwen2-VL-72B 모델을 능가하는 성과를 보여주었습니다.

### [Qwen2.5-1M Technical Report](https://arxiv.org/abs/2501.15383)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15383.png)

Vote: 22

Authors: Qin Zhu, Jiandong Jiang, Wenbiao Yin, Jingren Zhou, Yong Li, Minmin Sun, Mei Li, Rui Men, Jianwei Zhang, Chengyuan Li, Kexin Yang, Wenyuan Yu, Xinlong Yang, Haoyan Huang, Junyang Lin, Weijia Xu, Jianhong Tu, Fei Huang, Kai Dang, Bowen Yu, Xiafei Qiu, Zipeng Zhang, Le Yu, Dayiheng Liu, Xingzhang Ren, An Yang, Zhiying Xu, Tao He

- ***What's New***: Qwen2.5-1M은 컨텍스트 길이를 1백만 토큰까지 확장한 모델 시리즈로, 특히 롱-컨텍스트(1M Token) 작업에서 GPT-4o-mini를 능가하는 성능을 보여줍니다. 롱 데이터 합성(long data synthesis), 점진적 프리트레이닝(progressive pre-training), 멀티 스테이지 지도 세밀 조정(multi-stage supervised fine-tuning)과 같은 기술을 사용하여 롱 컨텍스트 성능을 크게 향상시켰습니다.
- ***Technical Details***: Qwen2.5-1M 모델은 Qwen2.5 모델 기반으로 개발되었으며, 트랜스포머 기반 아키텍처를 유지하여 호환성을 보장합니다. 주요 기술로는 긴 데이터 의존성을 강조한 합성 데이터(synthetic data)를 사용한 프리트레이닝, GQA(Grouped Query Attention)를 활용한 효율적 KV 캐시 운영, DCA(Dual Chunk Attention)와 YaRN(Attention Scaling) 방법을 활용한 길이 외삽(length extrapolation) 기법을 통해 긴 문맥에서의 롱-컨텍스트 성능을 최적화하였습니다.
- ***Performance Highlights***: Qwen2.5-1M 모델은 1M 문맥에서 Passkey Retrieval 테스트 결과에서 100% 정확도를 달성하였습니다. 7B 모델은 미세한 오류 외에 거의 완벽한 성능을 보여주었고, 또한 RULER, LV-Eval, Longbench-Chat 등의 벤치마크에서도 장기 문맥 작업에서 우수한 성능을 발휘하였습니다.

### [Towards General-Purpose Model-Free Reinforcement Learning](https://arxiv.org/abs/2501.16142)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16142.png)

Vote: 13

Authors: Amy Zhang, Michael Rabbat, Yuandong Tian, Pierluca D'Oro, Scott Fujimoto

- ***What's New***: 이 연구는 일반 목적의 모델 프리 강화 학습(General-Purpose Model-Free Reinforcement Learning)을 목표로 하며, 다양한 도메인과 문제 설정에 적용할 수 있는 통일된 모델 프리 강화 학습 알고리즘을 찾는 데 집중합니다. 이를 위해 MR.Q라는 알고리즘을 소개하며, 이는 모델 기반 표현(Model-based Representations)을 활용하여 가치 함수의 선형화를 시도함으로써, 플래닝(Planning)이나 시뮬레이션된 경로 비용을 최소화합니다.
- ***Technical Details***: MR.Q 알고리즘은 상태-행동쌍(state-action pairs)과 가치(value)의 선형 관계를 대략적으로 캡처하는 특징(feature)을 학습하는 방식으로 동작하며, 주요 접근 방식은 최신 역학 기반 표현 학습(Dynamics-based Representation Learning) 방법론에 기초합니다. 또한 상태 및 상태-행동쌍을 단일 임베딩(Unified Embedding)으로 매핑하여 입력 공간에서 환경 고유의 특성을 제거하고 표준화된 하이퍼파라미터 셋을 활용할 수 있게 합니다.
- ***Performance Highlights***: MR.Q는 메인 RL 벤치마크인 Gym, DMC(DeepMind Control Suite) 및 Atari에서 단일 하이퍼파라미터 셋으로 실험되었으며, 도메인 특화 및 일반 기준선을 상대로 경쟁력 있는 성능을 달성하였습니다. 특히 DMC 벤치마크에서 가장 높은 성능을 기록했으며, 일반적인 모델 프리 알고리즘인 PPO 등의 모델을 능가하는 결과를 보였습니다.

### [ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer](https://arxiv.org/abs/2501.15570)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15570.png)

Vote: 11

Authors: Peter Yue, Li Zhiyuan, Liu Xiao, Lin Yueyu

- ***What's New***: ARWKV는 전통적인 Transformer 모델의 사전학습이 필수적이지 않음을 보이기 위해 도입된 새로운 RNN-기반 언어 모델입니다. 이 모델은 특화된 RWKV-7 주의(attention) 모듈을 통합하여, 더 효율적인 지식 증류(Knowledge Distillation) 기법을 제안하고, 대형 모델의 성능을 더 작은 모델에 이전할 수 있도록 합니다.
- ***Technical Details***: ARWKV는 세 가지 주요 단계를 통해 개발됩니다. 단계 1에서는 Self-Attention을 Time Mixing 모듈로 대체하여 RNN의 잠재 상태 추적 기능을 향상시킵니다. 단계 2에서는 단어 레벨 KL 발산을 사용하여 지식을 증류하며, 이는 기존의 시퀀스 수준 지식 증류(SeqKD)보다 더 빠른 수렴을 달성합니다. 단계 3에서는 감독된 미세 조정(Supervised Fine-Tuning; SFT)과 직간접적인 선호도 최적화(Direct Preference Optimization; DPO)를 통해 모델의 컨텍스트 길이를 확대합니다.
- ***Performance Highlights***: ARWKV 모델은 다양한 벤치마크에서 테스트되었으며, 특히 32B 모델로부터 지식을 이전하는 과정에서 활성화 함수 게이트를 사용하지 않고 MLP를 고정할 경우 성능 저하가 발생했습니다. 이는 주의 메커니즘을 크기가 크게 다른 모델 간에 직접 전이하는 데 있어 구조적 부조화가 존재할 수 있음을 시사합니다. ARWKV 모델은 원본 RWKV 구현과 달리 float16(FP16)으로 추론 시 성능이 크게 향상되었습니다.

### [Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation](https://arxiv.org/abs/2501.15907)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15907.png)

Vote: 10

Authors: Chen Yang, Kai Chen, Hua Hua, Zhizheng Wu, Liwei Liu, Peiyang Shi, Xuyuan Li, Yicheng Gu, Chaoren Wang, Jiaqi Li, Zengqiang Shang, Pengyuan Zhang, Haorui He, Yuancheng Wang

- ***What's New***: Emilia는 최초의 대규모 다국어 학습 데이터셋(multi-lingual dataset)으로, 'in-the-wild' 음성 데이터를 활용하여 자발적이고 인간적인 말소리를 생성하도록 설계되었습니다. 이 데이터셋은 101,000시간 이상의 다양한 언어의 음성 데이터를 포함하며, 확장된 버전인 Emilia-Large는 216,000시간 이상으로 증가하여 업계 최대의 오픈 소스 음성 생성 데이터셋을 제공합니다.
- ***Technical Details***: Emilia-Pipe는 6단계의 전처리 파이프라인으로 구성되며, 원시 'in-the-wild' 음성 데이터를 표준화, 음원 분할, 화자 다이어리제이션(speaker diarization), 음성 활동 감지(VAD)를 통한 세분화, 자동 음성 인식(ASR), 필터링 과정을 통하여 고품질의 학습 데이터로 변환합니다. 이 과정은 데이터의 효율적이고 확장 가능한 처리와 다양한 언어에서의 다국어 음성 데이터셋 생성에 기여합니다.
- ***Performance Highlights***: Emilia 데이터셋은 기존의 오디오북 데이터셋에 비해 자발적이고 인간적인 말소리 생성에서 우수한 성능을 발휘합니다. 특히, 영어, 중국어, 독일어, 프랑스어, 일본어, 한국어 등의 다국어 모델에서 강력한 성능을 보이며, 교차언어 음성 생성에서도 안정적인 성능을 유지합니다. 데이터셋 크기의 확장은 성능을 지속적으로 증가시키며, 약 100,000시간 이상에서는 수확 체감의 법칙이 적용되는 점을 확인하였습니다.

### [iFormer: Integrating ConvNet and Transformer for Mobile Application](https://arxiv.org/abs/2501.15369)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15369.png)

Vote: 8

Authors: Chuanyang Zheng

- ***What's New***: iFormer는 사용자가 모바일 애플리케이션에서의 지연 시간(latency)과 정확도(accuracy)를 최적화하기 위해 설계된 새롭고 가벼운 하이브리드 비전 네트워크입니다. 이 모델은 컨벌루션(Convolution)의 빠른 지역 표현 능력과 비전 트랜스포머(Vision Transformer; ViT)의 전역 모델링 역량을 결합하여 모바일 디바이스에서의 실시간 분석을 가능하게 합니다.
- ***Technical Details***: iFormer는 4단계 계층형 아키텍처로 구성되어 있으며, 초기 고해상도 단계에서는 빠른 컨벌루션을 사용하여 지역 표현을 추출합니다. 최적화된 ConvNeXt를 활용하여 경량 네트워크를 설계하였으며, 나중에 낮은 해상도 단계에서는 리소스 제한적인 모바일 디바이스에서도 구현 가능한 싱글 헤드 변조 자기 주의(Single-Head Modulation Attention; SHMA)를 통합하여 긴 범위 컨텍스트를 모델링합니다.
- ***Performance Highlights***: iFormer는 ImageNet-1k에서 iPhone 13 기준으로 1.10ms의 지연 시간 내 80.4%의 Top-1 정확도를 달성하여, 최근 발표된 MobileNetV4보다 우수한 성능을 보여줍니다. 또한 COCO 객체 검출 및 ADE20k 의미적 세분화와 같은 다운스트림 작업에서도 뛰어난 성과를 보여주며, 모바일 디바이스에서의 고해상도 입력에 대해 낮은 지연 시간을 유지합니다.

### [Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity](https://arxiv.org/abs/2501.16295)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16295.png)

Vote: 4

Authors: Genghan Zhang, Weixin Liang, Junhong Shen, Ning Dong, Lili Yu, Luke Zettlemoyer

- ***What's New***: Mixture-of-Mamba는 State Space Models(SSMs)을 기반으로 하여 모달리티별 희소성(Modality-Aware Sparsity)을 직접적으로 도입한 새로운 아키텍처입니다. Mamba 블록 내에 직접 모달리티 인식 파라미터화를 통해 멀티모달 프리트레이닝에서 성능과 효율성을 개선하고자 합니다.
- ***Technical Details***: Mixture-of-Mamba는 모달리티에 따라 Win proj, Wx proj, Wdt proj, Wout proj와 같은 모든 입력 처리 프로젝션에 모달리티별 파라미터화를 적용합니다. 이 설계를 통해 하나의 모달리티에서 데이터를 처리하는 동안 효율적이고 안정적인 멀티모달 프리트레이닝이 가능합니다. 주요 실험 조건은 텍스트 및 연속적 이미지 토큰 간의 혼합 모달리티에 따른 Transfusion 설정, 텍스트와 이산 이미지 토큰간 Chameleon 설정, 그리고 여기에 음성 데이터까지 포함한 세 가지 모달리티 설정입니다.
- ***Performance Highlights***: Transfusion 설정에서 Mixture-of-Mamba는 1.4B 크기에서 기존의 34.76%의 FLOPs만으로 같은 이미지 손실 수준을 달성했습니다. Chameleon 설정에서는 42.50%의 FLOPs만 사용하여 유사한 이미지 손실을 기록했으며, 음성 모달리티에서는 24.80%의 FLOPs 만으로 동일한 손실 수준을 달성했습니다. 이러한 결과는 Mixture-of-Mamba가 다양한 설정하에서 SSMs의 효율성과 성능을 significantly 개선시킴을 보여줍니다.

### [Are Vision Language Models Texture or Shape Biased and Can We Steer Them?](https://arxiv.org/abs/2403.09193)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09193.png)

Vote: 4

Authors: Janis Keuper, Margret Keuper, Bianca Lamm, Robert Geirhos, Paul Gavrikov, Jovita Lukasik, Steffen Jung, Muhammad Jehanzeb Mirza

- ***What's New***: 이번 연구는 시각-언어 모델(Vision Language Models; VLMs)이 인간과 달리 형태와 질감 중 어느 쪽에 더 치중하는지, 그리고 언어를 통해 이러한 편향을 조정할 수 있는지를 조사합니다. 이를 통해 언어가 형태 편향을 49%에서 최대 72%까지 조정할 수 있음을 보여줍니다.
- ***Technical Details***: 연구는 다양한 유명 VLM을 분석하여 이들이 형태 vs. 질감 편향을 어떻게 나타내는지 살펴보았습니다. 중심적인 방법론은 스타일 전송 모델을 사용하여 생성된 1,280개의 이미지셋을 활용한 형태-질감 신호 대립 분류 작업(cue-conflict classification)을 포함하며, VLM의 응답을 통한 편향 측정도 포함됩니다.
- ***Performance Highlights***: VLM들은 전반적으로 이미지에 대한 형태 정보에 더 치중하였으나 인간(96% 형태 편향)보다는 낮았습니다. 실험에서 테스트한 모든 VLM이 인간의 형태 편향 수준에는 미치지 못했습니다. 예를 들어, GPT-4V 모델은 평가 시 약 47.9%의 질감 편향을 나타냈습니다.

### [CodeMonkeys: Scaling Test-Time Compute for Software Engineering](https://arxiv.org/abs/2501.14723)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.14723.png)

Vote: 4

Authors: Ryan Ehrlich, Bradley Brown, Jordan Juravsky, Ronald Clark, Azalia Mirhoseini, Christopher Ré

- ***What's New***: CodeMonkeys는 SWE-bench 데이터셋의 실제 GitHub 이슈를 해결하는 대형 언어 모델(LLM)의 기능을 테스트하는 시스템으로, 테스트 시간 스케일링(Test-Time Compute Scaling)을 통해 모델의 성능을 향상시키는 방법을 검토합니다. 이 시스템은 모델이 코드 베이스를 반복적으로 편집하고 테스트 스크립트를 생성하여 실제 문제 해법을 찾는 것이 가능하게 합니다.
- ***Technical Details***: CodeMonkeys 시스템은 관련 코드베이스 문맥을 식별, 코드 베이스 편집의 후보를 생성, 그리고 생성된 후보들 간의 최적의 선택을 하는 세 단계로 이뤄져 있습니다. 이 시스템은 Claude Sonnet API를 사용하여 순차적 스케일링을 위해 다중 상태 머신을 사용하며, 병렬 스케일링을 통해 여러 후보 솔루션을 샘플링합니다. 후보 선택은 다수결 투표와 모델 기반 선택을 결합한 방법을 사용합니다.
- ***Performance Highlights***: CodeMonkeys는 SWE-bench Verified의 이슈 중 57.4%를 해결하는 성과를 내며, 예산은 대략 2300 USD입니다. 기존 SWE-bench 제출물 중 상위 4개의 편집을 포함하는 'Barrel of Monkeys'를 통해 후보 편집 간의 선택을 수행하면 66.2% 점수를 얻어, 단일 편집으로 최적 점수를 기록한 참가자보다 뛰어난 성과를 냅니다.

### [Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models](https://arxiv.org/abs/2501.12370)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12370.png)

Vote: 3

Authors: Alaaeldin Mohamed Elnouby Ali, Josh Susskind, Vimal Thilak, Harshay Shah, Dan Busbridge, Samira Abnar

- ***What's New***: 이 논문은 Mixture-of-Experts (MoE) 언어 모델에서 파라미터 수와 FLOPs(Floating Point Operations) 간의 최적 균형을 위한 스케일링 법칙을 탐구합니다. 이는 MoE의 스파시티(sparsity)를 활용해 파라미터 수를 증가시키면서도 예제당 FLOPs를 비례적으로 늘리지 않는 방법을 제시합니다.
- ***Technical Details***: 스파스 MoE 모델에서 불활성 파라미터의 비율로 스파시티 수준을 정의하며, 이를 통해 파라미터 수와 예제당 FLOPs의 비율을 조정합니다. 다양한 스파시티, 모델 크기, 컴퓨팅 리소스를 평가하여 최적의 스파시티 수준을 산출하고, MoE의 스케일링 법칙에서 스파시티의 영향을 분석합니다.
- ***Performance Highlights***: 훈련과 추론에서 스파시티 수준을 증가시키면 성능과 효율성이 개선됩니다. 고정된 훈련 예산 하에서는 파라미터 수를 늘리는 것이 예제당 FLOPs를 늘리는 것보다 크게 성능을 향상시킵니다. 그러나 추론에서는 예제당 FLOPs가 더 중요한 역할을 하고, 스파시티가 높은 모델은 특정 다운스트림 태스크에서 성능이 낮아질 수 있습니다.

### [OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas](https://arxiv.org/abs/2501.15427)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15427.png)

Vote: 3

Authors: Wenhao Yu, Tao Ge, Hongming Zhang, Xiaoyang Wang, Dong Yu, Dian Yu

- ***What's New***: OpenCharacter는 대규모 합성 페르소나(Synthetic Personas)를 이용하여 맞춤형 역할극을 위한 대규모 언어 모델(Large Language Models; LLMs)을 훈련하는 새로운 접근법을 제시합니다. 이 접근법은 LLM에 사용자 지정 캐릭터로의 일반화(Generalization) 능력을 제공하여 다양한 사용자 요구를 수용할 수 있도록 합니다.
- ***Technical Details***: OpenCharacter는 Persona Hub에서 제공하는 대규모 합성 페르소나를 활용하여, 두 가지 전략을 연구합니다: 응답 재작성(OpenCharacter-R)과 응답 생성(OpenCharacter-G). 이 방법론은 LLM을 캐릭터 프로필에 맞춰 훈련된 데이터로 보강하여 새로운 캐릭터를 신속하게 일반화할 수 있도록 합니다. 주어진 캐릭터에 일치하는 응답을 생성하거나 기존 응답을 재작성하여 캐릭터 기반 역할 극에 맞는 데이터를 생성합니다.
- ***Performance Highlights***: LLaMA-3 8B 모델에 대한 감독 학습을 수행하여, 기존 인스트럭트를 개선하고 GPT-4o 모델에 비견할 만한 성능을 달성했습니다. 오픈 소스로 총 30만 6천개의 역할극 대화 쌍과 함께 2만 개의 합성 캐릭터를 공개하여 대중 연구를 지원합니다. 평가 결과, OpenCharacter 모델은 LLaMA-3 8B 인스트럭트보다 뛰어난 성과를 보였으며, 특히 GPT-4o 모델과 성능이 비슷하다는 점을 보여주었습니다.

### [Feasible Learning](https://arxiv.org/abs/2501.14912)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.14912.png)

Vote: 2

Authors: Simon Lacoste-Julien, Ignacio Hounie, Juan Elenter, Meraj Hashemizadeh, Jose Gallego-Posada, Juan Ramirez, Alejandro Ribeiro

- ***What's New***: Feasible Learning (FL)은 학습을 모든 훈련 샘플에 대해 손실을 제한하는 타당성 문제로 접근하는 새로운 학습 패러다임입니다. 이는 평균 성능을 최적화하는 기존의 경험적 위험 최소화(Empirical Risk Minimization; ERM)와 대조적이며, 각 데이터 포인트에 대해 만족스러운 성능을 요구합니다. FL은 수많은 적합해 중에서 선택지를 확장함으로써 초과적합을 방지할 수 있습니다.
- ***Technical Details***: FL은 최소 노름의 이완 변수(Slack Variables)를 도입하여 실제 사용에서 유의미한 임계값을 설정하는 도전을 해결합니다. 기존의 수렴 보장을 강화한 복합 플라이멀-듀얼 방법(Primal-Dual Approach)을 사용하여 문제를 해결하며, 이는 각 데이터 샘플의 중요성을 동적으로 조정합니다. 이 방법론은 이미지 분류, 연령 회귀, 대규모 언어 모델의 선호 최적화에서 더 나은 꼬리 분포 행동을 보이며 평균 성능에 거의 영향을 미치지 않습니다.
- ***Performance Highlights***: 실험 결과 FL을 사용한 모델이 ERM과 동등한 평균 성능을 유지하면서 꼬리 분포 행동이 개선된 것을 확인했습니다. RFL(Resilient Feasible Learning)을 통해 불가능한 문제를 관리하며 성능을 더욱 향상시킬 수 있음을 보여줍니다. 이러한 방식은 다양한 학습 시나리오에서 유용하게 적용될 수 있으며, 특히 데이터 포인트 간의 일관된 성능이 중요한 경우에 효과적입니다.

### [Visual Generation Without Guidance](https://arxiv.org/abs/2501.15420)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15420.png)

Vote: 2

Authors: Huayu Chen, Kai Jiang, Kaiwen Zheng, Jun Zhu, Hang Su, Jianfei Chen

- ***What's New***: 본 논문은 Guidance-Free Training (GFT)이라는 새로운 방법론을 제안하며, 이는 기존의 Classifier-Free Guidance (CFG)를 사용하지 않고도 유사한 성능을 발휘하면서도 샘플링 과정에서 필요한 컴퓨팅 비용을 절반으로 줄일 수 있습니다. GFT는 비지도 학습 방식으로 처음부터 훈련할 수 있으며, 기존 코드베이스에 최소한의 수정만으로 구현 가능합니다.
- ***Technical Details***: GFT는 CFG가 사용하는 동일한 최대 가능성 목표를 유지하면서 조건부 모델의 매개변수화를 변경합니다. 기존의 CFG와 달리 GFT는 조건부 모델을 명시적으로 정의하지 않으며, 샘플링 모델과 무조건 모델 간의 선형 보간을 통해 조건부 모델을 구축합니다. 이는 비주얼 생성에서 가이드 없는 샘플링 모델을 직접 최적화함으로써 가능해집니다.
- ***Performance Highlights***: GFT는 5가지 유형의 시각적 모델에 걸친 광범위한 실험을 통해 그 효과성과 다양성을 입증하며, CFG 대비 비슷하거나 더 낮은 FID 점수를 일관되게 달성합니다. DiT-XL/2 모델에 대한 예를 보면, GFT를 사용하여 FID 점수 1.99를 달성, CFG의 성능인 2.11을 넘어섭니다. 이는 GFT가 디퓨전, 자동회귀 및 마스킹 모델 전반에 적용 가능하다는 점에서 뛰어난 성능을 발휘함을 보여줍니다.

### [Return of the Encoder: Maximizing Parameter Efficiency for SLMs](https://arxiv.org/abs/2501.16273)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16273.png)

Vote: 2

Authors: Chad Voegele, Mohamed Elfeki, Rui Liu

- ***What's New***: 이 논문은 인코더-디코더 아키텍처의 기본적 이점을 두드러지게 하여 소형 언어 모델(SLMs)에서 더 나은 성능과 효율성을 구현했습니다. 새로운 지식 증류 프레임워크(Knowledge Distillation Framework)를 도입하여 소형 인코더-디코더 모델들이 대용량 디코더 전용 모델로부터 이점을 얻으면서도 본래의 아키텍처 이점을 유지할 수 있도록 했습니다.
- ***Technical Details***: 인코더-디코더 아키텍처는 GPU, CPU 및 NPU 플랫폼에서 디코더 전용 모델 대비 47% 낮은 첫 번째 토큰 대기 시간과 4.7배 높은 처리량을 달성합니다. 로터리 위치 내장(Rotary Positional Embeddings; RoPE)과 같은 현대적인 발전을 포함하고, 한 번의 입력 처리를 통해 효율적인 정보 분리와 생성 가능성을 제공합니다. 1/3-2/3, 1/2-1/2, 2/3-1/3의 세 가지 주요 인코더-디코더 분할 구성이 제안되었습니다.
- ***Performance Highlights***: 인코더-디코더 아키텍처는 GPU 및 NPU에서 첫 토큰 지연(latency)을 각각 42%, 47% 줄였으며 처리속도(Throughput)를 3.9배, 4.7배 증가시켰습니다. 330M 매개변수 모델 기준으로 SQuAD 2.0에서 성능 F1 점수 78을 기록하며 디코더 전용 모델을 뛰어넘었습니다. 이는 대규모 인코더-디코더 모델의 이점을 소규모 SLMs에서도 유지할 수 있음을 입증합니다.

