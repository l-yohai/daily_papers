## Daily Papers (2025-01-14)

### [The Lessons of Developing Process Reward Models in Mathematical Reasoning](https://arxiv.org/abs/2501.07301)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.07301.png)

Vote: 46

Authors: Junyang Lin, Jingren Zhou, Dayiheng Liu, Zhenru Zhang, Runji Lin, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Bowen Yu

- ***What's New***: 이 논문에서는 대규모 언어 모델(LLM)을 위한 수학적 추론의 과정 보상 모델(Process Reward Models; PRMs)을 개발하고자 하는 도전과제와 이를 해결하기 위한 새로운 합의 필터링 메커니즘(consensus filtering mechanism)을 도입했습니다. 이 메커니즘은 MC 추정(MC estimation)과 LLM-as-a-judge를 결합하여 더 정확한 데이터 평가와 더 나은 모델 성능을 제공합니다.
- ***Technical Details***: PRMs는 LLM의 수학적 추론 과정에서 중간 단계의 오류를 식별하고 줄이기 위해 설계되었습니다. 기존 MC 추정 기반의 데이터 생성은 일반적으로 LLM-as-a-judge 방식이나 인간 주석 방법에 비해 성능이 떨어졌으며, Best-of-N(BoN) 평가 방식에서도 편향이 나타났습니다. 이를 해결하기 위해 MC 추정과 LLM-as-a-judge의 일치를 확인하는 합의 필터링 메커니즘을 개발했습니다.
- ***Performance Highlights***: 새롭게 개발된 PRM은 Best-of-8 평가에서 기존 공개 소스 PRM보다 뛰어난 성능을 보였습니다. Qwen2.5-Math-PRM-7B와 Qwen2.5-Math-PRM-72B 모델은 PROCESSBENCH 벤치마크에서 높은 오류 식별 능력을 보여주었으며, 이는 데이터 효율성 및 모델 성능에서의 개선을 증명합니다.

### [Tensor Product Attention Is All You Need](https://arxiv.org/abs/2501.06425)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06425.png)

Vote: 35

Authors: Yifan Zhang, Yang Yuan, Huizhuo Yuan, Andrew Chi-Chih Yao, Yifeng Liu, Zhen Qin, Quanquan Gu

- ***What's New***: 이 논문에서는 새로운 주의 메커니즘인 텐서 곱 주의력(Tensor Product Attention; TPA)을 소개합니다. 이는 높은 차수 텐서를 사용하여 질의(Query), 키(Key), 값(Value)를 인코딩함으로써, 추론 시 저장하는 KV 캐시의 메모리 사용량을 크게 감소시키면서도 성능을 향상시킵니다.
- ***Technical Details***: TPA는 문맥적 저차수 요인화(Contextual Low-Rank Factorization)를 통해 질의, 키, 값을 표현하며, RoPE(Rotary Position Embedding)와도 자연스럽게 통합됩니다. 이를 기반으로 한 새로운 모델 아키텍처인 Tensor ProducT ATTenTion Transformer (T6)는 기존 MHA(Multi-Head Attention), MQA(Multi-Query Attention), GQA(Grouped-Query Attention), MLA(Multi-head Latent Attention)와 비교하여 더 우수한 모델 품질을 보입니다. TPA는 각 층에서 질의, 키, 값을 텐서 곱 형태로 표현하며, 학습과 추론 시 메모리 효율성을 높이는 것이 특징입니다.
- ***Performance Highlights***: TPA 기반의 T6 모델은 다양한 언어 모델링 작업에서 기존의 MHA, MQA, GQA, MLA보다 더 나은 성능을 보였습니다. 이는 예측된 perplexity와 다양한 벤치마크 평가 지표에서 입증되었습니다. 특히, TPA의 메모리 효율성 덕분에 고정된 자원 제약 하에서 훨씬 더 긴 시퀀스를 처리할 수 있게 되어, 현대 언어 모델의 확장성 문제를 해결합니다.

### [Migician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models](https://arxiv.org/abs/2501.05767)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.05767.png)

Vote: 23

Authors: Chao Huang, You Li, Heyu Huang, Ruixuan Li, Maosong Sun, Zhiyuan Liu, Kaiyu Huang, Zonghao Guo, Chi Chen, Jinan Xu, Yuhua Li

- ***What's New***: Migician은 다중 이미지 기반 비정형 그라운딩(Free-Form Multi-Image Grounding)을 수행할 수 있는 최초의 멀티모달 대규모 언어 모델(Multimodal Large Language Model; MLLM)입니다. 다중 이미지의 정밀한 그라운딩 문제를 해결하기 위해 개발되었으며, 여러 이미지에서 유연하고 정확한 그라운딩을 수행할 수 있는 모델입니다.
- ***Technical Details***: Migician은 MGrounding-630k라는 대규모 데이터셋을 이용해 2단계의 학습 절차를 통해 훈련되었습니다. 첫 번째 단계에서는 MGrounding-630k를 이용해 다중 이미지 그라운딩 능력을 강화하였고, 두 번째 단계에서는 고품질의 비정형 그라운딩 명령 데이터를 통해 모델을 더욱 정교화했습니다. 또한 MIG-Bench라는 벤치마크를 제안하여 다중 이미지 그라운딩 기능을 평가했습니다.
- ***Performance Highlights***: 실험 결과, Migician은 기존의 MLLM을 21.61% 초과하여 성능을 크게 향상시켰으며, 상당히 더 큰 70B 모델보다도 우수한 성능을 발휘했습니다. Migician의 성능은 여러 다중 이미지 이해 벤치마크에서도 뛰어난 성능을 보여줍니다. 예를 들어, 특정 차원에서는 GPT-4o 모델보다 향상된 성과를 보였습니다.

### [Transformer^2: Self-adaptive LLMs](https://arxiv.org/abs/2501.06252)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06252.png)

Vote: 19

Authors: Edoardo Cetin, Qi Sun, Yujin Tang

- ***What's New***: Transformer2는 전통적인 미세조정(fine-tuning) 방법이 아니라, 대형 언어 모델(LLMs)을 미리 알려지지 않은 작업에 실시간으로 적응시키는 새로운 프레임워크입니다. 기존의 무겁고 정적인 방법과 달리 개별 가중치 행렬의 특이성분만을 조정하여 새로운 작업을 처리할 수 있습니다. Transformer2는 적은 수의 매개변수로 더 나은 효율성 및 다양한 모달리티(vision-language tasks 등)에서도 뛰어난 성능을 보여줍니다.
- ***Technical Details***: Transformer2는 두 단계의 추론 메커니즘을 사용하여 작업 특성을 파악하고, 강화 학습(RL)을 통해 훈련된 '전문가' 벡터를 동적으로 혼합하여 주어진 프롬프트에 맞는 적절한 행동을 이끌어 냅니다. 새로운 파라미터 효율적 미세조정(PEFT) 방법인 특이값 미세조정(SVF)을 통해, 모델의 가중치 행렬의 특이값만을 조정하여 작업 성능을 직접 최적화하고 있습니다. 각 작업은 모듈화 되어, 무한히 새로운 기술을 추가할 수 있습니다.
- ***Performance Highlights***: Transformer2는 LoRA와 같은 기존 접근 방식을 능가하며, 적은 수의 매개변수로 더욱 높은 성능을 발휘합니다. LLAMA3 및 MISTRAL 모델과 같은 다양한 LLM 아키텍처에서 실험 결과, SVF는 기존 방법에 비해 일관된 성능 향상을 보여줍니다. 또한, Transformer2는 새로운 보상 체계와 함께 각 테스트 시간 조건에서 일관된 이점을 제공하며, 규모에 비해 매우 과소 평가된 파라미터를 활용합니다.

### [VideoAuteur: Towards Long Narrative Video Generation](https://arxiv.org/abs/2501.06173)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06173.png)

Vote: 18

Authors: Lu Qi, Alan Yuille, Liangke Gui, Jiepeng Cen, Junfei Xiao, Lu Jiang, Zhibei Ma, Feng Cheng

- ***What's New***: VideoAuteur는 요리 도메인에서 장기 내러티브 비디오 생성(Long Narrative Video Generation)을 향상시키기 위해 설계된 대규모 요리 비디오 데이터셋을 제안합니다. 이 데이터셋은 명확하고 정보가 가득한 사건 sequences을 제공하여 일관성 있는 내러티브를 지원하며, 비디오 생성 모델을 위한 장기 내러티브 비디오 감독 역할을 합니다. 이는 비디오 생성의 시각적 및 의미적 일관성을 높이기 위해 시각적 내재 공간(Visual Embeddings)을 사용하여 내러티브 흐름을 향상시킵니다.
- ***Technical Details***: VideoAuteur 파이프라인은 두 가지 주요 요소로 구성됩니다: 장기 내러티브 디렉터와 시각 조건 기반 비디오 생성 모델(Visual-Conditioned Video Generation Model)입니다. 장기 내러티브 디렉터는 내러티브 플로우를 캡처하는 시각적 임베딩 또는 키프레임 sequence을 생성합니다. 이와 함께, Vision-Language Model(VLM)을 사용하여 비주얼 내재 공간을 정렬하여 비디오 전체 품질을 개선했습니다. 이 훈련은 텍스트와 이미지 임베딩을 비디오 생성 과정 내에 통합하는 미세 조정 기법(Finetuning Techniques)으로 지원됩니다.
- ***Performance Highlights***: 제안된 파이프라인은 시각적 세부사항과 의미적 일관성이 개선된 장기 내러티브 비디오 생성에서 상당한 향상을 보여줩니다. 주어진 데이터셋에 대한 실험에서, 제안된 방법론이 공간적 및 시간적 일관성에서의 개선을 입증하였으며, 이는 시각적 상세함과 논리적 진행을 모두 포함하여 사용자에게 내러티브의 즐거움을 제공합니다.

### [O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning](https://arxiv.org/abs/2501.06458)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06458.png)

Vote: 15

Authors: Zhongzhen Huang, Haoyang Zou, Zhen Huang, Shaoting Zhang, Xiaofan Zhang, Gui Geng, Shengyi Hua, Pengfei Liu

- ***What's New***: 이 연구는 의료 추론 작업에서 대형 언어 모델(LLMs)의 성능을 향상시키기 위한 추론 시간 스케일링(inference-time scaling)의 잠재력을 탐구합니다. 이는 진단 결정부터 치료 계획까지 다양한 의료 과제를 포함합니다.
- ***Technical Details***: 이 연구에서는 MedQA, Medbullets, JAMA Clinical Challenges와 같은 다양한 복잡도의 의료 벤치마크에서 실험을 진행했습니다. 이때 의료 문제를 해결하기 위해 LLMs에 추론 시간 스케일링 기법을 적용하여 복잡한 작업을 단계별로 점진적으로 해결하는 방법을 제시합니다. 주요 데이터셋으로는 JAMA Clinical Challenge, Medbullets, MedQA 등이 사용되었습니다.
- ***Performance Highlights***: 연구 결과, 추론 시간을 늘림으로써 모델이 6%-11%의 성능 향상을 보였습니다. 추가로, LongMonolog와 LongStep와 같은 긴 형태의 추론 데이터셋에서 데이터를 학습한 모델은 JAMA, Medbullets, MedQA에서 각각 높은 정확도를 달성했습니다.

### [WebWalker: Benchmarking LLMs in Web Traversal](https://arxiv.org/abs/2501.07572)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.07572.png)

Vote: 14

Authors: Yong Jiang, Pengjun Xie, Wenbiao Yin, Zhenglin Wang, Deyu Zhou, Fei Huang, Runnan Fang, Zekun Xi, Jialong Wu

- ***What's New***: WebWalker는 LLMs(Large Language Models)의 웹 탐색 능력을 평가하는 새로운 벤치마크 WebWalkerQA를 소개합니다. 이는 LLMs가 웹사이트의 하위 페이지를 탐색하여 체계적으로 고품질 데이터를 추출하는 능력을 평가합니다. 사람과 같은 웹 탐색을 모방하는 multi-agent framework인 WebWalker를 제안하여 복합적인 정보를 효과적으로 처리합니다.
- ***Technical Details***: WebWalker는 탐색-비평(explore-critic) 패러다임을 채택한 multi-agent framework로, ReAct framework에 기반한 explorer agent와 memory를 관리하며 응답을 생성하는 critic agent로 구성됩니다. 이 벤치마크는 1373개의 웹 페이지에 걸쳐 680개의 쿼리로 구성되어 있으며, 질문-답변(QA) 형식으로 LLMs의 웹 탐색과 문제 해결 능력을 평가합니다. WebWalker는 다양한 주류 LLMs에서 구축되어 WebWalkerQA 벤치마크를 사용하여 평가됩니다.
- ***Performance Highlights***: WebWalker는 다양한 LLMs를 사용해 평가되었으며, 현재까지 가장 강력한 LLMs를 사용하더라도 WebWalkerQA에서 성능이 최적 수준에 미치지 못했습니다. 이는 WebWalkerQA가 LLMs에게 도전적인 과제를 제시함을 확인시켜 줍니다. WebWalker는 특히 복합적이고 다단계 웹 상호작용을 처리하는데 여전히 도전이 많다는 점을 지적합니다.

### [MinMo: A Multimodal Large Language Model for Seamless Voice Interaction](https://arxiv.org/abs/2501.06282)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06282.png)

Vote: 13

Authors: Fan Yu, Nan Zhao, Jiaqing Liu, Pei Zhang, Tianyu Zhao, Mengzhe Chen, Shiliang Zhang, Yexin Yang, Yanni Chen, Yuxuan Wang, Ruize Gao, Yabin Li, Haoneng Luo, Jinren Zhou, Yunlan Xu, Yafeng Chen, Hui Wang, Qian Chen, Chong Deng, Xiang Lv, Xian Yang, Hao Wang, Chong Zhang, Jialong Tang, Bin Ma, Zhihao Du, Wen Wang, Changfeng Gao, Zhijie Yan, Yingda Chen, Zhifu Gao, Qinglin Zhang, Baosong Yang, Guanrou Yang, Xian Shi, Chongjia Ni

- ***What's New***: MinMo는 사용자가 기존 텍스트 LLM(Text Large Language Model)의 능력을 보존하면서 오디오 이해와 생성 기능을 획득할 수 있도록 설계된 멀티모달 대형 언어 모델입니다. 이 모델은 1,400만 시간 이상의 다양한 음성 데이터 세트에서 훈련되어 몰입형 음성 상호작용을 지원합니다.
- ***Technical Details***: MinMo는 멀티 스테이지 음성-텍스트 정렬, 텍스트-음성 정렬, 음성-음성 정렬, 및 듀플렉스 상호작용 정렬 단계를 통해 훈련됩니다. 이러한 정렬 전략은 MinMo가 원활한 음성 이해, 생성 및 양방향 상호 작용 기능을 갖추는 데 도움을 줍니다. 또한 MinMo는 구조적으로 간단하고 경쟁력 있는 음성 생성 성능을 발휘하도록 설계된 새로운 음성 디코더(Voice Decoder)를 제안하여 Instruction-following을 개선합니다.
- ***Performance Highlights***: MinMo는 종합적인 벤치마크에서 최첨단 성능을 달성하며, 음성 인식, 음성 번역, 감정 인식, 스피커 분석에서 기존의 SOTA(State of the Art) 모델들을 능가합니다. 특히 Instruction-following 정확도에서 98.4%를 기록하며, 감정, 방언, 말하기 속도 등 다양한 뉘앙스의 음성 생성이 가능합니다. 음성-텍스트 지연은 약 100ms, 이론상 완전 듀플렉스 지연은 600ms, 실제 지연은 800ms입니다.

### [SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training](https://arxiv.org/abs/2501.06842)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06842.png)

Vote: 10

Authors: Tianjin Huang, Gaojie Jin, Zhangyang Wang, Ziquan Zhu, Shiwei Liu, Lu Liu

- ***What's New***: 이 논문에서는 대형 언어 모델(LLMs) 훈련 중 발생하는 불안정성을 해결하기 위해 새로운 옵티마이저, 스파이크-어웨어 애덤(Spike-Aware Adam; SPAM)을 소개합니다. 이 방법은 모멘텀 리셋(Momentum Reset)과 스파이크-어웨어 그래디언트 클리핑(Spike-Aware Gradient Clipping)을 활용하여 그래디언트 스파이크의 부정적인 영향을 줄이는 데 초점을 맞추고 있습니다.
- ***Technical Details***: SPAM은 애덤(Adam) 옵티마이저의 첫 번째 및 두 번째 모멘트 기록을 주기적으로 초기화하여 그래디언트 스파이크로 인한 영향을 최소화합니다. 또한 스파이크-어웨어 클리핑 기법을 도입하여 스파이크된 그래디언트를 식별하고 크기를 관리 가능한 수준으로 조정합니다. 이러한 기법은 모멘텀 리셋과 결합하여 훈련의 안정성을 개선합니다. 메모리 효율성을 높이기 위해 스파스 모멘텀(Sparse Momentum)을 적용하여 일부 모멘텀만 선택적으로 업데이트합니다.
- ***Performance Highlights***: SPAM은 다양한 LLM 크기에서 애덤 및 아다팩터(Adafactor) 같은 기존 옵티마이저보다 일관되게 우수한 성능을 보였습니다. 특히 메모리 제약 조건 하에서도 GaLore, Adam-Mini 등의 최신 메모리 효율 옵티마이저를 능가하는 결과를 나타냈습니다.

### [Generative AI for Cel-Animation: A Survey](https://arxiv.org/abs/2501.06250)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06250.png)

Vote: 9

Authors: Chao Huang, Liu He, Pinxin Liu, Hang Hua, Chenliang Xu, Mingqian Feng, Jing Bi, Susan Liang, Xinyang Li, Yunzhong Xiao, Zhiyuan Wang, Jia-Xing Zhong, Yizhi Song, Junjia Guo, Yunlong Tang, Zeliang Zhang, Luchuan Song

- ***What's New***: 이 논문은 칼 애니메이션(Cel-Animation) 제작에서 *Generative AI*의 통합이 기존 애니메이션 워크플로우를 혁신적으로 변화시키고 있음을 탐구합니다. AniDoc, ToonCrafter, AniSora와 같은 도구를 통해 창작자들에게 보다 광범위한 접근성을 제공하고, 창의적 표현과 예술적 혁신에 집중할 수 있는 환경을 조성합니다.
- ***Technical Details***: 칼 애니메이션의 전통적인 제작 파이프라인은 스토리보드 작성, 레이아웃 디자인, 키프레임 애니메이션, 인투비닝(*Inbetweening*), 컬러링 등 다양한 단계를 포함합니다. 이러한 과정들 중 반복적인 작업을 *Generative AI*로 자동화하여 시각적 일관성 및 스타일적 일관성을 유지하는 방법에 대해 논의합니다.
- ***Performance Highlights***: *Generative AI*는 다양한 제작 단계에서 효율성과 창의력을 높이지만, 시각적 일관성과 스타일 보전 등의 과제는 여전히 남아 있습니다. 이러한 도전 과제를 해결하기 위해 미래 연구 방향을 탐색하며, 학습 데이터셋의 개선, 보다 강력한 멀티모달 모델의 개발, 통합된 다중 프로세스 생성 프레임워크의 필요성을 제안합니다.

### [UnCommon Objects in 3D](https://arxiv.org/abs/2501.07574)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.07574.png)

Vote: 7

Authors: Piyush Tayal, Jiali Duan, Antoine Toisoul, Konstantinos Tertikas, Roman Shapovalov, Xingchen Liu, Jason Y. Zhang, Tom Monnier, Andrea Vedaldi, Jianyuan Wang, Natalia Neverova, David Novotny, Jesus Zarzar

- ***What's New***: UnCommon Objects in 3D (uCO3D)라는 새로운 대규모 데이터셋이 공개되었습니다. 이 데이터셋은 3D 심층 학습과 3D 생성 AI에 초점을 맞추었으며, 1,000개 이상의 객체 범주를 포괄하는 고해상도 동영상으로 구성되어 있습니다. uCO3D는 구조-이동(SfM) 시스템을 사용하여 3D 주석을 제공하며, 모든 객체가 캡션과 3D Gaussian Splat 재건과 함께 제공됩니다.
- ***Technical Details***: uCO3D는 360도 비디오 캡처 방식을 채택하여 객체를 모든 각도에서 촬영하며, VGGSfM이라는 최첨단 SfM 시스템을 활용하여 카메라 포즈와 점 구름을 생성합니다. 데이터셋은 LVIS 분류 체계를 기반으로 1,070개의 시각적 객체 범주를 포함하며, 이는 이전 데이터셋인 MVImgNet과 CO3Dv2와 비교해 훨씬 더 다양한 객체를 제공합니다. 데이터 품질 향상을 위해 각 비디오의 해상도를 엄격하게 검증했으며, 3D 재구성을 위해 3D Gaussian Splatting 방법을 사용했습니다.
- ***Performance Highlights***: uCO3D를 사용하여 각종 3D 모델을 훈련한 결과, 기존의 데이터셋인 MVImgNet과 CO3Dv2보다 더 나은 성능을 보여주었습니다. 특히, 대상 관점의 새로운 이미지를 합성할 때 질적으로 우수한 결과를 기록했으며, 이는 uCO3D 데이터셋이 3D 학습에 최적화된 고품질 데이터셋임을 증명합니다.

### [ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning](https://arxiv.org/abs/2501.06590)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06590.png)

Vote: 3

Authors: Zhuosheng Zhang, Mark Gerstein, Xiangru Tang, Yilun Zhao, Siru Ouyang, Wangchunshu Zhou, Arman Cohan, Xunjian Yin, Tianyu Hu, Pan Lu, Muyang Ye, Yanjun Shao

- **What's New**: ChemAgent는 대형 언어 모델(Large Language Models; LLMs)의 성능을 향상시키기 위해 동적, 자기 갱신 라이브러리를 도입한 새로운 프레임워크입니다. 이 시스템은 화학적 작업을 하위 작업으로 분해하고 이를 구조화된 컬렉션으로 컴파일하여 미래 쿼리를 위한 참고 자료로 사용합니다.
- **Technical Details**: ChemAgent는 하위 작업을 메모리라고 부르는 라이브러리에서 관련 정보를 검색하고 수정하여 효과적인 작업 분해와 솔루션 생성이 가능하게 합니다. 이 시스템은 계획 메모리, 실행 메모리, 지식 메모리라는 세 가지 주요 메모리로 구성되어 있으며, 외부적으로 저장되어 문제 해결 과정 동안 효율적으로 검색할 수 있도록 설계되었습니다.
- **Performance Highlights**: ChemAgent는 SciBench의 네 가지 화학 추론 데이터셋에서 최대 46%의 성능 향상을 보여줍니다. 실험 결과 StructChem과 같은 기존 방법보다 최대 15% 향상된 결과를 보여주며, 특히 강력한 LLMs에서 성능 향상 폭이 더 큽니다.

### [BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature](https://arxiv.org/abs/2501.07171)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.07171.png)

Vote: 3

Authors: Austin Wolfgang Katzer, Josiah Aklilu, Collin Chiu, Anita Rau, Jeffrey Gu, Alfred Seunghoon Song, James Burgess, Liangyu Chen, Robert Tibshirani, Min Woo Sun, Serena Yeung-Levy, Xiaohan Wang, Yuhui Zhang, Alejandro Lozano, Jeffrey J Nirschl, Ivan Lopez

- ***What's New***: BIOMEDICA는 PubMed Central Open Access (PMC-OA) 하위 집합에서 6백만 개 이상의 논문을 통해 2천 4백만 개의 고유한 이미지-텍스트 쌍을 추출하고 주석을 달아 웹 기반 데이터 집합으로 제공하는 오픈소스 프레임워크입니다. 이 데이터를 바탕으로 BMCA-CLIP 모델을 지속적으로 사전 학습하여 40개의 다양한 업무에서 전이 학습 없는(zero-shot) 분류 작업에서 평균 6.56%의 성능 향상을 달성했습니다.
- ***Technical Details***: BIOMEDICA 프레임워크는 PMC-OA 저장소의 딥러닝 준비를 위한 대규모 데이터 세트로, 27개의 고유한 메타데이터 필드를 포함하는 2천 4백만 개 이상의 이미지-텍스트 쌍을 포함합니다. 클러스터링과 전문가의 주석을 이용한 개념 분류 체계를 통해 다양한 생물의학 데이터를 구조화하였으며, 대량의 데이터를 스트리밍 방식으로 처리하여 훈련을 원격으로 접근할 수 있게 했습니다. BMCA-CLIP 모델은 이러한 구조화된 데이터로 지속적으로 사전 학습 및 최적화가 진행되었습니다.
- ***Performance Highlights***: BMCA-CLIP 모델은 병리학, 안과학, 피부과학 등 여러 생의학적 분야에서 주어진 40개의 태스크 대부분에서 이전 최첨단 모델을 초과하며 6.56%의 전이 학습 없는(zero-shot) 성능 향상을 보여주었습니다. 특히 피부과학에서는 29.8%, 안과학에서는 17.5%의 향상을 기록했으며, 비교적 적은 연산 비용으로 뛰어난 이미지-텍스트 검색 성능을 발휘했습니다.

### [Evaluating Sample Utility for Data Selection by Mimicking Model Weights](https://arxiv.org/abs/2501.06708)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06708.png)

Vote: 2

Authors: Manjot Bilkhu, Frederic Sala, Javier Movellan, Tzu-Heng Huang

- ***What's New***: 새로운 접근 방식인 Mimic Score는 데이터 샘플의 유용성을 평가하는 새로운 데이터 품질 지표로 도입되었습니다. 이는 미리 학습된 레퍼런스 모델(reference model)을 사용하여 새로운 모델의 가중치 공간(weight space)에서 벡터 방향과의 정렬을 측정합니다. 이 지표를 활용하여 새로운 데이터 선택 프레임워크인 Grad-Mimic이 개발되었으며, 학습에 유용한 샘플을 자동으로 식별하고 우선순위를 매겨 효과적인 필터를 생성합니다.
- ***Technical Details***: Mimic Score는 각 샘플의 그레디언트가 레퍼런스 모델의 가중치 공간의 방향과 얼마나 정렬되는지 측정하여 샘플의 기여도를 평가합니다. Grad-Mimic은 두 단계로 구성됩니다. 첫째, 학습 단계에서 mimic score를 사용해 샘플을 학습 우선순위로 정렬하고, 둘째, 샘플의 유용성을 식별하는 필터를 자동화하여 데이터 선택을 진행합니다. 실험 환경에서 레이블 노이즈 수준을 조절하며 Grad-Mimic의 샘플 식별 정확성을 검증했습니다.
- ***Performance Highlights***: Grad-Mimic을 사용한 모델 학습은 기존 필터링 방법보다 성능을 향상시키며, 6개의 이미지 데이터셋에서 일관된 성능 향상을 보였습니다. 특히, 대규모 웹 크롤링 데이터셋에서 CLIP 모델 학습 시 mimic score 기반 필터가 모델 성능을 크게 향상시켰음을 나타냈습니다. Mimic scores는 데이터셋 품질을 신뢰성 있게 추정하는 지표로서 유용성이 입증되었습니다.

