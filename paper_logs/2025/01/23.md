## Daily Papers (2025-01-23)

### [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12948.png)

Vote: 96

Authors: Yunxian Ma, Tao Yun, Y. K. Li, Shiyu Wang, Dejian Yang, Shuiping Yu, Shuting Pan, Dongjie Ji, Jiashi Li, Zhiyu Wu, Litong Wang, Ruisong Zhang, Hui Li, Huajian Xin, Shanhuang Chen, Honghui Ding, Shengfeng Ye, S. S. Li, Yishi Piao, Yuyang Zhou, R. L. Jin, Yaohui Li, Xuecheng Su, Junjie Qiu, Bochao Wu, Yuxiang You, Liang Zhao, H. Zhang, Huazuo Gao, Guowei Li, Zhihong Shao, Qinyu Chen, Jianzhong Guo, Xinxia Shan, Yang Zhang, Xiaoxiang Wang, Qihao Zhu, Mingchuan Zhang, Wanjia Zhao, Yuan Ou, Guangbo Hao, Tian Pei, Zhewen Hao, Xin Liu, Wen Liu, Yiyang Ma, Hui Qu, Kang Guan, Ying He, Yuxiang Luo, Yaohui Wang, Xinyuan Li, Leyi Xia, Zhe Fu, Xiaohan Wang, Zhen Huang, Yi Zheng, Zijun Liu, Bingxuan Wang, Liyue Zhang, Kai Dong, Wenjun Gao, Xingchao Liu, Yuduan Wang, Bei Feng, Xiaosha Chen, Wenqin Yu, Yiyuan Liu, Bing Xue, Zhibin Gou, Yu Wu, Minghua Zhang, Miaojun Wang, Ziyi Gao, Yifan Shi, Xin Cheng, Tianyu Sun, Ruiqi Ge, Yuting Yan, Shanghao Lu, Yanping Huang, Zehui Ren, Zhengyan Zhang, Fucong Dai, Panpan Huang, Yixuan Tan, Erhang Li, Yuheng Zou, Z. Z. Ren, Jiaqi Ni, Fangyun Lin, Yujia He, Shuang Zhou, Daya Guo, Kai Hu, Xiaotao Nie, Yukun Zha, Xinyi Zhou, Minghui Tang, Yuchen Zhu, Wei An, Junxiao Song, Deli Chen, Meng Li, Yongqiang Guo, Zhicheng Ma, Qiushi Du, Junlong Li, Chenyu Zhang, Y. X. Wei, Ying Tang, Ruyi Chen, Haowei Zhang, Xiangyue Jin, Wentao Zhang, Xiaokang Zhang, T. Wang, Haocheng Wang, Yichao Zhang, J. L. Cai, Lecong Zhang, Zhangli Sha, Zijia Zhu, Zhigang Yan, Zizheng Pan, Runxin Xu, Kuai Yu, Yi Yu, Shaoqing Wu, Zhen Zhang, Ruoyu Zhang, Xiaowen Sun, Chenggang Zhao, Ziyang Song, Z. F. Wu, Yunfan Xiong, Peiyi Wang, Xiaojin Shen, Peng Zhang, Yanhong Xu, Yao Zhao, Yisong Wang, Aixin Liu, Shirong Ma, Y. X. Zhu, Han Bao, Jingchang Chen, Lei Xu, DeepSeek-AI, Hanwei Xu, Kexin Huang, X. Q. Li, Qiancheng Wang, Xinyu Yang, Xianzu Wang, Xingkai Yu, Zhenda Xie, Xuheng Lin, Yao Li, Runji Wang, Chengqi Deng, Shunfeng Zhou, Y. Q. Wang, Shangyan Zhou, Zhuoshu Li, Wenfeng Liang, Guanting Chen, Mingming Li, Zilin Li, Jin Chen, W. L. Xiao, Zhongyu Zhang, Ning Tian, Yiliang Xiong, Jian Liang, Jiawei Wang, Yaofeng Sun, Lean Wang, Xiao Bi, Kaige Gao, Chong Ruan, Damai Dai, Yue Gong, Xiaokang Chen, Ruizhe Pan, Jingyang Yuan, Xin Xie, Chengda Lu, Ziwei Xie, Xinnan Song, Xiaodong Liu, Zhean Xu, Wangding Zeng, R. J. Chen, Yuxuan Liu, Zihui Gu, Fuli Luo, Zhipeng Xu

- ***What's New***: 이 논문에서는 순수한 강화학습(Reinforcement Learning; RL)을 통해 대규모 언어 모델(LLM)의 추론 능력을 향상시키는 DeepSeek-R1-Zero와 DeepSeek-R1을 소개합니다. DeepSeek-R1-Zero는 사전 감독학습(Supervised Fine-Tuning; SFT) 없이 RL만으로 학습되어 다양한 추론 행동을 자연스럽게 발현하지만 가독성과 언어 혼합 문제를 겪습니다. 이를 개선하기 위해 DeepSeek-R1은 다단계 학습과 콜드 스타트 데이터를 도입하여 OpenAI-o1-1217과 유사한 성능을 달성하였습니다.
- ***Technical Details***: DeepSeek-R1-Zero는 기본 모델에 직접 RL을 적용하여 장시간의 Chain-of-Thought(CoT) 추론을 탐색할 수 있게 하여 차별화된 추론 능력을 개발합니다. 반면, DeepSeek-R1은 콜드 스타트 데이터를 활용한 소규모 SFT와 논리적 강화를 통해 언어 혼합 문제와 가독성을 향상시킵니다. 특히, 강화학습 프레임워크로 GRPO를 사용하여 모델을 지속적으로 최적화합니다.
- ***Performance Highlights***: DeepSeek-R1은 MATH-500에서 97.3%의 Pass@1 점수를 기록하며 최고 성능을 자랑합니다. 또한, Codeforces에서 96.3%의 인간 참가자를 능가하며, 다양한 프로그래밍 및 수학적 추론에서 뛰어난 성과를 보입니다. distilled 모델인 DeepSeek-R1-Distill-Qwen-32B는 기존의 오픈소스 모델들보다 우수한 성능을 구현하며, 특히 AIME 2024에서 72.6%의 점수를 달성하는 등의 강점을 나타냅니다.

### [FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces](https://arxiv.org/abs/2501.12909)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12909.png)

Vote: 43

Authors: Zhouyi Li, Jun Yu, Zhenran Xu, Senbao Shi, Yiyu Wang, Xue Yang, Min Zhang, Longyue Wang, Baotian Hu, Jifang Wang

- ***What's New***: FILMAGENT는 가상 3D 공간에서 영화 제작을 자동화하기 위한 대규모 언어 모델(LLM)을 기반으로 한 최초의 다중 에이전트 협업 프레임워크입니다. 이 시스템은 여러 에이전트가 감독, 각본 작가, 배우, 촬영 감독 등으로 역할을 나눠 인간의 영화 제작 워크플로우를 모방합니다.
- ***Technical Details***: FILMAGENT는 세 가지 주요 단계로 나누어져 있으며, 각 단계마다 'Critique-Correct-Verify'와 'Debate-Judge' 같은 다중 에이전트 협업 알고리즘이 활용됩니다. 15곳의 가상 장소와 272개의 샷, 21가지의 액션으로 구성된 3D 공간에서 영화가 제작됩니다.
- ***Performance Highlights***: 인간 평가에서 FILMAGENT는 5점 만점에 평균 3.98점을 기록하며, 다른 기본 모델보다 우수한 성능을 보였습니다. 특히 플롯의 일관성과 카메라 선택의 적절함 측면에서 뛰어난 개선을 보였습니다. FILMAGENT는 GPT-4o 모델을 사용했음에도, 단일 에이전트보다 더 나은 성과를 냈습니다.

### [Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback](https://arxiv.org/abs/2501.12895)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12895.png)

Vote: 40

Authors: Yafu Li, Xuyang Hu, Linjie Li, Xiaoye Qu, Yu Cheng

- ***What's New***: 테스트 타임 선호도 최적화(Test-time Preference Optimization; TPO)는 대형 언어 모델(Large Language Models; LLMs)이 추론 시 인간의 선호도에 맞게 출력을 조정할 수 있도록 하는 경량의 실시간 최적화 방법을 제시합니다. 이는 모델 파라미터를 업데이트하지 않고 추론 과정에서 모델 출력을 개선합니다.
- ***Technical Details***: TPO는 모델의 추론 시점에서 텍스트 상의 피드백과 비판을 이용하여 모델의 출력을 개선하는 방법입니다. 이 과정에서는 '텍스트 보상(Textual Reward)' 및 '텍스트 기울기(Textual Gradient)'를 활용하여 모델 출력을 계속적으로 조정합니다. 수학적으로는 모델 출력과 보상 모델 간의 상호작용으로 피드백을 해석하여 반복적으로 피드백을 통해 출력을 수정합니다. 알고리즘은 변수를 정의하고 손실을 계산하며, 기울기를 계산하여 변수를 최적화하는 단계로 구성됩니다.
- ***Performance Highlights***: TPO는 테스트 타임에 두 단계의 반복만으로 학습된 모델과 비교해 더 나은 성능을 보이며, 특히 Llama-3.1-70B-SFT와 같은 정합되지 않은 모델이 TPO 적용 후 Llama-3.1-70B-Instruct와 같은 정합된 모델을 대부분의 벤치마크에서 웃도는 결과를 보였습니다. 테스트 타임에 TPO를 활용하면, 기존의 학습 기반 방법과 유사한 수준의 성능을 달성하면서도 학습에 소요되는 계산 비용을 크게 줄일 수 있습니다.

### [VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding](https://arxiv.org/abs/2501.13106)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13106.png)

Vote: 40

Authors: Yuqian Yuan, Fan Wang, Lidong Bing, Zesen Cheng, Xin Li, Deli Zhao, Peng Jin, Zhiqiang Hu, Guanzheng Chen, Yuming Jiang, Kehan Li, Wenqi Zhang, Boqiang Zhang, Sicong Leng, Hang Zhang

- ***What's New***: VideoLLaMA3는 이미지와 비디오 이해를 위한 멀티모달 모델로, 이미지 텍스트 데이터를 기반으로 비디오 이해 성능을 향상시키는 비전 중심의 학습 패러다임을 제안합니다. 또한, 다양한 입력 해상도를 처리할 수 있는 비전 인코더를 통해 시각적 입력의 유연성과 효율성을 강화합니다.
- ***Technical Details***: VideoLLaMA3는 4단계 학습 파라다임을 활용합니다. (1) 비전 인코더 적응 단계에서는 고품질 이미지 텍스트 데이터를 사용하여 비전 인코더의 성능을 향상시킵니다. (2) 비전-언어 사전 학습 단계에서는 멀티모달 이해의 기반을 마련합니다. (3) 다중 태스크 미세 조정 단계에서는 이미지와 텍스트 데이터를 활용하여 다양한 다운스트림 태스크에 맞게 모델을 조정합니다. (4) 비디오 중심의 미세 조정 단계에서는 비디오 이해 성능을 더욱 향상시킵니다. AVT(Any-resolution Vision Tokenization) 기술을 활용하여 다양한 해상도의 이미지와 비디오를 적절하게 처리하고, DiffFP(Differential Frame Pruner)를 통해 비디오 토큰을 압축하여 처리 효율성을 높입니다.
- ***Performance Highlights***: VideoLLaMA3는 이미지와 비디오 이해를 포함한 다양한 벤치마크에서 뛰어난 성과를 보여주었습니다. 예를 들어, 수학적 추론에서 MathVision 과제에서 이전 최첨단 모델보다 6.5% 높은 결과를 기록했으며, RealWorldQA와 같은 일반적인 지식 질문 응답에서 이전 모델보다 2% 향상된 성과를 보였습니다. 비디오 이해에서는 일반 비디오 이해 및 장기 비디오 이해에서 다양한 벤치마크에서 가장 높은 성과를 기록하였습니다.

### [Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/abs/2501.12599)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12599.png)

Vote: 33

Authors: Qiwei Pan, Zheng Zhang, Jia Chen, Weimin Xiong, Fengxiang Tang, Chonghua Liao, Yulun Du, Zhaowei Li, Shengling Ma, Haotian Zhao, Ying Yang, Yangyang Liu, Haozhen Yu, Xinxing Zu, Ling Ye, Hao Hu, Chenjun Xiao, Guangda Wei, Sihan Cao, Chenzhuang Du, Yang Li, Yuzhi Wang, Cheng Chen, Angang Du, Zhilin Yang, Tao Jiang, Junjie Yan, Hao Zhang, Yifeng Liu, Weixiao Huang, Shupeng Wei, Chuning Tang, Xianghui Wei, Weiran He, Zhen Zhu, Siying Huang, Wenhao Wu, Jianhang Guo, Jingyuan Liu, Zihao Huang, Kimi Team, Haoyu Lu, Weihao Gao, Huabin Zheng, Junyan Wu, Wenyang He, Hao Yang, Qucheng Gong, Dehao Zhang, Zhiqi Huang, Cheng Li, Mengnan Dong, Yidao Qin, Enming Yuan, Han Zhu, Hao Ding, Jianlin Su, Lidong Shi, Zaida Zhou, Yejie Wang, Xingzhe Wu, Xianqing Jia, Xuehai Pan, Jie Zhao, Changjiu Jiang, Yangyang Hu, Haiqing Guo, Zonghan Yang, Yiping Bao, Yuxin Wu, Huan Yuan, Ningchen Ma, Longhui Yu, Hongcheng Gao, Zhexu Wang, Flood Sung, Guokun Lai, Yanru Chen, Yibo Liu, Neo Zhang, Bowei Xing, Shaowei Liu, Bofei Gao, Haoze Li, Haotian Yao, Xinran Xu, Congcong Wang, Jianzhou Wang, Zhaoji Wang, Jin Zhang, Y. Charles, Ziyao Xu, Enzhe Lu, Xinyu Zhou

- ***What's New***: Kimi k1.5는 RL(Reinforcement Learning) 기법을 사용하여 대형 언어 모델(LLMs)의 확장성을 높이는 특징이 있습니다. 기존의 복잡한 방법 없이 정책 최적화와 긴 맥락 확대 방법을 통해 높은 성능을 보입니다.
- ***Technical Details***: Kimi k1.5는 멀티 모달 데이터로 훈련되었고, 긴 맥락(Window) 스케일링과 강화 학습 정책 최적화 기법이 핵심입니다. 몬테카를로 트리 탐색(Monte Carlo Tree Search)이나 가치 함수(Value Function) 같은 복잡한 기법에 의존하지 않는 간단한 강화를 사용합니다. 모델의 성능을 높이기 위해 다양한 샘플링 전략과 최적화된 데이터 기법을 사용했습니다.
- ***Performance Highlights***: Kimi k1.5는 다양한 벤치마크에서 높은 추론 성능을 달성했습니다. AIME에서 77.5, MATH500에서 96.2, Codeforces에서 94번째 퍼센타일을 기록하며, OpenAI o1 모델에 상응하는 성과를 보였습니다. 또한, 코딩 작업 최적화 방식인 Long2short 기법을 통해 짧은 맥락 모델에서의 성능을 크게 개선했습니다.

### [Autonomy-of-Experts Models](https://arxiv.org/abs/2501.13074)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13074.png)

Vote: 29

Authors: Songhao Wu, Rui Yan, Yining Qian, Ruobing Xie, Ang Lv, Di Wang, Xingwu Sun, Zhanhui Kang

- ***What's New***: Autonomy-of-Experts (AoE) 모델은 Mixture-of-Experts (MoE) 모형의 새 패러다임으로, 전문가들이 스스로 입력을 처리할지 여부를 결정하는 형태로 설계되었습니다. 이를 통해 전문가들이 자신의 능력을 스스로 평가하고 선택할 수 있도록 하여 기존 MoE의 라우터와 전문가 기능 분리로 인한 비효율을 극복합니다.
- ***Technical Details***: AoE에서는 각 전문가가 입력의 내부 활성화 규모를 사전에 계산해 그 노름(norm)을 기반으로 순위를 매깁니다. 상위 순위 전문가들만이 전방 패스(forward pass)를 진행하며 나머지는 중단됩니다. 이는 기존 라우터를 제거해 자율성을 부여하며, 저랭크(Low-rank) 가중치 분해를 통해 예비 계산 부담을 줄입니다. AoE는 700M에서 4B 파라미터까지 다양한 크기의 언어 모델을 사전 학습하여, 유사한 효율성으로 전통적인 MoE 모델을 능가합니다.
- ***Performance Highlights***: AoE는 성능 실험에서 전통적인 MoE 모델보다 우수한 다운스트림 성능을 보여주었으며, 전문가 선택의 개선 및 더 전문화된 학습으로 인해 더 나은 성능 개선 효과를 얻었습니다. 성능 보존율이 최대 95%에 달해 기존의 MoE 모델에서 71%에 머무는 성능보다 향상된 성능을 보였습니다. 또한, 최대 97%의 전통 MoE 모델 처리량에 이르면서도 메모리 사용량은 증가합니다.

### [Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament](https://arxiv.org/abs/2501.13007)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13007.png)

Vote: 13

Authors: Zijun Yao, Yantao Liu, Yixin Cao, Juanzi Li, Rui Min, Lei Hou

- ***What's New***: 이 논문에서는 기존의 보상 모델(Reward Model; RMs)의 임의적이고 불일치한 점수를 해결하기 위해, 수학 문제 해결에 대해 베스트 오브 엔(Best-of-N; BoN) 샘플링을 시행하는 새로운 페어와이즈 보상 모델(Pairwise Reward Model; Pairwise RM)과 녹아웃 토너먼트를 제안합니다.
- ***Technical Details***: 페어와이즈 RM은 수학 문제에 대해 두 후보 솔루션의 정답 여부를 동시에 평가하여 임의의 점수 부여를 없애고 평행 비교를 통해 솔루션의 크로스 검증을 가능케 합니다. 이를 위해, NumiaMath로부터 유도된 443K 쌍 비교(PAIRWISE-443K)를 포함하는 대규모 데이터셋을 구축하여 모델을 감독 기반 미세 조정(Supervised Fine-Tuning)으로 학습합니다.
- ***Performance Highlights***: MATH-500과 오림피아드 벤치에서의 실험 결과, 페어와이즈 RM은 기존의 보상 모델을 초월하여 상당한 성능 개선을 보여주며, 특히 가장 어려운 상위 50% 문제들에 대해 40%에서 60%까지 상대적인 개선을 달성하였습니다.

### [O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning](https://arxiv.org/abs/2501.12570)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12570.png)

Vote: 11

Authors: Naiqiang Tan, Xiaochun Cao, Li Shen, Yibo Wang, Dacheng Tao, Shiwei Liu, Wei Li, Haotian Luo, Haiying He

- ***What's New***: O1-Pruner는 O1-Like Reasoning 모델의 길이 조화를 위한 신규 Fine-Tuning 기법으로, 추론 과정을 최적화하여 모델의 정확성을 유지하면서도 불필요한 계산 오버헤드를 줄이는 효과를 가져옵니다. 이를 통해 더 효율적인 추론을 가능하게 하며, 다양한 수리적 추론 벤치마크에서 효율성과 정확성 모두에서 우수한 성과를 보여줍니다.
- ***Technical Details***: O1-Pruner는 강화 학습(RL) 스타일의 Fine-Tuning을 사용하며, LLM의 초기 성능을 선표본(pre-sampling)을 통해 예측한 후 정확성을 유지하며 더 짧은 추론 과정을 생성하도록 유도합니다. 이를 위해 길이 조화 기반 보상 함수(Length-Harmonizing Reward)를 설계, 모델이 정확성을 유지하면서도 추론 길이를 줄이도록 최적화합니다.
- ***Performance Highlights***: O1-Pruner는 MATH, GSM8K 및 GaoKao 데이터셋에서 평균 정확도가 76.8%에 이르며, 솔루션 길이를 약 40.5% 감소시키고 평균 ACE 점수도 향상되는 결과를 보였습니다. 이러한 성과는 다른 방법들과 비교했을 때 가장 효율적이며, 특히 inference time 측면에서 상당한 개선을 보여줍니다.

### [IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems](https://arxiv.org/abs/2501.11067)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11067.png)

Vote: 6

Authors: Elad Levi, Ilan Kadar

- ***What's New***: IntellAgent는 대화형 인공지능 시스템(Conversational AI Systems)의 평가를 위한 새로운 다중 에이전트 프레임워크(Multi-Agent Framework)입니다. 이 프레임워크는 정책 기반 그래프 모델링(Policy-Driven Graph Modeling), 현실적인 이벤트 생성(Realistic Event Generation), 상호작용 사용자-에이전트 시뮬레이션(Interactive User-Agent Simulations)을 결합하여 다양한 시나리오를 생성하고, 이를 통해 보다 정교한 진단을 제공하여 기존의 정적 벤치마크의 한계를 극복합니다.
- ***Technical Details***: IntellAgent는 정책 그래프(Policies Graph)를 사용하여 대화형 AI 에이전트를 평가합니다. 각 노드는 개별 정책과 그 복잡성을 나타내고, 엣지는 정책 간의 공동 발생 가능성을 나타냅니다. 이 시스템은 이벤트 생성기(Event Generator)가 생성한 정책 목록에 따라 사용자 요청과 초기 데이터베이스 상태를 포함한 이벤트를 시뮬레이션합니다. 시뮬레이션을 통해 다양한 복잡성 수준을 포괄하는 이벤트를 생성하여 대화형 에이전트를 철저히 평가할 수 있습니다. 또한, 모듈식 오픈소스 구조로 설계되어 새로운 도메인, 정책, API와의 통합을 용이하게 합니다.
- ***Performance Highlights***: IntellAgent를 통한 벤치마크 결과는 여러 모델의 성능과 복잡성 수준 간의 강한 상관 관계를 보여주며, 이는 모델 성능 하락 속도가 모델마다 상이함을 드러냈습니다. 예를 들어, Gemini-1.5-pro는 낮은 수준에서는 GPT-4o-mini보다 우수한 성능을 나타냈지만, 높은 복잡성에서는 두 모델의 성능이 수렴하였습니다. 이러한 상세 분석을 통해 사용자는 필요한 복잡성을 처리할 적합한 모델을 선택할 수 있습니다.

