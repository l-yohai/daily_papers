## Daily Papers (2025-01-08)

### [REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models](https://arxiv.org/abs/2501.03262)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03262.png)

Vote: 42

Authors: Jian Hu

- ***What's New***: REINFORCE++는 기존의 REINFORCE 알고리즘을 확장한 새로운 변형 버전으로, 대형 언어 모델(Large Language Models; LLMs)의 인간 선호 정렬(Alignment)에 간단하고 효율적인 접근 방식을 제공합니다. REINFORCE++는 Proximal Policy Optimization(PPO)의 주요 최적화 기법을 통합하면서도 비평가 네트워크(Critic Network)가 필요 없는 간소화를 통해 향상된 훈련 안정성과 감소된 계산 오버헤드를 달성합니다.
- ***Technical Details***: REINFORCE++는 PPO에서 영감을 받은 기술들을 REINFORCE 프레임워크에 결합하여 단순화 및 안정성을 강조합니다. 평가에서는 토큰-레벨 KL Penalty, PPO 클립손실, 미니-배치 업데이트 및 보상 정규화 등의 기법이 사용됩니다. 또한, KL 발산을 활용한 보상 모델링과 복잡한 데이터셋을 통한 평가가 이루어졌습니다.
- ***Performance Highlights***: 실험 결과에 따르면, REINFORCE++는 PPO 및 GRPO(Group Relative Policy Optimization)에 비해 훈련 안정성이 우수하며, 특히 보상 및 출력 길이 해킹 방지에서 두드러집니다. 또한, 수학적 문제 해결 시나리오에서는 KL 발산당 더 높은 보상 증가를 보여줍니다.

### [Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model](https://arxiv.org/abs/2501.02790)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02790.png)

Vote: 2

Authors: Mingyuan Zhou, Yujia Xie, Yuting Sun, Shentao Yang, Ziyi Yang, Yueqin Yin, Hany Awadalla, Weizhu Chen

- ***What's New***: 이 논문은 텍스트 분할 및 보상 학습을 통해 강화학습을 개선하는 방법을 제안합니다. 인간 피드백을 통한 강화학습 (Reinforcement Learning from Human Feedback; RLHF)에서, 텍스트의 의미적 완전성을 고려한 세그먼트 레벨 보상 모델 (Segment-level Reward Model)을 도입함으로써, 기존의 토큰 레벨 보상 모델이 가지는 한계를 극복하고자 합니다.
- ***Technical Details***: 제안된 방법은 텍스트 생성의 순차적 특성을 반영하여 의미적으로 완전한 텍스트 세그먼트에 보상을 할당합니다. 이를 통해 세그먼트 보상 학습을 일반적인 순서-선호 데이터셋과 호환되도록 설계합니다. 보상 학습을 위해, 동적 텍스트 분할과 위치 기반 보상 정규화 함수를 적용하여 보상 모델을 학습합니다. 또한 세그먼트 보상을 통해 강화를 더욱 조밀하게 만듭니다.
- ***Performance Highlights***: 제안된 방법은 AlpacaEval 2.0, Arena-Hard, MT-Bench 세 가지 RLHF 벤치마크에서 경쟁력 있는 성능을 보여주었습니다. 실험 결과 세그먼트 레벨 보상 모델이 클래식 밴딧 보상 접근 및 최근의 토큰 레벨 보상 접근보다 더 나은 성능을 제공함을 입증했습니다.

### [PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models](https://arxiv.org/abs/2501.03124)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03124.png)

Vote: 8

Authors: Yu Cheng, Mingyang Song, Xiaoye Qu, Zhaochen Su, Jiawei Zhou

- ***What's New***: PRMBench는 프로세스 레벨 보상 모델(Process-Level Reward Models; PRMs)의 세밀한 오류 탐지 기능을 평가하기 위해 특별히 설계된 벤치마크로, PRMs의 다양한 오류 탐지 능력을 체계적으로 평가하도록 설계되었습니다. 이를 통해 기존의 평가 기준을 넘어 다양한 오류 유형과 시나리오를 다룰 수 있습니다.
- ***Technical Details***: PRMBench는 6,216개의 신중하게 설계된 문제와 83,456개의 단계 레벨 라벨로 구성되어 있으며, 단순성, 건전성, 민감성을 포함한 여러 차원에서 모델을 평가합니다. 또한, 이 벤치마크는 오픈 소스 PRMs와 비공개 언어 모델을 포함한 15개의 모델을 대상으로 실험하여, 현재 PRMs의 약점을 드러냈습니다.
- ***Performance Highlights***: 평가된 모든 모델들은 다단계 프로세스 평가를 부분적으로 이해하고 있으며, 최고의 성능을 보인 모델도 무작위 추측을 약간 상회하는 수준의 결과를 보여 차후 개선의 여지가 큽니다. 특히, 오픈 소스 PRMs는 전반적으로 소유언어모델(Proprietary LLMs; P. LLMs)보다 낮은 성능을 보였습니다.

### [Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control](https://arxiv.org/abs/2501.03847)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03847.png)

Vote: 11

Authors: Zhen Dong, Zhiyang Dou, Cheng Lin, Wenping Wang, Chenyang Si, Yuan Liu, Rui Yan, Peng Li, Zekai Gu, Qifeng Liu, Jiahao Lu, Ziwei Liu

- ***What's New***: Diffusion as Shader (DaS)는 다양한 비디오 생성 제어 작업을 지원하는 새로운 3D 인식 비디오 확산 모델(video diffusion model)입니다. 이 모델은 카메라 조작, 객체 조작, 메시 애니메이션, 모션 전이와 같은 다중 제어 작업을 단일 아키텍처에서 수행할 수 있습니다.
- ***Technical Details***: DaS는 3D 추적 비디오(tracking videos)를 제어 신호로 사용하여 비디오 확산 프로세스에 3D 인식을 부여합니다. 이 추적 비디오는 첫 번째 프레임의 카메라 좌표에서 정의된 3D 포인트의 움직임 궤적을 포함합니다. 모델은 3D 추적 비디오에서 파생된 조건 기능을 디노이징 프로세스에 주입하여 비디오 생성을 제어합니다. 이는 제어 신호로서 3D 추적 비디오의 신호를 결합하여 VAE 인코더 및 Transformer 기반의 잠재 공간을 통한 비디오 생성을 실행합니다.
- ***Performance Highlights***: 실험 결과, DaS는 다양한 제어 작업에서 강력한 성능을 보였습니다. 카메라 제어와 모션 전이에서, DaS는 비교 모델 보다 우수한 성능을 보였습니다. 메시-비디오 생성 및 객체 조작 작업에서도 우수한 품질을 갖춘 비디오를 생성하여 질적 평가에서 높은 평가를 받았습니다. 3D 추적 비디오는 뛰어난 시간적 일관성을 제공하며, 이는 생성된 비디오의 품질을 현저히 향상시킵니다.

### [MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control](https://arxiv.org/abs/2501.02260)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02260.png)

Vote: 3

Authors: Guoying Zhao, Huai-Qian Khor, Mengting Wei, Tuomas Varanka, Xingxun Jiang

- ***What's New***: MagicFace는 얼굴의 세세한 표정을 AU(행동 단위; Action Unit) 변화를 통해 고해상도로 편집할 수 있는 새로운 모델입니다. 이 모델은 안정적 확산 모델(Stable-Diffusion)과 ID 인코더(ID Encoder)를 활용하여 얼굴의 지문 세부 사항을 보존하며 연속적이고 해석 가능한 표정 변화를 가능하게 합니다. 이와 함께 배경과 자세를 일관성 있게 유지하며 다양한 AU 조합을 통해 고품질의 표정 편집을 제공합니다.
- ***Technical Details***: MagicFace는 사전 학습된 안정적 확산 모델을 기반으로 하며, AU 변화를 조건으로 활용하여 얼굴 표정을 편집하는 기법을 사용합니다. ID 인코더는 대칭적 UNet 구조로 입력 정체성의 공간적 세부 정보를 포착하여 얼굴의 세부 사항을 일관되게 유지합니다. 백그라운드와 포즈의 일관성을 유지하기 위해 Attribute Controller를 도입하여 목표 배경과 포즈를 명시적으로 모델에 전달합니다. AU 변화를 디노이징 UNet에 주입하여 다양한 AU 결합과 함께 원할한 표정 편집을 지원합니다.
- ***Performance Highlights***: MagicFace의 성능은 기존의 얼굴 표정 편집 방법과 비교하여 정밀하고 유연하며 사용자 친화적인 편집 방식을 제공합니다. AU 정확성, 정체성 보존 및 이미지 유사성 측면에서 MagicFace는 AU 조건의 더 정확한 해석 및 고품질 이미지 생성과 관련하여 상당한 개선을 보여주었습니다. 다양한 인종과 연령대의 실제 인간 이미지뿐만 아니라 만화 스타일과 같은 도메인 외 이미지에서도 강력한 일반화 능력을 나타냅니다.

### [Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers](https://arxiv.org/abs/2501.02393)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02393.png)

Vote: 1

Authors: Markus J. Buehler

- ***What's New***: 이 논문에서는 Transformer 아키텍처를 개선하기 위해 그래프-인지(attention mechanism)를 도입하여 GNN과 언어 모델링 개념을 결합한 방법을 제안합니다. 이는 Transformer의 주의 메커니즘을 그래프 연산으로 다시 구성하며, Graph Isomorphic Networks (GIN)과 Principal Neighborhood Aggregation (PNA)을 포함한 고급 그래프 모델링 전략을 활용하여 복잡한 종속성을 포착하고 과제 전반에 걸쳐 일반화 성능을 개선합니다.
- ***Technical Details***: Graph-Aware Isomorphic Attention은 Attention 메커니즘을 그래프 연산으로 재구성하여, 그래프 신경망인 GIN과 PNA를 통합합니다. 이 방법은 다양한 활성화 함수(Activation Functions)를 통해 어텐션을 강화하고, Sparse GIN-Attention을 도입하여 사전 훈련된 모델을 최소한의 계산 비용으로 튜닝하여 그래프-인지 능력을 부여합니다. 이러한 혁신을 통해 모델은 시퀀스 및 관계 데이터 모델링을 효과적으로 수행합니다.
- ***Performance Highlights***: Graph-aware attention 메커니즘은 전통적인 어텐션을 훈련 효율 및 검증 성능에서 능가하며, Sparse GIN-Attention 튜닝은 낮은 랭크 적응(LoRA)과 같은 대안 대비 개선된 학습 역학 및 일반화 성능을 제공합니다. 실험 결과, 제안된 어텐션 메커니즘은 생물정보학, 재료 과학 및 언어 모델링 등의 분야에서 강력한 가능성을 보여줍니다.

### [ProTracker: Probabilistic Integration for Robust and Accurate Point Tracking](https://arxiv.org/abs/2501.03220)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03220.png)

Vote: 0

Authors: Zhiyang Dou, Jiahui Lei, Tingyang Zhang, Lingjie Liu, Baoquan Chen, Qingzhe Gao, Chen Wang

- ***What's New***: ProTracker는 비디오에서 임의의 점을 장기적으로 추적할 때 강인하고 정확한 성능을 제공하는 새로운 프레임워크를 제안합니다. 이 방법은 Optical Flow와 Semantic Feature로부터의 예측을 확률적으로 통합하여 Robust(강인한) 단기 및 장기 추적을 수행합니다.
- ***Technical Details***: ProTracker는 Kalman Filter에 영감을 받아 Bidirectional(양방향) Probabilistic Integration 방법을 제안하며, Optical Flow 추정과 장기적 Correspondence를 통합합니다. 필터링 단계에서는 RAFT 모델을 사용하여 거친 Optical Flow 예측을 얻고, Object-level Filtering과 Geometry-aware Feature Filtering을 통해 부정확한 예측을 제거합니다. 그런 다음, Gaussian 분포로 각각의 예측을 취급하여 최적의 포인트 예측을 진행합니다.
- ***Performance Highlights***: ProTracker는 TAP-Vid 벤치마크에서 Self-supervised 및 Unsupervised 접근 방식 중에서 최고 성능을 기록하였으며, Supervised 방법과 비교해도 경쟁력 있는 성과를 보여주었습니다. 특히 Kinetics 데이터셋에서 모든 지표에서 최상위를 차지하면서 전반적인 Tracking 성능이 향상되었음을 입증하였습니다.

### [PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides](https://arxiv.org/abs/2501.03936)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03936.png)

Vote: 7

Authors: Xinyan Guan, Jia Zheng, Hao Zheng, Le Sun, Yaojie Lu, Hao Kong, Xianpei Han, Hongyu Lin, Ben He

- ***What's New***: PPTAgent는 자동 프레젠테이션 생성 프로세스를 사람의 작업흐름을 모방한 두 단계의 편집 기반 접근 방식을 통해 개선하는 혁신적인 프레임워크입니다. PPTAgent는 참조 프레젠테이션을 분석하여 구조적 패턴과 콘텐츠 스키마를 이해한 후, 코드를 통해 슬라이드를 초안 작성하고 생성하여 일관성과 정렬을 보장합니다. 또한 세 가지 차원(콘텐츠, 디자인, 일관성)에 대해 프레젠테이션을 평가하는 프레임워크인 PPTEval을 소개합니다.
- ***Technical Details***: PPTAgent는 두 단계로 구성됩니다. 첫 번째 단계에서 참조 프레젠테이션을 분석하여 슬라이드를 군집화하고 콘텐츠 스키마를 추출합니다. 두 번째 단계에서는 문서와 분석된 참조 프레젠테이션을 기반으로 새로운 프레젠테이션을 생성합니다. 이때, 문서의 특정 섹션과 참조 슬라이드를 각각의 슬라이드에 할당하며, API를 사용해 참조 슬라이드에 대한 동적 편집을 수행합니다. PPTEval은 MLLM-as-a-judge 패러다임을 활용하며 내용, 디자인, 일관성을 평가하기 위한 포괄적인 프레임워크입니다.
- ***Performance Highlights***: PPTAgent는 PPTEval 평가에서 평균 3.67점을 기록하며, 다양한 도메인에서 97.8%의 높은 성공률을 자랑합니다. PPTAgent의 강력한 성능은 세 가지 주요 요소 덕분입니다: 콘텐츠를 변화시키는 데 집중하여 스타일링 복잡성을 회피, API의 간소화된 디자인을 통한 과제 실행 용이, 코드 인터랙션 모듈로 슬라이드 이해도와 정확한 행동 생성 능력 향상. 이러한 접근 방식은 기존의 자동 프레젠테이션 생성 방법을 크게 능가하며 다양한 분야에 걸쳐 그 강점을 입증했습니다.

### [Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback](https://arxiv.org/abs/2501.03916)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03916.png)

Vote: 3

Authors: Tao Chen, Wanli Ouyang, Botian Shi, Lei Bai, Bowen Zhou, Xiangchao Yan, Yu Qiao, Bo Zhang, Jiakang Yuan

- ***What's New***: DOLPHIN은 기존의 과학 연구 패러다임을 혁신하기 위해 제안된 첫 번째 폐회로 개방형 자동 연구 프레임워크입니다. 이 시스템은 새로운 연구 아이디어 생성, 실험 수행, 결과 피드백을 통한 고품질 연구 아이디어 생성을 지원하여 자동 과학 연구의 전 과정을 구축합니다.
- ***Technical Details***: DOLPHIN은 주어진 주제와 관련된 논문을 검색하여 새로운 아이디어를 생성하며, 이러한 아이디어는 LLMs에 의해 실험 계획이 생성되어 코드가 작성되고 디버깅됩니다. 각 실험의 결과는 자동으로 분석되어 다음 아이디어 생성에 반영됩니다. 이 과정에서 논문의 주제 관련성과 작업 속성 관련성을 기반으로 한 논문 순위 매기기 및 오류 추적 기반 디버깅 과정을 도입하여 아이디어의 질과 코드 실행률을 높였습니다.
- ***Performance Highlights***: ModelNet40, CIFAR-100, SST-2 등을 포함한 다양한 벤치마크 데이터셋에서 DOLPHIN은 기존의 방법들과 비교하여 각 분야에서 성능을 향상시키는 아이디어를 생성했습니다. 특히, 3D 분류 작업에서 DOLPHIN이 제안한 PointNet-CSR 모델은 수작업으로 설계된 최신 3D 분류법과 유사한 성능을 구현하며 연구 효율성을 크게 개선했습니다.

### [MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2501.03714)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03714.png)

Vote: 5

Authors: Jihyong Oh, Joonsoo Kim, Jun Young Jeong, Won-Sik Cheong, Sangwoon Kwak, Munchurl Kim

- ***What's New***: MoDec-GS는 전 세계적인 3D Gaussian Splatting(3DGS)의 기술적인 한계를 극복하기 위해 개발된 최첨단 프레임워크입니다. 이 프레임워크는 복잡한 글로벌 및 로컬 모션을 동적으로 포착하여 데이터 저장 요구사항을 크게 줄이면서도 고성능 렌더링 품질을 유지합니다.
- ***Technical Details***: MoDec-GS는 복잡한 동적 모션을 수집하기 위해 Global-to-Local Motion Decomposition(GLMD)을 사용합니다. Global Canonical Scaffold-GS와 Local Canonical Scaffold-GS를 결합하여 Global Anchor Deformation(GAD)을 이용한 글로벌 모션과 Local Gaussian Deformation(LGD)을 이용한 세밀한 로컬 조정을 통해 복잡한 모션을 효과적으로 처리합니다. Temporal Interval Adjustment(TIA) 기능을 도입하여 외부 모션 데이터 없이도 매우 효율적인 시간 간격 세분화를 가능하게 합니다.
- ***Performance Highlights***: MoDec-GS는 최첨단 기술들에 비해 모델 크기를 평균 70%까지 줄였음에도 불구하고 렌더링 품질을 저하시키지 않거나 오히려 개선합니다. 다양한 벤치마크 실험에서 MoDec-GS는 질감이 복잡한 동적 씬을 현저히 압축된 저장 크기로도 높은 정확도로 재구성할 수 있음을 입증했습니다.

### [Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers](https://arxiv.org/abs/2501.03931)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03931.png)

Vote: 4

Authors: Bohao Peng, Bin Xia, Yaoyang Liu, Yuechen Zhang, Jiaya Jia, Eric Lo, Zexin Yan

- ***What's New***: Magic Mirror는 Video Diffusion Transformers 기반으로 ID-보존형 비디오 생성(ID-Preserved Video Generation)을 최초로 구현한 프레임워크로, 영화 수준의 품질과 동적인 움직임을 유지합니다. 기존 텍스트-비디오 생성 방법들이 ID 일관성을 유지하는 데 어려움을 겪는 문제를 해결하려고 합니다.
- ***Technical Details***: Magic Mirror는 Video Diffusion Transformers를 기반으로 ID 및 구조적 특성을 잡아내는 이중 브랜치(facial feature extractor)와, ID 통합을 위한 경량 복합 모달 어댑터(cross-modal adapter)를 도입했습니다. 또, 합성된 ID 쌍과 비디오 데이터를 결합한 2단계 훈련 전략을 사용합니다.
- ***Performance Highlights***: Magic Mirror는 다양한 지표에서 기존 방법들보다 뛰어난 성능을 발휘하며, 사람별 맞춤화를 하지 않고도 비디오 DiT 기반 맞춤형 비디오 생성을 최초로 달성했습니다. 사용자 평가를 통한 전체적인 선호 점수에서도 Magic Mirror는 다른 방법들보다 높았습니다.

### [Cosmos World Foundation Model Platform for Physical AI](https://arxiv.org/abs/2501.03575)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03575.png)

Vote: 25

Authors: Stella Shi, Arsalan Mousavian, Xiaowei Ren, Alice Luo, Kaichun Mo, Yu Zeng, Zeeshan Patel, Heng Wang, Bartosz Stefaniak, Daniel Dworakowski, Wei Yang, Haoxiang Wang, Lindsey Pavao, Ting-Chun Wang, Shitao Tang, Arslan Ali, Xiaohui Zeng, Hanzi Mao, Laura Leal-Taixe, Michele Fenzi, Shiyi Lan, Seungjun Nah, Qingqing Zhao, Jibin Varghese, Wei-Cheng Tseng, Yongxin Chen, Sanja Fidler, Tiffany Cai, Ming-Yu Liu, Francesco Ferroni, Erik Barker, Xinyue Wei, Yifan Ding, Despoina Paschalidou, Vasanth Rao Naik Sabavat, Jingyi Jin, Prithvijit Chattopadhyay, Maciej Bala, Jing Zhang, Dieter Fox, Yunhao Ge, Huan Ling, Fitsum Reda, Sriharsha Niverty, Qianli Ma, Yogesh Balaji, Jiashu Xu, Chen-Hsuan Lin, Lyne Tchapmi, Siddharth Gururani, Artur Zolkowski, Hao Wang, Niket Agarwal, Yuxuan Zhang, Yin Cui, Ethan He, David Page, Jay Zhangjie Wu, Seung Wook Kim, Anqi Li, NVIDIA, Jiaojiao Fan, Lin Yen-Chen, Jiahui Huang, Gergely Klár, Morteza Ramezanali, Zhaoshuo Li, Ed Schmerling, Jacob Huffman, Przemek Tredak, Pooya Jannaty, Tsung-Yi Lin, Jinwei Gu, Songwei Ge, Qinsheng Zhang, Fangyin Wei, Grace Lam, Xian Liu

- ***What's New***: NVIDIA의 Cosmos World Foundation Model Platform은 AI가 물리적 환경을 안전하게 시뮬레이션할 수 있도록 하는 World Foundation Model(WFM)을 제안합니다. 이 플랫폼은 물리적 인공지능(Physical AI) 개발자에게 디지털 쌍둥이 모델을 제공하여 다양한 응용 프로그램을 지원할 수 있도록 설계되었습니다.
- ***Technical Details***: Cosmos 플랫폼은 비디오 큐레이션 파이프라인, 사전 학습된 세계 기초 모델(WFM), 사후 학습 예제, 비디오 토크나이저로 구성되어 있습니다. 사전 학습된 WFM은 다양한 비디오 데이터셋을 사용해 일반적인 물리적 시뮬레이션을 가능하게 하며, 사후 학습을 통해 특정 물리적 AI 환경에 맞춤화할 수 있습니다. 토크나이저는 비디오 데이터를 효율적으로 처리하여 높은 스루풋을 제공합니다.
- ***Performance Highlights***: Cosmos 플랫폼에서 사전 학습된 WFM은 새로운 시뮬레이션 및 물리적 AI 설계에 사용될 수 있으며, 3D 일관성과 물리적 정확성에서 뛰어난 성능을 보여줍니다. 최근의 비디오 생성 모델과의 비교에서 높은 품질과 정확성을 유지하면서 3D 세계의 시뮬레이션을 가능하게 합니다.

### [Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos](https://arxiv.org/abs/2501.04001)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04001.png)

Vote: 16

Authors: Xiangtai Li, Yunhai Tong, Jiashi Feng, Shunping Ji, Lu Qi, Zilong Huang, Haobo Yuan, Shilin Xu, Ming-Hsuan Yang, Tao Zhang

- ***What's New***: 이 논문은 Sa2VA라는 새로운 모델을 제시하며, 이는 시각적 기반이 되는 중요한 두 모델 SAM-2와 LLaVA를 결합하여 이미지와 비디오에 대한 밀도 있는 근거 있는 이해를 제공하는 것을 목표로 합니다. Sa2VA는 참조 세분화(referring segmentation), 비주얼 질문 응답(VQA), 근거 있는 대화 생성(GCG)과 같은 작업을 지원하며, Ref-SAV라는 새로운 데이터셋을 도입하여 모델의 성능을 향상시키고 평가합니다.
- ***Technical Details***: Sa2VA는 SAM-2와 LLaVA 모델을 결합하여 작업을 단일의 시각 지시 조정(one-shot instruction tuning) 형태로 재구성합니다. 이 모델은 입력된 이미지, 비디오, 시각적 프롬프트를 LLM 토큰 공간으로 통합하며, '[SEG]'라는 특수 토큰을 활용하여 SAM-2의 데코더로 세그먼트 마스크를 생성합니다. 또한, Ref-SAV라는 72k 이상의 객체 표현이 포함된 데이터셋을 도입하여 모델을 자동으로 라벨링된 데이터로 학습시킵니다.
- ***Performance Highlights***: Sa2VA는 다양한 이미지 및 비디오 참조 세분화 데이터셋에서 최첨단 성능을 기록하며, MMLM과 결합하여 비단순히 참조 세분화뿐만 아니라 이미지/비디오 대화 및 근거 있는 캡션 생성에서도 강력한 능력을 보여줍니다. 특히, Ref-SAV 데이터셋에서 기존 방법보다 15% 이상 높은 성능을 보이고, 복잡한 실제 응용 프로그램에서의 잠재력을 강조합니다.

### [MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models](https://arxiv.org/abs/2501.02955)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02955.png)

Vote: 30

Authors: Yean Cheng, Yuxiao Dong, Xiaotao Gu, Jie Tang, Zhuoyi Yang, Wenyi Hong, Lefan Wang, Weihan Wang, Shiyu Huang

- ***What's New***: MotionBench는 비전 언어 모델(Vision Language Models; VLMs)을 평가하는 포괄적인 벤치마크로, 세분화된 동작 이해를 중심으로 설계되었습니다. 기존 벤치마크들이 동작 수준의 이해에 대한 평가를 충분히 다루지 못한 공백을 메우기 위해 개발되었습니다.
- ***Technical Details***: MotionBench는 다양한 출처에서 수집한 데이터를 포함하여 실세계 비디오 콘텐츠의 다양한 표현을 보장합니다. 주어진 시퀀스 길이 내에서 VLM의 압축 성능을 개선하기 위해 새로운 Through-Encoder(TE) Fusion 방법을 제안하며, 이를 통해 인접한 프레임의 깊은 복합 처리가 가능합니다.
- ***Performance Highlights***: 실험 결과는 현존 VLM들이 세분화된 동작을 이해하는 데 미흡함을 보여줍니다. TE Fusion을 적용한 GLM-4V-9B 모델은 동작 이해에서 최첨단 결과를 달성하며, 높은 압축 시나리오에서 특히 유리함을 입증했습니다. 주요 모델들은 MotionBench에서 평균 60% 이하의 정확도를 기록했습니다.

### [LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://arxiv.org/abs/2501.03895)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03895.png)

Vote: 19

Authors: Zhe Yang, Shaolei Zhang, Yang Feng, Qingkai Fang

- ***What's New***: LLaVA-Mini는 하나의 비전 토큰(Vision Token)을 활용하여 대형 멀티모달 모델(Large Multimodal Models; LMMs)의 효율성을 대폭 개선시키는 새로운 접근을 제안합니다. 이는 시각 토큰의 수를 극단적으로 줄이면서도 이미지와 동영상 이해 성능을 유지 혹은 향상시킵니다.
- ***Technical Details***: LLaVA-Mini는 사전 연합 모듈(Modality Pre-fusion)을 도입하여 시각 정보를 텍스트 토큰에 사전에 융합함으로써 LLM 백본에 공급되는 시각 토큰을 하나로 줄입니다. 이 모델은 CLIP Vision encoder와 대응되는 LLM 구조에 Compression 모듈을 추가하여 효율성을 높입니다.
- ***Performance Highlights***: LLaVA-Mini는 11가지 이미지 기반 및 7가지 동영상 기반 벤치마크에서 기존의 LLaVA-v1.5보다 적은 비전 토큰을 사용하면서도 더 높은 효율성을 기록했습니다. 이러한 접근으로 77%의 FLOPs를 절감하고, RTX 3090 (24GB)에서 10,000프레임 이상의 긴 동영상 처리도 가능하게 했습니다. 이는 일반적인 LMMs와 비교할 때 월등한 효율성을 제공합니다.

