## Daily Papers (2025-01-15)

### [MiniMax-01: Scaling Foundation Models with Lightning Attention](https://arxiv.org/abs/2501.08313)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08313.png)

Vote: 193

Authors: Kecheng Xiao, Peikai Huang, Guojun Zhang, Mozhi Zhang, Xiaodong Han, Yongyi Hu, Meizhi Ju, Cheng Zhu, Sichen Li, Weixuan Sun, Yingjie Zhu, Jingtao Han, Songquan Zhu, Xinjie Zhang, Jiadai Zhu, Junbin Xie, Yunan Huang, Junjie Yan, Bo Yang, Lin Zheng, Liheng Feng, Yuhao Li, Pengyu Zhao, Pengfei Li, Congchao Guo, Weigao Sun, Yuxin Mao, Yipeng Zhou, Yuanxiang Fan, Zhen Qin, Pengcheng Niu, Kaishun Zhang, Kexi Kang, Le Han, Leyang Wang, Jiaqi Zhuang, Zhihang Yu, Zhuo Jiang, Shengmin Shi, Qidi Xu, Yue Yu, Qin Wang, Qiexiang Wang, Yunzhi Xu, Yan Gong, Zekang Li, Mingyuan Chi, Xiangjun Song, Yiran Zhong, Yunpeng Huang, Zehan Li, Bangwei Gong, Tianrun Liang, MiniMax, Chang Liu, Junhao Xu, Shuqi Yu, Jin Zhu, Yunji Li, Zewen Ying, Tao Huang, Dong Li, Enwei Jiao, Xun Zou, Xuyang Shen, Jingyang Li, Zijia Wu, Qi Yang, Wenkai Li, Zhaoyang Cong, Long Xing, Yufeng Yang, Aonian Li, Haohai Sun, Chunhao Zhang, Ruitao Leng, Jiayuan Song, Zhenhua Fan, Zewei Tao, Lianfei Yu, Xiao Su, Xinzhu Hou, Weiyu Cheng, Houze Dong, Linbo Chai, Da Chen, Qiuhui Li, Boji Shan, Gengxin Li, Xu Min

- ***What's New***: MiniMax-01 시리즈는 MiniMax-Text-01과 MiniMax-VL-01을 포함하며, 기존 최고 성능 모델과 유사한 능력을 가지면서 더욱 긴 콘텍스트를 효율적으로 처리할 수 있는 능력을 제공합니다. 특히 Lightning Attention과 32개의 전문가로 구성된 Mixture of Experts (MoE) 구조를 통합하여 4560억 개의 총 파라미터를 가진 모델을 구축했습니다.
- ***Technical Details***: MiniMax-01은 Lightning Attention을 통해 모델의 복잡도를 줄이며, 수백억 개의 파라미터를 가진 모델의 훈련 및 추론을 효율적으로 수행할 수 있도록 최적화된 병렬 전략과 통신 기술을 사용합니다. 특히 MiniMax-Text-01은 훈련 시 최대 100만 토큰의 콘텍스트 윈도우를 처리할 수 있으며 추론 시 400만 토큰까지 확장할 수 있습니다. MiniMax-VL-01은 5120억 개의 비전-언어 토큰을 사용하여 추가 훈련을 거쳤습니다.
- ***Performance Highlights***: MiniMax-Text-01 및 MiniMax-VL-01 모델은 GPT-4o와 Claude-3.5-Sonnet 같은 최신 상용 모델과 유사한 성능을 보여주면서도 20-32배 긴 콘텍스트 윈도우를 지원합니다. 이는 다양한 벤치마크에서 확인되며, 특히 Long Context RULER 벤치마크에서 우수한 성능을 나타냈습니다. 공공 벤치마크 외에 실사용 시나리오에서 높은 성능을 보여줍니다.

### [MangaNinja: Line Art Colorization with Precise Reference Following](https://arxiv.org/abs/2501.08332)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08332.png)

Vote: 31

Authors: Hao Ouyang, Zhiheng Liu, Jie Xiao, Yu Liu, Qifeng Chen, Ping Luo, Kai Zhu, Xi Chen, Ka Leong Cheng, Yujun Shen

- ***What's New***: MangaNinja는 참조에 기반한 선화 채색 방법으로, 참조색 이미지와 대상 선화 이미지 간의 정밀한 일치성과 사용자의 정밀한 색상 매칭을 위한 포인트 기반 제어(Point-driven Control)의 새로운 설계를 도입했습니다.
- ***Technical Details***: MangaNinja는 듀얼-브랜치 구조와 패치 셔플링 모듈(Patch Shuffling Module)을 사용해 참조 이미지와 선화 이미지 간의 지역적 매칭 능력을 학습합니다. PointNet을 통해 사용자가 정의한 포인트를 기반으로 세밀한 제어를 가능하게 하며, 이를 통해 선화의 어색함을 줄이고 매칭 능력을 극대화합니다. 또한, 훈련 데이터로는 애니메이션 비디오 프레임을 사용하여 자연적 시맨틱 일치성을 활용합니다.
- ***Performance Highlights***: MangaNinja는 기존 방법들에 비해 더 나은 색상 충실도와 정체성 보존을 달성하여 복잡한 채색 작업에서도 높은 품질의 결과를 생성했습니다. 이는 생성된 이미지와의 시맨틱 이미지 유사도(CLIP, DINO)와 컬러링 정확도를 측정하여 입증되었습니다.

### [Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models](https://arxiv.org/abs/2501.06751)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06751.png)

Vote: 27

Authors: Hadas Orgad, Ido Galil, Rinon Gal, Yoad Tewel, Michael Toker, Yonatan Belinkov, Gal Chechik

- ***What's New***: Padding Tone 연구는 텍스트-이미지 변환(T2I) 모델에서 패딩 토큰(Padding Tokens)이 이미지 생성에 미치는 영향을 처음으로 분석한 데 중점을 둡니다. 이 연구는 패딩 토큰이 텍스트 인코딩, 확산 과정에서의 모델 출력에 어떻게 작용하는지 검토하며, 무시되는 시나리오까지 총 3가지 경우를 밝혀냅니다. 이 결과는 향후 T2I 모델의 설계 및 훈련 방식에 대한 의미 있는 통찰을 제공합니다.
- ***Technical Details***: 이 연구에서는 텍스트 인코더 출력의 개입(Intervention in the Text Encoder Output; ITE)과 확산 과정에서의 개입(Intervention in the Diffusion Process; IDP)이라는 두 가지 방법을 개발했습니다. 이러한 방법은 특정 입력 또는 중간 표현을 변경하고 그 결과를 관찰하는 인과매개분석(causal mediation analysis)을 기반으로 하며, 이를 통해 패딩 토큰이 이미지 생성 과정에 미치는 영향을 효과적으로 평가합니다.
- ***Performance Highlights***: 실험에서는 총 6개의 T2I 모델을 분석했으며, 텍스트 인코더가 얼음 상태로 훈련되는 경우 패딩 토큰이 무시되지만, 학습 또는 미세조정이 된 경우에는 의미 있는 정보를 인코드하는 것으로 나타났습니다. 특히 FLUX 같은 모델에서는 확산 과정에서 패딩 토큰이 정보를 저장하고 기억하는 '레지스터' 역할을 하여 이미지의 시각적 정보를 보완하는 것으로 관찰되었습니다.

### [Diffusion Adversarial Post-Training for One-Step Video Generation](https://arxiv.org/abs/2501.08316)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08316.png)

Vote: 19

Authors: Lu Jiang, Yuxi Ren, Xuefeng Xiao, Shanchuan Lin, Ceyuan Yang, Xin Xia

- ***What's New***: 이 논문에서는 Diffusion 모델을 활용한 이미지 및 비디오 생성을 단일 단계(one-step)로 가속화하는 새로운 방법인 Adversarial Post-Training(APT)을 소개합니다. 이는 기존의 Distillation과는 달리 실제 데이터를 기반으로 한 적대적 훈련을 통해 수행됩니다.
- ***Technical Details***: APT는 사전 훈련된 DiT(Diffusion Transformer) 모델을 초기화로 사용하며, 적대적 훈련 목표를 설정합니다. 훈련 안정성을 개선하기 위해 디스크리미네이터에 대한 몇 가지 주요 설계를 도입하여 R1 정규화 손실을 근사화하였습니다. 이 방법은 비디오와 이미지 모두를 단일 평가 단계에서 생성할 수 있습니다.
- ***Performance Highlights***: 제안된 APT 모델은 1280×720 해상도, 24fps의 2초짜리 비디오를 단일 단계로 실시간 생성할 수 있으며, 이는 이전 최첨단 모델들에 비해 탁월한 성능을 보여줍니다. 특히, 시각적 사실성과 세부사항 면에서 우수한 평가를 받았지만, 구조적 일관성과 텍스트 정렬에서는 일부 제한점이 발견되었습니다.

### [A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following](https://arxiv.org/abs/2501.08187)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08187.png)

Vote: 19

Authors: Kangwei Liu, Penghui Yang, Jingyang Qian, Yin Fang, Ningyu Zhang, Xinle Deng, Huajun Chen, Xiaohui Fan

- ***What's New***: InstructCell은 생명과학 연구에서 단일 세포 분석을 위해 자연어를 매개로 하여 더욱 직접적이고 유연한 분석을 가능하게 하는 멀티모달 AI 코파일럿입니다. 다양한 조직과 종에서 수집한 단일 세포 RNA 시퀀싱(scRNA-seq) 데이터와 텍스트 지침을 쌍으로 구성하여 다중 모달 지시 데이터세트를 구축하였습니다.
- ***Technical Details***: InstructCell은 Q-Former 모듈을 포함한 멀티모달 셀 언어 모델 아키텍처를 기반으로 합니다. 이 모델은 단일 세포의 유전자 발현 데이터를 임베딩하고, 백본으로 사전 학습된 언어 모델(LM), 그리고 세포 재구성 블록을 포함하여 두 가지 모달리티를 통합하여 처리할 수 있습니다. 또한, 다양한 생물학적 속성을 자연어 지침으로 변환하여 모델이 더욱 상호작용적으로 사무환경을 조정할 수 있게 합니다.
- ***Performance Highlights***: InstructCell은 반복적인 생물학적 맥락을 보존하면서도 다양한 연구 상황에 적응함으로써 기존의 단일 세포 모델의 성능을 지속적으로 초과하거나 만족시킵니다. 실험 결과는 대조적인 생물학적 구조를 정확하게 복제하고 세포 위치를 유지하면서 생물학적 통찰력을 발견할 수 있는 능력을 강조합니다.

### [FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors](https://arxiv.org/abs/2501.08225)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08225.png)

Vote: 12

Authors: Yabo Zhang, Hui Li, Wangmeng Zuo, Hang Xu, Yihan Zeng, Xinpeng Zhou

- ***What's New***: FramePainter는 기존의 상호작용 이미지 편집 기법을 영상 생성 문제로 재구성하여 비디오 확산 모델(Video Diffusion Models)의 매끄러운 priors을 활용한 새로운 접근 방식을 제안합니다. 이 방법은 데이터 및 계산 요구사항을 감소시키고, 뛰어난 일반화 성능을 제공합니다.
- ***Technical Details***: FramePainter는 Stable Video Diffusion(SVD)로 초기화되며, 가벼운 sparse control encoder를 사용하여 편집 신호를 U-Net에 주입합니다. Temporal attention의 한계를 극복하기 위해, 우리는 매칭 어텐션을 도입하여 편집된 이미지 토큰들과 소스 이미지 토큰들 사이의 밀도 있는 correspondence를 촉진합니다.
- ***Performance Highlights***: FramePainter는 높은 시각적 일관성과 플로우의 보존성을 유지하면서, 비용이 드는 훈련 과정 없이 다양한 편집 신호에 대해 우수한 성능을 발휘합니다. 특히, 현실 비디오에 존재하지 않는 시나리오에 대한 뛰어난 일반화 능력을 보여줍니다.

### [Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks](https://arxiv.org/abs/2501.08326)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08326.png)

Vote: 11

Authors: Miran Heo, Seon Joo Kim, Subhashree Radhakrishnan, Ryo Hachiuma, Min-Hung Chen, Sifei Liu, De-An Huang, Yu-Chiang Frank Wang

- ***What's New***: Omni-RGPT는 이미지와 비디오에서의 지역 수준(region-level) 이해를 통합하기 위해 토큰 마크(Token Marks)를 도입한 멀티모달 모델입니다. 이 모델은 시공간 차원에서 일관성을 유지하기 위해 사전 정의된 토큰을 사용해 목표 지역을 명확히 하고, 새로운 대규모 지역 수준 비디오 데이터셋(RegVID-300k)을 도입하여 성능을 최적화합니다.
- ***Technical Details***: Omni-RGPT는 범위 프롬프트(region prompts)와 텍스트 프롬프트를 결합하여 시각적 피처 공간에서 목표 지역을 강조하는 Token Mark를 도입했습니다. 이 토큰들은 섹션 프롬프트(예: 상자, 마스크)를 사용해 공간 지역에 직접 삽입되며 텍스트 프롬프트에도 포함되어 시각 및 텍스트 토큰 간의 직접적인 연결을 구축합니다. 또한, 트랙릿(tracklet)을 필요로 하지 않는 비디오 이해를 지원하기 위해 auxiliary task를 포함하여 비디오 전반의 지역 해석의 안정성을 강화했습니다.
- ***Performance Highlights***: Omni-RGPT는 이미지 및 비디오 기반 영리 추론(commonsense reasoning)의 벤치마크에서 최첨단 성능을 달성했습니다. 특히 이미지 기반(VCR) 및 비디오 기반(Causal-VidQA) 작업에서 우수한 성능을 보였으며, 구체적인 설명 및 지칭 표현 이해 작업에서 강한 성능을 입증했습니다.

### [Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens](https://arxiv.org/abs/2501.07730)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.07730.png)

Vote: 10

Authors: Chenglin Yang, Xiaohui Shen, Suha Kwak, Liang-Chieh Chen, Dongwon Kim, Ju He, Qihang Yu

- ***What's New***: 이 연구에서는 텍스트-이미지 생성 모델의 민주화를 목표로 하여 새로운 Text-Aware Transformer 기반의 1차원 토크나이저(TA-TiTok)를 소개했습니다. 이 토크나이저는 텍스트 정보와 통합하여 학습 수렴을 가속화하고 성능을 향상시킵니다. 또한, 공개 데이터만을 사용해 훈련된 Masked Generative Models(MaskGen) 가족을 도입하여 프라이빗 데이터로 훈련된 모델과 비슷한 성능을 달성합니다.
- ***Technical Details***: TA-TiTok는 1차원 토큰화 기술을 사용하여 이미지의 누락 정보를 복원하는 과정에서 텍스트 정보를 통합하여 텍스트와의 의미론적 정렬을 강화합니다. TA-TiTok의 주요 개선점은 효율적인 1 스테이지 학습 절차로의 변환과 연속적인 VAE 표현으로의 확장이며, 이는 다량의 데이터셋에도 적합하게 설계되었습니다. MaskGen은 텍스트의 CLIP 임베딩과 이미지 토큰을 결합하여, 각 이미지의 연속 및 불연속적인 토큰들을 출력하는 모델로, 확산 손실 (diffusion loss)를 사용하여 학습됩니다.
- ***Performance Highlights***: 공개 데이터만을 사용했음에도 불구하고, MaskGen은 다양한 텍스트-이미지 생성 벤치마크에서 강력한 성능을 보여줍니다. 예를 들어, MJHQ-30K 기준으로 MaskGen-L은 FID 7.74를 기록하며, PixArt-α보다 적은 자원으로 더 나은 성능을 구현했습니다. MaskGen-XL은 6.53의 FID를 달성하며 고성능의 텍스트-이미지 합성 성능을 지니고 있습니다.

### [PokerBench: Training Large Language Models to become Professional Poker Players](https://arxiv.org/abs/2501.08328)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08328.png)

Vote: 9

Authors: Richard Zhuang, Richard Yang, Zhengyu Li, Akshat Gupta, Gopala Anumanchipalli, Aniket Rahane

- ***What's New***: POKERBENCH는 대형 언어 모델(LLMs)의 포커 능력을 평가하기 위한 새로운 벤치마크로, 기존의 NLP 작업을 넘어 복잡한 전략 게임 평가의 새로운 도전을 제시합니다. 포커의 불확실성 요소와 전략적 결정을 반영한 11,000개의 시나리오를 포함하고 있습니다.
- ***Technical Details***: POKERBENCH는 프로 포커 플레이어와 협력하여 개발된 11,000개의 포커 시나리오를 포함하며, 게임 이론에 기초한 최적의 플레이 방법을 모델이 수행하도록 평가합니다. 데이터셋은 1,000개의 프리플랍 시나리오와 10,000개의 포스트플랍 시나리오로 구성되어 있으며, 학습 데이터 및 코드가 공개되어 있습니다.
- ***Performance Highlights***: GPT-4는 POKERBENCH에서 53.55%의 정확도로 가장 높은 성능을 보였지만, 최적의 포커 전략을 완벽히 구현하지 못했습니다. 파인튜닝 이후, LLAMA-3-8B 모델은 성능이 크게 개선되어 GPT-4보다 뛰어난 결과를 보여주었습니다. 각기 다른 체크포인트 플레이어 간의 50,000 핸드 시뮬레이션을 통해 POKERBENCH 점수가 실제 포커 성능의 향상에 기여함을 확인했습니다.

### [3DIS-FLUX: simple and efficient multi-instance generation with DiT rendering](https://arxiv.org/abs/2501.05131)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.05131.png)

Vote: 9

Authors: Ji Xie, Dewei Zhou, Yi Yang, Zongxin Yang

- ***What's New***: 3DIS-FLUX는 3DIS의 확장판으로 FLUX 모델을 이용하여 보다 뛰어난 렌더링 기능을 제공합니다. FLUX 모델을 통해 세세한 특성을 정확하게 렌더링할 수 있으며, 기존 3DIS와 비교해 성능과 이미지 품질에서 현저히 향상되었습니다.
- ***Technical Details***: 3DIS-FLUX는 장면 깊이 맵(scene depth map)을 생성한 후, FLUX.1-Depth-dev 모델을 통해 깊이 맵을 기반으로 이미지를 생성합니다. 이 과정에서 FLUX의 공동 주의(Joint Attention) 메커니즘을 이용하여 각 인스턴스의 상세 속성을 정확하게 제어합니다. 주목할 점은 Attention Controller를 통해 각 인스턴스의 이미지 토큰이 해당 텍스트 토큰에만 주의를 기울이게 하는 것입니다.
- ***Performance Highlights***: COCO-MIG 벤치마크에서 3DIS-FLUX는 이전의 3DIS-SDXL 대비 인스턴스 성공률(Instance Success Ratio; ISR)에서 6.9% 향상을 기록했습니다. 또한, SOTA(State-Of-The-Art) 어댑터 기반 방법과 비교해도 12.4% 높은 ISR을 보였으며, FLUX 모델을 사용하여 더 우수한 이미지 품질을 달성했습니다.

### [Enhancing Automated Interpretability with Output-Centric Feature Descriptions](https://arxiv.org/abs/2501.08319)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08319.png)

Vote: 7

Authors: Atticus Geiger, Roy Mayan, Chen Agassy, Mor Geva, Yoav Gur-Arieh

- ***What's New***: 본 연구는 출력 중심(feature-centric) 기법을 활용하여 대형 언어 모델(LLMs) 내 피처(feature)의 자동 해석 가능성을 향상시키는 방법을 제안합니다. 새로운 피처 설명 생성 방법을 통해 기존 입력 중심(input-centric)의 한계를 극복하고, 피처가 출력에 미치는 인과적 영향을 더욱 잘 반영합니다.
- ***Technical Details***: 두 가지 출력 중심의 새로운 설명 생성 방법을 제안했습니다. 첫 번째는 '어휘 투영(VocabProj)' 방법으로, 피처를 모델의 어휘 공간으로 투영하여 피처 벡터를 아래로 해석하는 것입니다. 두 번째는 '토큰 변경(TokenChange)' 방법으로, 피처가 증폭될 때 출력 토큰의 확률이 어떻게 변하는지를 추적하여 설명을 생성합니다. 이 두 가지 방법은 계산 비용이 적고, 입력 중심의 '최대 활성화(MaxAct)' 방식보다 효율적입니다.
- ***Performance Highlights***: 실험 결과, 입력-출력 중심 방법을 결합하면 설명의 정확성이 향상됩니다. 특히, '최대 활성화' 방식보다 출력 중심의 피처 설명이 모델의 출력을 보다 효과적으로 포착하였고, 이는 설명이 피처의 인과적 역할을 더 잘 반영했음을 시사합니다. 이러한 결과는 피처 해석의 자동화 파이프라인에 있어 출력 중심 방법이 유용할 수 있음을 보여줍니다.

### [OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training](https://arxiv.org/abs/2501.08197)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08197.png)

Vote: 5

Authors: Yijiong Yu, Ji Pei, Ran Chen, Zekun Wang, Wei Wang, Ziyun Dai

- ***What's New***: OpenCSG 중국어 코퍼스는 대형 언어 모델(LLM)의 사전 학습을 위해 고품질 중국어 데이터셋으로 구성된 새로운 시리즈입니다. 이 코퍼스는 다양한 도메인에 걸쳐 양질의 텍스트와 확장 가능한 데이터 큐레이션 프로세스로 중국어 LLM의 성능을 향상시키는 데 중점을 두고 있습니다.
- ***Technical Details***: OpenCSG 중국어 코퍼스에는 Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, Smoltalk-chinese의 네 가지 데이터셋이 포함되어 있습니다. Fineweb-edu 데이터셋은 교육적 가치를 강조하는 필터링을 통해 다양한 중국 웹 출처에서 파생된 고품질 콘텐츠에 중점을 두고 있으며, Cosmopedia-chinese는 교과서 스타일의 합성 데이터를 제공하여 지식 집약적 학습을 지원합니다. Smoltalk-chinese는 대화 형식의 데이터를 다양하게 제공하여 스타일적 발전을 강조하며, 자동화된 스코어링 및 중복 제거 프로세스를 통합하고 있습니다.
- ***Performance Highlights***: 제안된 데이터셋으로 학습된 모델은 주요한 높은 성능 개선을 보여줍니다. Fineweb-Edu-Chinese 데이터셋은 사전 학습 시 높은 교육 콘텐츠를 통해 모델의 효율성과 성능을 크게 향상시킵니다. Cosmopedia-Chinese는 합성 텍스트로 잘 구성된 응답을 생성하여 유용성을 강조하지만, 벤치마크에서 큰 향상을 보이진 않았습니다. Smoltalk-Chinese는 다양한 대화 데이터를 포함하여 사용자의 지시와 일치하는 모델 행동을 조정하고 다양한 지표에서 강력한 개선을 보여주었습니다.

### [Potential and Perils of Large Language Models as Judges of Unstructured Textual Data](https://arxiv.org/abs/2501.08167)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08167.png)

Vote: 5

Authors: Elizabeth Conjar, Satya Kapoor, Rewina Bedemariam, David Theil, Natalie Perez, Alex Gil, Aman Chadha, Ikkei Itoku, Naumaan Nayyar, Sreyoshi Bhaduri

- ***What's New***: 이 연구는 대형 언어 모델(Large Language Models; LLMs)을 다른 LLM이 생성한 요약의 주제적 정렬을 평가하는 평가자로 사용하는 방법의 효과를 분석했습니다. Anthropic Claude 모델을 통해 오픈형 설문 응답의 주제적 요약을 생성하고, Amazon의 Titan Express, Nova Pro, Meta의 Llama를 평가자로 사용했습니다. 이는 전통적인 인간 중심 평가 방법의 대안으로 제안되었습니다.
- ***Technical Details***: LLM-as-judge 접근법은 한 AI 모델의 출력물을 다른 LLM에 입력하여 평가하는 방법론을 채택합니다. 이 연구에서는 ‘주제적 정렬’을 평가하는 평가 기준을 설정하여 LLM이 인간의 판단을 얼마나 잘 복제할 수 있는지를 분석했습니다. 평가 지표로 Cohen's kappa, Spearman's rho, Krippendorff's alpha를 이용하여 LLM-as-judge 모델과 인간 평가 간의 신뢰도를 검증했습니다.
- ***Performance Highlights***: 인간 평가자와 LLM 간의 일치도를 분석한 결과, Sonnet 3.5가 Cohen's kappa에서 0.44로 가장 높은 일치도를 보였으며, 이는 적절한 수준의 일치도를 나타냅니다. LLM과 인간 간의 평균 일치도는 76%~79%의 범위였으며, 모델 간의 일치도는 더욱 높았습니다(예: Claude 2.1 vs Titan Express에서 91% 일치). 이러한 결과는 단일 모델뿐만 아니라 여러 모델 간의 평가 결과에 대한 신뢰성을 보여줍니다.

### [HALoGEN: Fantastic LLM Hallucinations and Where to Find Them](https://arxiv.org/abs/2501.08292)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08292.png)

Vote: 5

Authors: David Wadden, Abhilasha Ravichander, Yejin Choi, Shrusti Ghela

- ***What's New***: HALoGEN은 생성형 대형 언어 모델(LLM; Large Language Models)의 허위 정보 탐지를 위한 처음이자 포괄적인 벤치마크입니다. 이는 프로그래밍, 과학적 인용, 요약 등 9개의 도메인을 아우르는 10,923개의 프롬프트와 각각의 사용 사례에 대해 생성된 내용을 원자 단위로 분해하고 신뢰할 수 있는 지식 소스를 통해 검증하는 자동 검증기를 포함합니다.
- ***Technical Details***: HALoGEN은 모델의 생성 내용을 원자 단위로 분해하여 각 단위가 사실에 기반한 것인지 고품질의 지식 소스를 통해 검증합니다. 실험은 14개의 LLM을 대상으로 약 150,000개의 생성을 평가했으며, 각 사용 사례마다 자동 검증기를 사용하여 끝없는 생성 문장을 조각화하고, 각 조각을 외부 도구, 프로그램, LLM 기반 분류기를 통해 사실성을 검증합니다.
- ***Performance Highlights***: 전반적으로 가장 성능이 좋은 모델들도 최대 86%에 이르는 허위 정보가 포함된 원자 정보를 생성했음을 발견했습니다. GPT-4와 같은 모델은 응답 기반 작업에서 높은 사실성을 보였지만 일부 영역에서는 여전히 잘못된 정보를 생성했습니다. 다양한 도메인에서 일관되지 않은 모델의 거짓 정보 발생 패턴은 다양한 도메인 벤치마크 필요성을 강조합니다.

### [Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding](https://arxiv.org/abs/2501.07888)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.07888.png)

Vote: 5

Authors: Yuan Lin, Yuchen Zhang, Haomiao Sun, Jiawei Wang, Liping Yuan

- ***What's New***: Tarsier2는 최첨단 대규모 시각-언어 모델(Large Vision-Language Models; LVLM)로, 자세하고 정확한 비디오 설명을 생성하는 동시에 뛰어난 일반 비디오 이해 능력을 보입니다. Tarsier2는 11M 비디오-텍스트 쌍에서 40M 쌍으로의 훈련 데이터 확장, 감독된 미세 조정 시 세분화된 시간적 정렬 수행, DPO 훈련을 통한 최적화를 위해 모델 기반 샘플링을 통한 데이터 생성의 3가지 주요 업그레이드를 통해 중요한 발전을 이뤄냈습니다.
- ***Technical Details***: Tarsier2의 훈련은 사전 훈련, SFT(감독된 미세 조정), 강화 학습(RL)의 세 단계로 구성됩니다. 사전 훈련 데이터는 40M 비디오-텍스트 쌍으로 확대되었고, 세분화된 시계열 정렬이 있는 데이터셋을 구축해 모델의 시간적 정렬 감독을 강화했습니다. DPO(Direct Preference Optimization) 훈련은 자동 생성된 성향 데이터를 사용하여 비디오 설명의 품질을 향상시켰습니다.
- ***Performance Highlights***: Tarsier2-7B는 DREAM-1K 벤치마크에서 GPT-4o보다 2.8%, Gemini-1.5-Pro보다 5.8% 더 나은 F1 성능을 보였습니다. 인간 측면 평가에서 Tarsier2-7B는 GPT-4o 대비 8.6%, Gemini-1.5-Pro 대비 24.9% 성능 우위를 보여줍니다. 15개의 공용 벤치마크에서 새로운 SOTA를 세웠으며, 이를 통해 Tarsier2의 견고한 일반적 비전-언어 모델로서의 다재다능함이 입증되었습니다.

### [AfriHate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for African Languages](https://arxiv.org/abs/2501.08284)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08284.png)

Vote: 3

Authors: Samuel Rutunda, Nelson Odhiambo Onyango, Seid Muhie Yimam, David Ifeoluwa Adelani, Rooweither Mabuya, Abigail Oppong, Andiswa Bukula, Tadesse Destaw Belay, Oumaima Hourrane, Ebrahim Chekol Jibril, Hagos Tesfahun Gebremichael, Chiamaka Ijeoma Chukwuneke, Tesfa Tegegne Asfaw, Tadesse Kebede Guge, Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Saminu Mohammad Aliyu, Lilian D. A. Wanzare, Nedjma Ousidhoum, Meriem Beloucif, Salomey Osei, Esubalew Alemneh, Abinew Ali Ayele, Lukman Jibril Aliyu, Elyas Abdi Ismail, Ibrahim Said Ahmad, Paul Röttger

- ***What's New***: AfriHate는 아프리카 언어 15개로 다언어 혐오 발언 및 남용 언어 데이터셋을 처음으로 구성한 연구입니다. 이는 혐오 발언 탐지에서 아프리카 언어의 데이터를 연구하고자 하는 공동체에 소중한 기초를 제공합니다.
- ***Technical Details***: AfriHate는 알제리아 아랍어, 암하라어, 이보어 등 15개 아프리카 언어로 2012년부터 2023년까지의 트윗을 수집했습니다. 이 데이터셋은 각 언어에 대해 원어민이 참여하여 혐오, 남용/공격적, 중립의 세 가지 클래스 중 하나로 주석을 달았습니다. 또한, 혐오 발언의 목표를 인종, 정치, 종교, 성별 등 여섯 개의 일반적인 속성으로 추가 레이블링했습니다.
- ***Performance Highlights***: 다중 언어로 미세 조정된 AfroXLMR-76L 모델은 대부분의 AfriHate 언어에서 최고 성능을 기록하며, 평균 매크로 F1 점수 78.16을 달성했습니다. GPT-4o는 제로샷 설정에서 61.89, 20샷 설정에서 평균 F1 점수 70.79를 기록하며 가장 높은 성능을 보여주었습니다. 이 결과는 아프리카 언어에 대한 혐오 발언 및 남용 언어 탐지에서 다중 언어 및 맥락 특화 모델의 이점을 강조합니다.

### [In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR](https://arxiv.org/abs/2501.08120)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08120.png)

Vote: 1

Authors: Markus J. Buehler

- ***What's New***: 이 논문은 Graph-PReFLexOR라는 새로운 프레임워크를 소개하며, 그래프 기반의 추론과 상징적 추상화를 결합하여 분야 지식을 확장하는 방식을 제안합니다. Reinforcement Learning에서 영감을 받아, 이것은 탐색적 최적화를 위한 새로운 접근법으로, 과학적 발견과 패턴 인식에서 새로운 가능성을 열어줍니다.
- ***Technical Details***: Graph-PReFLexOR는 과제(T)를 지식 그래프(G), 추상 패턴(P), 최종 답변(A)으로 변환하는 구조적 매핑을 정의합니다. 카테고리 이론에서 영감을 받아, 개념을 노드로, 관계를 엣지로 표현하여 계층적 추론과 적응 학습을 지원합니다. 특히, 이 접근법은 그래프 이형성(GIN)을 포함하여 구조적 동등성을 캡처하는 것으로, 각 요소들이 어떻게 상호작용하는지를 나타냅니다.
- ***Performance Highlights***: 3억 개의 파라미터를 가진 Graph-PReFLexOR 모델은 깊은 추론 능력과 적응력을 보여주며, 투명하고 다학문적인 AI 기반 발견의 가능성을 강조합니다. 이는 일반적인 자율 추론 솔루션을 위한 토대를 마련합니다.

### [MatchAnything: Universal Cross-Modality Image Matching with Large-Scale Pre-Training](https://arxiv.org/abs/2501.07556)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.07556.png)

Vote: 0

Authors: Hujun Bao, Xiaowei Zhou, Zehong Shen, Hao Yu, Sida Peng, Dongli Tan, Xingyi He

- ***What's New***: MatchAnything은 다양한 학문 분야에서 필수적인 크로스모달리티 이미지 매칭(Cross-Modality Image Matching)의 일반화를 크게 개선한 새로운 대규모 사전 훈련(large-scale pre-training) 프레임워크입니다. 이 프레임워크는 다양한 합성 크로스모달 데이터 신호를 활용하여 기본 구조를 인식하고 매칭하도록 모델을 훈련하여 실제 세계에서 보지 못한 다양한 크로스모달리티 작업에 적응할 수 있게 합니다.
- ***Technical Details***: 제안된 프레임워크는 픽셀 정렬 이미지 번역 네트워크를 사용하여 각기 다른 모달리티로 이미지 쌍을 합성하여 크로스모달 훈련 쌍을 구성하는 방법을 사용합니다. 다양한 데이터 출처와 혼합 훈련 접근법이 통합되어, 다양한 모달의 훈련 데이터를 제공합니다. 이 과정은 영상 시퀀스의 연속성을 활용하여 아직 보지 못한 구조에도 모델이 일반화하도록 돕습니다.
- ***Performance Highlights***: 우리의 프레임워크로 사전 훈련된 매칭 모델은 8가지 이상의 미지의 크로스모달 등록 작업에서 같은 네트워크 가중치를 사용하여 상당한 일반화 능력을 보여주며, 각 분야의 기존 방법을 크게 능가합니다. 실험 결과, ELoFTR와 같은 모델이 저장된 학습 데이터 없이도 가장 높은 정확도를 달성했으며, ROMA 모델은 하버드 브레인 데이터셋에서 76.9%의 상대적 향상을 보였습니다.

