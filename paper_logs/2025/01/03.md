## Daily Papers (2025-01-03)

### [Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models](https://arxiv.org/abs/2501.01423)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01423.png)

Vote: 25

Authors: Xinggang Wang, Jingfeng Yao

- ***What's New***: 이 논문에서는 잠재 확산 모델(Latent Diffusion Models)의 최적화 딜레마를 해결하기 위해 Vision Foundation 모델을 활용한 VA-VAE(Vision foundation model Aligned Variational AutoEncoder)와 개선된 DiT(Diffusion Transformers)인 LightningDiT를 제안합니다. 이를 통해 고차원 잠재 공간에서의 재구성과 생성 성능의 패러독스를 효과적으로 해결할 수 있습니다.
- ***Technical Details***: VA-VAE는 Vision Foundation 모델과의 정렬을 통해 잠재 공간을 최적화하며, VF 손실(VF Loss)을 통해 고차원 잠재 공간에서의 재구성과 생성을 동시에 개선합니다. 아키텍처나 학습 파이프라인을 수정하지 않고도 잠재 공간을 구조화하는 VF 손실은 요소 유사성과 쌍 대 쌍의 유사성을 고려하여 고차원 잠재 공간의 포괄적 정규화를 가능하게 합니다. LightningDiT는 향상된 학습 전략과 아키텍처 디자인을 추가한 DiT입니다.
- ***Performance Highlights***: 제안된 시스템은 ImageNet 256×256 생성에서 FID 1.35라는 최첨단 성능을 달성하였고, 64 에포크 동안 FID 2.11을 기록하며 오리지널 DiT 대비 21배의 수렴 속도 향상을 보여줍니다. 이는 현재 잠재 확산 모델이 채택한 방법론 중 가장 효율적인 구성 중 하나로 평가됩니다.

### [CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings](https://arxiv.org/abs/2501.01257)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01257.png)

Vote: 27

Authors: Binyuan Hui, Bofei Gao, Yunlong Feng, Shanghaoran Quan, Yibo Miao, Bowen Yu, Jiaxi Yang, Junyang Lin, Dayiheng Liu, Yang Fan, Zeyu Cui, Jian Yang, Yichang Zhang, An Yang, Bo Zheng, Zekun Wang, Xuancheng Ren

- ***What's New***: CODEELO는 처음으로 LLMs(대형 언어 모델)의 경쟁 수준 코딩 능력을 평가하기 위한 표준화된 벤치마크를 제공합니다. 이 벤치마크는 CodeForces 플랫폼을 기반으로 하며, 플랫폼과 일치하는 Elo 평가 시스템을 통해 사람과 비교 가능한 평가를 수행합니다.
- ***Technical Details***: CODEELO 벤치마크는 최근 6개월 간의 CodeForces 대회 문제를 바탕으로 경쟁 부문, 문제 난이도 평가, 문제 알고리즘 태그와 같은 세부 정보를 포함합니다. 문제는 플랫폼에 직접 제출되어 정확한 판단을 받으며, 특수 심사(special judge)가 지원되어 정확한 실행 환경을 보장합니다. Elo 평가 시스템은 플랫폼의 Elo 평가를 기반으로 하며, 모델의 성능에 따른 예측 Elo 평가를 제공합니다.
- ***Performance Highlights***: OpenAI의 o1-mini 모델은 Elo 점수 1578을 기록하며 상위 90% 인간 참가자를 능가했으며, 오픈소스 모델 중에서는 QwQ-32B-Preview 모델이 1261점으로 두드러진 성과를 보였습니다. 대부분의 모델들은 가장 쉬운 문제에서도 어려움을 겪어 인간 참가자의 하위 20%에 위치합니다. 각 알고리즘과 C++과 Python을 사용한 성능 비교 실험에서, 대다수의 모델이 Python보다 C++에서 더 나은 성능을 보여주었습니다.

### [2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining](https://arxiv.org/abs/2501.00958)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00958.png)

Vote: 40

Authors: Jiashuo Sun, Deli Zhao, Wenqi Zhang, Yueting Zhuang, Yongliang Shen, Lidong Bing, Xin Li, Hang Zhang, Weiming Lu

- ***What's New***: 2.5년 동안의 멀티모달 교과서(Multimodal Textbook)는 비전-언어 사전학습(Vision-Language Pretraining)을 위한 고품질 데이터 세트를 제안합니다. 이 데이터는 22,000시간의 강의 동영상을 기반으로 하여 보다 풍부한 기초 지식과 더 나은 이미지-텍스트 정렬을 제공하여 기존의 웹페이지에서 수집한 데이터셋의 한계를 극복합니다.
- ***Technical Details***: 이 시스템은 LLM이 제안한 지식 분류 체계를 사용하여 교육 동영상을 체계적으로 수집합니다. 이후 동영상에서 시각적 정보(주요 프레임), 청각적 정보(ASR), 텍스트적 정보(OCR)를 추출하여 시간 순서에 따라 이미지-텍스트 혼합 코퍼스로 구성합니다. 특히, 비디오 중심의 디자인으로 콘텐츠의 논리적 일관성을 높이고 지식 밀도를 향상시킵니다.
- ***Performance Highlights***: 우리의 멀티모달 교과서로 사전 학습한 Vision-Language Models는 과학 QA(ScienceQA) 및 수학 비스타(MathVista)와 같은 지식 및 추론 강도 벤치마크에서 두드러진 성과를 보였습니다. 이 모델은 이미지와 텍스트 단서들을 활용하여 과제를 해결하는 능력이 뛰어난 것으로 나타났으며, 기존의 데이터셋보다 평균적으로 4% 정도 성능 향상을 이루었습니다.

### [Unifying Specialized Visual Encoders for Video Language Models](https://arxiv.org/abs/2501.01426)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01426.png)

Vote: 10

Authors: Tyler Zhu, Max Gonzalez Saez-Diez, Juan Carlos Niebles, Jihoon Chung, Honglu Zhou, Olga Russakovsky

- ***What's New***: MERV는 여러 고정된 비주얼 엔코더(frozen visual encoders)를 활용하여 비디오의 통합 표현을 생성함으로써 VideoLLM(Video Large Language Models)에 보다 포괄적인 비주얼 지식을 제공하는 새로운 방법론을 제안합니다. 이는 InVideoLLM의 비주얼 처리에서 여러 엔코더의 기능을 결합하여 시공간적 특성을 정렬하고 다양한 비디오 이해 질문에서 이전의 최고 성능을 뛰어넘습니다.
- ***Technical Details***: MERV는 영상 인코더의 정보를 혼합하기 위해 교차 주의 엔코더 믹서(cross-attentive encoder mixer)를 사용하여 비디오에서 서로 다른 유형의 비주얼 엔코더의 정보를 결합합니다. 이를 통해 엔코더의 고유한 강점과 약점을 상호 보완할 수 있도록 설계되었습니다. 이 방식은 최소한의 추가 매개변수를 도입하며, 데이터 병렬 처리(parallelize)를 통해 시각적 처리 속도를 높여줍니다.
- ***Performance Highlights***: MERV는 비디오 이해 벤치마크에서 Video-LLaVA에 비해 최대 3.7% 향상된 정확도를 보여주며, 특히 zero-shot Perception 테스트 정확도에서 지난 SeViLA보다 2.2% 개선된 성능을 보였습니다. 실험 결과 MERV는 단일 엔코더 방식에 비해 더욱 빠르게 학습하며, 경량화된 방법으로 더 나은 성능을 보장합니다.

### [Are Vision-Language Models Truly Understanding Multi-vision Sensor?](https://arxiv.org/abs/2412.20750)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.20750.png)

Vote: 12

Authors: Sangyun Chung, Se Yeon Kim, Youngjoon Yu, Youngchae Chee, Yong Man Ro, Byung-Kwan Lee

- ***What's New***: 이 연구는 대규모 시각-언어 모델(Vision-Language Models; VLMs)이 다양한 멀티-비전 센서 데이터를 실제로 이해하는지를 평가하는 MS-PR(Multi-vision Sensor Perception and Reasoning) 벤치마크를 제안합니다. 이를 통해 VLMs의 센서별 추론 역량을 체계적으로 평가하며, 데이터가 부족한 상황에서도 깊이 있는 추론을 가능하게 하는 새로운 학습 기법으로 Diverse Negative Attributes(DNA) 최적화를 도입하였습니다.
- ***Technical Details***: MS-PR 벤치마크는 멀티-비전 센서 이미지를 이해하고 분석하는 두 가지 과제인 멀티-비전 인지와 멀티-비전 추론으로 구성됩니다. 다양한 부정적 예제를 활용하는 DNA 최적화는 데이터가 부족한 상황에서도 센서별 맥락 이해를 촉진합니다. 이 과정에서 다양한 부정적 샘플은 VLMs의 추론 과정을 강화하고 단순한 RGB 기반 추정의 한계를 넘어서도록 돕습니다. 연구진은 이 벤치마크를 통해 총 10개의 최신 VLMs을 평가하였고, DNA 최적화 방법이 멀티-비전 센서 추론 능력을 크게 향상시킨다는 것을 실험 결과로 입증했습니다.
- ***Performance Highlights***: 실험 결과, 제안된 DNA 최적화 방식은 멀티-비전 센서 추론 과제에서 현저한 성능 향상을 보여주었습니다. Phantom-7B와 Qwen2-VL-7B 모델은 DNA 최적화를 통해 멀티-비전 추론 능력이 개선되었고, 특히 센서별 정보 이해가 필요한 작업에서 두드러진 향상을 거두었습니다. 이는 각 센서 유형의 고유한 속성에 대한 심층적인 이해를 가능하게 합니다.

### [A3: Android Agent Arena for Mobile GUI Agents](https://arxiv.org/abs/2501.01149)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01149.png)

Vote: 14

Authors: Yuxiang Chai, Jiayu Zhang, Siyuan Huang, Liang Liu, Shuai Ren, Hongsheng Li, Guozhi Wang, Hanhao Li

- ***What's New***: Android Agent Arena(A3)는 모바일 GUI 에이전트를 위한 새로운 평가 플랫폼으로 실제 환경에서 AI 에이전트의 성능을 평가합니다. A3는 (1) 실시간 정보 검색 및 작업 지시와 같은 의미 있는 실용적 작업, (2) 모든 데이터셋에서 학습된 에이전트와 호환 가능한 확장된 액션 공간, (3) 자동화된 LLM 기반의 비즈니스 수준 평가 프로세스를 제공하여 인간의 개입과 코딩 전문성의 필요성을 줄였습니다.
- ***Technical Details***: A3는 Appium을 기반으로 구축된 경량 시스템으로, Android 기기와의 자동 상호작용을 지원합니다. 21개의 널리 사용되는 서드파티 앱에서 파생된 200개 이상의 과제를 포함하며, 각 과제는 고유하고 반복되지 않도록 선택되었습니다. A3의 액션 공간은 모든 데이터셋의 학습 에이전트와의 호환성을 보장하기 위해 확장되었습니다. 평가 방법은 과제에 대한 평가 기능과 LLM을 활용한 평가 시스템으로 나뉘며, 실시간 상태를 기반으로 과제를 평가합니다.
- ***Performance Highlights***: 현재 InternVL2, GPT-4o, 그리고 AppAgent와 같은 다양한 에이전트가 A3에서 평가되었으며, 특히 AppAgent가 다른 에이전트보다 높은 성공률을 보였습니다. 하지만, 여전히 여러 실질적인 과제에서 에이전트들이 고군분투하고 있으며, 다프레임 정보 쿼리 작업에서는 에이전트들의 성능이 매우 부족한 것으로 나타났습니다. 이 결과는 에이전트들이 실시간 데이터와 상호작용할 때의 복잡성을 해결하는 데 더욱 발전이 필요하다는 것을 제시합니다.

### [MLLM-as-a-Judge for Image Safety without Human Labeling](https://arxiv.org/abs/2501.00192)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00192.png)

Vote: 11

Authors: Felix Juefei-Xu, Shiyu Zhao, Dimitris N. Metaxas, Xiaowen Lin, Jianfa Chen, Zhuowei Li, Shuming Hu, Zhenting Wang, Shiqing Ma, Nan Jiang, Ankit Jain, Li Chen, Lingjuan Lyu, Ligong Han, Harihar Subramanyam

- ***What's New***: 사람의 레이블 없이 이미지 안전을 판단할 수 있는 MLLM-as-a-Judge 방법을 소개합니다. 이 연구는 AI 생성 콘텐츠(AIGC)가 증가하면서 온라인 플랫폼에서의 이미지 콘텐츠 안전이 중요한 문제로 대두됨에 따라, 미리 학습된 멀티모달 대형 언어 모델(MLLMs)을 활용하여 안전 규정에 기반한 제로샷 이미지 검토의 신뢰성과 효율성을 높이는 방법을 제시합니다.
- ***Technical Details***: CLUE(Constitutional MLLM JUdgE) 방법은 주관적이고 모호한 안전 규칙을 객체화하고, 규칙과 이미지 간의 관련성을 평가하며, 편향된 토큰 확률을 통해 빠른 판단을 내리고, 필요시 더욱 깊은 연쇄적 사고 과정을 통한 깊은 추론을 수행합니다. 또한 멀티모달 대비 모델(CLIP)을 사용하여 이미지와 관련 없는 규칙 쌍을 필터링하여 모델로 전달 전 선별하는 과정을 돕습니다.
- ***Performance Highlights***: CLUE 방법은 여러 MLLM에서 시행한 실험 결과, 제로샷 이미지 안전 판단에 있어 높은 정확도와 신뢰성을 입증했으며, 예를 들어 InternVL2-76B 모델에서는 불안전한 이미지를 구별하는 데 있어 95.9%의 재현율과 94.8%의 정확도, 0.949의 F-1 점수를 기록했습니다.

### [Dynamic Scaling of Unit Tests for Code Reward Modeling](https://arxiv.org/abs/2501.01054)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01054.png)

Vote: 12

Authors: Sijia Luo, Zeyao Ma, Jie Tang, Xiaokang Zhang, Jing Zhang, Jifan Yu

- ***What's New***: 이 논문에서는 코드 생성에 대한 보상 신호를 향상시키기 위해 유닛 테스트(Unit Tests)의 규모를 동적으로 조정하는 새로운 방법론을 제안합니다. 연구자들은 더 많은 솔루션을 생성하는 것이 큰 언어 모델(LLMs)의 성능을 향상시키고 있음을 기반으로, 테스트 결과가 보상 신호의 품질에 어떻게 영향을 미치는지를 초기 실험을 통해 탐구하였습니다.
- ***Technical Details***: CodeRM-8B라는 경량의 유닛 테스트 생성기(Unit Test Generator)를 도입하였으며, 문제의 난이도에 따라 유닛 테스트의 수를 동적으로 조정하는 메커니즘을 적용하였습니다. 이를 위해 문제 난이도 분류기를 사용하여 문제의 난이도를 평가하고, 이를 기반으로 컴퓨팅 자원을 효율적으로 분배하는 탐욕 알고리즘을 활용하였습니다.
- ***Performance Highlights***: CodeRM-8B는 HumanEval Plus 벤치마크에서 Llama3-8B 모델이 18.43%의 성능 향상을 기록하는 등 다양한 모델에 대해 성능을 크게 개선시켰습니다. 또한, Llama3-70B나 GPT-4o-mini와 같은 대형 모델에서도 각각 4.95%와 3.42%의 향상을 보였습니다. 이러한 결과들은 CodeRM-8B의 효과와 컴퓨팅 효율성을 강조합니다.

### [LTX-Video: Realtime Video Latent Diffusion](https://arxiv.org/abs/2501.00103)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00103.png)

Vote: 9

Authors: Eitan Richardson, Poriya Panet, Benny Brazowski, Dudu Moshe, Eran Levin, Nir Zabari, Yoav HaCohen, Sapir Weissbuch, Ofir Bibi, Ori Gordon, Nisan Chiprut, Yaki Bitterman, Daniel Shalem, Guy Shiran, Victor Kulikov, Zeev Melumian

- ***What's New***: LTX-Video는 텍스트와 이미지를 기반으로 하는 비디오 생성에 혁신을 일으킨 변형기 기반의 잠재 확산 모델입니다. 이 모델은 비디오-VAE(Video-VAE)와 노이즈 제거 변형기(denoising transformer)의 상호작용을 최적화하여 실시간보다 빠른 비디오 생성 속도를 구현하였습니다.
- ***Technical Details***: LTX-Video는 VAE의 입력단에서 패치화(patchifying) 작업을 수행하여 1:192의 높은 압축비를 달성합니다. 이는 변형기가 효율적으로 공간적·시간적 자가 주의(spatiotemporal self-attention)를 수행할 수 있게 하여 고해상도 비디오 생성의 일관성을 유지합니다. 추가로, VAE 디코더는 노이즈 제거와 잠재 변수에서 픽셀로의 변환을 함께 수행하며, 이는 고압축 환경에서도 세부사항을 유지합니다. 회전식 위치 임베딩(Rotary Positional Embeddings; RoPE)을 사용하여 공간적·시간적 일관성을 개선하였습니다.
- ***Performance Highlights***: LTX-Video는 Nvidia H100 GPU에서 24fps, 768x512 해상도의 5초 비디오를 2초만에 생성하여, 유사한 규모의 기존 모델들을 성능 측면에서 앞서나갑니다. 인간 평가 실험에서 LTX-Video는 텍스트 기반 비디오 및 이미지-비디오 작업에서 경쟁 상대들을 상회하는 시각적 품질과 모션 충실도, 프롬프트 충실성을 보여주었습니다.

### [Nested Attention: Semantic-aware Attention Values for Concept Personalization](https://arxiv.org/abs/2501.01407)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01407.png)

Vote: 1

Authors: Or Patashnik, Rinon Gal, Sergey Tulyakov, Daniel Cohen-Or, Kfir Aberman, Daniil Ostashev

- ***What's New***: 이 논문은 Nested Attention이라는 새로운 메커니즘을 소개하여 텍스트-이미지 모델의 개인화를 개선하였습니다. 이 메커니즘은 기존 크로스-어텐션 레이어(s)에 부드럽게 통합될 수 있는 풍부하고 표현력 있는 주제 표현을 주입합니다. 이를 통해 모델의 사전 지식을 유지하면서 높은 정체성 보존과 텍스트 프롬프트와의 일치를 향상시킵니다.
- ***Technical Details***: Nested Attention 메커니즘은 새로운 주제를 단일 텍스트 토큰에 묶으면서 쿼리 의존 주제 값을 생성합니다. 이는 두 개의 어텐션 레이어로 구성된 구조로, 외부 레이어는 전통적인 텍스트-이미지 크로스-어텐션 레이어로 작동하고, 내부 '네스티드' 레이어는 쿼리별 값을 구성하여 이미지의 각 영역에 대해 가장 관련성이 높은 주제 특성을 학습합니다. 이러한 네스티드 레이어는 인코더 기반의 개인화 방법에 통합됩니다.
- ***Performance Highlights***: Nested Attention을 적용한 모델은 동일한 데이터 및 교육 예산 하에서 가장 뛰어난 ID 유사성과 편집 가능성을 보여줍니다. 기존의 주제-주입 방법들과 비교했을 때 쿼리 의존 값을 제공함으로써 성능이 향상되었습니다. 또한, 새로운 테스트 사례로 추가적인 이미지 입력 없이 성능을 더욱 높일 수 있음을 보여주었습니다.

### [Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing](https://arxiv.org/abs/2501.00658)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00658.png)

Vote: 5

Authors: Pragya Srivastava, Zhangyang Wang, Yuehao Wang, Peihao Wang, Jiajun Zhu, Pan Li, Ruisi Cai

- ***What's New***: 이 논문에서는 상태공간모델(SSMs)의 최근성 편향(Recency Bias) 및 과도평활화(Over-smoothing) 문제를 해결하기 위한 새로운 기법인 폴라리제이션(Polarization) 기법을 제안했습니다. 이는 상태 전이 행렬의 두 채널을 각각 0과 1로 고정함으로써 두 가지 문제를 동시에 해결하는 방법입니다.
- ***Technical Details***: SSMs는 최근 트랜스포머의 대안으로 주목받고 있습니다. 하지만 SSMs는 강한 최근성 편향으로 인해 장기 의존성을 유지하는 데에 한계가 있으며, 이로 인해 모델의 견고성에도 문제가 발생합니다. 깊은 구조가 장기 문맥 학습을 촉진할 수는 있지만, 오히려 과도하게 평활화되어 토큰 표현이 구별 불가능해지는 경향이 있습니다. 이러한 근본적인 딜레마를 해결하기 위해, 이 논문에서는 상태 전이 행렬에 두 개의 채널을 마련하여 각각 0과 1로 고정시키는 폴라리제이션 기법을 제안했습니다.
- ***Performance Highlights***: 폴라리제이션 기법을 적용한 SSM 모델은 장기 의존성 토큰의 연상기억 정확도를 일관되게 향상시켜주었으며, 보다 깊은 구조에서도 과도평활화 문제를 완화하였습니다. 실험결과, 폴라리제이션 기법은 SSMs의 장기 문맥 모델링 능력을 크게 향상시키면서 과도평활화를 완화한다는 것이 확인되었습니다.

### [Population Aware Diffusion for Time Series Generation](https://arxiv.org/abs/2501.00910)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00910.png)

Vote: 2

Authors: Haipeng Chen, Zhenyu Bi, Han Meng, Ingolv T. Urnes, Yang Li

- ***What's New***: Population-aware Diffusion for Time Series (PaD-TS)는 새로운 시계열 생성 모델로, 기존 모델들이 간과했던 인구 통계 수준의 속성을 더 잘 보존할 수 있도록 설계되었습니다. PaD-TS는 시계열 데이터의 인구 수준 속성을 명시적으로 학습에 통합하고, 이중 채널 인코더 모델 아키텍처를 도입하여 시계열 데이터 구조를 더 잘 포착하는 데 중점을 두고 있습니다.
- ***Technical Details***: PaD-TS는 새로운 훈련 목적을 통해 인구 수준의 분포 이동을 최소화하며, mini-batch 내에서 동일한 diffusion step을 비교하는 새로운 샘플링 전략(SSS)를 사용합니다. 또한, 트랜스포머(Transformer) 기반 인코더 구조를 사용하여 시계열 데이터의 시간적 및 교차 차원 정보를 더 잘 캡처합니다. 모델 아키텍처는 다측면 시계열 데이터가 갖는 복잡한 구조를 더 잘 이해할 수 있도록 설계되었습니다.
- ***Performance Highlights***: PaD-TS는 주요 벤치마크 데이터셋에서 실험적으로 우수한 성능을 입증하였으며, 기존 최첨단 모델들과 비교하여 평균적으로 CC 분포 이동 점수를 5.9배까지 개선하면서 개인 수준의 진정성을 유지했습니다. 또한, 다양한 실험에서 장기 시계열 생성에서도 탁월한 성능을 보였습니다.

### [ProgCo: Program Helps Self-Correction of Large Language Models](https://arxiv.org/abs/2501.01264)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01264.png)

Vote: 17

Authors: Yanan Wu, Xiaoshuai Song, Jiaheng Liu, Wenbo Su, Weixun Wang, Bo Zheng

- ***What's New***: ProgCo는 대형 언어 모델(LLM)의 초기 반응을 외부 피드백 없이 스스로 검증하고 수정할 수 있도록 도와주는 혁신적인 프로그램 주도 자기 수정 기법입니다. 이를 통해 복잡한 추론 작업에서 LLM의 자기 수정 기능을 향상시킵니다.
- ***Technical Details***: ProgCo는 두 가지 주요 구성 요소인 프로그램 주도 검증(ProgVe)와 프로그램 주도 수정(ProgRe)로 구성됩니다. ProgVe는 입력된 작업에 대한 검증 가상 프로그램을 생성하고 이를 실행하여 복잡한 검증 논리를 수행합니다. ProgRe는 ProgVe에서 피드백을 받아 반응과 검증 프로그램에 대해 이중 반영 및 수정을 진행하여 오류 피드백의 오도를 완화합니다.
- ***Performance Highlights***: ProgCo는 세 가지 벤치마크 실험에서 강력한 자기 수정 성능을 달성했습니다. 예를 들어, GPT-3.5에서는 하나의 자기 수정 라운드 만으로 IFEval의 초기 반응 대비 4.62%, GSM8K에서 5.84%, MATH에서 5.8% 향상된 성능을 보였습니다. 이러한 결과는 ProgCo의 복잡한 추론 작업에서의 효과성을 보여줍니다.

### [SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration](https://arxiv.org/abs/2501.01320)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01320.png)

Vote: 3

Authors: Lu Jiang, Meng Wei, Zhijie Lin, Yang Zhao, Jianyi Wang, Ceyuan Yang, Chen Change Loy

- ***What's New***: SeedVR는 대규모 확산 변환모델(Diffusion Transformer)을 통해 임의의 해상도와 길이의 일반 비디오 복원을 가능하게 하는 혁신적인 접근을 제시했습니다. 새롭게 도입된 화소 이동 기반 윈도우 주의 메커니즘을 바탕으로 복잡한 해상도 제약 문제를 해결하였습니다.
- ***Technical Details***: SeedVR는 MM-DiT를 기반으로 하여 각 윈도우에 3D 회전 위치 임베딩을 적용하여 해상도 제약을 회피합니다. 또한, 원활한 비디오 인코딩을 위해 유발 비디오 VAE(Causal Video VAE)를 설계하며, 대규모 이미지 및 비디오 데이터로의 공동 훈련을 도입하였습니다. 대구경 소재 창 주의 메커니즘을 사용하여 텍스트 프롬프트와의 상호작용을 통해 긴 거리 의존성을 포착할 수 있도록 하였습니다.
- ***Performance Highlights***: SeedVR는 기존의 확산 기반 비디오 복원 방법들에 비해 2배 이상 빠른 속도를 자랑하며 SPMCS, UDM10, YouHQ40, AIGC38 등 다양한 벤치마크에서 탁월한 성능을 보여주었습니다. LPIPS, DISTS, NIQE 등 다양한 지표에서 최고의 성능을 기록하였으며, AIGC의 세부 디테일 복원에서도 뛰어난 능력을 입증하였습니다.

### [MapQaTor: A System for Efficient Annotation of Map Query Datasets](https://arxiv.org/abs/2412.21015)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.21015.png)

Vote: 5

Authors: Md Rizwan Parvez, Mahir Labib Dihan, Mohammed Eunus Ali

- ***What's New***: MAPQATOR은 지도 기반 질문 응답(QA) 데이터셋을 효율적으로 주석 달기 위한 시스템으로, 지도 서비스에 자연어 공간질문을 손쉽게 처리하고 생성된 데이터셋의 신뢰성을 보장합니다. 사용자에게 맞춤형으로 다양한 지도 API에 즉시 연결할 수 있으며, API 응답을 캐싱하여 동일한 데이터의 지속적인 사용을 가능하게 하고, 대규모 언어모델(LLMs)의 지리적 추론 능력을 강화할 수 있도록 설계되었습니다.
- ***Technical Details***: MAPQATOR는 다양한 지도 API와의 통합을 지원하는 플러그앤플레이(Plug-and-Play) 아키텍처로 구성되어 있으며, 각 지도 API에 대해 표준화된 요청과 응답 형식을 통해 다양한 제공자와의 손쉬운 확장을 가능하게 합니다. API 응답은 PostgreSQL 데이터베이스에 캐쉬됨으로써 데이터의 일관성과 지속성을 보장하며, 시각화 도구를 통해 직관적인 데이터 탐색을 지원합니다.
- ***Performance Highlights***: MAPQATOR는 수동 방법보다 최소 30배 빠른 주석 처리 속도를 보이며, Place Details, Nearby Search, Compute Routes 등의 작업에서 상당한 시간 절약을 제공합니다. 이를 통해 복잡한 지도 추론 데이터셋 개발에 있어 매우 효율적임이 입증되었습니다.

### [VMix: Improving Text-to-Image Diffusion Model with Cross-Attention Mixing Control](https://arxiv.org/abs/2412.20800)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.20800.png)

Vote: 4

Authors: Wei Liu, Fei Ding, Qian He, Mengqi Huang, Shaojin Wu

- ***What's New***: VMix는 텍스트-투-이미지 확산 모델(text-to-image diffusion model)의 미학적 품질을 향상시키기 위해 크로스-어텐션 믹싱 컨트롤(cross-attention mixing control)을 사용하는 새로운 플러그앤플레이 어댑터입니다. VMix는 입력된 텍스트 프롬프트를 콘텐츠와 미학적 설명으로 분리하고, 미학적 조건을 디노이징 과정에 통합하여 시각적 성능을 향상시킵니다.
- ***Technical Details***: VMix는 보편적인 사진 표준에 따라 손으로 선택된 고품질 이미지 하위 집합을 기반으로 어댑터를 미세 조정합니다. 여기에는 U-Net 아키텍처에 미학적 레이블을 추가 조건으로 통합하기 위한 미학적 임베딩 초기화 모듈과 크로스-어텐션 믹싱 컨트롤 모듈이 포함됩니다. VMix는 다른 커뮤니티 모듈들과도 높은 호환성을 보여주며, 재훈련 없이 플러그인으로 사용 가능합니다.
- ***Performance Highlights***: VMix는 다양한 베이스 모델과 통합되어 미학적 성능을 크게 향상시킵니다. MJHQ-30K 및 LAION-HQ10K 벤치마크에서 VMix는 Aes Score에서 최고 점수를 기록하였으며, CLIP Score와 FID 또한 우수함을 나타냅니다. 사용자 연구 결과에 따르면, 인간 사용자들은 VMix를 통합한 모델이 생성하는 이미지에 대해 더 높은 선호도를 보였습니다.

### [MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models](https://arxiv.org/abs/2501.00316)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00316.png)

Vote: 15

Authors: Md Rizwan Parvez, Muhammad Aamir Cheema, Md Almash Alam, Md Tanvir Hassan, Md Tanvir Parvez, Mohammed Eunus Ali, Md Hasebul Hasan, Mahir Labib Dihan

- ***What's New***: MAPEVAL은 기초 모델(Foundation Models)의 지리적 추론 능력을 평가하기 위한 새로운 벤치마크로, 다양한 복잡성을 가진 지도 기반 사용자 쿼리를 통해 모델을 테스트합니다. 이를 통해 자율 내비게이션 최적화, 자원 탐색 용이화, 물류 효율화 등 실제 응용에 필수적인 영역을 다룹니다.
- ***Technical Details***: MAPEVAL은 180개 도시와 54개국을 아우르는 700개의 다중 선택 질문으로 이루어져 있으며, 3가지 과제 유형(텍스트 기반, API 기반, 비주얼 기반)으로 구성되어 있습니다. 모델은 지도 도구를 통해 세계 정보를 수집하고 각종 지리적 맥락을 처리해야 하며, 복합적인 논리를 필요로 합니다.
- ***Performance Highlights***: 전체적으로, 모든 평가 유형에서 Claude-3.5-Sonnet이 선도적인 성능을 보였지만, MAPEVAL-API에서 특히 두드러진 성과 격차가 관찰되었습니다. GPT-4o및 Gemini-1.5-Pro를 각각 16% 및 21%로 앞섰으며, 코딩 활용 시 모든 모델은 인간의 성능에 평균 20% 이상 뒤처지는 것으로 나타났습니다.

### [VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control](https://arxiv.org/abs/2501.01427)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01427.png)

Vote: 30

Authors: Sihui Ji, Xiang Bai, Hengshuang Zhao, Yuanpeng Tu, Xi Chen, Hao Luo

- ***What's New***: 비디오 오브젝트 삽입 분야에서 VideoAnydoor라는 새로운 기술이 소개되었습니다. 이는 높은 디테일 유지와 정밀한 모션 컨트롤을 제공하는 제로-샷(zero-shot) 비디오 오브젝트 삽입 프레임워크입니다. 사용자는 상자(box)나 포인트 트랙터지(point trajectories)를 사용하여 모션을 제어할 수 있으며, 다중 오브젝트 추가 삽입이나 오브젝트 교환도 가능합니다. 이는 기존 방법에 비해 상당한 우수성을 보입니다.
- ***Technical Details***: VideoAnydoor는 텍스트-투-비디오(text-to-video) 모델에서 시작하여 ID 추출기를 사용하여 전역 아이덴티티를 주입하고 상자 시퀀스를 이용하여 전체적인 모션을 제어합니다. 동시에 픽셀 워퍼(pixel warper)를 설계하여 레퍼런스 이미지의 임의의 키포인트(key-points)와 해당 키포인트 트랙터지를 입력으로 사용합니다. 이를 통해 픽셀 디테일을 경로에 따라 워핑(warping)하고 차별적 ID 토큰을 주입하여 세밀한 모션 컨트롤을 지원합니다. 또한, 비디오 및 정적 이미지와 함께 훈련 전략을 도입하여 삽입 품질을 향상시켰습니다.
- ***Performance Highlights***: VideoAnydoor는 다양한 결정적 애플리케이션, 예를 들어 비디오 얼굴 교환, 비디오 가상 착용, 다중 지역 편집 등을 자연스럽게 지원합니다. 현재의 선행 연구에 비해 ID 보존과 모션 제어에서 비교적 높은 성능을 보여주며, 다양한 실험 결과 이를 입증합니다. CLIP-Score, DINO-Score 같은 평가지표에서 우수한 결과가 나타났으며, 유저 스터디에서 높은 평가를 받았습니다.

### [Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding](https://arxiv.org/abs/2501.00712)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00712.png)

Vote: 2

Authors: Zhangyang Wang, Peihao Wang, Jason D. Lee, Pan Li, Jiajun Zhu, Ruisi Cai

- ***What's New***: 이 논문은 언어 모델의 위치 기반 어드레싱을 개선하기 위해 'TAPE'(ConTextualized EquivAriant Positional Encoding)라는 새로운 프레임워크를 제안합니다. 이는 시퀀스 내용을 통합하여 동적이고 문맥 인식적인 위치 인코딩을 도입함으로써 기존 고정 패턴의 제한을 극복합니다.
- ***Technical Details***: TAPE는 다차원 텐서를 사용하는 향상된 위치 인코딩 방식을 제안합니다. 이는 토큰 특징과 위치 간의 상호작용을 강화하며, 전역과 국소 정보를 동시에 처리합니다. 또한 주의 메커니즘에 위치 인코딩의 직교 등변성을 도입하여 모델 업데이트 동안의 안정성을 보장합니다. 이를 통해 기존의 표준 Transformer에 쉽게 통합이 가능합니다.
- ***Performance Highlights***: TAPE는 기존의 위치 인코딩 기법과 비교하여 언어 모델링, 산술 추론 및 긴 문맥 검색 작업에서 우수한 성능을 보였습니다. 특히, 긴 시퀀스에서의 perplexity 감소 및 다양한 도메인에 대한 일반화 능력이 뛰어납니다. 또한, TAPE는 효율적인 파라미터 튜닝을 통해 기존 사전 훈련된 모델에 통합할 수 있으며, 플래쉬 어텐션과 같은 가속화된 메커니즘과 호환됩니다.

### [VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM](https://arxiv.org/abs/2501.00599)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00599.png)

Vote: 27

Authors: Zesen Cheng, Deli Zhao, Long Li, Jianke Zhu, Yuqian Yuan, Boqiang Zhang, Yueting Zhuang, Wentong Li, Wenqiao Zhang, Lidong Bing, Xin Li, Hang Zhang

- ***What's New***: VideoRefer Suite는 비디오 대형 언어 모델(Video Large Language Models; Video LLMs)의 공간-시간 객체 이해 능력을 향상시키기 위해 고안되었습니다. 이 Suite는 고품질의 객체 수준 비디오 지시 데이터셋인 VideoRefer-700K, 다재다능한 공간-시간 객체 인코더를 갖춘 VideoRefer 모델, 그리고 포괄적인 성능 평가를 위한 VideoRefer-Bench로 구성됩니다.
- ***Technical Details***: VideoRefer Suite는 세 가지 핵심 구성 요소로 이루어져 있습니다. 첫째, VideoRefer-700K는 멀티 에이전트 데이터 엔진을 사용하여 신중하게 편집한 대규모 고품질 객체 수준의 비디오 데이터셋으로 구성됩니다. 둘째, VideoRefer 모델은 임의의 객체에 대한 정밀한 지역 및 시간적 표현을 캡처하기 위한 다재다능한 공간-시간 객체 인코더를 탑재하고 있습니다. 마지막으로 VideoRefer-Bench는 모델의 공간-시간 이해 능력을 다양한 관점에서 평가하는 벤치마크로, 묘사 생성(VideoRefer-BenchD)과 다양한 선택 질문 답변(VideoRefer-BenchQ)을 포함합니다.
- ***Performance Highlights***: VideoRefer 모델은 VideoRefer-Bench에서 뛰어난 성능을 보였으며, 이는 기존의 방법들보다 우수한 성과를 보입니다. VideoRefer는 공간-시간적 이해 능력에서 최고의 성능을 발휘하며, 이는 세밀한 비디오 객체 참조 및 복잡한 객체 관계 분석에서 특히 두드러집니다. 이는 또한 일반 비디오 이해 능력을 향상시키는 데 기여합니다.

### [SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization](https://arxiv.org/abs/2501.01245)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01245.png)

Vote: 3

Authors: Zhenbang Xu, Haodong Chen, Zihan Jia, Yongle Huang, Haozhou Sun, Dian Shao

- ***What's New***: SeFAR는 시간적 교란(Temporal Perturbation)과 학습 안정화(Learning Stabilization)를 통해 반지도 세밀한 액션 인식(Fine-grained Action Recognition; FAR)을 수행하는 새로운 프레임워크입니다. 이를 통해 높은 비용의 주석 데이터 없이 간단한(Semi-supervised) 방법으로 멀티모달 모델의 세밀하고 특정 도메인별 이해 능력을 강화할 수 있습니다.
- ***Technical Details***: SeFAR는 FixMatch 기반 반지도학습(SSL) 구조에 따라 설계되었습니다. 1) 이중 레벨 시간 요소(Dual-level Temporal Elements)를 통해 충분한 시각적 세부사항을 포착하여 미세한 동작 차이를 구별합니다. 2) 약-강(Weak-Strong) 데이터 대비 쌍을 만들기 위해 중간(Moderate) 시간 교란을 도입한 새로운 증강(Augmentation) 전략을 설계하였습니다. 3) 학습 과정의 불안정을 최소화하기 위해 적응형 조절(Adaptive Regulation)을 사용하여 교사 모델의 예측을 안정화합니다.
- ***Performance Highlights***: SeFAR는 FineGym과 FineDiving 데이터셋을 기반으로 한 세밀한 및 거친(coarse) 액션 인식에서 최고 성능을 달성했습니다. 또한 UCF101과 HMDB51와 같은 고전적 데이터셋에서도 다른 반지도학습 방법을 초과하는 성능을 보여줬습니다. 더 나아가, SeFAR로 추출한 특징은 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 세밀하고 도메인 특화된 시맨틱 이해를 크게 향상시킬 수 있음을 보여줍니다.

