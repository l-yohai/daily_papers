## Daily Papers (2025-01-06)

### [Virgo: A Preliminary Exploration on Reproducing o1-like MLLM](https://arxiv.org/abs/2501.01904)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01904.png)

Vote: 12

Authors: Bingning Wang, Yuqi Huo, Ji-Rong Wen, Zhongyuan Wang, Wayne Xin Zhao, Yifan Li, Yifan Du, Zheng Liu, Zikang Liu, Weipeng Chen

- ***What's New***: 이 논문에서는 멀티모달 느림 사고(Multimodal Slow-thinking) 시스템인 Virgo를 소개합니다. 이는 MLLM(Multimodal Large Language Models)을 텍스트 기반의 장시간 사고 데이터로 미세 조정(fine-tuning)하여 구현된 시스템으로, 이러한 방법이 시각적 추론 능력을 효과적으로 향상시킬 수 있음을 보여줍니다.
- ***Technical Details***: Virgo 모델은 Qwen2-VL-72B-Instruct를 기반으로 하며, 텍스트 기반 장시간 사고 데이터와 시각적 장시간 사고 데이터를 활용해 느림 사고 추론 능력을 발휘하도록 합니다. 여기서 중점은 텍스트 기반의 장시간 사고 데이터가 MLLMs의 느림 사고 능력을 유도하는데 시각적 데이터보다 더 효과적일 수 있음을 시사합니다.
- ***Performance Highlights***: Virgo는 MathVerse, MathVision, OlympiadBench, 그리고 MMMU와 같은 네 가지 어려운 벤치마크에서 상업 시스템에 필적하는 성능을 보여주었습니다. 특히 텍스트 기반 장시간 사고 데이터로만 미세 조정 했을 때도 매우 경쟁력 있는 결과를 보였으며, OlympiadBench에서 12.4%의 성능 향상을 보여 더 어려운 과제가 더 큰 혜택을 받는다는 것을 발견했습니다.

### [VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation](https://arxiv.org/abs/2412.21059)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.21059.png)

Vote: 11

Authors: Zhuoyi Yang, Jiale Cheng, Yuxiao Dong, Xiaotao Gu, Qunlin Jin, Jiajun Xu, Yuanming Yang, Yu Huang, Shen Yang, Ming Ding, Jie Tang, Yuan Wang, Wenbo Duan, Xiaohan Zhang, Shiyu Huang, Jiayan Teng, Shurun Li, Xiao Liu, Wendi Zheng, Minlie Huang, Jiazheng Xu

- ***What's New***: VisionReward는 시각적 생성 모델을 인간의 선호도에 맞추는 새로운 전략을 제시합니다. 이 모델은 이미지와 비디오에서 인간의 선호도를 여러 차원으로 분해하여 세밀하고 다차원적인 보상 모델(Beward Model)을 구축하였습니다.
- ***Technical Details***: VisionReward 시스템은 이미지 및 비디오 생성과 관련된 요소들을 세분화하여 각 요소별로 질문과 답변을 통해 인간의 선호도를 예측합니다. 특히 비디오 품질 평가의 난점을 해결하기 위해 다양한 동적 요소를 체계적으로 분석하여 VideoScore를 17.2% 초과하는 성능을 달성했습니다. 또한, VisionReward를 기반으로 다목적 선호 학습 알고리즘을 개발하여 선호 데이터의 혼재 요소 문제를 효과적으로 해결하였습니다.
- ***Performance Highlights***: VisionReward는 기존의 이미지 및 비디오 평가 방법을 크게 능가하며, 특히 비디오 선호도를 정확하게 예측하여 VideoScore를 17.2% 초과하였습니다. VisionReward를 사용한 다목적 최적화는 사람의 주석 없이도 모델의 성능을 안정적으로 향상시킵니다.

### [EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation](https://arxiv.org/abs/2501.01895)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01895.png)

Vote: 41

Authors: Pengfei Zhou, Peng Gao, Zhengkai Jiang, Maoqing Yao, Yue Hu, Siyuan Huang, Liliang Chen, Guanghui Ren, Hongsheng Li, Shengcong Chen

- ***What's New***: EnerVerse는 로봇 조작 작업을 위한 '장래 체화 공간'(embodied future space) 생성 프레임워크로, 컨볼루션과 쌍방향 주의(attention) 메커니즘을 통합하여 일관성과 지속성을 보장합니다. Free Anchor View(FAV) 공간을 도입하여 관찰과 분석을 강화하고 로봇의 일반화 및 적응성을 개선합니다.
- ***Technical Details***: EnerVerse는 내구적인 연속 메모리 컨텍스트와 청크 단위의 일방향 생성 파라다임을 사용하여 제로-데이터 유출을 보장하며, 다중 뷰 동영상 확산 및 4D Gaussian Splatting(4DGS) 모델을 결합한 데이터 엔진 파이프라인을 제공합니다. 이 파이프라인은 데이터 품질과 다양성을 향상시키는 ‘데이터 플라이휠’ 효과를 발휘합니다.
- ***Performance Highlights***: EnerVerse 모델은 멀티뷰 비디오 생성과 긴 범위 로봇 조작 작업에서 최첨단(SOTA) 성능을 보여주며, 생성 전의 공간(pollicy)의 예측 능력을 크게 강화하여 사용성이 입증되었습니다. 특히, LIBERO 벤치마크에서 다중 시각 입력 설정 시, 평균 점수 88.5를 기록하며 다른 모델을 능가했습니다.

### [VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction](https://arxiv.org/abs/2501.01957)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01957.png)

Vote: 19

Authors: Chaoyou Fu, Xing Sun, Yunhang Shen, Yi-Fan Zhang, Ke Li, Haojia Lin, Xiong Wang, Xiaoyu Liu, Zuwei Long, Heting Gao, Xiawu Zheng, Rongrong Ji, Yangze Li, Caifeng Shan, Ran He

- ***What's New***: 이 논문은 VITA-1.5라는 시각 및 음성 상호작용 모델을 소개합니다. 이 모델은 비전-언어 및 음성 데이터를 통합하여 실시간 대화 기능을 제공하는 것으로, 기존의 자동 음성 인식(ASR)과 텍스트 음성 변환(TTS) 모듈을 별도로 사용하지 않고도 빠른 반응 속도를 자랑합니다.
- ***Technical Details***: VITA-1.5는 세 단계의 교육 방법을 포함합니다: 1단계에서는 시각-언어 조정 및 시각적 QA(질문 답변) 튜닝을 진행하고, 2단계에서는 음성 입력 조정을 통해 ASR 기능을 강화하며, 최종 3단계에서는 수천 시간의 텍스트-음성 쌍 데이터를 이용하여 음성 출력을 가능하게 합니다.
- ***Performance Highlights***: VITA-1.5는 시각-언어 이해와 영상 이해 벤치마크에서 우수한 성능을 보여주며, 일부 상용 모델과 경쟁할 만큼 우위에 있습니다. 또한, VITA-1.5는 중국어와 영어 ASR 작업에서 선도적인 정확도를 보이며, 기존의 전문 음성 모델보다 더 나은 결과를 달성했습니다.

### [Graph Generative Pre-trained Transformer](https://arxiv.org/abs/2501.01073)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01073.png)

Vote: 9

Authors: Xiaolin Xu, Yuanqi Du, Yinkai Wang, Xiaohui Chen, Jiaxing He, Soha Hassoun, Li-Ping Liu

- ***What's New***: Graph Generative Pre-trained Transformer (G2PT)는 그래프를 시퀀스로 표현하는 새로운 방법을 도입하며, auto-regressive 모델로서 그래프 구조를 다음 토큰 예측을 통해 학습합니다. 이 모델은 그래프 설계와 속성 예측과 같은 다운스트림 작업에 맞춰 조정 가능한 범용적인 기초 모델로 제시됩니다.
- ***Technical Details***: G2PT는 그래프를 노드 정의와 엣지 정의로 나누어 시퀀스로 인코딩하는 방법을 사용합니다. 노드의 인덱스와 타입을 정의한 후, 엣지가 어떻게 연결되는지 엣지 라벨을 사용해 명시적으로 표현합니다. transformer 디코더를 활용하여 시퀀스 분포를 근사화하고, 다음 토큰 예측 손실을 통해 모델을 학습시킵니다.
- ***Performance Highlights***: G2PT는 일반적인 그래프와 분자 데이터셋에서 탁월한 생성 성능을 보여주었습니다. 또한, 목표 지향적 생성 및 그래프 속성 예측과 같은 다운스트림 작업에서도 뛰어난 적응성과 다양성을 나타냈습니다. 실험은 G2PT가 기존의 최첨단(SOTA) 기법을 초과하거나 유사한 성능을 기록함을 보여주었습니다.

### [BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery](https://arxiv.org/abs/2501.01540)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01540.png)

Vote: 4

Authors: Lyle Goodyear, Aditi Bhaskar, Kanishk Gandhi, Mohammed Zaman, Noah D. Goodman, Michael Y. Li, Louise Li

- ***What's New***: BoxingGym은 LLM 기반 과학 에이전트의 실험 설계 및 모델 발견 능력을 체계적으로 평가하기 위한 새로운 벤치마크입니다. 다양한 실제 과학 도메인을 기반으로 한 10개의 환경을 포함하고 있어 과학적 이론 제안, 실험 데이터 수집 및 데이터에 기반한 이론 수정 능력을 평가할 수 있습니다.
- ***Technical Details***: BoxingGym은 실험 설계(e.g., 과학 이론 테스트를 위한 데이터 수집) 및 모델 발견(e.g., 과학 이론 제안 및 수정)을 평가하기 위한 생성을 포함하는 환경에서 시행됩니다. 각 환경은 상호실험을 수행할 수 있는 생성적 확률 모델로 구현되었습니다. 유익한 실험 데이터 수집 및 모델 발견을 평가하기 위해 정보 이론적 수량(EIG)을 사용합니다. 모델 발견의 평가는 설명 기반 방식으로 진행되며, 에이전트가 발견한 모델을 설명하여 다른 과학 에이전트가 이 환경에 대한 신뢰할 수 있는 예측을 수행할 수 있는지를 평가합니다.
- ***Performance Highlights***: 현재의 LLM, 예를 들어 GPT-4o,는 실험 설계 및 모델 발견에 어려움을 겪는 것으로 나타났습니다. LLM 기반 에이전트에 명시적인 통계 모델을 추가해도 결과가 크게 개선되지 않았습니다. BoxingGym의 퍼포먼스 테스트에서는 과학적 이론 제안, 실험 설계, 관찰 데이터 통합의 도전 과제를 강조하며, 앞으로의 연구 방향을 제시합니다.

### [SDPO: Segment-Level Direct Preference Optimization for Social Agents](https://arxiv.org/abs/2501.01821)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01821.png)

Vote: 10

Authors: Yuchuan Wu, Aobo Kong, Yong Qin, Fei Huang, Ke Wang, Yongbin Li, Xiaoqian Liu, Qicheng Li, Shiwan Zhao, Wentao Ma

- ***What's New***: SDPO(Segment-Level Direct Preference Optimization)은 소셜 에이전트(Social Agents)의 행동을 특정 인터랙션 세그먼트에 맞춰 최적화함으로써 대형 언어 모델(LLM)의 다중 턴 대화에서 소셜 지능을 높이는 혁신적인 접근 방식을 제시합니다. SDPO는 세션 레벨 혹은 턴 레벨 접근 방식의 한계를 극복하기 위해 설계되었습니다.
- ***Technical Details***: 기존의 DPO(Direct Preference Optimization)는 턴 혹은 세션 단위로 이루어져 반드시 정교한 목표를 이루지 못했지만, SDPO는 잘못된 턴을 식별하고 그 이전의 인터랙션 히스토리를 이용해 세그먼트를 생성하여 최적화합니다. 이를 통해 정교한 세그먼트 레벨에서 에이전트 행동을 조정합니다. SDPO는 새로운 DPO 손실 함수(adapted DPO loss)를 계산하여 긍정적인 세션에서 영향을 주는 주요 세그먼트를 찾아 최적화하는 방법을 사용합니다.
- ***Performance Highlights***: SOTOPIA 벤치마크에서 SDPO로 튜닝된 에이전트는 기존의 DPO 기반 방법들과 OTP-4o와 같은 독점 LLM들을 꾸준히 능가하며, 목표 및 관계 항목 모두에서 현저히 뛰어난 성과를 보였습니다. SDPO는 멀티 턴 대화에서 에이전트의 소셜 인텔리전스를 향상시키는데 효과적이며, 다양한 영역에 적용 가능성을 제시합니다.

### [LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models](https://arxiv.org/abs/2501.00874)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00874.png)

Vote: 7

Authors: Thien Huu Nguyen, Hieu Man, Viet Dac Lai, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi

- ***What's New***: LUSIFER는 대형 언어 모델(Large Language Models; LLMs)에 기반한 임베딩 모델을 다중언어 작업에 적응시키는 새로운 제로샷(Zero-Shot) 접근 방식을 제안합니다. 멀티리턴(multi-turn) 통합 구조를 통해 멀티언어 인코더의 언어 이해 능력을 특화된 임베딩 모델로 효과적으로 전이할 수 있습니다. 추가적으로, 다중언어 임베딩 성능을 평가하기 위해 5개의 주요 임베딩 작업과 14개의 언어를 포함하는 새로운 벤치마크를 소개합니다.
- ***Technical Details***: LUSIFER의 아키텍처는 멀티언어 인코더와 임베딩 특정 작업에 최적화된 LLM 기반 모델로 구성되어 있으며, 이들 컴포넌트는 최소한의 학습 가능한 파라미터로 매끄럽게 연결됩니다. 이를 통해 LUSIFER는 XLM-R과 같은 다중언어 표현을 활용하여 LLM의 목표적 LLM의 임베딩 모델로 전이할 수 있게 만듭니다. 두 단계의 훈련 프로세스를 통해 멀티언어 인코더의 표현을 영어 LLM의 나타내기 공간과 정렬시키고, 대조학습을 통해 텍스트 표현을 미세 조정합니다.
- ***Performance Highlights***: E5-Mistral과 같은 기존 모델을 평균 3.19점 향상시키며, 특히 중간 및 저자원의 언어에서 22.15의 개선을 보여줍니다. 또한, 교차언어 시나리오에서 평균 5.75 점 더 높은 성능을 보여주어 LUSIFER의 멀티언어 능력 향상 효과를 입증하였습니다.

