## Daily Papers (2025-01-09)

### [rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking](https://arxiv.org/abs/2501.04519)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04519.png)

Vote: 102

Authors: Fan Yang, Li Lyna Zhang, Xinyu Guan, Youran Sun, Yifei Liu, Mao Yang, Ning Shang, Yi Zhu

- ***What's New***: rStar-Math는 Self-Evolved Deep Thinking을 통해 작은 언어 모델(SLMs)이 OpenAI o1 모델의 수학적 추론 능력을 뛰어넘을 수 있게 하였습니다. 이 접근법은 더 큰 모델에서 증류 없이도 수행됩니다.
- ***Technical Details***: rStar-Math는 Monte Carlo Tree Search (MCTS)를 활용하여 수학 정책 SLM이 테스트 시간에 검색을 수행하고, SLM 기반의 프로세스 보상 모델에 의해 안내받습니다. 세 가지 주요 혁신으로는 (1) 코드보강(CoT) 데이터 합성 방법, (2) 새로운 프로세스 보상 모델 훈련 방법, (3) 자기 진화(self-evolution) 레시피가 포함됩니다.
- ***Performance Highlights***: rStar-Math는 MATH 벤치마크에서 Qwen2.5-Math-7B의 성능을 58.8%에서 90.0%로, Phi3-mini-3.8B를 41.4%에서 86.4%로 개선하며, OpenAI o1-preview를 초과합니다. AIME 올림피아드에서는 평균 53.3%의 문제를 해결하여 상위 20%의 수준을 나타냈습니다.

### [Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though](https://arxiv.org/abs/2501.04682)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04682.png)

Vote: 43

Authors: Chase Blagden, Duy Phung, Jan-Philipp Franken, Charlie Snell, Dakota Mahan, Violet Xiang, Louis Castricato, Anikait Singh, Alon Albalak, Nathan Lile, Kanishk Gandhi, Chelsea Finn, Rafael Rafailov, Nick Haber

- ***What's New***: 이 논문에서는 기존의 체인 오브 생각(Chain-of-Thought; CoT) 프레임워크를 확장하여 메타 체인 오브 생각(Meta Chain-of-Thought; Meta-CoT)라는 새로운 프레임워크를 제안합니다. 이는 복잡한 추론 문제의 데이터를 생성하는 과정에서 누락되었던 진정한 사고의 과정을 명시적으로 모델링하여 더 인간적인 추론을 강화하려는 시도입니다.
- ***Technical Details***: 메타-CoT는 고급 추론 문제를 해결하기 위해 비선형, 반복적 탐색 및 검증의 잠재적 과정을 체계적으로 모델링합니다. 이 프레임워크는 프로세스 감독, 인공 데이터 생성 및 검색 알고리즘을 통해 구현되며, 확률론적 맥락에서 몬테카를로 트리 탐색(Monte Carlo Tree Search; MCTS)와 A* 탐색을 포함합니다. 또한, 이렇게 생성된 모델들을 인스트럭션 튜닝과 강화 학습(RL)을 통해 강화합니다.
- ***Performance Highlights***: Meta-CoT 프레임워크는 고급 수준의 문제 해결에 유리한 결과를 보이며, 기존 모델보다 더욱 인간에 가까운 추론 능력을 갖추었음을 보여줍니다. 특히 OpenAI의 o1 및 DeepSeek-R1 모델 등 최신 모델에 대한 실험 결과는 이러한 구문 내 검색 과정이 메타-CoT를 통해 내재화된다고 주장합니다.

### [URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics](https://arxiv.org/abs/2501.04686)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04686.png)

Vote: 35

Authors: Xinzhe Ni, Jin Zeng, Zicheng Lin, Yujiu Yang, Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu

- ***What's New***: URSA는 멀티모달 수학(Multimodal Mathematics)에서 사슬형 추론(Chain-of-Thought; CoT) 개선을 위한 혁신을 도입하여 고품질 추론 데이터를 생성하는 전략을 제안했습니다. 이로 인해 MMathCoT-1M이라는 멀티모달 수학 CoT 학습 데이터 셋이 탄생했으며, URSA-7B 모델은 여러 벤치마크에서 최신 성능(State-of-the-Art; SOTA)을 기록했습니다.
- ***Technical Details***: URSA는 고품질 CoT 추론 데이터를 생성하기 위해 CoT 증류(CoT Distillation), 경로-형식의 수정(Trajectory-format Rewriting), 형식 통합(Format Unification)을 포함한 세 가지 모듈로 구성된 종합적인 합성 전략을 사용합니다. 테스트 시 확장성을 위해 DualMath-1.1M이라는 프로세스 주석 데이터 셋을 자동 생성하여 URSA-RM-7B 훈련에 사용됩니다.
- ***Performance Highlights***: URSA-7B는 여러 멀티모달 수학 벤치마크에서 SOTA 성능을 달성하며, 일부 문제 유형에서 강력한 비공개 기반 모델조차 능가합니다. URSA-RM-7B는 OOD(Out-of-Distribution) 검증 능력을 발휘하여 일반화 가능성을 보여줍니다.

### [Agent Laboratory: Using LLM Agents as Research Assistants](https://arxiv.org/abs/2501.04227)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04227.png)

Vote: 34

Authors: Xiaodong Yu, Ze Wang, Zicheng Liu, Yusheng Su, Samuel Schmidgall, Jialian Wu, Jiang Liu, Ximeng Sun, Emad Barsoum

- ***What's New***: Agent Laboratory는 자율적인 LLM 기반 프레임워크로, 연구 아이디어를 입력받아 문헌 검토, 실험, 보고서 작성의 세 가지 단계를 통해 연구 결과를 생성할 수 있습니다. 이를 통해 연구자는 창의적인 아이디어 개발에 더 많은 시간을 할애할 수 있으며, 연구 비용은 이전의 자율 연구 방법에 비해 84% 감소하였습니다.
- ***Technical Details***: Agent Laboratory는 다양한 최신 LLMs를 배경으로 사용하여 실험의 질, 보고서의 질, 유용성을 평가하였으며 각 단계에서 사람의 피드백을 수집하고 연구의 최종 평가를 수행합니다. 특히 o1-preview가 가장 우수한 연구 결과를 생성하며, 사람의 피드백이 연구의 전반적인 질을 크게 향상시킵니다.
- ***Performance Highlights***: Agent Laboratory는 연구 비용을 GPT-4o 백엔드 사용 시 논문 당 약 $2.33 USD로, 이전의 자율 연구 워크플로우에 비해 현저히 감소하였습니다. 또한, 제안된 mle-solver를 통해 MLE-Bench의 일부 과제에서 최첨단 성능을 보여주었으며, 다른 솔버들보다 더 높은 일관성과 점수를 달성했습니다.

### [LLM4SR: A Survey on Large Language Models for Scientific Research](https://arxiv.org/abs/2501.04306)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04306.png)

Vote: 18

Authors: Wei Yang, Zexin Xu, Zonglin Yang, Xinya Du, Ziming Luo

- ***What's New***: LLM4SR은 대형 언어 모델(LLM)이 과학 연구에서 어떻게 혁신을 이끌고 있는지를 체계적으로 탐구한 최초의 종합적인 설문 조사 연구입니다. 연구는 가설 발견, 실험 계획 및 시행, 과학적 글쓰기, 피어 리뷰의 네 가지 주요 단계에서 LLM의 역할을 분석합니다. 이 연구는 LLM들이 연구 과정 전반에서 과학적 탐구를 어떻게 촉진할 수 있는지에 대한 방향성과 영감을 제공합니다.
- ***Technical Details***: LLM4SR은 과학적 연구 프로세스에서 LLM의 적용을 다룹니다. 특히, LLM이 새로운 연구 아이디어를 제안할 수 있는 과학적 가설 발견과 실험 설계의 최적화, 자동 과학적 글쓰기, 그리고 피어 리뷰 지원을 통해 기여할 수 있는 방법을 체계적으로 분석합니다. 이 연구는 각 작업에 대한 방법론, 벤치마크, 평가 방법을 종합적으로 검토하고 현재의 과제를 식별하며 발전 방향을 제시합니다.
- ***Performance Highlights***: 현재 LLM 기술은 과학적 탐구의 모든 단계에서 상당한 잠재력을 보이며, 특히 과학적 가설의 발견과 실험 설계 최적화에 있어 두드러진 성과를 나타내고 있습니다. GPT-4와 같은 모델이 새로운 연구 아이디어를 제시할 수 있는 능력을 갖추고 있으며, LLM으로 인해 과학적 생산성이 크게 향상될 것으로 기대됩니다.

### [InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection](https://arxiv.org/abs/2501.04575)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04575.png)

Vote: 14

Authors: Zishu Wei, Xueyu Hu, Fei Wu, Shengyu Zhang, Pengxiang Li, Xiaotian Han, Xinchen Xu, Congkai Xie, Hongxia Yang, Yuhang Liu

- ***What's New***: InfiGUIAgent는 다양한 GUI 작업을 처리할 수 있고, 고유의 추론(Native Reasoning)과 반영 능력을 갖춘 멀티모달 일반 GUI 에이전트로, 이를 위해 두 단계로 구분된 감독 중심의 미세 조정 파이프라인을 도입했습니다. 이는 GUI 이해 및 시각-언어 연계와 같은 기본 기능을 강화하고, 계층적 추론(Hierarchical Reasoning) 및 기대-반영 추론(Expectation-Reflection Reasoning) 기술을 도입함으로써 GUI 자동화 작업의 효율성을 크게 향상시킵니다.
- ***Technical Details***: InfiGUIAgent는 두 단계의 감독적 미세 조정(Supervised Fine-Tuning)을 통해 훈련됩니다. 1단계에서는 GUI 이해, 대화형 데이터 셋을 사용해 기초 능력을 강화합니다. 2단계에서는 MLLM이 생성한 데이터를 통해 계층적 추론과 기대-반영 추론 기술을 데이터에 결합하여 에이전트가 복잡한 추론을 자연스럽게 수행할 수 있도록 합니다. 이러한 과정을 통해 에이전트는 GUI 상호작용을 정밀하게 조정하며, 사용자와 더 원활히 소통합니다.
- ***Performance Highlights***: InfiGUIAgent는 여러 GUI 벤치마크에서 경쟁력 있는 성능을 보여줍니다. ScreenSpot 벤치마크에서 76.3%의 정확도를 기록했으며, 이는 Strong Baseline 솔루션보다 우수한 결과입니다. AndroidWorld 환경에서도 InfiGUIAgent의 전반적인 성공률은 0.09로, 유사한 규모의 오픈소스 모델을 능가하는 성과를 보였습니다. 이는 GUI 작업을 처리할 때 에이전트의 실제 적용 가능성을 강조합니다.

### [OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment across Language with Real-time Self-Aware Emotional Speech Synthesis](https://arxiv.org/abs/2501.04561)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04561.png)

Vote: 13

Authors: Fei Huang, Xiong Liu, Ting-En Lin, Run Luo, Min Yang, Longze Chen, Jiaming Li, Yuchuan Wu, Hamid Alinejad-Rokny, Yongbin Li, Lei Zhang, Yangyi Chen, Haonan Zhang

- ***What's New***: OpenOmni는 실시간 감정 인식 음성 합성을 통해 언어 간 제로샷의 옴니모달 정렬을 가능하게 하는 큰 언어 모델을 제안합니다. 이 모델은 감정을 인식하는 자연스러운 대화와 실시간 감정 음성 생성 능력을 갖춘 최신 옴니모달 언어 모델을 개발하기 위한 두 단계의 훈련 방법을 사용합니다.
- ***Technical Details***: OpenOmni는 언어를 피벗으로 하여 옴니모달 정렬을 수행하는 모델입니다. 정렬 단계에서는 사전 훈련된 음성 모델을 텍스트-이미지 태스크에서 추가적으로 훈련하여 비즈니스 모델보다 더 나은 성능을 보여줍니다. 음성 생성단계에서는 경량화된 디코더를 사용하여 감정적 음성 생성을 실시간으로 훈련합니다. 이 과정에는 O2S-300K 및 EO2S-9K라는 고품질 양방향 음성 생성 데이터셋 사용이 포함됩니다.
- ***Performance Highlights***: OpenOmni는 OmniBench 벤치마크에서 VITA 모델을 네 개의 절대 점수로 초과하며, 더 적은 양의 데이터와 적은 모델 파라미터(7B vs 7x8B)를 가지고도 더 우수한 성능을 발휘합니다. 여러 이중 모드 태스크에서도 경쟁력 있는 성능을 발휘하며 실시간 감정 음성 생성, 음성 이해 및 이미지-텍스트 질문 답변 등을 포함하여 낮은 비용으로 높은 성과를 보였습니다.

### [GeAR: Generation Augmented Retrieval](https://arxiv.org/abs/2501.02772)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02772.png)

Vote: 11

Authors: Weiwei Deng, Shaohan Huang, Feng Sun, Haoyu Liu, Furu Wei, Yuefeng Zhan, Jianfeng Liu, Hao Sun, Qi Zhang

- ***What's New***: GeAR은 기존의 바이엔코더 기반 검색 방법의 한계를 극복하기 위해 설계된 새로운 접근법으로, 검색과 로컬라이제이션 능력을 강화해주는 'Generation-Augmented Retrieval' 메서드를 소개합니다.
- ***Technical Details***: GeAR은 쿼리(query)와 문서(document)의 정보 삼중합(query-document-information triples)을 구성하여, 대조 학습(contrastive learning)과 텍스트 디코더(text decoder)를 통해 세밀한 정보 로컬라이제이션과 텍스트 생성 능력을 향상시킵니다. 이 방법은 기존의 바이엔코더와 동일한 연산 부담을 유발하지 않고, 대규모 언어 모델(LLM)을 통해 고품질 데이터를 효율적으로 합성해냅니다.
- ***Performance Highlights***: 다양한 검색 시나리오와 데이터셋에서 GeAR은 경쟁력 있는 검색 성능과 세밀한 정보 로컬라이제이션 성능을 보여주었습니다. GeAR 모델은 쿼리와 문서를 기반으로 관련 정보를 생성함으로써 검색 결과 해석에 새로운 통찰을 제공하며, 전통적인 검색 과정에 새로운 관점을 제시합니다.

### [SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images](https://arxiv.org/abs/2501.04689)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04689.png)

Vote: 9

Authors: Varun Jampani, Zixuan Huang, James M. Rehg, Aaryaman Vasishta, Mark Boss

- ***What's New***: SPAR3D는 단일 이미지에서 고품질 3D 메쉬를 재구성하는 최신 모델입니다. 이 모델은 빠른 복원 속도(0.7초)를 자랑하며, 사용자 인터랙티브 편집을 지원합니다.
- ***Technical Details***: SPAR3D는 두 단계로 작업을 수행하며, 첫 번째 단계에서는 경량 포인트 확산 모델(point diffusion model)을 사용하여 희소 3D 포인트 클라우드를 생성하고, 두 번째 단계에서는 샘플링된 포인트 클라우드와 입력 이미지를 활용하여 고도로 상세한 메쉬를 생성합니다. 중간 표현으로 포인트 클라우드를 사용함으로써 상호작용 사용자 편집이 가능하도록 하였습니다.
- ***Performance Highlights***: SPAR3D는 다양한 데이터셋에서 최첨단 메서드보다 우수한 성능을 보여줍니다. 단일 객체를 0.7초 미만의 인퍼런스 시간 내에 복원 가능하며, 다수의 벤치마크에서 우수한 정량적 및 정성적 결과를 기록하였습니다. AI 생성 이미지와 다양한 이미지에서도 강력한 일반화 능력을 보여줍니다.

### [Chirpy3D: Continuous Part Latents for Creative 3D Bird Generation](https://arxiv.org/abs/2501.04144)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04144.png)

Vote: 9

Authors: Jia Wei Sii, Jiankang Deng, Tao Xiang, Chee Seng Chan, Yi-Zhe Song, Xiatian Zhu, Jing Yang, Kam Woh Ng

- ***What's New***: Chirpy3D는 자연스러운 부품 간섭 및 샘플링을 가능하게 하여 완전히 새로운 유형의 조류를 3D로 생성할 수 있는 능력을 제공합니다. 이 시스템은 두 가지 주요 혁신을 통해 2D 세부 이해를 3D로 끌어올리면서 새로운 종을 창조하는 최초의 솔루션입니다.
- ***Technical Details***: 새로운 기술적 접근 방식은 다중 뷰 디퓨전 모델(차용: MVDream)과 연속적인 부품 잠재 공간을 사용하여 세부적으로 나누어진 부품을 생성합니다. 자가 감독된 기능 일관성 손실(Feature Consistency Loss)을 통해, 새로운 부품을 안정적으로 생성하고, LoRA 레이어를 U-Net의 교차-주의 레이어에 통합하여 효율적으로 모델 교육을 진행하는 방법론입니다. 부분 잠재 공간은 가우시안 분포로 모델링되며 이는 부품 간 보간과 무작위 샘플링을 통해 새 종을 생성할 수 있습니다.
- ***Performance Highlights***: Chirpy3D는 200 여종의 CUB-200-2011 데이터 세트를 활용하여 세분화된 조류 종을 3D로 생성할 수 있었으며, PartCraft와 비교하여 FID 점수에서 약 4% 개선된 성과를 보였습니다. 자동화된 부품 구성 평가에서는 소품 인버전(Textual Inversion) 방식보다 높은 효과를 입증하며, 비시각적 부품 생성에서 가시적 일관성을 유지합니다.

### [DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization](https://arxiv.org/abs/2501.03271)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03271.png)

Vote: 5

Authors: Aman Chadha, Vasu Sharma, Vinija Jain, Rajarshi Roy, Gurpreet Singh, Aishwarya Naresh Reganti, Danush Khanna, Amitava Das, Yaswanth Narsupalli, Suranjana Trivedy, Basab Ghosh

- ***What's New***: DPO-Kernels는 Direct Preference Optimization (DPO)의 개량 버전으로, 커널을 활용하여 다양하고 복잡한 인간 가치 및 선호도를 모델링하려는 AI 전략을 제시합니다. Mahalanobis, 폴리노미얼, RBF, 스펙트럴 커널을 통합하여 더 풍부하고 표현력이 강한 특징 변환을 가능케 하며, Jensen-Shannon, Hellinger, Bhattacharyya와 같은 다양한 발산 척도를 포함시켜 안정성과 견고성을 향상시켰습니다. 이를 통해 데이터 주도적 접근에서 최적의 커널-발산 쌍을 시설 없이 선택할 수 있는 방법을 소개합니다.
- ***Technical Details***: DPO-Kernels는 28개 조합(4개의 커널 × 7개의 발산 Maßstäbe) 중 최적의 커널-발산 쌍을 선택하기 위한 자동화 메트릭을 사용합니다. 자동 데이터 분석 도구를 이용해 세밀한 정렬 알고리즘을 최적화하며, 하이브리드 손실을 통해 확률 기반 손실과 임베딩 기반 손실을 결합하여 고전적인 DPO의 한계를 넘어서 최적화를 강화합니다.
- ***Performance Highlights***: 12개의 데이터셋을 통한 평가 결과, DPO-Kernels는 사실성, 안전성, 추론 및 명령 수행 과제에서 최첨단 일반화 성능을 달성했습니다. 특히, Mahalanobis 커널을 사용한 경우 높은 차원의 연관성을 고려하여 강력한 일반화 성능을 보여주었습니다. 그러나 이러한 성과는 기존 DPO보다 3-4배 높은 연산 비용을 요구하며, 이를 개선하기 위한 연구가 필요합니다.

### [EpiCoder: Encompassing Diversity and Complexity in Code Generation](https://arxiv.org/abs/2501.04694)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04694.png)

Vote: 4

Authors: Xiao Liu, Yangyu Huang, Qi Chen, Ying Xin, Haoling Li, Xin Zhang, Wenxiang Hu, Yujiu Yang, Jinsong Su, Scarlett Li, Jie Wu, Zhongxin Guo, Yaoxiang Wang

- ***What's New***: EpiCoder는 다채롭고 복잡한 코드 생성(Code Generation)을 위한 새로운 기능 트리 기반의 합성 프레임워크를 소개합니다. 이 프레임워크는 추상 구문 트리(Abstract Syntax Trees; AST)에 영감을 받아, 코드 요소 간의 의미론적 관계를 모델링할 수 있는 기능 트리로 구성됩니다.
- ***Technical Details***: 기능 트리 기반의 코드 데이터 합성 프레임워크는 기능 집합을 추출하여 트리 구조를 시연하는 단계인 기능 트리 추출(Feature Tree Extraction), 기능 트리 진화(Feature Tree Evolution), 진화된 기능 트리를 활용하여 다양한 복잡도의 코드 지침 데이터를 생성하는 기능 트리 기반 코드 생성(Feature Tree-Based Code Generation)의 세 가지 주요 단계로 구성됩니다.
- ***Performance Highlights***: EpiCoder 시리즈는 평균 다섯 가지 함수 수준의 코드 생성 벤치마크에서 동일한 크기의 다른 모델들을 초월하는 최첨단 성능(State-of-the-Art)을 달성했습니다. 또한 파일 수준 벤치마크인 XFileDep에서도 파일레벨 트레이닝 데이터로 훈련된 모델이 뛰어난 성능을 보였습니다.

### [Generalizable Origin Identification for Text-Guided Image-to-Image Diffusion Models](https://arxiv.org/abs/2501.02376)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02376.png)

Vote: 1

Authors: Yi Yang, Zongxin Yang, Zhentao Tan, Yifan Sun, Zhengdong Hu, Wenhao Wang

- ***What's New***: 이 논문은 텍스트 기반 이미지-투-이미지(diffusion models)를 악용하기 쉽게 하거나 원본 이미지를 식별할 새로운 방법론을 소개합니다. 이 기술은 산발적으로 발생하는 보안 문제를 해결하기 위해 특수한 심층 임베딩 모델(deep embedding model)을 훈련시키는 것을 목표로 합니다.
- ***Technical Details***: 이 연구는 'OriPID'라는 첫 데이터셋을 구축하고, 생성된 이미지와 원본 사이의 거리 차이를 최소화하는 선형 변환의 존재를 증명하여 일반화 문제를 해결합니다. 다양한 diffusion models에 대해 이 이론이 얼마나 일반화될 수 있는지를 실험을 통해 입증하고, 생성된 이미지와 참조 이미지 간의 기능 벡터를 비교하는 방법을 사용합니다.
- ***Performance Highlights***: 제안된 방법은 기존의 유사성 기반 접근법보다 평균 정확도(Mean Average Precision; mAP)에서 +31.6%의 향상을 보이며 일반화 성능을 크게 능가합니다. 7개의 다양한 diffusion models을 대상으로 실험했을 때, 각각 88.8%, 81.5%, 87.3%, 89.3%, 85.7%, 85.7%, 90.3% mAP를 기록합니다.

### [Multi-task retriever fine-tuning for domain-specific and efficient RAG](https://arxiv.org/abs/2501.04652)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.04652.png)

Vote: 1

Authors: Patrice Béchard, Orlando Marquez Ayala

- ***What's New***: 이 논문은 Retrieval-Augmented Generation (RAG)의 실제 응용 분야에서 도메인별 고효율의 다중 작업 검색기를 미세 조정하는 방법을 제시합니다. 여기서 소형 검색기 인코더를 다양한 도메인별 작업에 대해 명령어를 사용해 미세 조정함으로써 여러 사용 사례를 지원 가능한 단일 검색기를 배포할 수 있는 방법을 소개합니다.
- ***Technical Details***: 제안된 검색기는 내부 데이터베이스에서 추출한 데이터를 활용하여 여러 작업에 대해 명령어 기반 미세 조정을 거칩니다. 특히, mGTE라는 모델을 fine-tuning 하여 구조화된 데이터를 포함한 점에서 효율적이고 도메인 특화된 검색을 지원합니다. BM25 및 멀티링구얼 임베딩 모델(mE5, mGTE)과 비교하여 성능을 평가하였습니다.
- ***Performance Highlights***: 제안된 다중 작업 미세 조정 도움을 받는 모델은 BM25 및 기타 오픈 소스 대안들보다 좋은 성능을 보였습니다. RAG(데이터 검색 기반 생성) 설정을 위해 필드 검색의 어려움에도 불구하고 도메인 전반에 걸쳐 잘 일반화되는 모습을 보여, 다양한 데이터 검색이 필요한 실제 환경 평가에서도 우수한 결과를 입증하였습니다.

