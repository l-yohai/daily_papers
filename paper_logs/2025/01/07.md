## Daily Papers (2025-01-07)

### [ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use](https://arxiv.org/abs/2501.02506)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02506.png)

Vote: 5

Authors: Junjie Ye, Weijian Lin, Yufei Xu, Zhiheng Xi, Qi Zhang, Xuesong Yao, Zaiyuan Wang, Sining Zhu, Xuanjing Huang, Jiechao Chen, Tao Gui, Siyu Yuan, Zehui Chen, Zhengyin Du

- ***What's New***: ToolHop은 대형 언어 모델(LLMs)의 멀티-홉 도구 사용(multi-hop tool use) 능력을 평가하기 위해 설계된 새로운 데이터셋입니다. 995개의 사용자 쿼리와 3,912개의 관련 도구로 구성되어 있으며, 쿼리 주도 데이터 구축 방식(query-driven data construction approach)을 통해 도구 생성, 문서 정제(document refinement), 코드 생성(code generation)을 포함합니다.
- ***Technical Details***: ToolHop은 멀티-홉 쿼리를 원자적 쿼리로 분해하여 적절한 도구를 선택하고, 도구 피드백을 반복적으로 검색하여 최종 답변에 도달하는 과정으로 LLM의 이해, 추론 및 함수 호출 능력을 평가합니다. 모델은 LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, GPT의 다섯 개 모델군에서 14개의 LLM을 평가하여 멀티-홉 도구 사용 시나리오의 상당한 도전 과제를 드러냈습니다.
- ***Performance Highlights***: 모델 중 최고 성능을 보인 GPT-4o가 49.04%의 정확도를 기록하였으나, 이는 멀티-홉 도구 사용에서 상당한 개선의 여지를 남깁니다. 연구 결과를 통해 모델 군에 따라 도구 사용 전략이 다양하게 나타났으며, Qwen2.5는 병렬 호출을 중시하여 환상을 초래한 반면, GPT 군은 도구 피드백을 활용하여 성능을 개선했습니다.

### [Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation](https://arxiv.org/abs/2501.03059)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03059.png)

Vote: 10

Authors: Shelly Sheynin, Yaniv Taigman, Adam Polyak, Yuval Kirstain, Amit Zohar, Guy Yariv, Yossi Adi, Sagie Benaim

- ***What's New***: Through-The-Mask는 이미지에서 비디오 생성으로의 과제를 해결하기 위한 혁신적인 두 단계 프레임워크를 소개합니다. 이 방법은 마스크 기반 모션 트랙터리스를 중간 표현으로 사용하여 여러 객체의 모션을 일관성 있게 구현함으로써 기존 모델이 겪던 문제를 해결했습니다.
- ***Technical Details***: 이 연구는 두 단계의 구성적 프로세스를 제안합니다: (i) 입력 이미지를 기반으로 한 명시적 중간 표현 생성 단계 (ii) 이 표현을 조건으로 활용한 비디오 생성 단계. 모션 트랙터리스는 객체 수준의 주의 목표(object-level attention objectives)를 사용하여 입력된 이미지, 세분화된 마스크(segmentation mask), 텍스트 프롬프트를 조건으로 생성됩니다.
- ***Performance Highlights***: 복잡한 다 객체와 고 모션 시나리오가 포함된 벤치마크에서 본 방법이 시간적 일관성, 모션 현실성 및 텍스트 프롬프트 충실도 측면에서 최첨단 결과를 달성했음을 실험을 통해 입증했습니다. 새로운 SA-V-128 벤치마크가 싱글 객체 및 복수 객체 비디오 생성의 복잡한 문제를 해결하도록 설계되었습니다.

### [Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models](https://arxiv.org/abs/2501.01830)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01830.png)

Vote: 5

Authors: Huijia Zhu, Ben He, Xianpei Han, Weiqiang Wang, Le Sun, Yaojie Lu, Shuhen Zhou, Hongyu Lin, Yanjiang Liu

- ***What's New***: AUTO-RT는 대규모 언어 모델(LLMs)의 취약성을 탐색하고 최적화하기 위한 자동 강화 학습 프레임워크입니다. 이전 방법이 개별적인 안전성 결함에 초점을 맞췄던 것과 달리, AUTO-RT는 공격 전략의 최적화를 통해 복잡한 취약성을 효과적으로 발견할 수 있습니다.
- ***Technical Details***: AUTO-RT는 강화 학습을 기반으로 하여 공격 전략을 자동으로 탐색하고 최적화합니다. 주요 기여는 두 가지 알고리즘을 통합한 것입니다. 첫째, Early-terminated Exploration(조기 종료 탐색)을 통해 고위험 공격 전략에 집중하여 중요하지 않은 경로를 실시간으로 중지합니다. 둘째, Progressive Reward Tracking(점진적 보상 추적)은 First Inverse Rate(FIR)라는 새로운 메트릭을 사용하여 보상 신호의 밀도를 높입니다.
- ***Performance Highlights***: AUTO-RT는 다양한 LLM들에 대해 공격 탐색의 효율성을 크게 향상시켰음이 입증되었으며, 기존 방법들보다 16.63% 더 높은 성공률을 기록했습니다. 특히 컴퓨터 자원과 태스크 최적화를 동시에 고려하며, 빠른 취약성 발견 속도를 제공합니다.

### [Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction](https://arxiv.org/abs/2501.03218)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03218.png)

Vote: 20

Authors: Shuangrui Ding, Pan Zhang, Yuhang Cao, Dahua Lin, Xiaoyi Dong, Yuhang Zang, Jiaqi Wang, Rui Qian

- ***What's New***: Dispider는 비디오 LLMs(Video Large Language Models)에서 실시간 상호작용을 가능하게 하는 시스템으로, 인식(Perception), 결정(Decision), 반응(Reaction)을 분리하여 비디오 입력을 지속적으로 처리하면서 즉각적이고 맥락에 맞는 반응을 제공합니다. 이는 비디오LLM-online 모델의 반응 생성 및 비디오 처리의 차단 문제를 해결해 실시간 상호작용을 향상시키는 고유한 패러다임을 제시합니다.
- ***Technical Details***: Dispider는 경량 스트리밍 비디오 처리 모듈을 특징으로 하여 비디오 스트림을 추적하고 최적의 상호작용 순간을 파악합니다. 상호작용이 촉발되면 비동기 상호작용 모듈이 세부적인 응답을 제공합니다. 장면 기반 인식 모듈은 비디오 스트림을 장면 경계를 기반으로 비고르게 세분화해 구조적 정보를 보존하고, 실시간 응답 결정 모듈은 비동기 상호작용을 통해 비디오 분석과 응답 생성을 병렬로 처리합니다.
- ***Performance Highlights***: 실험 결과 Dispider는 StreamingBench와 ETBench와 같은 벤치마크에서 VideoLLM-online 및 기존 오프라인 모델들을 능가하여, 특히 장시간 비디오 스트림에서 프로엑티브 반응 생성 및 시간적 추론에 뛰어난 성능을 보였습니다. 이러한 비동기 설계는 Dispider가 실시간 상호작용에 이상적인 솔루션임을 증명합니다.

### [BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning](https://arxiv.org/abs/2501.03226)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03226.png)

Vote: 21

Authors: Yuhong Liu, Pan Zhang, Yuhang Cao, Dahua Lin, Beichen Zhang, Yuhang Zang, Xiaoyi Dong, Haodong Duan, Jiaqi Wang

- ***What's New***: BoostStep는 대형 언어 모델(Large Language Models)의 수학적 능력을 한 단계씩 개선된 단일 단계 추론(single-step reasoning)을 통해 향상시키는 일반적이고 강력한 방법입니다. 이전에는 적합했던 문제 수준의 예제 검색을 세부적인 단계 수준(step level)으로 정밀화하여 각 추론 단계에 높은 관련성을 지닌 예시를 제공하는 최초의 '첫 시도(first-try)' 전략을 도입했습니다.
- ***Technical Details***: BoostStep는 문제 수준의 비슷한 예제를 검색하는 전통적인 방법 대신 단계 수준의 그라뉼러리티(granularity)로 검색을 정밀화합니다. 이를 통해 각 단계의 추론에서 더 알맞은 예제를 찾고, '첫 시도' 전략을 통해 모델이 현재 필요한 추론을 파악하여 유사한 단계를 검색할 수 있게 합니다. 이 방법은 더 세밀한 지도를 제공하여 개별 단계의 이유정확성을 높입니다.
- ***Performance Highlights***: BoostStep는 다양한 수학적 벤치마크에서 GPT-4o의 수학적 성능을 3.6%, Qwen2.5-Math-72B를 2.0% 향상시켰으며, Monte Carlo Tree Search (MCTS)와 결합했을 때는 7.5%의 성능 향상을 이끌어냈습니다. 이는 새로운 예제 검색 방법과 첫 시도 전략이 기존의 문제-수준의 몇발 학습(few-shot learning)과 비교하여 평균적으로 4%의 성능 개선을 가져왔습니다.

### [Test-time Computing: from System-1 Thinking to System-2 Thinking](https://arxiv.org/abs/2501.02497)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02497.png)

Vote: 15

Authors: Min Zhang, Kaixin Wu, Yixin Ji, Juntao Li, Jia Xu, Hai Ye, Linjian Mo

- ***What's New***: 이 논문은 테스트 타임 컴퓨팅(Test-time Computing)을 통해 시스템-1 모델에서 시스템-2 모델로의 진화 과정을 탐색하며, 시스템-2 사고를 통해 모델의 복잡한 추론 능력을 확장하는 방법을 제안합니다. 이 연구는 모델의 추론 능력을 고양하기 위해 테스트 타임 컴퓨팅 확장이 모델의 잠재력을 어떻게 풀어주는지를 보여줍니다.
- ***Technical Details***: 테스트 타임 컴퓨팅은 시스템-1 모델에서는 파라미터 업데이트, 입력 수정, 표현 수정, 출력 보정을 통해 분포 변화에 대처하고 강인성을 향상시킵니다. 반면, 시스템-2 모델에서는 반복 샘플링, 자기 교정, 트리 탐색을 통해 복잡한 문제를 해결하는 추론 능력을 강화합니다. 이는 시스템-1 모델이 약한 시스템-2 모델로, 그리고 더 강한 시스템-2 모델로 전환하는 데 핵심적인 역할을 합니다.
- ***Performance Highlights***: 최신 연구 결과에 따르면, 테스트 타임 컴퓨팅을 통해 복잡한 추론 작업에서 시스템-2 사고를 가진 모델의 성능이 눈에 띄게 개선될 수 있습니다. 테스트 타임에서의 계산 노력을 증가시킬수록 모델의 추론 성능이 향상된다는 것을 보여줍니다.

### [Samba-asr state-of-the-art speech recognition leveraging structured state-space models](https://arxiv.org/abs/2501.02832)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02832.png)

Vote: 4

Authors: Kruthika KR, Kartik Basavaraj Angadi, Syed Abdul Gaffar Shakhadri

- ***What's New***: Samba-ASR는 최초로 상태공간 모델(Structured State-Space Models; SSMs)을 활용하여 첨단 성능을 보이는 자동 음성 인식(ASR) 모델입니다. Transformer 기반의 ASR 모델들이 가지는 한계, 예를 들어 입력 길이에 따른 복잡한 계산과 긴 범위의 의존성 처리의 어려움을 해결하여 뛰어난 정확성과 효율성을 달성하였습니다.
- ***Technical Details***: Samba-ASR는 Mamba 아키텍처를 사용하여 인코더와 디코더를 설계하였으며, 이는 선택적 순환 및 하드웨어에 최적화된 연산을 포함합니다. Mamba는 시퀀스 길이에 선형적으로 확장되면서도 효율적으로 긴 범위의 의존성을 모델링합니다. 또한, 학습 및 추론 중 메모리 부하를 최소화하는 하드웨어 중심의 구현을 통해 연산 효율성을 최적화하였습니다.
- ***Performance Highlights***: 대규모 벤치마크 데이터셋에서 Samba-ASR는 경쟁 모델을 능가하는 성능을 보였습니다. 평균 단어 오류율(Word Error Rate; WER)은 3.65%로, 특히 LibriSpeech Clean에서는 1.17%의 WER을 기록하여 새로운 기준을 세웠습니다. GigaSpeech와 SPGISpeech에서도 각각 9.12%, 1.84%의 WER로 우수한 결과를 얻었습니다.

### [Scaling Laws for Floating Point Quantization Training](https://arxiv.org/abs/2501.02423)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02423.png)

Vote: 4

Authors: Yu Cheng, Jie Jiang, Zhanhui Kang, Shuai Li, Di Wang, Kan Wu, Yixing Li, Xingwu Sun, Weidong Han, Ruobing Xie, Zhen Yang, An Wang, Yangyu Tao, Chengzhong Xu, Shuaipeng Li, Jinbao Xue

- ***What's New***: 이 논문에서는 부동 소수점 양자화 훈련(Floating-Point Quantization Training)의 스케일링 법칙을 다룹니다. 저자들은 지수 비트(Exponent Bits)와 맨티사 비트(Mantissa Bits)의 최적 비율을 제공하여 하드웨어 제조사들이 참조할 수 있도록 하였습니다. 또한, 저자들은 저정밀 LLM 훈련에서 중요한 데이터 크기의 형성을 발견하였으며, 계산 능력과 권장 정확도 사이의 비례 관계를 제시하였습니다.
- ***Technical Details***: 부동 소수점 양자화 훈련의 성능을 조사하기 위해 다양한 정확도 설정에서 366개의 모델을 검토하였습니다. 논문에서 제안된 스케일링 법칙은 모델 크기(N), 데이터 크기(D), 지수(E), 맨티사(M), 블록 크기(B)에 대한 영향을 포괄적으로 분석하며, 특히 저정밀 훈련에서 발생하는 '지식 강도'와 '정보 손실'을 정량화합니다.
- ***Performance Highlights***: 실험 결과, 부동 소수점 양자화는 저정밀 훈련 설정에서 예상한 모델 성능보다 큰 편차를 나타냈습니다. 논문에서는 최적의 맨티사와 지수 비트 수가 각각 FP4, FP8, FP16으로 제안되었으며, 권장되는 비용-성능 비율은 4-8 비트 범위 안에 있다고 평가하였습니다.

### [METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring](https://arxiv.org/abs/2501.02045)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02045.png)

Vote: 12

Authors: Jeff Kaufman, Johannes Hagemann, Jason Wiemels, Ollie Liu, Willie Neiswanger, Shangshang Wang, Sami Jaghouar

- ***What's New***: METAGENE-1은 새로운 메타지놈 데이터를 기반으로 전염병 모니터링을 위한 기초 모델(Foundation Model)을 제시합니다. METAGENE-1은 인간의 폐수에서 수집된 다양한 DNA 및 RNA 서열을 분석하여 팬데믹 모니터링 및 병원체 탐지에 활용할 수 있는 잠재력을 보여줍니다.
- ***Technical Details***: METAGENE-1은 70억 개의 파라미터를 가진 자동회귀 변환기 모델(Autoregressive Transformer Model)로, 1.5조개 이상의 염기쌍으로 구성된 메타지놈 데이터를 바이트-쌍 인코딩(Byte-Pair Encoding; BPE) 방법을 사용하여 전처리합니다. 모델은 곱게 조정된 메타지놈 데이터를 통해 자연어 처리(NLP)에서 사용되는 기술과 인프라를 활용하여 교육되었습니다.
- ***Performance Highlights***: METAGENE-1은 병원체 탐지 벤치마크에서 다른 최신 모델보다 뛰어난 성과를 보였으며, 특히 다양한 시퀀싱 조건에서 강력한 성능을 발휘하였습니다. 또한, 일반적인 유전체 평가 작업에서 최첨단의 성과를 나타내며 공중보건 응용 분야에 큰 잠재력을 보여주고 있습니다.

### [TransPixar: Advancing Text-to-Video Generation with Transparency](https://arxiv.org/abs/2501.03006)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03006.png)

Vote: 8

Authors: Zhifei Chen, Yingcong Chen, Zhifei Zhang, Yijun Li, Luozhou Wang, He Zhang, Jui-Hsien Wang, Zhe Lin

- ***What's New***: TransPixar는 텍스트 투 비디오 생성(Text-to-Video Generation)에 투명성(Transparency)을 도입하여 RGBA 비디오 생성을 가능하게 하는 방법론을 제안합니다. 이 방법은 시각적 특수 효과(VFX)와 인터랙티브 콘텐츠 제작을 위해 알파 채널(Alpha Channel)을 포함한 투명 비디오 생성의 가능성을 확장합니다.
- ***Technical Details***: TransPixar는 기존 RGB 비디오 생성 모델에 디퓨전 트랜스포머(DiT)를 기반으로 알파-특정 토큰(Alpha-specific tokens)을 추가하고 LoRA 기반 세밀 조정(LoRA-based fine-tuning)을 통해 RGBA 채널을 동시에 생성합니다. 이 과정에서 셀프 어텐션(self-attention) 메커니즘을 최적화하여 적은 데이터로도 RGB와 알파 채널 간의 일관성을 유지합니다.
- ***Performance Highlights***: 제안된 방법은 다양한 RGBA 영상을 생성하며 RGB와 알파 채널 간의 강력한 정렬을 유지하며 적은 교육 데이터로도 우수한 성능을 보여줍니다. 실험 결과, 기존의 예측 기반 파이프라인과의 비교에서 뛰어난 성능을 입증하며, 사용자 연구에서도 긍정적인 피드백을 받았습니다.

### [Personalized Graph-Based Retrieval for Large Language Models](https://arxiv.org/abs/2501.02157)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02157.png)

Vote: 16

Authors: Ojasmitha Pedirappagari, Nikos Kanakaris, Hanieh Deilamsalehy, Cameron J. Dimacali, Franck Dernoncourt, Yu Wang, Nesreen K. Ahmed, Ryan A. Rossi, Steven Au, Namyong Park

- ***What's New***: 이 논문에서는 개인화된 텍스트 생성에서의 제한점을 극복하기 위해 사용자 중심의 지식 그래프(User-centric Knowledge Graph)를 활용한 PGraphRAG(Personalized Graph-based Retrieval-Augmented Generation) 프레임워크를 제안합니다. 기존의 방법들은 사용자의 히스토리에만 의존하여 초기 데이터가 부족한 상황에서는 한계를 보였으나, PGraphRAG는 구조화된 사용자 정보로 프롬프트를 보강하여 개인화의 문맥 이해와 출력의 질을 향상시킵니다.
- ***Technical Details***: PGraphRAG 시스템은 사용자의 히스토리와 상호작용을 바탕으로 사용자 중심의 그래프를 구축하여 관련 정보를 언어 모델에 제공합니다. 주어진 입력과 사용자 중심의 지식 그래프가 결합되어 보다 정밀하고 개인화된 텍스트를 생성하게 됩니다. 함께 제안된 Personalized Graph-based Benchmark는 LLMs의 개인화된 텍스트 생성 능력을 평가하기 위해 만들어졌습니다. 이 벤치마크는 실세계 환경 속에서 사용자의 히스토리가 부족한 시나리오를 다룹니다.
- ***Performance Highlights***: 실험 결과, PGraphRAG는 다양한 작업에서 최신 개인화 방법들을 능가하는 성능을 보였습니다. 예를 들어, 장문 생성 작업에서 ROUGE 및 METEOR 지표에서 이전 방법들에 비해 뛰어난 성과를 거두었고, 단문 생성 작업에서는 더 작은 개선폭이지만 여전히 나은 성과를 기록하였습니다. 이는 사용자 중심의 지식 그래프를 통한 개인화된 텍스트 생성이 유리함을 시사합니다.

### [DepthMaster: Taming Diffusion Models for Monocular Depth Estimation](https://arxiv.org/abs/2501.02576)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02576.png)

Vote: 5

Authors: Li Liu, Bo Li, Tianzhu Zhang, Ruijie Zhu, Zerong Wang, Ziyang Song, Peng-Tao Jiang, Hao Zhang

- ***What's New***: DepthMaster는 단일 단계에서 퓨전 모델(Diffusion Models)을 활용하여 단안 깊이 추정(Monocular Depth Estimation)을 개선하는 새로운 접근 방식을 제안합니다. 본 연구에서는 기능 정렬 모듈(Feature Alignment Module)과 푸리에 향상 모듈(Fourier Enhancement Module)을 도입하여 모델의 일반화 능력과 세부 보존 능력을 크게 향상시켰습니다.
- ***Technical Details***: DepthMaster는 단일 단계에서 RGB 이미지를 입력받아 깊이 맵을 출력하는 결정론적 패러다임을 채택합니다. 기능 정렬 모듈은 외부 인코더의 고품질 시맨틱 기능을 융합하여 세부 텍스처에 대한 과적합을 완화합니다. 푸리에 향상 모듈은 주파수 도메인에서 작동하며 저주파 구조와 고주파 세부 사항을 균형 잡기 위해 설계되었습니다. 이 모듈들은 두 단계의 교육 전략을 통해 최적화됩니다.
- ***Performance Highlights***: DepthMaster는 다양한 데이터셋에서 최신 성능(state-of-the-art)을 기록하며, 특히 일반화 및 세부 보존 능력에서 다른 확산 기반 방법을 능가합니다. 데이터 드리븐(data-driven) 접근법과 모델 드리븐(model-driven) 접근법의 격차를 효과적으로 좁힐 수 있음을 입증하였습니다.

### [Ingredients: Blending Custom Photos with Video Diffusion Transformers](https://arxiv.org/abs/2501.01790)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.01790.png)

Vote: 6

Authors: Di Qiu, Debang Li, Mingyuan Fan, Changqian Yu, Zhengcong Fei

- ***What's New***: 이 논문은 비디오 디퓨전 트랜스포머(Video Diffusion Transformers)를 활용해 여러 개의 특정 ID 사진을 통합하여 맞춤형 비디오를 생성할 수 있는 강력한 프레임워크인 'Ingredients'를 소개합니다. 이 방법은 여러 얼굴 ID를 유지하면서 사용자 정의 프롬프트를 수용하여 개인화된 비디오 콘텐츠를 생성하는 데 초점을 맞춥니다.
- ***Technical Details***: Ingredients는 세 가지 주요 모듈로 구성되어 있습니다: (i) 각 ID의 글로벌 및 로컬 관점을 고려하여 다재다능하고 정확한 얼굴 특징을 캡처하는 얼굴 추출기(Facial Extractor); (ii) 얼굴 임베딩을 비디오 디퓨전 트랜스포머의 이미지 쿼리 컨텍스트 공간으로 매핑하는 멀티스케일 프로젝터(Multi-scale Projector); (iii) 여러 ID 임베딩을 해당 시공간 영역에 동적으로 할당하는 ID 라우터(ID Router). 이 프레임워크는 다단계 훈련 프로세스를 통해 고유 ID의 보존과 비디오 생성을 최적화합니다.
- ***Performance Highlights***: Ingredients는 기존의 개인화된 비디오 생성 기술보다 개선된 높은 품질의 비디오를 생성할 수 있으며, 실험 결과에서 높은 페이스 시뮬러리티(Face Similarity)와 낮은 프리딩적 거리(FID)를 달성했습니다. 이는 텍스트와 이미지 간의 일관성을 유지하면서도 정교한 사용자 정의 컨트롤이 가능하다는 강점을 보여줍니다.

### [Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation](https://arxiv.org/abs/2501.03225)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.03225.png)

Vote: 1

Authors: Chenyu Wang, Serena Yeung-Levy, Alejandro Lozano, Anjiang Wei, Yiming Liu, Josiah Aklilu, Ludwig Schmidt, Yuchang Su, Elaine Sui, Xiaohan Wang, James Burgess, Yuhui Zhang

- ***What's New***: 이 연구에서는 비전 언어 모델(Vision Language Models; VLMs)이 보다 정확하고 객관적인 평가를 받을 수 있도록 개방형 질문을 자동적으로 다지선다형(Multiple-Choice)으로 변환하는 AutoConverter라는 새로운 프레임워크를 소개합니다. 이를 활용해 20개의 기존 VQA 데이터셋을 통합 다지선다형 질문 형식으로 변환하여 VMCBench라는 새로운 벤치마크를 구성하였습니다.
- ***Technical Details***: AutoConverter는 GPT-4o 기반의 다중 에이전트 시스템을 사용하여 개방형 질문을 다지선다형 질문으로 자동 변환합니다. 정확성 향상과 난이도 증강을 위해 특수한 제안자(Proposer) 에이전트와 리뷰어(Reviewer) 에이전트가 협력하며, 최종적으로 선택된 오답의 정확성은 평가 에이전트(Evaluator)에 의해 판별됩니다. 이런 프로세스를 통해 각 선택지가 서로 명확하게 구별되도록 보장합니다.
- ***Performance Highlights***: AutoConverter로 생성된 다지선다형 질문은 사람이 만든 질문보다 비슷하거나 더 낮은 정확도를 보이며 VLM들의 성능을 보다 명확하게 차별화합니다. VMCBench를 통해 33개의 최신 VLM을 평가한 결과, GPT-4o가 전체적으로 80.6% 정확도를 보이면서 최상의 성능을 기록하였고, 이는 오픈소스 모델인 Qwen2-VL-7B와 차이가 2%에 불과하여 성능 차이가 점차 줄어들고 있음을 시사합니다.

### [STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution](https://arxiv.org/abs/2501.02976)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02976.png)

Vote: 36

Authors: Ying Tai, Jun Zhou, Jian Yang, Yinhong Liu, Kai Zhang, Zhenyu Zhang, Chen Zhao, Rui Xie, Penghao Zhou, Zhenheng Yang

- ***What's New***: STAR는 실세계 비디오 초해상도를 위한 T2V(Text-to-Video) 모델을 활용한 시공간 증강 기법을 제안하여 실감나는 공간적 세부사항과 강력한 시간적 일관성을 달성합니다. 이는 복잡한 열화 상황에서의 인위적 흔적을 줄이고, 강력한 T2V 모델의 생성 능력으로 인한 품질 저하를 보완합니다.
- ***Technical Details***: STAR는 글로벌 주의 블록 전의 로컬 정보 강화 모듈(Local Information Enhancement Module; LIEM)을 도입하여 로컬 세부사항을 풍부하게 하고 열화 흔적을 완화합니다. 또한, 역 확산 과정에서 낮은 주파수와 높은 주파수의 구성 요소에 초점을 맞춰야 함을 모델에 유도하는 동적 주파수 손실(Dynamic Frequency Loss)을 제안합니다. 이는 모델이 확산 단계에서 서로 다른 주파수 구성 요소에 초점을 맞추도록 안내함으로써 충실도를 향상시킵니다.
- ***Performance Highlights***: STAR는 OpenVid30 및 REDS30과 같은 합성 데이터셋과 VideoLQ와 같은 실세계 데이터셋에서 최첨단 방법을 능가하는 우수한 성능을 입증합니다. 특히, STAR는 DOVER와 같은 동영상 선명도 점수에서 최고 점수를 기록하였으며, SSIM(Structural Similarity) 및 LPIPS(Perceptual Similarity) 지표에서도 우수한 결과를 보입니다. 이는 STAR가 공간적 세부 사항이 뛰어나고 시간적 일관성이 높아 실감나는 비디오를 복원할 수 있음을 나타냅니다.

### [AutoPresent: Designing Structured Visuals from Scratch](https://arxiv.org/abs/2501.00912)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.00912.png)

Vote: 3

Authors: Alane Suhr, Maarten Sap, Daniel Fried, Graham Neubig, Jiaxin Ge, Trevor Darrell, Qinyue Tan, Xuhui Zhou, Sanjay Subramanian, Zora Zhiruo Wang, Yi-Hao Peng

- ***What's New***: 이 연구에서는 자연어(Natural Language; NL) 지시를 통해 슬라이드를 생성하는 자동화된 슬라이드 생성 방안을 제안합니다. AUTOPRESENT라는 새로운 모델을 소개하며, 이 모델은 슬라이드 생성을 위한 프로그램 코드와 자연어 지시 쌍 7,000개로 학습되었습니다. 또한 이 모델은 비공개 모델인 GPT-4O와 비교할 수 있는 성능을 보여줍니다. SLIDESBENCH라는 최초의 슬라이드 생성 벤치마크도 도입하여 다양한 도메인에서 생성된 슬라이드에 대해 평가할 수 있게 했습니다.
- ***Technical Details***: SLIDESBENCH 벤치마크는 여러 디자인 원칙을 기반으로 한 참조 기반과 비기준 방식의 평가지표를 제공합니다. AUTOPRESENT 모델은 8B 파라미터의 LLAMA 기반 모델로, 프로그램 생성 방식을 활용하여 자연어 지시로부터 Python 프로그램을 생성하고 이를 실행하여 PPTX 슬라이드를 얻습니다. SLIDESLIB라는 라이브러리를 개발해 슬라이드 프로그램 생성 과정을 단순화했습니다. 이 라이브러리는 타이틀, 텍스트 추가, 이미지 검색 및 생성 함수 등을 포함하여 모델이 더 간결하고 모듈식의 프로그램을 생성할 수 있습니다.
- ***Performance Highlights***: AUTOPRESENT 모델은 성능 면에서 주요 GPT 모델에 필적하며, 특히 SLIDESLIB를 사용할 때 더 나은 결과를 보였습니다. GPT-4O와 비교해도 상세한 지시 없이도 강력한 성능을 보여주었습니다. SLIDESLIB를 사용한 프로그램 생성 방식은 이미지 생성 방식보다 더 구조적이고 명료한 슬라이드를 생성할 수 있었습니다.

### [GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking](https://arxiv.org/abs/2501.02690)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.02690.png)

Vote: 11

Authors: Yijin Li, Fu-Yun Wang, Hongsheng Li, Xiaoyu Shi, Zhaoyang Huang, Weikang Bian

- ***What's New***: 이 논문에서는 영상 생성(video generation)을 위한 새로운 프레임워크 GS-DiT를 소개합니다. 이 프레임워크는 효율적인 조밀한 3D 포인트 트래킹(Dense 3D Point Tracking; D3D-PT) 기술을 사용하여 의사 4D 가우시안 필드(pseudo 4D Gaussian fields)를 구축하고, 다양한 카메라 파라미터 조작을 통해 4D 콘텐츠 제어를 가능하게 합니다. GS-DiT는 비디오 생성에서의 렌즈 기법과 카메라 내재적 제어의 한계를 뛰어넘어 더 창의적인 영상 제작을 지원합니다.
- ***Technical Details***: 제안된 GS-DiT 프레임워크는 먼저 입력 영상의 기초로 조밀한 3D 포인트를 추적하여 의사 4D 가우시안 필드를 구축합니다. 그 후, 이 가우시안 필드를 렌더링하여 모든 비디오 프레임에 적용하고, 미세조정(finetuning) 과정을 통해 사전 훈련된 Video Diffusion Transformer(DiT)를 개선하여 새로운 영상 생성을 지시합니다. GS-DiT의 핵심 아이디어는 기존의 렌더된 영상이 비록 아티팩트를 포함하고 있더라도 영상 생성을 위한 강력한 단서를 제공한다는 점입니다.
- ***Performance Highlights***: GS-DiT는 최첨단 밀도 3D 포인트 트래킹 기법인 SpatialTracker보다 두 배 이상의 속도로 추론을 가속화하면서도 높은 정확도를 자랑합니다. 다중 카메라 촬영 영상 생성에서는, PSNR, SSIM, LPIPS 측면에서 GCD 및 MonST3R를 비롯한 다른 기법을 성능적으로 초과하며, 카메라 위치와 객체 움직임까지 제어할 수 있는 강력한 4D 제어 능력을 입증합니다. 또한, GS-DiT는 카메라 내부적 특성 조정과 객체 모션 편집을 통해 돌리 줌(dolly zoom)과 같은 고급 영화 효과를 지원합니다.

