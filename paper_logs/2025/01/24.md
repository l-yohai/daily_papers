## Daily Papers (2025-01-24)

### [SRMT: Shared Memory for Multi-agent Lifelong Pathfinding](https://arxiv.org/abs/2501.13200)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13200.png)

Vote: 44

Authors: Yuri Kuratov, Alsu Sagirova, Mikhail Burtsev

- ***What's New***: SRMT (Shared Recurrent Memory Transformer)는 멀티 에이전트 지속 경로 찾기 문제를 해결하기 위한 새로운 접근 방식을 제안합니다. 이 연구는 멀티 에이전트 강화 학습(MARL) 문제에서 에이전트 간 정보 교환을 가능하게 하여 협력을 향상시키고 교착 상태를 예방합니다. SRMT는 부분적으로 관찰 가능한 멀티 에이전트 경로 찾기 문제에서 기존의 강화 학습 베이스라인보다 우수한 성능을 보여줍니다.
- ***Technical Details***: SRMT는 기억(transformer) 변형기를 멀티 에이전트 설정으로 확장하여 개별 작업 기억을 풀링하고 전체적으로 방송합니다. 이 메커니즘은 에이전트가 개별 기억 표현을 공유된 공간에 읽고 쓰기를 학습하여 더 나은 의사결정을 가능하게 합니다. SRMT는 두 개의 작은 방을 좁은 통로로 연결하는 '목마름(narrow corridor)' 환경에서 테스트되었습니다.
- ***Performance Highlights***: SRMT는 다양한 보상 함수 설정에서도 일관되게 우수한 성과를 보이며, 특히 환경의 피드백이 최소화된 상황에서 에이전트 간의 공유 기억을 통한 협력이 중요한 역할을 합니다. SRMT는 실험에서 협력 성공률(CSR), 개인 성공률(ISR), 시간 소요 비용(SoC) 측면에서 기존 벤치마크를 능가했습니다.

### [Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models](https://arxiv.org/abs/2501.13629)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13629.png)

Vote: 31

Authors: Xiao Liang, Ziyue Yang, Xuan Feng, Peng Cheng, Yaoxiang Wang, Yu Yan, Zhenghao Lin, Guoshuai Zhao, Yuting Jiang, Hang Li, Zihao Tang, Kun Kuang, Jian Jiao, Xiao Liu, Hao Ni, Yuqing Xia, Yasen Hu, Wenjie Li, Yelong Shen, Zheheng Luo, Qi Chen, Yeyun Gong, Jui-Hao Chiang, Chen Lin, Binyang Li, Zhongxin Guo, Yi Cheng, Lei Qu, Feiyang Chen, Mao Yang, Shuai Lu, Yiming Huang, Kailai Yang, Ying Xin

- ***What's New***: SIGMA는 시스템 도메인에 특화된 효율적인 대형 언어 모델로, 새롭게 제안된 DiffQKV attention 아키텍처를 사용합니다. 이 아키텍처는 Query(Q), Key(K), Value(V) 요소의 차별적 압축을 통해 성능과 효율성을 향상시킵니다.
- ***Technical Details***: DiffQKV attention 메커니즘은 Q, K, V 구성 요소가 다른 수의 헤드와 다양한 차원의 헤드를 가질 수 있도록 허용하며, K, V 캐시를 각각의 전략으로 출력 값을 계산합니다. SIGMA의 효율성을 높이기 위해 K 헤드 수를 크게 줄이고, V 벡터를 선택적으로 적재하는 방식으로 구성됩니다.
- ***Performance Highlights***: SIGMA는 전통적인 그룹 쿼리 어텐션(GQA) 대비 최대 33.36% 빠른 추론 속도를 자랑합니다. 시스템 도메인에서 SIGMA 모델은 AIMICIUS 벤치마크에서 GPT-4보다 최대 52.5% 우수한 성능을 보여주었습니다.

### [Improving Video Generation with Human Feedback](https://arxiv.org/abs/2501.13918)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13918.png)

Vote: 30

Authors: Xiaohong Liu, Yujiu Yang, Wenyu Qin, Ziyang Yuan, Di Zhang, Qiulin Wang, Xiaokun Liu, Xiele Wu, Jie Liu, Mingwu Zheng, Jiajun Liang, Pengfei Wan, Wanli Ouyang, Xintao Wang, Menghan Xia, Gongye Liu, Fei Yang, Kun Gai

- ***What's New***: 이 논문은 인간 피드백 (Human Feedback)을 이용하여 비디오 생성 모델을 개선하는 방법을 제안합니다. 특히, 비디오 생성의 움직임 부드럽지 못한 문제와 비디오와 프롬프트 간의 불일치 문제를 해결하기 위해 인간 선호 데이터 셋의 구축과 새로운 다차원 비디오 보상 모델, VideoReward를 소개합니다.
- ***Technical Details***: 이 연구에서는 강력한 비디오 생성 시스템을 학습시키기 위해 강화학습 (Reinforcement Learning)의 관점에서 접근하여, KL 정규화(KL Regularization)를 최대화하는 세 가지 정렬 알고리즘을 제안합니다. 이는 흐름 기반 모델 (Flow-based Models)에게 차수최적화(Direct Preference Optimization)와 보상 가중 회귀 (Reward Weighted Regression)라는 두 가지 학습 전략과, 흐름을 기반으로 하는 추론 시 기술, Flow-NRG를 포함합니다.
- ***Performance Highlights***: 실험 결과, 제안한 비디오 보상 모델, VideoReward는 기존의 보상 모델을 능가하였으며, Flow-DPO는 Flow-RWR과 기존의 지도 기반 세분화 방법보다 뛰어난 성능을 보여주었습니다. 또한, Flow-NRG는 사용자가 추론 중에 다양한 목표에 맞춘 가중치를 적용할 수 있게 하여 개인화된 비디오 품질 요구를 충족시킬 수 있었습니다.

### [Temporal Preference Optimization for Long-Form Video Understanding](https://arxiv.org/abs/2501.13919)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13919.png)

Vote: 12

Authors: Zeyu Wang, Xiaohan Wang, Yuhui Zhang, Serena Yeung-Levy, Rui Li

- ***What's New***: 이 논문에서는 대규모 비디오 멀티모달 모델(video-LMMs)이 긴 비디오는 물론이고 시간적인 시점 파악을 개선할 수 있는 새로운 포스트 트레이닝 기법인 Temporal Preference Optimization(TPO)을 소개합니다. TPO는 두 가지 시간적 세분성을 가진 선호 학습(self-training)을 통해 모델이 시간적으로 잘 맞춰진 응답을 우선시할 수 있도록 합니다.
- ***Technical Details***: TPO는 모델이 선호된 응답과 비선호 응답의 대비를 사용해 더욱 시간적으로 잘 맞는 응답을 우선시할 수 있는 기회를 제공합니다. 로컬라이즈드(localized)와 종합적인(comprehensive) 시간적 시점(preference) 데이터셋을 생성하여 이를 통해 모델의 시간적 기반 이해를 향상시킵니다. Direct Preference Optimization(DPO) 방법론을 사용해 모델 최적화가 이루어집니다.
- ***Performance Highlights***: LongVideoBench, MLVU, Video-MME 세 가지 벤치마크에 대한 실험 결과 TPO는 LongVideoBench에서 2.9%, MLVU에서 3.1%, Video-MME에서 2.5%의 성능 향상을 보였습니다. 특히 LLaVA-Video-TPO 모델은 Video-MME 벤치마크에서 최신 7B 모델로서 그 성능을 입증했습니다.

### [Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step](https://arxiv.org/abs/2501.13926)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13926.png)

Vote: 11

Authors: Peng Gao, Zhizheng Zhao, Renrui Zhang, Chengzhuo Tong, Hongsheng Li, Pheng-Ann Heng, Ziyu Guo

- ***What's New***: 이 논문은 대규모 아키텍처의 체인-오브-생각(Chain-of-Thought; CoT) 추론 전략을 이미지 생성에 적용하는 최초의 체계적인 연구를 수행했습니다. 이를 통해 이미지 생성 성능이 상당히 향상되며, 이를 위해 PARM과 PARM++라는 새로운 보상 모델을 제안했습니다.
- ***Technical Details***: PARM(Potential Assessment Reward Model)은 자기 회귀적 이미지 생성에서 각 생성 단계별로 적절히 평가하여 단계별 잠재력 평가를 수행하는 데 중점을 둔 보상 모델로, 결과적으로 고품질의 최종 이미지를 생성하는 데 도움을 줍니다. PARM++는 PARM에 반영 기제를 추가해 생성된 이미지를 자체 수정하는 능력을 제공합니다. 이 모델들은 Show-o라는 최신 모델을 기반으로 사용되며 GenEval 벤치마크에서 그 효과를 입증했습니다.
- ***Performance Highlights***: GenEval 벤치마크에서 Show-o 모델은 PARM과 PARM++을 통합하여 +24%의 향상을 이루었고, 이는 Stable Diffusion 3를 +15% 상회하는 결과입니다. 특히, '두 개체', '색상', '위치', '속성 결합'과 같은 복잡한 속성에서 상당한 향상이 관찰되었습니다.

### [Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos](https://arxiv.org/abs/2501.13826)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13826.png)

Vote: 8

Authors: Penghao Wu, Wang Xiao, Bo Li, Fanyi Pu, Xiang Yue, Kairui Hu, Yuanhan Zhang, Ziwei Liu

- ***What's New***: Video-MMMU는 대규모 멀티모달 모델(Large Multimodal Models; LMMs)의 동영상 기반 지식 획득 능력을 평가하는 새로운 벤치마크로, 영상에서 지식을 획득하고 이를 활용할 수 있는지를 평가하기 위해 설계되었습니다. 300개의 전문가 수준의 영상과 900개의 질문을 통해 다양한 학문 분야에 걸쳐 평가합니다.
- ***Technical Details***: Video-MMMU는 6개의 학문 분야에서 전문가들이 선정한 주제에 기반한 300개의 교육 비디오로 구성되어 있습니다. 각 비디오는 지식 획득의 세 가지 인지 단계(Perception, Comprehension, Adaptation)에 맞춰 설계된 질문-답변 쌍을 포함하며, 지식 향상 척도인 Δknowledge를 통해 성능 개선을 수치화합니다.
- ***Performance Highlights***: 인간과 모델의 지식 획득 능력을 비교한 결과, 인간은 비디오 시청 후 Δknowledge에서 33.1%의 성능 향상을 보인 반면, GPT-4o와 같은 최상위 모델은 15.6%의 향상을 기록했습니다. 이는 현재 모델들이 비디오 기반 학습에서 해결해야 할 도전 과제를 나타냅니다.

### [IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models](https://arxiv.org/abs/2501.13920)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13920.png)

Vote: 8

Authors: Peng Gao, Weifeng Lin, Xiangfei Hu, Zhongyu Li, Renrui Zhang, Xinyue Li, Yiting Lu, Shitian Zhao, Ruoyi Du, Le Zhuo, Zhen Li, Hongsheng Li, Wenjian Sun, Ziyu Guo, Jiayi Lei

- ***What's New***: IMAGINE-E는 최신 텍스트-이미지(T2I) 모델들의 성능을 포괄적으로 평가하기 위해 개발된 프레임워크입니다. 이 프레임워크는 FLUX.1, Ideogram2.0, Midjourney, Dall-E3, Stable Diffusion 3, Jimeng 등 6개의 대표적인 T2I 모델을 5가지 주요 영역에서 평가했습니다. 이 연구는 T2I 모델이 일반화된 활용 가능성으로 나아갈 수 있는 잠재력을 강조합니다.
- ***Technical Details***: IMAGINE-E는 구조적 출력 생성, 현실감 및 물리적 일관성, 특정 도메인 생성, 도전적인 시나리오 생성, 다양한 스타일 창작 과제를 포함하는 5개의 주요 영역으로 나누어 모델을 평가합니다. 평가 지표로는 CLIPScore, HPSv2, Aesthetic Score, GPT-4o 점수가 사용되며, 이러한 점수들과 인간 평가의 일관성을 비교해 평가했습니다.
- ***Performance Highlights***: FLUX.1과 Ideogram2.0은 구조적 출력 작업과 특정 도메인 생성 과제에서 우수한 성능을 보였습니다. 반면, GPT-4o와 인간 평가의 결과는 다른 대부분의 평가 지표들과 비교했을 때 인간 인식과 더 잘 일치했습니다. MIDjourney는 스타일 일관성과 관련된 작업에서 더 높은 미적 품질을 보여주었습니다.

### [Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback](https://arxiv.org/abs/2501.10799)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10799.png)

Vote: 7

Authors: Chen Zhu, Yuandong Tian, Jason Weston, Hao Ma, Yun He, Yun-Nung Chen, Tianhao Wu, Han Fang, Arash Rahnama, Sinong Wang, Tengyu Xu, Di Jin, Yen-Ting Lin, Sainbayar Sukhbaatar

- ***What's New***: 이 연구는 수학적 추론 작업에서 중간 추론 단계와 최종 답변 모두에 대해 단순한 이진 피드백을 제공하여 대형 언어 모델(LLM)의 논리적이고 신뢰할 수 있는 추론 경로를 유도하는 Step-KTO라는 새로운 학습 및 최적화 프레임워크를 제안합니다. Step-KTO는 중간 추론 단계의 논리적 진행을 강조하여 기존 방법의 최종 답변 정확성에 치중한 한계를 극복합니다.
- ***Technical Details***: Step-KTO는 과정 피드백(Process-level feedback)과 결과 피드백(Outcome-level feedback)을 결합하여 논리적이고 일관된 추론 경로를 유도합니다. 각 중간 단계는 PRM(Process Reward Model)을 사용하여 평가하고 최종 답변은 Outcome Reward Model을 통해 평가합니다. Kahneman-Tversky에서 영감을 받은 가치 함수를 사용하여 사람과 유사한 위험 회피 성향을 모델에 주입합니다. 이 접근은 문제 해결 패턴과 일치하는지에 대한 이진 피드백을 단계별로 제공하여 학습 목표를 형성합니다.
- ***Performance Highlights***: 수학적 추론 벤치마크 MATH-500에서 Step-KTO는 Pass@1 정확도를 53.4%에서 63.2%로 향상시켰으며, 이외에도 AMC23과 AIME24 데이터셋에서도 뛰어난 성능 개선을 보였습니다. 이 모델은 다른 최신 기법들을 능가하며 복잡하고 긴 추론 경로에 있어서 더욱 신뢰할 수 있고 일관된 결과를 제공합니다.

### [DiffuEraser: A Diffusion Model for Video Inpainting](https://arxiv.org/abs/2501.10018)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10018.png)

Vote: 7

Authors: Xiaowen Li, Haolan Xue, Peiran Ren, Liefeng Bo

- ***What's New***: DiffuEraser는 스테이블 확산 모델(Stable Diffusion Model)을 기반으로 한 비디오 인페인팅(Video Inpainting) 모델로, 마스크된 영역을 보다 자세하고 일관된 구조로 채우도록 설계되었습니다. 이 모델은 초기화를 위한 약한 조건을 제공하는 사전 정보를 포함하여 노이즈 아티팩트를 줄이고 환각을 억제합니다.
- ***Technical Details***: DiffuEraser는 확산 모델의 강력한 생성 능력을 활용하여 묘사된 구조를 완성하고 더 자세한 콘텐츠를 생성합니다. 또한, 이전 모델과 DiffuEraser의 시간적 수용 범위를 확장하여 긴 시퀀스 추론 동안 시간적 일관성을 개선했습니다. 기술적으로 BrushNet을 기반으로 하여 추가적인 분기를 통해 마스크된 이미지에서 특징을 추출하고 주가 되는 디노이징 UNet에 단계적으로 통합합니다.
- ***Performance Highlights***: 실험 결과에 따르면 제안된 방법은 콘텐츠 완전성과 시간적 일관성 모두에서 최첨단 기술보다 우수한 성과를 보였으며, 효율성을 적절히 유지합니다. 특히, Propainter와 비교하여 텍스처 품질과 시간적 일관성에서 더 뛰어난 성능을 발휘했습니다.

### [One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt](https://arxiv.org/abs/2501.13554)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13554.png)

Vote: 5

Authors: Senmao Li, Joost van de Weijer, Tao Liu, Ming-Ming Cheng, Kai Wang, Fahad Shahbaz Khan, Jian Yang, Yaxing Wang, Shiqi Yang

- ***What's New***: 이 연구에서는 One-Prompt-One-Story(1Prompt1Story)라는 새로운 훈련이 필요 없는 방법을 제안하여 컨텍스트 일관성을 활용해 텍스트-이미지 변환(T2I)에서 일관된 아이덴티티 표현을 보장합니다. 이는 싱글 프롬프트(Single Prompt)를 사용하여 다양한 장면에서 일관된 아이덴티티를 유지합니다.
- ***Technical Details***: 1Prompt1Story는 텍스트 내의 아이덴티티 정보를 유지하는 컨텍스트 일관성(context consistency)을 응용하였으며, Singular-Value Reweighting(SVR) 및 Identity-Preserving Cross-Attention(IPCA) 기법을 도입하여 텍스트와 이미지 간의 정렬을 개선하고 주제 일관성을 강화하였습니다.
- ***Performance Highlights***: 이 방법은 ConsiStory+ 벤치마크에서 주요한 일관성과 텍스트 정렬 개선을 보여주었으며, 다양한 장면에서 동일한 주제를 유지하면서 현재 널리 사용되는 기술들을 능가하는 결과를 제공합니다. 1Prompt1Story는 다양한 T2I 생성 모델에서 호환 가능하며, 텍스트 모델의 특성이 특정 생성 모델과 무관하게 작동합니다.

### [Hallucinations Can Improve Large Language Models in Drug Discovery](https://arxiv.org/abs/2501.13824)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13824.png)

Vote: 4

Authors: Michael Färber, Shuzhou Yuan

- ***What's New***: 이 논문에서는 약물 발견 분야에서 창의성이 중요한 역할을 하는데, Large Language Models(LLMs)의 잘못된 사실 생성(hallucinations)이 이를 향상시킬 수 있다는 가설을 제시했습니다. 이 가설을 검증하기 위해, LLMs가 분자의 SMILES 문자열을 자연어로 설명하고, 이를 특정 약물 탐색 작업에 활용하여 성능을 평가하였습니다.
- ***Technical Details***: 약물 발견에 LLMs의 환상(hallucinations)이 긍정적인 영향을 줄 수 있는지를 확인하기 위해 다양한 LLMs를 7가지로 평가하고, 5가지 분류 작업에 이러한 개념을 적용했습니다. 비교를 위해, 분자 텍스트 설명은 MolT5를 참고 기준으로 하였고, Llama-3.1-8B 및 기타 LLMs로부터 생성된 환상을 입력 프롬프트에 포함시켜 ROC-AUC 성능을 비교하였습니다.
- ***Performance Highlights***: Llama-3.1-8B는 환상 포함 프롬프트에서 SMILES 기준보다 18.35%, MolT5 기준보다 13.79% 높은 ROC-AUC 성능 향상을 보였습니다. 특히 GPT-4o로 생성된 환상이 여러 LLMs에서 가장 일관된 성능 개선을 제공했으며, 언어, 모델 크기, 생성 온도 등의 추가 인자가 성능에 미치는 영향을 조사했습니다.

### [EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion](https://arxiv.org/abs/2501.13452)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13452.png)

Vote: 4

Authors: Wenfeng Lin, Renjie Chen, Jiangchuan Wei, Mingyu Guo, Shiyue Yan, Boyuan Liu

- ***What's New***: EchoVideo는 멀티모달 피처 융합(Multimodal Feature Fusion)을 통해 정체성을 보존하는 인간 비디오 생성 방법을 제안합니다. 기존의 '복사-붙여넣기' 문제를 해결하면서 얼굴뿐만 아니라 전체 몸체의 일관성을 유지할 수 있는 혁신적인 접근법을 제공합니다.
- ***Technical Details***: EchoVideo는 높은 수준의 의미론적 피처를 포착하는 Identity Image-Text Fusion (IITF) 모듈을 통해 텍스트, 이미지, 얼굴 이미지를 통합합니다. IITF는 얼굴 식별 정보를 깨끗하게 추출하고, 모달리티 간의 의미론적 갈등을 해결하며, 플러그 앤 플레이가 가능하도록 설계되어 다양한 태스크에 적용할 수 있습니다. DiT (Diffusion Transformers)를 기반으로 하는 이 모델은 개인화된 비디오 콘텐츠를 생성합니다.
- ***Performance Highlights***: EchoVideo는 FaceSim 측면에서 ConsisID보다 낮은 성능을 보이지만, CLIPScore(텍스트 지침의 준수도)와 FID(Fréchet Inception Distance)를 포함하여 비디오 품질 측면에서 우수한 성능을 보입니다. 또한, 기존의 방법에 비해 '복사-붙여넣기'와 같은 문제를 개선하여 더 자연스러운 결과물을 생성할 수 있습니다.

### [Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass](https://arxiv.org/abs/2501.13928)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13928.png)

Vote: 4

Authors: Hao Tang, Joyce Chai, Mikael Henaff, Matt Feiszli, Ang Cao, Jianing Yang, Alexander Sax, Franziska Meier, Kevin J. Liang

- ***What's New***: Fast3R는 1000장 이상의 비정렬, 비배치 사진을 단일 한 번에 3D로 재구성할 수 있는 방법을 제안합니다. 이 연구는 DUSt3R를 확장하여 여러 뷰에서 병렬로 사진을 처리하는 Transformer 기반 구조를 제안하여 보다 효율적이고 확장 가능한 3D 재구성을 달성합니다.
- ***Technical Details***: Fast3R는 Transformer 기반 아키텍처를 사용하여 다수의 이미지를 병렬로 처리하며, 다중 뷰 포인트맵(Pointmap) 추정에 있어서 글로벌 후처리를 제거하여 속도, 계산 오버헤드 및 확장성에서 상당한 개선을 제공합니다. 각 프레임이 입력 세트의 다른 모든 프레임에 동시에 주의를 기울일 수 있도록 하여 오류 누적을 크게 줄입니다.
- ***Performance Highlights***: Fast3R는 CO3Dv2 데이터셋에서 포즈 추정에서 15도 이내의 정확성 99.7%를 달성하며, DUSt3R에 비해 오류를 14배 감소시켰습니다. 또한 단일 A100 GPU에서 1500장 이상의 이미지를 한 번에 처리할 수 있으며, DUSt3R에 비해 100배 이상 빠른 추론 속도를 갖췄습니다.

### [Debate Helps Weak-to-Strong Generalization](https://arxiv.org/abs/2501.13124)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13124.png)

Vote: 3

Authors: Hao Lang, Yongbin Li, Fei Huang

- ***What's New***: 이 논문은 강력한 사전 학습 모델과 대화를 통해 약한 인간 감독을 강화하고 이를 사용하여 강력한 모델을 감독하는 방법을 제안합니다. 강한 모델과 약한 모델의 팀이 어떻게 더 나은 약한 감독자를 생성하여 강한 모델의 능력을 이끌어낼 수 있는지에 대한 정확한 정보를 얻는 데 초점을 맞추고 있습니다.
- ***Technical Details***: 이 연구에서는 강력한 사전 학습 모델(Debaters)을 사용하여 대화를 통해 진실성을 이끌어내고, 약한 모델(Weak Model)의 훈련 시 샘플에 대한 맥락 정보로 활용합니다. 약한 모델 앙상블(Weak Model Ensemble)을 사용하여 더 강력한 감독 평가를 얻을 수 있으며, 이러한 접근 방식을 통해 강력한 학생 모델을 더 잘 훈련할 수 있습니다.
- ***Performance Highlights***: OpenAI의 Weak-to-Strong NLP 벤치마크를 사용한 실험 결과, 이 접근 방식은 다른 솔루션보다 일관되고 우수한 성능을 보여주었습니다. 특히 대화 앙상블(Debate Ensemble)을 활용한 예측이 단일 약한 모델(Single Weak Model) 및 보정된 약한 감독 모델들과 비교하여 가장 견고한 감독 추정치를 제공합니다.

### [Evolution and The Knightian Blindspot of Machine Learning](https://arxiv.org/abs/2501.13075)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13075.png)

Vote: 3

Authors: Kenneth O. Stanley, Joel Lehman, Tarin Ziyaee, Elliot Meyerson, Tarek El-Gaaly

- ***What's New***: 이 논문에서는 강화 학습(Reinforcement Learning; RL)이 개방적인 세계에서 대처하기 어려운 부분이 있다고 주장하며, 이를 자연 진화와 비교하여 강화 학습의 형식이 미래의 예기치 않은 상황에 대한 강건함을 충분히 다루지 않는다고 설명합니다. 컴퓨터 비전, 자연어 처리 등 다양한 분야에서 머신러닝이 인간 지식보다 계산의 활용을 통해 더욱 발전했지만, 개방형 미래의 질적 강건함을 확보하는 데에는 한계가 있음을 강조합니다.
- ***Technical Details***: 논문은 경제학의 나이티안 불확실성(Knightian Uncertainty; KU) 개념을 ML 알고리즘에 적용할 수 있는지 탐구하며, 이를 강화 학습과 자연 진화를 비교하여 설명합니다. RL은 마르코프 의사결정 과정(Markov Decision Process; MDP)에 기반하여 동작하며, 이는 일반적으로 폐쇄적인 세계에서 작동하도록 설계되었기 때문에 현실 세계의 불확실성과 적절히 맞지 않는다고 말합니다. 이에 따라 강화 학습이 미래의 예측 불가능한 복잡한 환경에서 약점을 보일 가능성을 제기합니다.
- ***Performance Highlights***: 자연 진화와 달리 RL 알고리즘은 미래의 질적 변화를 예측하고 대처하기 어렵습니다. 이는 RL의 각종 형식이 실험자가 환경의 모든 변화를 미리 예측할 것을 요구하기 때문입니다. 진화는 모든 구조와 학습 알고리즘을 개방적으로 수정할 수 있는 형태로 작용하기 때문에 더 폭넓은 강건함을 얻을 수 있습니다. 연구의 결론은 현재의 ML이 포괄적인 강건함을 확보하기 위해 KU 문제를 직접 다뤄야 한다는 점을 강조합니다.

### [GSTAR: Gaussian Surface Tracking and Reconstruction](https://arxiv.org/abs/2501.10283)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10283.png)

Vote: 1

Authors: Lixin Xue, Juan Zarate, Chengwei Zheng, Jie Song

- ***What's New***: GSTAR는 역동적인 장면의 변화하는 지형(topology)에 대응하여 정확한 3D 추적과 표면 재구성을 구현하는 새로운 방법론입니다. 이 방법은 정적 장면에 대해 포토리얼리스틱 렌더링을 가능케 하였던 3D 가우시안 스플래팅 기술을 동적인 장면으로 확장하였습니다.
- ***Technical Details***: GSTAR는 다중 뷰 캡처(multi-view capture)를 입력으로 받고, 가우시안을 메쉬의 면에 바인딩함으로써 동적 개체를 표현합니다. 일관된 토폴로지를 지닌 표면에서는 메쉬 토폴로지를 유지하여 가우시안을 사용하여 메쉬를 추적하고, 토폴로지가 변하는 지역에서는 가우시안을 메쉬와 분리하여 새로운 표면을 생성합니다. 추가적으로 프레임 간 트래킹 초기화를 돕는 표면 기반 장면 흐름(scene flow) 방법을 제안하였습니다.
- ***Performance Highlights***: GSTAR는 기존의 SOTA(State-of-the-art) 방법과 비교하여 외관 및 지오메트리 표현에서 우수한 성능을 보였습니다. 또한 다중 뷰 영상 데이터에 의존하므로 범용적으로는 제한적일 수 있으나, 고품질 3D 렌더링 응용 분야에 강력한 표현력을 제공합니다.

### [Control LLM: Controlled Evolution for Intelligence Retention in LLM](https://arxiv.org/abs/2501.10979)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10979.png)

Vote: 1

Authors: Haichao Wei, Yunxiang Ren, Aman Lunia, Alice Leung, Ya Xu, Zhoutong Fu, Yi-Lin Chen

- ***What's New***: Control LLM은 대형 언어 모델(Large Language Models; LLMs)의 연속 학습 과정에서 발생하는 지연된 기억손실(catastrophic forgetting; CF)을 효과적으로 완화하기 위한 새로운 방법입니다. 이 방법은 두 가지 병렬 변환기 블록(pre-trained and expanded transformer blocks)을 활용해, 이들 간의 히든 스테이트(hidden states)를 보존함으로써 최신 작업을 통합하면서 기존 성능을 유지합니다.
- ***Technical Details***: Control LLM은 고정된(pre-trained) 블록과 학습 가능한(expanded) 블록 간의 히든 스테이트 정렬을 위한 보간(interpolation) 전략을 사용하여 모델을 확장합니다. 이 방식의 다양한 구현에는 선형 보간(Lerp) 및 동적 선형 보간(Dlerp)이 포함됩니다. 그림 4는 이러한 구조와 매커니즘을 시각적으로 설명합니다.
- ***Performance Highlights***: Control LLM은 Llama3.1-8B-Instruct 모델에서 수학적 추론(Math-Hard)에서 14.4%의 향상과, MBPP-PLUS에서 코딩 성능이 10% 향상되었습니다. 멀티링궐 성능도 C-Eval에서 10.6%, CMMLU에서 6.8%, CMMLU-0shot-CoT에서 30.2% 증가했습니다. 이 방법론은 기존 학습 능력을 35% 이상 저하시키지 않으면서 성과를 높이며, LinkedIn의 제품에도 성공적으로 적용되었습니다.

