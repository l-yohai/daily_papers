## Daily Papers (2024-01-29)

### [SliceGPT: Compress Large Language Models by Deleting Rows and Columns](https://arxiv.org/abs/2401.15024)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/X2ZXpHxYkhtCfKRwLZ0eP.png)

Vote: 40

Authors: James Hensman, Saleh Ashkboos, Marcelo Gennari do Nascimento, Torsten Hoefler, Maximilian L. Croci

- 큰 언어 모델은 자연어 처리의 핵심 요소로 자리잡았으나, 이를 사용하는 데 많은 계산 및 메모리 자원이 소요된다.
- 자원 제약을 줄이기 위한 해결책으로 스파시피케이션(희소화)이 제안되었고, 훈련된 모델들이 사후에 스파시파이(희소화) 될 수 있다는 것이 최근 연구로 밝혀졌다.
- 기존의 희소화 기술은 추가적인 데이터 구조가 필요하며 현재의 하드웨어에서 제한적인 속도 향상을 제공한다는 문제가 있다.
- 본 논문에서는 SliceGPT라는 새로운 사후 훈련 희소화 방식을 제시하는데, 이는 각 가중치 행렬을 더 작은 밀집 행렬로 대체하여 네트워크의 임베딩 차원을 줄인다.
- 광범위한 실험을 통해 SliceGPT가 LLAMA2-70B, OPT 66B, Phi-2 모델들에 대해서 각각 밀집 모델의 제로샷 작업 성능의 99%, 99%, 90%를 유지하면서 모델 매개변수(임베딩 포함) 중 최대 25%를 제거할 수 있음을 보여준다.
- 슬라이스된 모델은 더 적은 GPU를 사용하고 추가적인 코드 최적화 없이 빠르게 실행되며, 24GB 소비자용 GPU에서는 LLAMA2-70B의 밀집 모델 대비 총 계산을 64%로, 40GB A100 GPU에서는 66%로 줄일 수 있다.
- 트랜스포머 네트워크에서의 계산 불변성이라는 새롭게 발견된 인사이트를 통해 SliceGPT를 가능하게 하였으며, 이는 사전 훈련된 모델들의 메모리 및 계산 요구 사항을 줄이기 위한 미래 연구의 영감과 가능성을 제공할 것으로 기대된다.
- 관련 코드는 https://github.com/microsoft/TransformerCompression에서 확인할 수 있다.

### [From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities](https://arxiv.org/abs/2401.15071)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-OZna4DZ1W3AWNufdsqoC.png)

Vote: 25

Authors: Ming Zhang, Jie Zhang, Hongxing Fan, Jingyi Deng, Lijun Li, Yan Teng, +, Qibing Ren, Sirui Chen, Chaochao Lu, Guodong Zheng, Jinlan Fu, Limin Wang, Kunchang Li, Tao Gui, Yali Wang, Lu Sheng, Chen Qian, Kexin Huang, Wanli Ouyang, Meiqi Chen, Hongzhi Gao, Jing Shao

- 본 논문은 다양한 모달리티(텍스트, 코드, 이미지, 비디오)에서의 일반화, 신뢰성, 인과 추론 능력을 통해 최근 다중 모달 대규모 언어 모델(MLLM)의 성능과 대중의 기대 사이의 격차에 대한 이해를 높이고자 한다.
- 이 연구는 OpenAI의 GPT-4와 Google의 Gemini를 포함, 6개 오픈 소스 대규모 언어 모델(LLM) 및 MLLM을 평가하여 230개의 사례를 종합적으로 분석하였다.
- 연구의 결과는 12개의 점수(4개 모달리티 각각에 대한 3가지 속성)로 요약되며, 각각의 모델들이 실제 다중 모달 애플리케이션에서 얼마나 신뢰할 수 있는지를 정의하는 대표적인 요소를 탐구한다.
- 이 과정에서 보유하고 있는 독점적 및 오픈 소스 MLLM의 능력과 한계를 이해하는 데 유용한 14가지 실증적 발견을 밝혔으며, 이는 더 신뢰할 수 있는 다중 모달 애플리케이션 개발을 위한 기반이 될 것이다.

### [Learning Universal Predictors](https://arxiv.org/abs/2401.14953)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GfKbUTI-puokUCecuKfAI.png)

Vote: 9

Authors: Laurent Orseau, Jordi Grau-Moya, Elliot Catt, Christopher Mattern, Joel Veness, Grégoire Delétang, Anian Ruoss, Marcus Hutter, Li Kevin Wenliang, Tim Genewein, Matthew Aitchison

- 메타러닝은 한정된 데이터로부터 신속하게 새로운 작업을 학습하는 뉴럴 네트워크를 훈련시키는 강력한 접근법으로 부상했습니다.
- 다양한 작업에 대한 광범위한 노출은 일반적인 문제 해결을 가능하게 하는 다재다능한 표현(representations)을 이끌어냅니다.
- 이 연구에서는 메타러닝의 한계를 극대화하여 가장 강력한 보편적 예측기인 솔로몬오프 유도(Solomonoff Induction, SI)를 뉴럴 네트워크에 통합하는 잠재력을 탐구합니다.
- 우리는 보편적 튜링 머신(Universal Turing Machines, UTMs)을 사용하여 네트워크가 다양한 패턴에 노출되도록 하는 훈련 데이터를 생성합니다.
- 저자들은 UTM 데이터 생성 프로세스와 메타-훈련 프로토콜에 대한 이론적 분석을 제공합니다.
- 실험은 LSTM, 트랜스포머 같은 뉴럴 아키텍처와 복잡성 및 보편성이 다양한 알고리즘 데이터 생성기를 사용하여 실시되었습니다.
- 결과는 UTM 데이터가 메타러닝에 유용한 자원임을 시사하며, 이를 통해 보편적 예측 전략을 학습하는 능력을 가진 뉴럴 네트워크를 훈련시킬 수 있다고 제시합니다.

### [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty](https://arxiv.org/abs/2401.15077)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/dYbXUEAvwAyyAxJ-51e9M.png)

Vote: 8

Authors: Fangyun Wei, Yuhui Li, Hongyang Zhang, Chao Zhang

- 대형 언어 모델의 자동 회귀 디코딩이 추론 속도를 느리게 하므로, EAGLE(Extrapolation Algorithm for Greater Language-model Efficiency)이라는 간단한 프레임워크를 제안하여 속도를 높입니다.
- 기존의 추측 샘플링 방법과 달리, EAGLE은 더 정규화된 (두 번째 최상위 레이어의) 특징 수준에서 자동 회귀적 드래프팅 과정을 운영하며, 다음-특징 예측의 샘플링 불확실성 문제를 한 타임 스텝 앞선 토큰을 통합함으로써 해결합니다.
- EAGLE에 의한 가속화는 손실이 없으며, 타깃 LLM을 미세 조정할 필요가 없고, 생성된 텍스트는 기존 자동 회귀 디코딩과 동일한 분포를 유지합니다.
- 이 논문이 제출된 시점에서, EAGLE은 추측 샘플링 범주 내에서 가장 빠른 프레임워크로, MT-bench에서 기존 디코딩 대비 3배, Lookahead 대비 2배, Medusa 대비 1.6배 빠른 속도를 보여줍니다.
- EAGLE은 단일 RTX 3090 GPU에서 LLaMA2-Chat 13B를 사용하여 평균 160 토큰/초의 속도로, Huggingface의 구현체들이 24 토큰/초의 속도와 비교될 때 높은 성능을 달성합니다.

### [Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support](https://arxiv.org/abs/2401.14688)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Jqw4MUhTG3s_cCkSPrYL5.png)

Vote: 7

Authors: Junyu Lu, Jiaxing Zhang, Ziwei Wu, Xiaojun Wu, Yan Song, Ruyi Gan, Dixiang Zhang, Renliang Sun, Pingjian Zhang

- 최근 텍스트-이미지 모델의 발전은 이미지 생성 능력을 크게 강화했지만, 오픈소스 모델에서의 양어(한국어와 중국어) 지원에 여전히 공백이 존재한다.
- 이러한 필요를 충족시키기 위해, 저자들은 CLIP과 Stable-Diffusion-XL의 기능을 확장하여 양어 연속적인 사전 학습을 통한 새로운 중국어 및 영어 양어 텍스트-이미지 모델 Taiyi-Diffusion-XL을 소개한다.
- 이 접근 방법에는 CLIP의 토큰화기 및 임베딩 층에 가장 자주 사용되는 중국어 문자를 효율적으로 통합하는 어휘 확장과 절대 위치 인코딩 확장이 포함된다.
- 대규모 시각-언어 모델에 의한 텍스트 프롬프트의 풍부화를 통해, 더 나은 이미지 캡션을 생성하고 시각적 품질이 높은 이미지를 생성한다.
- 이러한 개선은 후속 텍스트-이미지 모델에 적용된다.
- 경험적인 결과에 의하면 개발된 CLIP 모델은 양어 이미지-텍스트 검색에서 뛰어난 성능을 보여준다.
- 또한, Taiyi-Diffusion-XL의 양어 이미지 생성 능력은 이전 모델들을 뛰어넘는다.
- 이 연구를 통해 Taiyi-Diffusion-XL 모델이 개발되고 오픈소스화되어, 특히 중국어 어플리케이션에서의 이미지 생성 분야에서 주목할 만한 발전을 대표한다.
- 이 기여는 다양한 언어 지원에 대한 모달 연구의 필요성에 대응하기 위한 한 걸음이며, 해당 모델과 데모는 https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/에서 공개적으로 이용할 수 있으며, 이 분야에서의 추가 연구와 협력을 촉진한다.

### [TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts](https://arxiv.org/abs/2401.14828)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ZpqNbxN8MUer8cEyyUJlq.png)

Vote: 3

Authors: Ying Shan, Guanbin Li, Liang Lin, Yan-Pei Cao, Jingyu Zhuang, Di Kang

- TIP-Editor는 텍스트와 이미지 프롬프트 모두에 의한 3D 장면 편집 프레임워크로, 사용자는 편집 지역을 지정하기 위해 3D 경계 상자를 사용할 수 있습니다.
- 이미지 프롬프트를 통해 사용자는 텍스트 설명을 보완하여 대상 콘텐츠의 세부적인 외관/스타일을 명확히 지정할 수 있어, 외관의 정확한 제어가 가능합니다.
- TIP-Editor는 기존 장면과 참조 이미지의 표현을 더 잘 학습하기 위해 점진적인 2D 개인화 전략을 사용하며, 경계 상자에 의해 지정된 올바른 객체 배치를 장려하는 정착 손실을 제안합니다.
- TIP-Editor는 배경을 변경하지 않고 지역 편집을 용이하게 하는 명시적이고 유연한 3D 가우시안 스플레팅을 3D 표현으로 활용합니다.
- 광범위한 실험을 통해 TIP-Editor는 지정된 경계 상자 영역 내에서 텍스트 및 이미지 프롬프트에 따라 정확한 편집을 수행하며, 편집 품질 및 프롬프트와의 정렬에 있어 베이스라인을 일관되게 능가함을 보여주었습니다.

### [Generative Expressive Robot Behaviors using Large Language Models](https://arxiv.org/abs/2401.14673)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WS00gHIo4OAvmheD8kGIN.png)

Vote: 2

Authors: Leila Takayama, Andy Zeng, Zhuo Xu, Carolina Parada, Jonathan Chien, Noah Brown, Fei Xia, Karthik Mahadevan, Dorsa Sadigh

- 사람들은 고개를 끄덕여 상대의 시선을 확인하거나 붐비는 복도에서 앞지르기 위해 "실례합니다"라고 말하는 등 표현적인 행동을 사용하여 효과적으로 의사소통하고 다른 사람과 조정합니다.
- 로봇 또한 인간-로봇 상호작용에서 표현적인 행동을 보여주기를 원하지만, 기존의 규칙 기반 방법은 새로운 의사소통 방식이나 사회적 상황에 적용이 어렵고 데이터 중심의 방법은 로봇이 사용되는 각각의 사회적 상황에 대해 특화된 데이터셋이 필요합니다.
- 본 논문에서는 대규모 언어 모델(LLMs)이 가진 풍부한 사회적 맥락과 지시에 기반한 움직임 생성 능력을 활용하여 조정 가능하고 결합 가능한 표현적인 로봇 움직임을 생성하는 방법을 제안합니다.
- 우리는 휴먼 언어 지시를 로봇의 가능하고 학습된 기술을 사용하는 매개변수화된 제어 코드로 변환하기 위해 'few-shot chain-of-thought prompting' 기법을 활용합니다.
- 사용자 연구와 시뮬레이션 실험을 통해, 우리의 접근 방식이 사용자에게 이해하기 쉽고 유능하다고 느껴지는 행동을 생성함을 보여주었습니다.
- 추가 자료는 https://generative-expressive-motion.github.io/ 에서 찾아볼 수 있습니다.

