## Daily Papers (2024-01-11)

### [PIXART-δ: Fast and Controllable Image Generation with Latent Consistency Models](https://arxiv.org/abs/2401.05252)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/l4v9s2xQHdqOJ8LY6hoXg.png)

Vote: 24

Authors: Junsong Chen, Junsong Chen, Junsong Chen, Yue Wu, Simian Luo, Simian Luo, Simian Luo, Enze Xie, Enze Xie, Enze Xie, Sayak Paul, Sayak Paul, Sayak Paul, Ping Luo, Hang Zhao, Hang Zhao, Hang Zhao, Zhenguo Li, Zhenguo Li, Zhenguo Li

- 이 기술 보고서는 텍스트-이미지 합성 프레임워크인 PIXART-δ를 소개하며, 이는 Latent Consistency Model(LCM)과 ControlNet을 통합하여 고급 PIXART-α 모델을 개선한다.
- PIXART-α는 뛰어난 효율의 트레이닝 과정을 통해 1024px 해상도의 고품질 이미지 생성으로 인정받고 있다.
- PIXART-δ는 LCM 통합을 통해 추론 속도를 크게 향상시켜, 2-4 단계 만에 고품질 이미지를 생성할 수 있다.
- 특히 1024x1024 픽셀 이미지 생성 시 0.5초라는 돌파구를 이루어, PIXART-α 대비 약 7배의 성능 향상을 달성했다.
- 32GB V100 GPU에서 단 하루만에 효율적으로 교육(training)할 수 있게 설계되어 있다.
- PIXART-δ는 8GB GPU 메모리 제약 내에서 1024px 이미지를 합성할 수 있는 8비트 추론 능력을 가지고 있어 사용성과 접근성을 크게 향상시킨다.
- ControlNet과 유사한 모듈을 통합함으로써 텍스트-이미지 확산 모델에 대한 미세한 제어가 가능하다.
- Transformers에 특별히 맞춘 새로운 ControlNet-Transformer 아키텍처를 도입하여, 고품질 이미지 생성과 함께 명시적인 제어 가능성을 실현한다.
- PIXART-δ는 최첨단 오픈소스 이미지 생성 모델로, 안정된 확산(Stable Diffusion) 모델군에 대한 유망한 대안을 제시하며 텍스트-이미지 합성 분야에 상당한 기여를 한다.

### [InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes](https://arxiv.org/abs/2401.05335)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/KHwPYpncQy7UBY4FauF7X.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/KHwPYpncQy7UBY4FauF7X.mp4" muted="false"></video></div>

Vote: 15

Authors: Mohamad Shahbazi, Liesbeth Claessens, Liesbeth Claessens, Liesbeth Claessens, Michael Niemeyer, Michael Niemeyer, Michael Niemeyer, Edo Collins, Alessio Tonioni, Alessio Tonioni, Alessio Tonioni, Luc Van Gool, Federico Tombari

- 'InseRF'는 사용자가 제공한 텍스트 설명과 레퍼런스 뷰포인트의 2D 바운딩 박스를 기반으로 3D 장면에서 새로운 객체를 생성하는 새로운 방법을 소개합니다.
- 최근에 텍스트-이미지 확산 모델의 강력한 사전 정보를 사용하여 3D 생성 모델링이 획기적으로 변모했으나, 새로운 객체를 생성하는 것은 여전히 도전적인 과제입니다.
- 이 연구는 3D 객체 삽입을 3D 장면의 레퍼런스 뷰에서 2D 객체 삽입과 연계하여 제안합니다.
- 2D 편집은 단일 뷰 객체 재구성 방법을 이용하여 3D로 상승시키며, 단안 깊이 추정 방법의 선행 정보에 의해 가이드되어 장면에 객체를 삽입합니다.
- 다양한 3D 장면에 대해 우리의 방법을 평가하고 제안된 구성 요소에 대한 심층 분석을 제공합니다.
- 여러 3D 장면에서 객체를 생성적으로 삽입하는 실험은 우리 방법의 효과를 기존 방법들에 비해 확인시켜줍니다.
- InseRF는 명시적인 3D 정보를 입력으로 요구하지 않고도 조절 가능하고 3D 일관성 있는 객체 삽입이 가능합니다.
- 프로젝트 페이지인 https://mohamad-shahbazi.github.io/inserf 에서 더 많은 정보와 결과를 확인할 수 있습니다.

### [URHand: Universal Relightable Hands](https://arxiv.org/abs/2401.05334)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GRSnQqwcEocqHX-S6sWv6.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GRSnQqwcEocqHX-S6sWv6.mp4" muted="false"></video></div>

Vote: 11

Authors: Zhaoxi Chen, Zhaoxi Chen, Zhaoxi Chen, Gyeongsik Moon, Kaiwen Guo, Chen Cao, Stanislav Pidhorskyi, Tomas Simon, Rohan Joshi, Yuan Dong, Yichen Xu, Bernardo Pires, He Wen, Lucas Evans, Bo Peng, Julia Buffalini, Autumn Trimble, Kevyn McPhail, Melissa Schoeller, Shoou-I Yu, Javier Romero, Michael Zollhöfer, Yaser Sheikh, Ziwei Liu, Ziwei Liu, Ziwei Liu, +

- 기존의 사실적인 재조명 가능한 손 모델은 다양한 관점, 자세, 조명 하에서의 특정 정체성에 대한 광범위한 관찰을 필요로 하며, 자연스러운 조명과 새로운 정체성에 일반화하는 데 어려움이 있습니다.
- 이러한 격차를 해소하기 위해, URHand는 관점, 자세, 조명, 정체성에 걸쳐 일반화할 수 있는 최초의 범용 재조명 가능한 손 모델을 제시합니다.
- URHand 모델은 휴대전화로 촬영한 이미지를 사용하여 소수의 샷으로 개인화할 수 있으며, 새로운 조명 하에서 사실적인 렌더링이 가능합니다.
- 사실주의를 유지하면서 개인화 과정을 단순화시키기 위해, 수백 개의 정체성을 포함하는 조명 스테이지에서 손의 다중 뷰 이미지로부터의 신경 재조명을 기반으로 한 강력한 범용 재조명 사전을 구축했습니다.
- 개인의 신원에 대한 충실도와 세부 사항의 날카로움을 유지하는 동시에 자연스러운 조명 하에서의 일반화를 저해하지 않으면서 정체성 간의 훈련 확장의 핵심적인 도전과제를 해결하기 위해 물리적으로 영감을 받은 셰이딩을 입력 특징으로 하는 공간적으로 변화하는 선형 조명 모델을 제안합니다.
- 비선형 활성화와 편향을 제거함으로써, 우리가 특별히 설계한 조명 모델은 광 전송의 선형성을 명시적으로 유지합니다.
- 이는 조명 스테이지 데이터로부터 단일 단계의 훈련을 가능하게 하며, 다양한 정체성에 걸쳐 임의의 연속적인 조명 하에서 실시간 렌더링에 일반화됩니다.
- 또한, 물리 기반 모델과 신경 재조명 모델의 공동 학습을 소개하여 충실도와 일반화를 더욱 향상시킵니다.
- 광범위한 실험을 통해 우리의 접근법이 품질과 일반화 측면에서 기존 방법들에 비해 우수한 성능을 달성한다는 것을 보여줍니다.
- 또한, 우리는 URHand를 사용하여 본 적 없는 정체성의 짧은 전화 스캔에서 빠르게 개인화하는 것을 시연합니다.

### [The Impact of Reasoning Step Length on Large Language Models](https://arxiv.org/abs/2401.04925)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/33G4qvyyqfGsMGMYGXjIj.png)

Vote: 10

Authors: Mingyu Jin, Mingyu Jin, Mingyu Jin, Qinkai Yu, Dong shu, Haiyan Zhao, Wenyue Hua, Wenyue Hua, Wenyue Hua, Yanda Meng, Yanda Meng, Yanda Meng, Yongfeng Zhang, Yongfeng Zhang, Yongfeng Zhang, Mengnan Du

- 사고의 연쇄(Chain of Thought, CoT)는 대규모 언어 모델(Large Language Models, LLMs)의 추론 능력 향상에 중요하지만, CoT의 효과와 프롬프트 내 추론 단계 길이 사이의 상관관계는 아직 잘 알려져 있지 않다.
- 이를 밝히기 위해, 다양한 실험을 통해 CoT 데모에서 논리적 추론 단계를 확장 및 압축하는 연구를 설계했으며, 다른 모든 요소는 일정하게 유지했다.
- 주요 발견으로는, 새로운 정보를 추가하지 않더라도 프롬프트 내 추론 단계를 늘릴 때 LLMs의 추론 능력이 여러 데이터셋에 걸쳐 현저하게 향상된다는 것을 확인했다.
- 반대로, 핵심 정보를 유지하면서 추론 단계를 짧게 할 때는 모델의 추론 능력이 크게 저하된다.
- 또한, 잘못된 추론 과정이라 하더라도, 추론의 필요한 길이를 유지한다면 긍정적 결과를 낼 수 있다는 흥미로운 관계 또한 밝혀냈다.
- 마지막으로, 추론 단계를 늘리는 것의 이점이 과제에 따라 다르며, 간단한 과제는 적은 단계가 필요하지만 복잡한 과제는 더 긴 추론 시퀀스로 상당한 이득을 볼 수 있음을 관찰했다.

### [Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk](https://arxiv.org/abs/2401.05033)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/krSRbXOs5mGCADXGa_929.png)

Vote: 7

Authors: Dennis Ulmer, Dennis Ulmer, Dennis Ulmer, Elman Mansimov, Elman Mansimov, Elman Mansimov, Kaixiang Lin, Kaixiang Lin, Kaixiang Lin, Justin Sun, Xibin Gao, Yi Zhang

- LLM (대규모 언어 모델)은 대화형 에이전트로서 강력하지만 특정 기능을 수행하도록 전문화하는 것이 어려울 수 있습니다.
- 인간이 생성한 지시사항과 샘플 응답에 모델을 튜닝하는 것이 효과적이지만, 많은 양의 데이터 샘플이 필요하며 이는 a) 사용할 수 없거나 b) 생성하는 데 비용이 많이 들 수 있습니다.
- 대화 중 특정 워크플로우를 따르도록 LLM을 만들려는 경우 이 비용은 더욱 증가합니다.
- 강화학습의 '자기 대화(self-play)' 기법과 LLM을 사용하여 인간 에이전트를 시뮬레이션하는 아이디어에서 영감을 받아 본 연구에서는 다양한 역할에서 대화에 참여하는 LLM을 통한 보다 효과적인 데이터 수집 방법을 제안하고 있습니다.
- 이 접근법은 LLM의 "자기 대화(self-talk)"를 통해 훈련 데이터를 생산하며, 이를 미세 조정(fine-tuning)을 위해 활용하고 정제할 수 있습니다.
- 대화의 (부분적) 성공을 측정하기 위한 자동화된 방법을 도입하고, 이 메트릭을 사용하여 훈련을 위해 LLM에 다시 피드백되는 대화 데이터를 필터링합니다.
- 자동화된 평가 및 인간의 대화 품질 평가를 통해 이러한 '자기 대화' 데이터가 결과를 개선한다는 것을 보여주고 있습니다.
- 또한 생성된 대화의 다양한 특성을 조사하고, 이러한 특성이 훈련 데이터로서의 잠재적 유용성과 어떻게 연결될 수 있는지를 탐색하였습니다.

### [ANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of Video](https://arxiv.org/abs/2401.05314)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6cFgjM4Ek9tW3hFNXVtyz.png)

Vote: 5

Authors: Kevin Cai, Chonghua Liu, Chonghua Liu, Chonghua Liu, David M. Chan, David M. Chan, David M. Chan

- 인터넷 콘텐츠의 60%가 영어로 제작되어 전 세계 인구 중 영어 사용자는 18.8%, 모국어로 사용하는 사람은 5.1%에 불과하여 온라인 정보 접근에 불평등이 생깁니다.
- 비디오 더빙 자동화, 즉 비디오의 오디오 트랙을 번역된 대체 트랙으로 교체하는 과정은 정확한 타이밍, 얼굴 움직임 동기화 및 운율 일치가 필요하여 복잡합니다.
- 종단간(end-to-end) 더빙은 해결책을 제시하지만, 데이터 부족은 종단간 및 파이프라인 기반 방법의 발전을 저해하고 있습니다.
- 이 연구에서는 자동 더빙을 지원하는 42만 5천 개가 넘는 일본어와 영어로 정렬된 애니메이션 비디오 세그먼트로 이루어진 포괄적인 데이터셋인 Anim-400K를 소개합니다.
- Anim-400K 데이터셋은 자동 더빙, 동시 번역, 가이드 비디오 요약, 장르/테마/스타일 분류와 같은 다양한 비디오 관련 작업을 지원합니다.
- 연구 목적으로 공개적으로 이용할 수 있으며 https://github.com/davidmchan/Anim400K에서 데이터셋을 다운로드할 수 있습니다.

### [Score Distillation Sampling with Learned Manifold Corrective](https://arxiv.org/abs/2401.05293)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/1uTkfPtj2XaNvyKPtXB6S.png)

Vote: 5

Authors: Thiemo Alldieck, Thiemo Alldieck, Thiemo Alldieck, Nikos Kolotouros, Nikos Kolotouros, Nikos Kolotouros, Cristian Sminchisescu

- 'Score Distillation Sampling (SDS)'은 이미지 확산 모델을 사용하여 텍스트 프롬프트를 이용해 최적화 문제를 제어하는 최근 등장한 방법으로, 널리 인기를 얻고 있습니다.
- 이 논문에서는 SDS 손실 함수에 대한 심층 분석을 수행하고, 기존 공식의 본질적인 문제점을 확인하며, 놀랄 만큼 간단하지만 효과적인 해결책을 제시합니다.
- 손실을 다양한 요소로 분해하여 잡음이 많은 그래디언트를 일으키는 구성 요소를 분리합니다.
- 원래의 방식은 잡음을 보완하기 위해 높은 텍스트 가이던스를 사용했으나, 이는 원하지 않는 부작용을 초래했습니다.
- 대신, 우리는 이미지 확산 모델의 시간에 따른 노이즈 제거 미비를 모방하는 얕은 네트워크를 훈련시켜 효과적으로 그 부분을 제거합니다.
- 새로운 손실 공식의 다재다능함과 효과성을 최적화 기반 이미지 합성 및 편집, 제로샷 이미지 변환 네트워크 교육, 텍스트-3D 합성 등 여러 정성적 및 정량적 실험을 통해 입증합니다.

