## Daily Papers (2024-01-03)

### [DocLLM: A layout-aware generative language model for multimodal document understanding](https://arxiv.org/abs/2401.00908)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/QLjS9pkeHviuaNyoBvkvu.png)

Vote: 48

Authors: Dongsheng Wang, Natraj Raman, Mathieu Sibue, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu

- 본 논문에서는 텍스트 및 공간적 모달리티(intersection of textual and spatial modalities)의 교차점에서 풍부한 의미를 담고 있는 양식, 송장, 영수증, 보고서, 계약서 등과 같은 기업 문서들의 이해를 위해 공간 레이아웃을 고려하는 DocLLM이라는 새로운 경량 확장 언어 모델을 제시합니다.
- 기존의 다모달 언어 모델과 달리 비싼 이미지 인코더를 피하고, 문서의 공간 레이아웃 구조를 포함하기 위해 경계 상자(bounding box) 정보에만 초점을 맞춥니다.
- 텍스트와 공간적 모달리티 간의 교차 정렬을 고전적인 트랜스포머의 주의 메커니즘을 해체된 행렬 집합으로 분해함으로써 포착합니다.
- 또한, 비정형 레이아웃과 다양한 콘텐츠가 자주 발생하는 시각적 문서의 문제를 해결하기 위해 텍스트 세그먼트를 채우는(pre-training objective that learns to infill text segments) 학습 목표를 고안합니다.
- 네 가지 주요 문서 지능 작업을 다루는 대규모 지시 데이터셋을 사용하여 사전 훈련된 모델을 추가 학습(fine-tune) 합니다.
- DocLLM은 모든 작업에 걸쳐 16개 데이터셋 중 14개에서 최신 상태의 언어 모델들을 능가하는 성능을 보이며, 이전에 보지 못한 5개 데이터셋 중 4개에서도 잘 일반화함을 입증합니다.

### [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/abs/2401.01335)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Toi20XWM03alxbQB04LiF.png)

Vote: 18

Authors: Zixiang Chen, Yihe Deng, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, Quanquan Gu

- 감독된 미세조정(Supervised Fine-Tuning, SFT)은 대규모 언어 모델들(Large Language Models, LLM)의 발전을 위해 인간이 주석한 데이터의 힘을 활용하는 것이 중요하다.
- 본 논문에서는 추가적인 인간 주석 데이터 없이 약한 LLM을 강한 것으로 성장시킬 수 있는 가능성에 대해 연구한다.
- 연구진은 자기 대결을 통한 정교화(Self-Play fIne-tuNing, SPIN)라는 새로운 미세조정 방법을 제안하며, 이는 감독된 미세조정된 모델로부터 시작한다.
- SPIN의 핵심은 언어 모델이 자기 자신의 인스턴스와 경기를 진행함으로써 능력을 정제하는 자기 대결 메커니즘에 있다.
- 구체적으로 LLM은 이전 반복에서 자체적으로 훈련 데이터를 생성하고, 이 자체 생성된 답변들을 인간 주석 데이터로부터 얻은 답변들과 구별함으로써 정책을 세련화한다.
- 이 방법은 초보 모델에서 뛰어난 모델로 LLM을 단계적으로 향상시키며, SFT를 위한 인간 주석 데이터의 전체 잠재력을 해제한다.
- 이론적으로, 우리의 방법의 훈련 목적 함수에 대한 전역 최적이 LLM 정책이 목표 데이터 분포와 일치할 때만 달성된다는 것을 증명한다.
- 우리는 여러 벤치마크 데이터셋들, HuggingFace Open LLM Leaderboard, MT-Bench, 그리고 Big-Bench에서 온 데이터셋들을 사용해 우리 방법을 평가한다.
- 우리의 결과는 SPIN이 다양한 벤치마크들에서 LLM의 성능을 상당히 향상시킬 수 있으며, 추가적인 GPT-4 선호 데이터로 보충된 직접 선호 최적화(Direct Preference Optimization, DPO)를 통해 훈련된 모델들을 능가함을 보여준다.
- 이는 자기 대결의 가능성을 빛내며 전문가 상대 없이 LLM에서 인간 수준의 성능 달성을 가능하게 하는 것을 시사한다.

### [LLaMA Beyond English: An Empirical Study on Language Capability Transfer](https://arxiv.org/abs/2401.01055)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/du9QCMtyGPRUBK-H3tLnT.png)

Vote: 17

Authors: Jun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, Xuanjing Huang

- 최근 다양한 복잡한 작업에서 뛰어난 수행능력을 보여준 대규모 언어 모델(LLM)들이 있지만, 많은 주요 LLM들이 영어 중심 코퍼스로 사전학습되어 있어 다른 비영어권 언어로의 성능이 제한적입니다.
- 본 논문에서는 비영어권 언어로의 언어 생성 및 지시 사항 이해 능력을 효과적으로 전이하는 방법에 대해 집중하여 연구를 수행하였습니다.
- LLaMA를 기반으로 하는 폭넓은 실증적 조사를 통해 1440 GPU 시간이 넘게 축적되었으며, 이를 통해 사전학습 데이터의 어휘 확장, 추가 사전학습, 지시 조정의 영향을 분석했습니다.
- 모델의 지식 수준을 정확히 평가하기 위해 C-Eval, MMLU, AGI-Eval, GAOKAO-Bench 등 네 가지 널리 사용되는 표준화된 테스트 벤치마크를 사용했습니다.
- 모델의 응답 품질에 대한 종합적인 평가를 LLM-Eval 벤치마크를 사용하여, 정확성, 유창성, 정보성, 논리적 일관성, 무해성 등의 측면을 고려하여 수행하였습니다.
- 평가 결과는 정보 정렬 및 응답 품질 측면에서 사전학습 데이터의 1% 미만을 사용함으로써 최신 전이 모델과 비교할 수 있는 성능을 달성할 수 있음을 보여줍니다.
- 또한, 실험 결과는 13개의 저자원 언어에 대해서도 유사한 추세를 나타냅니다.
- 연구를 통해 밝혀진 결론들이 비영어권 LLM 개발에 도움이 될 것으로 기대합니다.

### [LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning](https://arxiv.org/abs/2401.01325)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/oObO580r0gF98RhnXnf0u.png)

Vote: 12

Authors: Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, Xia Hu

- 본 연구에서는 세부 조정 없이도 장문의 콘텍스트를 처리할 수 있는 대규모 언어 모델(LLM)의 내재적 능력을 확인했습니다.
- 훈련 중에 사용된 훈련 시퀀스의 길이 제한이 추론 시 긴 입력 시퀀스에 LLM을 적용하는 데 제약이 될 수 있음을 지적하며, 기존 LLM들은 본질적으로 긴 콘텍스트를 다룰 수 있는 능력을 가지고 있다고 주장합니다.
- 이러한 능력을 완전히 활용하기 위해, LLM의 콘텍스트 윈도우를 스스로 확장하는 방안을 제시하였습니다.
- 'Self-Extend'라 불리는 방법론을 제안하여, 이는 그룹 수준과 이웃 수준의 이중 레벨 주의 정보를 구성하여 LLM의 장문 콘텍스트 처리 잠재력을 자극합니다.
- 제안된 방법은 기존 모델의 자체 주의(self-attention)를 사용하여 계산되므로, 추가적인 훈련이 필요하지 않습니다.
- 코드 수정을 단 네 줄만으로 기존 LLM의 콘텍스트 윈도우를 손쉽게 확장할 수 있으며, 세부 조정이 필요 없습니다.
- 실시된 광범위한 실험을 통해 제안된 방법이 기존 LLM의 콘텍스트 윈도우 길이를 효과적으로 확장할 수 있음을 보여줍니다.

### [Boundary Attention: Learning to Find Faint Boundaries at Any Resolution](https://arxiv.org/abs/2401.00935)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ybhSS6FpM0jqe-knYjKE_.png)

Vote: 10

Authors: Mia Gaia Polansky, Charles Herrmann, Junhwa Hur, Deqing Sun, Dor Verbin, Todd Zickler

- 본 논문에서는 경계, 윤곽, 모서리 및 접합부와 같은 경계선을 명확히 모델링하는 새로운 메커니즘인 '경계 주의(Boundary Attention)'를 사용한 새로운 차별화 모델을 제시합니다.
- 해당 모델은 경계 신호가 매우 약하거나 노이즈에 묻혀있을 때에도 정확한 결과를 제공합니다.
- 이전의 전통적인 경계 탐지 방법들과 비교했을 때, 모델은 차별화가 가능하며 더 큰 이미지로 확장 가능하고, 이미지의 각 부분에 적절한 기하학적 세부 정보에 자동으로 적응합니다.
- 깊은(deep) 방법을 통해 끝에서 끝까지(end-to-end) 경계를 찾는 이전의 방법들에 비해, 본 모델은 서브픽셀 정밀도를 제공하며, 노이즈에 더 강하고, 이미지의 원래 해상도 및 종횡비를 유지하며 처리할 수 있는 이점을 가지고 있습니다.

### [VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM](https://arxiv.org/abs/2401.01256)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0L798mT8zqHIuc2xj9SXC.png)

Vote: 9

Authors: Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei

- 최근 확산 모델의 혁신으로 주어진 프롬프트에 대한 고품질 비디오 생성 가능성이 크게 확장되었지만, 대부분은 단일 배경에서 발생하는 하나의 비디오 이벤트에 초점을 맞추고 있다.
- 다중 장면 비디오를 생성하는 것은 각 장면 사이의 논리를 잘 관리하면서 주요 콘텐츠의 일관된 시각적 모양을 유지해야 하기 때문에 간단한 문제가 아니다.
- 이 논문에서는 다중 장면 비디오 생성을 위해 내용 일관성을 유지하는 새로운 프레임워크인 VideoDrafter를 제안한다.
- VideoDrafter는 대규모 언어 모델(LLM)을 활용하여 입력된 프롬프트를 LLM이 학습한 논리적 지식이 반영된 포괄적인 다중 장면 스크립트로 변환한다.
- 각 장면의 스크립트에는 이벤트를 설명하는 프롬프트, 전경/배경 엔티티, 카메라 움직임이 포함되어 있다.
- VideoDrafter는 스크립트를 통해 공통 엔티티를 식별하고 LLM에 각 엔티티의 세부 사항을 요청하여 결과적인 엔티티 설명을 이미지 생성 모델에 입력하여 각 엔티티에 대한 참조 이미지를 생성한다.
- 마지막으로, VideoDrafter는 참조 이미지, 이벤트의 서술 프롬프트, 카메라 움직임을 고려하여 확산 과정을 통해 각 장면 비디오를 생성함으로써 다중 장면 비디오를 출력한다.
- 확산 모델은 참조 이미지를 조건으로 사용하고 정렬하여 다중 장면 비디오의 내용 일관성을 강화한다.
- 광범위한 실험을 통해 VideoDrafter가 시각적 품질, 내용의 일관성 및 사용자 선호도 측면에서 기존의 상태(state-of-the-art, SOTA) 비디오 생성 모델들을 능가함을 보여준다.

### [A Comprehensive Study of Knowledge Editing for Large Language Models](https://arxiv.org/abs/2401.01286)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/MjFLMp59hzGsc9Db5ghvt.png)

Vote: 8

Authors: Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen

- 대규모 언어 모델(Large Language Models, LLMs)은 인간의 소통을 거의 재현할 수 있는 텍스트 이해 및 생성 능력을 보여주었으나, 방대한 매개변수화로 인한 훈련 중 상당한 계산 요구 때문에 주요 제한 사항이 있습니다.
- 세계는 끊임없이 변화하므로, LLMs는 구식 정보를 수정하거나 새로운 지식을 통합하여 지속적으로 관련성을 유지하기 위해 자주 업데이트해야 합니다.
- 훈련 후 모델 수정이 필요한 많은 응용 프로그램이 있으며, 이를 위해 현장에서 모델을 경량화하여 효율적으로 수정하는 방법에 대한 관심이 증가하고 있습니다.
- 이 연구에서는 특정 분야에서 LLMs의 행동을 효율적으로 수정하면서도 다양한 입력에 걸쳐 전반적인 성능을 유지하는 데 목적을 둔 지식 편집 기법에 초점을 맞추고 있습니다.
- 지식 편집 문제를 정의하고 최신 접근법에 대한 포괄적인 검토를 제공하며, 교육 및 인지 연구 이론에서 영감을 받아 지식 편집 방법을 외부 지식 사용, 모델 내 지식 통합, 그리고 내재적 지식 편집의 세 그룹으로 분류합니다.
- 대표적인 지식 편집 접근법의 포괄적인 실증 평가를 위한 새로운 벤치마크, KnowEdit를 소개하고, LLMs 내에 내재되어 있는 지식 구조에 대한 깊이 있는 이해를 제공할 수 있는 지식 위치에 대한 심층 분석을 제공합니다.
- 지식 편집의 몇 가지 잠재적 응용 분야를 논의하고, 그것이 가지는 넓고 영향력 있는 함의를 개요합니다.

### [TrailBlazer: Trajectory Control for Diffusion-Based Video Generation](https://arxiv.org/abs/2401.00896)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/KnI3nKdVgESsMdOQ4yCJI.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/KnI3nKdVgESsMdOQ4yCJI.mp4" muted="false"></video></div>

Vote: 8

Authors: Wan-Duo Kurt Ma, J. P. Lewis, W. Bastiaan Kleijn

- 텍스트에서 비디오로 (T2V) 생성하는 최근 접근법들에서, 생성된 비디오의 제어성을 달성하는 것은 종종 도전적인 문제입니다.
- 일반적으로 이 문제는 테두리 맵, 깊이 맵 또는 변형할 기존 비디오와 같은 형태의 프레임별 저수준 가이드를 제공함으로써 해결됩니다.
- 그러나 이런 가이드를 얻는 과정은 노동 집약적일 수 있습니다.
- 본 논문은 신경망 훈련, 미세조정, 추론 시 최적화 또는 기존 비디오 사용 없이도 다양한 방식으로 비디오 내 객체를 안내하는 경계 상자(bounding box)를 활용하여 비디오 합성의 제어성을 향상시키는 데 중점을 둡니다.
- TrailBlazer 알고리즘은 사전 훈련된 T2V 모델에 기반하고 있으며, 구현이 간단합니다.
- 제안하는 시공간 주의 맵(spatial and temporal attention map) 편집을 통해 경계 상자로 객체를 지시합니다.
- 또한, 자세한 마스크를 제공할 필요 없이 움직이는 경계 상자와 해당 프롬프트로 객체의 궤적과 전반적인 외형을 안내해주는 키프레이밍(keyframing) 개념을 소개합니다.
- 이 방법은 사전 훈련된 모델에 비해 추가적인 계산이 미미할 정도로 효율적입니다.
- 경계 상자 가이드가 단순함에도 불구하고, 생성된 동작은 상당히 자연스럽고, 상자 크기가 커짐에 따라 가상 카메라로의 움직임과 관점 변화와 같은 효과가 나타납니다.

### [Taming Mode Collapse in Score Distillation for Text-to-3D Generation](https://arxiv.org/abs/2401.00909)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-E5TUjsOuIbKPCICnbSBx.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-E5TUjsOuIbKPCICnbSBx.mp4" muted="false"></video></div>

Vote: 6

Authors: Peihao Wang, Dejia Xu, Zhiwen Fan, Dilin Wang, Sreyas Mohan, Forrest Iandola, Rakesh Ranjan, Yilei Li, Qiang Liu, Zhangyang Wang, Vikas Chandra

- 텍스트에서 3D 생성으로의 속성 추출에서 뛰어난 성능을 보임에도 불구하고, 이러한 기술은 일명 "얀누스" 아티팩트로 알려진 시점 불일치 문제에서 고통받습니다. 여기서 생성된 객체는 여러 정면 얼굴로 각 뷰를 허위로 나타냅니다.
- 경험적으로 효과적인 방법들이 점수 편향 해소 또는 프롬프트 엔지니어링을 통해 이 문제에 접근했지만, 이 문제를 설명하고 해결하기 위한 보다 엄밀한 관점은 여전히 어렵습니다.
- 본 논문에서는 기존의 점수 추출 기반 텍스트-to-3D 생성 프레임워크가 각각의 시점에서 독립적으로 최대 가능성 추구로 퇴화되어 모드 붕괴 문제에 시달리며, 실제로 얀누스 아티팩트로 나타난다는 것을 밝혔습니다.
- 모드 붕괴를 억제하고자, 우리는 생성된 3D 자산들 간에 다양성을 격려하여 얀누스 문제를 완화하는 엔트로피 항을 재확립함으로써 점수 추출을 향상시킵니다.
- 이 새로운 목적에 기반하여, 엔트로픽 점수 추출(ESD)이라 불리는 새로운 3D 점수 추출 업데이트 규칙을 도출했습니다.
- 이론적으로 ESD가 변분 점수 추출에 분류자 자유 안내 트릭을 채택함으로써 간단하게 줄이고 실행될 수 있음을 밝혔습니다.
- 놀랍도록 단순하지만, 우리의 광범위한 실험은 ESD가 점수 추출에서 얀누스 아티팩트에 대한 효과적인 치료법이 될 수 있음을 성공적으로 입증했습니다.

### [Q-Refine: A Perceptual Quality Refiner for AI-Generated Image](https://arxiv.org/abs/2401.01117)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WocqK3CI1NQvzNK2jOpQw.png)

Vote: 4

Authors: Chunyi Li, Haoning Wu, Haoning Wu, Zicheng Zhang, Hongkun Hao, Kaiwei Zhang, Lei Bai, Xiaohong Liu, Xiongkuo Min, Weisi Lin, Guangtao Zhai

- 최근 몇 년 동안 급속한 발전을 이룬 텍스트-투-이미지(Text-to-Image, T2I) 모델의 불만족스런 결과물이 도전 과제로 부상하고 있으며, 이에 품질이 다른 AI 생성 이미지(AI-Generated Images, AIGIs)를 동일하게 세밀하게 조정하는 것은 저품질 AIGIs의 최적화 능력에 한계를 두고, 고품질 AIGIs에 부정적인 최적화를 가져오고 있다.
- 이 문제를 해결하기 위해, 인간 시각 시스템(Human Visual System, HVS)의 선호도를 기반으로 하여 이미지 품질 평가(Image Quality Assessment, IQA) 메트릭을 처음으로 활용한 품질 중심의 세밀 조정기인 Q-Refine이 제안되었다.
- Q-Refine은 세 가지 적응형 파이프라인을 통해 서로 다른 품질의 이미지를 수정하며, 실험 결과에 따르면 주요 T2I 모델에 대해서 Q-Refine은 다양한 품질의 AIGIs에 효과적인 최적화를 수행할 수 있음을 보여준다.
- Q-Refine은 신뢰성과 미학적 품질 차원에서 AIGIs를 최적화할 수 있는 일반적인 세밀 조정기로서 T2I 생성 모델의 응용 범위를 확장할 수 있는 잠재력을 가지고 있다.

### [En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data](https://arxiv.org/abs/2401.01173)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/7PApdOnzbhAQorzDDZe8T.png)

Vote: 2

Authors: Yifang Men, Biwen Lei, Yuan Yao, Miaomiao Cui, Zhouhui Lian, Xuansong Xie

- 본 연구에서는 고품질의 3D 인간 아바타를 제작하기 위한 향상된 생성 기법인 En3D를 제시합니다.
- 이전 연구들이 희귀한 3D 데이터셋이나 불균형한 시점과 부정확한 포즈 사전 정보를 가진 제한된 2D 컬렉션에 의존한 반면, 저희의 방법론은 기존의 3D 나 2D 자산에 의존하지 않고 시각적으로 현실적이고 기하학적으로 정확하며 콘텐츠의 다양성을 갖는 3D 인간을 생성하는 제로샷 3D 생성 체계를 개발하는 것을 목표로 합니다.
- 우리는 합성된 2D 데이터로부터 향상된 3D 생성 모델을 학습하기 위해 정확한 물리적 모델링을 구현하는 세심하게 제작된 워크플로우를 소개합니다.
- 추론 과정에서는 현실적인 외형과 거친 3D 형태 간의 격차를 메우기 위해 최적화 모듈을 통합합니다.
- 구체적으로, En3D는 세 가지 모듈로 구성됩니다: 균형 있고 다양하며 구조화된 인간 이미지로부터 현실적 외관을 가진 일반적인 3D 인간을 정확히 모델링 할 수 있는 3D 생성기, 복잡한 인간 해부학에 대해 다중 시점 정규 제약을 사용하여 형상 품질을 향상시키는 기하학 조각가, 그리고 의미적 UV 분할 및 차별화 가능한 래스터라이저를 활용하여 신뢰성과 편집 가능성을 가진 명시적 텍스처 맵을 분리하는 텍스처링 모듈입니다.
- 실험 결과는 이미지 품질, 기하학 정확도 및 콘텐츠 다양성 측면에서 저희의 접근 방식이 이전의 작업들보다 현저하게 우수함을 보여줍니다.
- 또한 애니메이션 및 편집을 위한 생성된 아바타의 적용 가능성과 콘텐츠 스타일 자유 적응을 위한 저희 방법의 확장 가능성을 보여줍니다.

