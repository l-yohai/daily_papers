## Daily Papers (2024-01-18)

### [Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model](https://arxiv.org/abs/2401.09417)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png)

Vote: 31

Authors: Lianghui Zhu, Lianghui Zhu, Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Xinlong Wang, Xinlong Wang, Wenyu Liu, Xinggang Wang, Xinggang Wang, Xinggang Wang

- 최근 Mamba와 같이 효율적인 하드웨어를 고려한 상태 공간 모델(SSM)이 긴 시퀀스 모델링에 큰 가능성을 보여주었습니다.
- 순수한 SSM에 기반한 효율적이고 범용적인 비전 백본을 구축하는 방향은 매력적이나, SSM은 시각 데이터의 위치 감도와 글로벌 컨텍스트 요구 때문에 시각 데이터를 표현하는 데 어려움이 있습니다.
- 본 논문에서는 자기주의가 시각 표현 학습에 필수적이지 않다는 것을 보이며 양방향 Mamba 블록(Vim)이 포함된 새로운 범용 비전 백본을 제안합니다.
- Vim은 이미지 시퀀스에 위치 임베딩을 표시하고 양방향 상태 공간 모델로 시각 표현을 압축하여 ImageNet 분류, COCO 객체 감지, ADE20k 의미 분할 작업에서 DeiT 같은 잘 알려진 비전 변환기보다 더 높은 성능을 달성했습니다.
- Vim은 예를 들면, 1248x1248 해상도 이미지에서 배치 인퍼런스를 수행할 때 DeiT보다 2.8배 빠른 속도로 86.8%의 GPU 메모리를 절약합니다.
- 결과적으로 Vim은 고해상도 이미지에 대한 Transformer 스타일 이해에서의 계산 및 메모리 제약을 극복하고, 비전 파운데이션 모델을 위한 차세대 백본이 될 큰 잠재력을 가진 것으로 나타났습니다.
- 관련 코드는 https://github.com/hustvl/Vim 에서 사용할 수 있습니다.

### [SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding](https://arxiv.org/abs/2401.09340)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SpPYrZ75Uee17OWAQrm1q.png)

Vote: 13

Authors: Baoxiong Jia, Baoxiong Jia, Baoxiong Jia, Yixin Chen, Yixin Chen, Yixin Chen, Huangyue Yu, Huangyue Yu, Huangyue Yu, Yan Wang, Yan Wang, Yan Wang, Xuesong Niu, Xuesong Niu, Xuesong Niu, Tengyu Liu, Tengyu Liu, Tengyu Liu, Qing Li, Siyuan Huang, Siyuan Huang, Siyuan Huang

- 3D 시각-언어 지상화는 언어를 3D 물리적 환경과 연결하는 것으로, 구현된 에이전트 개발의 핵심으로 여겨진다.
- 2D 영역의 최근 발전과 비교할 때, 3D 장면에서 언어를 지상화하는 것은 3D 장면의 복잡성, 희소한 3D 시각-언어 데이터, 그리고 통합 학습 프레임워크의 부재로 인한 여러 도전 과제에 직면해 있다.
- 본 연구는 실내 환경에서의 3D 시각-언어 학습을 체계적으로 확대하는 가능성을 탐구하여 이러한 도전 과제들을 해결하는 것을 목표로 한다.
- "SceneVerse"라는 최초의 백만 규모 3D 시각-언어 데이터셋을 소개하며, 이는 약 68K개의 3D 실내 장면과 인간 주석 및 확장 가능한 장면 그래프 기반 생성 접근법을 통해 유도된 2.5M개의 시각-언어 쌍으로 구성되어 있다.
- GPS(Grounded Pre-training for Scenes)라 불리는 통합 사전 훈련 프레임워크를 통해 3D 시각-언어 학습에 대한 확장 가능성을 실증하며, 모든 기존 3D 시각적 지상화 벤치마크에서 최신 성능을 달성함을 증명해 보였다.
- SceneVerse와 GPS의 광범위한 잠재력은 3D 시각-언어 작업에서의 도전적인 제로샷 전이 실험을 통해 밝혀졌다.
- 프로젝트 웹사이트: https://scene-verse.github.io

### [ReFT: Reasoning with Reinforced Fine-Tuning](https://arxiv.org/abs/2401.08967)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/bPp1x6IrywnkFJUhY7QLH.png)

Vote: 12

Authors: Trung Quoc Luong, Trung Quoc Luong, Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Zhanming Jie, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li

- 대규모 언어 모델(Large Language Models, LLMs)의 추론 능력을 향상시키기 위해, 연쇄 사고(Chain-of-Thought, CoT) 주석을 사용한 감독된 미세 조정(Supervised Fine-Tuning, SFT)을 수행하는 방법을 연구함.
- 그러나 기존 SFT 방법은 제한된 CoT 데이터에만 의존하여 일반화 능력이 충분히 강하지 않음이 밝혀짐.
- 이 문제를 해결하기 위해, 본 논문에서는 강화 학습 기반의 미세 조정 방법인 Reinforced Fine-Tuning (ReFT)을 제안함, 특히 수학 문제 해결을 예시로 들어 설명함.
- ReFT는 모델을 SFT로 워밍업한 다음, 실시간 강화 학습(여기서는 PPO 알고리즘)을 적용하여 모델을 추가로 미세 조정함. 이 과정에서 여러 가지 추론 경로가 자동으로 샘플링되고, 정답에서 자연스럽게 보상을 도출함.
- GSM8K, MathQA, SVAMP 데이터셋에 대한 광범위한 실험을 통해 ReFT가 SFT를 크게 능가함을 보여주며, 다수결 투표나 재순위 배정과 같은 추론 시간 전략을 조합함으로써 성능을 더욱 향상시킬 수 있는 가능성을 보여줌.
- ReFT는 추가적인 트레이닝 질문이나 보강된 트레이닝 질문 없이 SFT가 사용했던 같은 트레이닝 질문으로부터 학습하여 성능 향상을 달성, 이는 ReFT의 우수한 일반화 능력을 시사함.

### [UniVG: Towards UNIfied-modal Video Generation](https://arxiv.org/abs/2401.09084)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/4FyqtWZf5J8vgDRF6xFK7.png)

Vote: 9

Authors: Ludan Ruan, Ludan Ruan, Ludan Ruan, Lei Tian, Chuanwei Huang, Xu Zhang, Xinyan Xiao

- 확산 기반 비디오 생성에 대한 관심이 높아지고 학계와 산업계에서 상당한 성공을 거두었으나 현재의 노력은 주로 텍스트, 이미지 또는 텍스트와 이미지의 조합에 의한 단일 목표 또는 단일 작업 비디오 생성에 집중되어 있습니다.
- 사용자는 이미지와 텍스트 조건을 유연하게 개별적으로 또는 함께 입력할 수 있으므로, 이러한 현실 세계의 응용 시나리오 요구를 충족시키지 못합니다.
- 이를 해결하기 위해, 저희는 텍스트 및 이미지 모달리티에 걸쳐 다양한 비디오 생성 작업을 처리할 수 있는 통합 모달 비디오 생성(Unified-modal Video Generation) 시스템을 제안합니다.
- 시스템 내에서 다양한 비디오 생성 작업을 생성적 자유도의 관점에서 재검토하고, 고자유도 및 저자유도 비디오 생성 카테고리로 분류합니다.
- 고자유도 비디오 생성에서는 입력 이미지 또는 텍스트의 의미에 부합하는 비디오를 생성하기 위해 다중 조건 교차 주의(Multi-condition Cross Attention)를 사용합니다.
- 저자유도 비디오 생성을 위해서는 입력 조건의 내용을 더 잘 보존하는 데 도움이 되는 편향된 가우시안 잡음(Biased Gaussian Noise)을 순수 무작위 가우시안 잡음으로 대체합니다.
- 제안된 방법은 공개적인 학술 벤치마크인 MSR-VTT에서 가장 낮은 프레쳇 비디오 거리(Fréchet Video Distance, FVD)를 달성하였고, 인간 평가에서 현재 공개된 방법들을 능가하며, 현재 비공개 방법인 Gen2와 비슷한 수준의 성능을 보여줍니다.
- 추가 샘플은 https://univg-baidu.github.io 에서 확인할 수 있습니다.

### [GARField: Group Anything with Radiance Fields](https://arxiv.org/abs/2401.09419)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/v-0P4LwFSz1TA8CKoElRZ.webm)

Vote: 8

Authors: Chung Min Kim, Chung Min Kim, Chung Min Kim, Mingxuan Wu, Justin Kerr, Justin Kerr, Justin Kerr, Ken Goldberg, Matthew Tancik, Angjoo Kanazawa, Angjoo Kanazawa, Angjoo Kanazawa

- GARField는 사진 입력으로부터 3차원 씬(scene)을 의미론적으로 중요한 그룹으로 분해하는 새로운 접근법을 제시한다.
- 이 방법은 물리적 규모를 통하여 그룹의 모호성을 포용하며, 스케일에 조건을 건 3D 친화력 특성 필드를 최적화함으로써 세계 내의 한 지점이 여러 크기의 다른 그룹에 해당할 수 있게 한다.
- Segment Anything (SAM)에서 제공하는 2D 마스크 세트를 이용하여 조화로운 계층 구조를 유지하며 다양한 관점에서 모순되는 마스크를 일관성 있게 융합시켜 필드를 최적화한다.
- 이 필드로부터 자동 트리 구성이나 사용자 상호작용을 통한 가능한 그룹화의 계층을 도출할 수 있다.
- GARField는 실제 현장의 다양한 장면에 대한 평가를 통해 객체 클러스터, 개체 및 다양한 하위 부분 등 다양한 수준의 그룹을 효과적으로 추출하는 것으로 나타났다.
- 이 방법은 다중 관점에서 일관된 그룹을 본질적으로 나타내고, 입력 SAM 마스크보다 높은 정밀도의 그룹을 생성한다.
- GARField의 계층적 그룹화는 3D 자산 추출 또는 동적 장면 이해와 같은 흥미로운 후속 응용 분야에 사용될 수 있다.
- 해당 연구와 관련된 더 많은 정보는 프로젝트 웹사이트 https://www.garfield.studio/에서 확인할 수 있다.

### [DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference](https://arxiv.org/abs/2401.08671)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kHRcfLyUO_kQLUihKFoGL.png)

Vote: 7

Authors: Connor Holmes, Connor Holmes, Connor Holmes, Masahiro Tanaka, Masahiro Tanaka, Masahiro Tanaka, Michael Wyatt, Michael Wyatt, Michael Wyatt, Ammar Ahmad Awan, Ammar Ahmad Awan, Ammar Ahmad Awan, Jeff Rasley, Jeff Rasley, Jeff Rasley, Samyam Rajbhandari, Samyam Rajbhandari, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Heyang Qin, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, Lev Kurilenko, Lev Kurilenko, Yuxiong He

- DeepSpeed-FastGen 시스템은 대규모 언어 모델(LLMs)의 배포 및 확장 문제를 해결하기 위해 새로운 프롬프트 및 생성 합성 전략인 Dynamic SplitFuse를 사용하여 기존 시스템보다 최대 2.3배 더 높은 처리량과 평균 2배 더 낮은 지연 시간, 그리고 토큰 수준에서 최대 3.7배 낮은 지연 시간을 제공합니다.
- 이 시스템은 DeepSpeed-MII와 DeepSpeed-Inference의 조화로운 조합을 통해 효율적이고 사용하기 쉬운 LLM 서비스 시스템을 제공합니다.
- DeepSpeed-FastGen은 다양한 모델을 지원하고, 대화형 세션부터 장기 실행 애플리케이션까지 다양한 사용자 시나리오에 맞는 비영구 및 영구적 배포 옵션을 제공합니다.
- 자세한 벤치마킹 방법론을 제시하고, 지연 시간-처리량 곡선을 통해 성능을 분석하며, 부하 분산을 통해 확장성을 조사합니다.
- 다양한 모델 및 하드웨어 구성에 대한 평가를 통해 처리량과 지연 시간에서 상당한 개선을 입증합니다.
- 더 넓은 모델 지원과 새로운 하드웨어 백엔드를 포함하는 미래의 개선사항에 대한 로드맵을 논의합니다.
- DeepSpeed-FastGen 코드는 커뮤니티 참여와 기여를 위해 공개적으로 제공됩니다.

### [Asynchronous Local-SGD Training for Language Modeling](https://arxiv.org/abs/2401.09135)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wV15FTUGHbZ62WNxqwYgF.png)

Vote: 6

Authors: Bo Liu, Rachita Chhaparia, Arthur Douillard, Arthur Douillard, Arthur Douillard, Satyen Kale, Andrei A. Rusu, Jiajun Shen, Arthur Szlam, Marc'Aurelio Ranzato

- 본 연구는 언어 모델을 훈련하기 위한 비동기 Local-SGD(지역 확률적 경사 하강법)에 대한 실증적 연구를 제시합니다.
- 비동기 Local-SGD는 각 워커가 SGD(확률적 경사 하강법) 단계를 마친 후에 글로벌 파라미터를 즉시 업데이트하는 방식을 의미합니다.
- 연구는 워커의 하드웨어 이질성, 모델 크기, 워커 수, 최적화 방법이 학습 성능에 미치는 영향에 대한 광범위한 조사를 수행하였습니다.
- 결과적으로, 비동기 Local-SGD는 동기화 방식에 비해 글로벌 모델 파라미터를 더 자주 업데이트하고 있음에도 불구하고 수렴하는 데 더 많은 반복을 필요로 한다는 것을 발견했습니다.
- 이는 작업자의 그래디언트가 구식일 때 글로벌 파라미터에 대한 모멘텀 가속이 핵심적인 도전 과제로 식별되었습니다.
- 본 논문에서는 지연된 Nesterov 모멘텀 업데이트를 활용하고, 각 워커의 로컬 훈련 단계를 그들의 계산 속도에 기반하여 조정하는 새로운 방법을 제안합니다.
- C4 데이터 세트에서 최대 150M 파라미터의 모델로 평가해본 결과, 이 접근법은 업데이트 단계당 perplexity(혼란도) 측면에서 동기화 Local-SGD의 성능과 동등하며 벽시계 시간 측면에서는 현저히 뛰어난 성능을 보여주었습니다.

### [VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models](https://arxiv.org/abs/2401.09047)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/jgPqbzAZBFM7Anlxsjxpi.png)

Vote: 5

Authors: Haoxin Chen, Yong Zhang, Xiaodong Cun, Xiaodong Cun, Xiaodong Cun, Menghan Xia, Menghan Xia, Menghan Xia, Xintao Wang, Xintao Wang, Xintao Wang, Chao Weng, Ying Shan, Ying Shan, Ying Shan

- 텍스트를 기반으로 비디오를 생성하는 것은 주어진 프롬프트에 기반한 비디오를 생산하는 것을 목표로 한다.
- 상업적으로 사용 가능한 비디오 모델들은 적은 노이즈, 뛰어난 세부 사항 그리고 높은 미적 점수를 지닌 설득력 있는 비디오를 생성할 수 있다.
- 그러나, 이러한 모델들은 대규모이며, 잘 필터링된 고품질의 비디오에 의존하는데, 이는 커뮤니티에서 접근할 수 없는 데이터이다.
- 현재의 많은 연구들은 낮은 품질의 WebVid-10M 데이터셋을 사용하여 모델을 훈련시키며, 그 결과 고품질의 비디오를 생성하는데 어려움을 겪는다.
- 본 연구에서는 Stable Diffusion에서 확장된 비디오 모델의 훈련 방식을 탐구하며, 낮은 품질의 비디오와 합성된 고품질의 이미지를 이용하여 고품질 비디오 모델을 얻는 가능성을 조사한다.
- 우리는 비디오 모델의 공간 및 시간 모듈 간의 연결성과 낮은 품질 비디오로의 분포 변화를 분석한다.
- 전체 모듈을 훈련시키는 것이 시간 모듈만 훈련시키는 것보다 공간 및 시간 모듈 간의 더 강한 연결성을 결과로 한다는 것을 관찰한다.
- 이 더 강한 연결성을 기반으로하여, 우리는 고화질 이미지로 공간 모듈을 미세 조정함으로써 동작 저하 없이 고품질로의 분포를 이동시키는데 성공한다.
- 비디오 품질, 움직임 및 개념 구성 면에서 제안된 방법의 우수성을 입증하기 위해 평가가 수행된다.

### [TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion](https://arxiv.org/abs/2401.09416)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/h7DGdKotisn9wcJciHHNX.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/h7DGdKotisn9wcJciHHNX.mp4" muted="false"></video></div>

Vote: 3

Authors: Yu-Ying Yeh, Jia-Bin Huang, Jia-Bin Huang, Jia-Bin Huang, Changil Kim, Lei Xiao, Thu Nguyen-Phuoc, Thu Nguyen-Phuoc, Thu Nguyen-Phuoc, Numair Khan, Cheng Zhang, Manmohan Chandraker, Carl S Marshall, Zhao Dong, Zhao Dong, Zhao Dong, Zhengqin Li, Zhengqin Li, Zhengqin Li

- 'TextureDreamer'는 적은 수의 입력 이미지(3~5장)를 사용하여 임의의 범주에 걸친 대상 3D 모양에 재조명이 가능한 텍스처를 전달하는 새로운 이미지 가이드 텍스처 합성 방법입니다.
- 텍스처 생성은 비전과 그래픽에서 중요한 도전 과제이며, 산업 회사들은 3D 자산에 대한 텍스처를 수작업으로 만드는 숙련된 예술가들을 고용합니다.
- 기존의 방법들은 밀집된 시점과 정확한 기하학적 정렬을 요구하는 반면, 학습 기반 방법들은 데이터셋 내 특정 종류의 모형에 국한됩니다.
- 반면에 'TextureDreamer'는 소수의 우연히 캡처된 이미지만을 사용하여 현실 세계 환경에서 세밀하고 복잡한 텍스처를 임의의 객체에 전달할 수 있으며, 이로써 텍스처 생성을 대중화할 가능성을 지닙니다.
- 핵심 아이디어인 맞춤형 기하학-인식 점수 증류(PGSD)는 최신 확산 모델 개발에서 영감을 받아 텍스처 정보 추출을 위한 개인화 모델링, 세부적인 외모 합성을 위한 변형 점수 증류, ControlNet을 통한 명시적 기하학 안내를 포함합니다.
- 이들의 통합과 몇 가지 중요한 수정은 텍스처 품질을 상당히 향상시킵니다.
- 다양한 카테고리의 실제 이미지에 대한 실험은 'TextureDreamer'가 실제감 있고 의미 있는 텍스처를 임의의 객체에 성공적으로 전달 할 수 있으며, 이전 최신 기술의 시각적 품질을 뛰어넘는 것을 보여줍니다.

### [SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers](https://arxiv.org/abs/2401.08740)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/hdPDnPoXo91-ivR-5d52c.png)

Vote: 2

Authors: Nanye Ma, Mark Goldstein, Mark Goldstein, Mark Goldstein, Michael S. Albergo, Michael S. Albergo, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, Saining Xie, Saining Xie, Saining Xie

- 본 논문에서는 확산 변환기(Diffusion Transformers, DiT)의 기반 위에 구축된 생성 모델의 새로운 가족인 Scalable Interpolant Transformers(SiT)를 소개합니다.
- SiT는 표준 확산 모델보다 더 유연하게 두 분포를 연결할 수 있는 인터폴란트(interpolant) 프레임워크를 통해 동적 전송에 기반한 생성 모델의 다양한 설계 선택사항을 모듈식으로 연구할 수 있게 합니다.
- 이 프레임워크는 이산 시간 대 연속 시간 학습, 모델 목표 결정, 분포 연결 인터폴란트 선택, 결정론적 혹은 확률론적 샘플러 배치 등을 고려할 수 있게 해줍니다.
- SiT는 동일한 백본, 매개변수 수, GFLOPs를 사용하여 조건부 ImageNet 256x256 벤치마크에서 DiT를 일관되게 능가하는 성능을 보입니다.
- 학습과 별개로 조정할 수 있는 다양한 확산 계수를 탐색함으로써, SiT는 FID-50K 점수에서 2.06이라는 높은 성과를 달성합니다.

### [ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization](https://arxiv.org/abs/2401.08937)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/h9IUGJ-Cp4Beq_v5JaZDt.png)

Vote: 2

Authors: Weiyao Wang, Pierre Gleize, Hao Tang, Xingyu Chen, Kevin J Liang, Matt Feiszli

- ICON(Incremental CONfidence)은 2D 비디오 프레임에서 뉴럴 레디언스 필드(NeRF)를 훈련시키기 위한 최적화 절차를 제시합니다.
- 이 방법은 정확한 카메라 포즈를 필요로 하는 일반적인 NeRF 훈련과 달리, 부드러운 카메라 운동을 가정하여 초기 포즈 추정치를 계산합니다.
- ICON은 모델 품질을 측정하는 적응적인 '신뢰도(confidence)' 메트릭을 도입하여, 경사도 재가중치를 동적으로 조정합니다.
- 높은 신뢰도를 가진 포즈를 사용하여 NeRF를 학습하며, NeRF에 의해 인코딩된 고품질 3D 구조를 통해 포즈를 배웁니다.
- 기존의 SfM(Structure-from-Motion) 포즈 정보 없이도 ICON은 CO3D와 HO3D 데이터셋에 대해 상위 성능을 달성했다는 점을 보여줍니다.

### [Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis](https://arxiv.org/abs/2401.09048)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-yf5DxmvTrVDfbGeLFsZd.png)

Vote: 1

Authors: Jonghyun Lee, Hansam Cho, Hansam Cho, Hansam Cho, Youngjoon Yoo, Youngjoon Yoo, Youngjoon Yoo, Seoung Bum Kim, Yonghyun Jeong

- 본 논문에서는 텍스트 조건부 확산 모델에서 정확한 레이아웃을 표현하는 텍스트의 한계를 해결하기 위해 생성된 이미지 내 특정 속성을 조건화하는데 추가적인 신호를 결합하는 작업을 소개한다.
- 이전 연구들이 성공적이긴 했지만, 이들은 특성을 삼차원 평면으로 확장하여 특정 위치에 배치하는 것을 고려하지 않았다.
- 이에, 저자들은 다중 예시 이미지에서 글로벌 스타일릭 의미론을 분리하여 통제하면서 3차원 객체 배치를 통합하는 조건부 확산 모델을 제시한다.
- 구체적으로는, 상대적 깊이를 활용하여 모델이 합성 이미지 삼중쌍을 사용하여 보이지 않는 객체의 절대 위치를 식별할 수 있게 하는 깊이 분리 훈련을 도입한다.
- 또한, 추가적인 위치 결정 신호 없이 전역 의미론을 목표 지역에 적용하는 소프트 가이던스 방법을 소개한다.
- Compose and Conquer(CnC)라는 통합 프레임워크는 이러한 기술을 결합하여 다중 조건을 분리된 방식으로 지역화한다.
- 이 접근법은 다양한 깊이에 있는 객체들을 인지할 수 있게 하면서도 다른 글로벌 의미론이 담긴 지역화된 객체들을 구성할 수 있는 다재다능한 프레임워크를 제공하는 것으로 나타났다.
- 관련 코드는 https://github.com/tomtom1103/compose-and-conquer에서 제공된다.

