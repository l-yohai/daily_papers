## Daily Papers (2024-01-23)

### [Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text](https://arxiv.org/abs/2401.12070)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ad7cj1qc923Uo4nZCu7Jz.png)

Vote: 25

Authors: Avi Schwarzschild, Aniruddha Saha, Micah Goldblum, Abhimanyu Hans, Jonas Geiping, Valeriia Cherepanova, Tom Goldstein, Hamid Kazemi

- 최신 대형 언어 모델(Large Language Models, LLM)이 생성한 텍스트를 탐지하는 것이 어렵다고 여겨지지만, 두 가지 밀접한 언어 모델을 대조하는 기반 스코어를 통해 인간이 생성한 텍스트와 기계가 생성한 텍스트를 정확하게 구분할 수 있다는 것을 발견했습니다.
- 이 메커니즘에 기반하여, 저자들은 사전 훈련된 LLM 쌍만을 이용한 간단한 계산을 필요로 하는 새로운 LLM 탐지기 'Binoculars'를 제안합니다.
- 'Binoculars'는 어떠한 훈련 데이터도 없이 최신의 정확성을 달성하며, 모델 특정 수정 없이 다양한 현대 LLM에서 기계 텍스트를 식별할 수 있는 능력이 있습니다.
- 다양한 텍스트 자료원 및 상황에 걸쳐 'Binoculars'를 종합적으로 평가한 결과, ChatGPT 및 다른 LLM에서 생성된 샘플을 0.01%의 낮은 오진율로 90% 이상 탐지할 수 있음을 확인하였으며, 이는 ChatGPT 데이터에 대해 훈련되지 않았음에도 불구하고 얻은 결과입니다.

### [CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/abs/2401.11944)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/a6AjTDn9HkcaocwE3tHfy.png)

Vote: 20

Authors: Ge Zhang, Yu-Hsuan Tsai, Yizhi Li, +, Ruibin Yuan, Shuyue Guo, Fengji Zhang, Kang Zhu, Yuyang Cheng, Bei Chen, Yiming Liang, Tongxu Luo, Chunpu Xu, Wenhao Huang, Tianyu Zheng, Wenhu Chen, Xingwei Qu, Junjie Wang, Chenghua Lin, Haoran Zhang, Yudong Liu, Xinrun Du, Zekun Wang

- 대규모 다중모달 모델(LMM)의 성능을 평가하는 것이 점점 더 중요해짐에 따라, 중국어 맥락에서 LMM의 고급 지식과 추론 능력을 평가하기 위한 간격이 더욱 확대되고 있습니다.
- CMMMU는 중국어 맥락에서 대학 수준의 과목 지식과 신중한 추론을 요구하는 작업에서 LMM을 평가하기 위해 설계된 새로운 벤치마크입니다.
- 이 벤치마크는 MMMU의 주석 및 분석 패턴에 영감을 받아 엄격하게 따르고 있으며, 예술 및 디자인, 비즈니스, 과학, 건강 및 의학, 인문 및 사회 과학, 기술 및 공학 등 6개의 핵심 학문 분야를 포함합니다.
- CMMMU에는 차트, 다이어그램, 지도, 표, 악보, 화학 구조물 등 39가지의 매우 이질적인 이미지 유형을 포함하는 30개의 과목에 걸쳐 12,000개 이상의 수동으로 수집된 다중모달 문제들이 포함되어 있습니다.
- 이 벤치마크는 중국어 맥락에서 특정 분야 지식을 사용하는 복잡한 인식 및 추론에 중점을 두고 있습니다.
- 저자들은 11개의 오픈소스 LLM과 사유화된 GPT-4V(ision)를 평가하였으며, GPT-4V조차도 42%의 정확도를 달성하여 향후 개선 여지가 크다는 것을 보여줍니다.
- CMMMU는 다양한 언어 맥락을 제공함으로써 전문 인공 지능을 향한 차세대 LMM 구축을 지원하고 LMM의 민주화를 촉진할 것입니다.

### [Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs](https://arxiv.org/abs/2401.11708)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/x8Du5h62UaslJ3-VwzFK7.png)

Vote: 16

Authors: Bin Cui, Chenlin Meng, Minkai Xu, Zhaochen Yu, Ling Yang, Stefano Ermon

- 본 논문에서는 다중 객체와 속성, 관계가 포함된 복잡한 텍스트 프롬프트를 처리할 때 종종 문제에 직면하는 기존의 텍스트-이미지 변환 모델들을 향상시키기 위해 새로운 훈련이 필요 없는 텍스트-이미지 생성/편집 프레임워크인 'Recaption, Plan and Generate (RPG)'를 제안한다.
- 이 방법은 다중 모달 LLM(Multimodal Large Language Models)의 강력한 사고 연쇄 추론 능력을 활용하여 텍스트-이미지 확산 모델의 구성성을 강화한다.
- MLLM을 글로벌 플래너로 활용하여 복잡한 이미지 생성 작업을 여러 간단한 생성 작업으로 분해하고 각 서브 리전 내에서 실행한다.
- 지역별 조합 생성을 가능하게 하는 보완적인 지역 확산 기술을 제안한다.
- 텍스트 가이드 이미지 생성과 편집을 RPG 프레임워크 내에서 폐쇄 루프 방식으로 통합하여 일반화 능력을 향상시킨다.
- RPG는 다양한 MLLM 아키텍처(예: MiniGPT-4) 및 확산 백본(예: ControlNet)과 광범위하게 호환된다.
- 실시된 광범위한 실험을 통해 RPG가 다중 카테고리 객체 구성과 텍스트-이미지 의미 정렬 면에서 DALL-E 3 및 SDXL과 같은 최신 텍스트-이미지 확산 모델들을 능가하는 성능을 보였다는 것을 입증했다.
- 해당 연구의 코드는 GitHub에서 제공된다: https://github.com/YangLing0818/RPG-DiffusionMaster.

### [CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation](https://arxiv.org/abs/2401.12208)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/mkUrcgNzR_f4O7h7FOwnw.png)

Vote: 14

Authors: Sergios Gatidis, Zhihong Chen, Andrew Johnston, Akshay S. Chaudhari, Tanishq Mathew Abraham, Curtis Langlotz, Jean-Benoit Delbrouck, Alaa Youssef, Eduardo Pontes Reis, Joseph Paul Cohen, Magdalini Paschali, Dave Van Veen, Cameron Olsen, Jeya Maria Jose Valanarasu, Maya Varma, Louis Blankemeier, Emily B. Tsai

- 가슴 엑스레이(CXR)는 임상에서 가장 자주 수행되는 영상 검사입니다.
- CheXagent는 CXR 해석을 자동으로 수행하는 지침 기반의 기초 모델(FM)로, 클리닉 의사의 의사결정을 도와 환자 결과를 향상시킬 수 있습니다.
- CheXagent 개발에는 대규모의 비전-언어 데이터셋 부족, 의료 데이터의 복잡성을 포착할 수 있는 비전 및 언어 인코더의 부재, CXR 해석 능력을 벤치마킹할 평가 프레임워크의 부재 등의 도전이 있었습니다.
- 이러한 문제들을 해결하기 위해 연구팀은 28개 공개 데이터셋에서 추출한 대규모 지침 튜닝 데이터셋인 CheXinstruct를 소개합니다.
- 또한, 방사선 보고서를 분석하기 위한 임상 대규모 언어 모델(LLM), CXR 이미지를 표현하기 위한 비전 인코더, 그리고 비전과 언어 모달리티를 연결하는 네트워크로 CheXagent를 구축했습니다.
- 이와 함께, 8개의 임상적으로 중요한 CXR 해석 작업에서 FM을 체계적으로 평가할 수 있는 새로운 벤치마크인 CheXbench를 소개합니다.
- CheXagent는 CheXbench 작업에서 일반 및 의료 분야의 기존 FM보다 뛰어난 성능을 보이며, 5명의 전문 방사선과 전문의와의 평가를 통해 양적 및 질적으로 높은 평가를 받았습니다.
- 모델의 투명성을 높이기 위해 성별, 인종, 연령에 따른 성능 차이를 강조하는 공정성 평가를 수행했습니다.
- CheXagent 프로젝트에 대한 자세한 정보는 https://stanford-aimi.github.io/chexagent.html에서 확인할 수 있습니다.

### [SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities](https://arxiv.org/abs/2401.12168)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/J0kW5ndf_m_5uGKxQ2WXf.png)

Vote: 14

Authors: Brian Ichter, Dorsa Sadigh, Leonidas Guibas, Boyuan Chen, Fei Xia, Pete Florence, Sean Kirmani, Zhuo Xu, Danny Driess

- 시각 질문 답변(VQA)과 로봇공학에서 공간 관계에 관한 이해와 추론은 기본적인 능력이지만, 현재의 시각-언어 모델(VLM)들은 3D 공간 추론, 예를 들어 물리적 대상들의 거리나 크기 차이를 인식하는 능력이 부족합니다.
- 이 연구는 VLM의 공간 추론 능력의 한계가 3D 공간 지식을 포함하지 않은 교육 데이터에 기인한다고 가설을 세우고, 인터넷 규모의 공간 추론 데이터로 VLM을 교육하여 이 문제를 해결하고자 합니다.
- 본 논문은 2백만 개의 VQA 예시와 천만 장의 실세계 이미지를 활용하여 자동 3D 공간 VQA 데이터 생성 프레임워크를 개발하고, 이를 통해 인터넷 규모의 3D 공간 추론 데이터셋을 제작했습니다.
- 데이터 품질, 교육 파이프라인, VLM 아키텍처 등 다양한 요소에 대한 훈련 방법을 조사함으로써 이러한 데이터를 사용하여 VLM을 교육할 때, 질적 및 양적 공간 VQA에 대한 능력이 크게 향상 되었다는 것을 증명했습니다.
- 양적 추정 능력 덕분에 연구진은 체인-오브-소트(chain-of-thought) 공간 추론과 로보틱스 분야에서의 새로운 다운스트림 응용 프로그램을 제시했습니다.
- 프로젝트 웹사이트 - https://spatial-vlm.github.io/ -에는 실증 결과와 추가 정보가 공개되어 있습니다.

### [EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models](https://arxiv.org/abs/2401.11739)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xyrEB9_HYQv8Rd2MkFtie.jpeg)

Vote: 13

Authors: Sanja Fidler, Seung Wook Kim, Amirmojtaba Sabour, Koichi Namekata

- 확산 모델들이 의미 분할(semantics segmentation) 작업을 위한 전이 능력을 입증하였음에도 불구하고, 세밀한 분할 마스크를 생성하기 위해선 보통 추가적인 주석이 달린 데이터셋에서의 추가 훈련이 요구되었다.
- 이 논문에서는 사전 훈련된 확산 모델들이 생성한 이미지의 의미적 연관성을 얼마나 이해하는지를 명확히 하기 위해 Stable Diffusion(SD)에서 추출된 의미 지식을 활용하여 추가 훈련 없이도 세밀한 분할 지도를 생성할 수 있는 이미지 분할기를 개발하고자 한다.
- 의미론적으로 유의미한 특징 맵이 일반적으로 공간적으로 낮은 차원의 레이어에만 존재하기 때문에, 이러한 특징 맵으로부터 픽셀 수준의 의미적 관계를 직접 추출하는 것은 주요한 어려움이었다.
- 본 프레임워크는 SD의 생성 과정을 활용하여 이미지 픽셀과 저차원 특징 맵의 공간 위치 사이의 의미론적 일치를 식별하고, 이를 사용하여 이미지 해상도의 분할 지도를 구축한다.
- 실시한 광범위한 실험에서 생성된 분할 지도는 상세한 이미지 부분을 잘 표현하고 정교하게 구분되어 있으며, 이는 확산 모델들 안에 고도로 정확한 픽셀 수준 의미 지식의 존재를 시사한다.

### [Make-A-Shape: a Ten-Million-scale 3D Shape Model](https://arxiv.org/abs/2401.11067)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Nhho8fb4yMSngXHbgwSyC.png)

Vote: 11

Authors: Kamal Rahimi Malekshan, Ka-Hei Hui, Aditya Sanghi, Arianna Rampini, Chi-Wing Fu, Zhengzhe Liu, Hooman Shayani

- 자연어 및 이미지를 위한 대규모 생성 모델에 상당한 발전이 있었지만, 3D 생성 모델은 훈련에 상당한 자원을 요구하고 비효율적이며, 덜 간결하고, 표현력이 떨어져 진전이 더뎠습니다.
- 본 논문에서는 1000만 개의 공개 가능한 형태를 사용할 수 있는 효율적인 대규모 훈련을 위해 설계된 새로운 3D 생성 모델인 Make-A-Shape을 소개합니다.
- 기술적으로, 우리는 웨이블릿 트리 표현을 혁신하여 형태를 간결하게 인코딩하고, 계수 관계를 효율적으로 이용하기 위한 서브밴드 계수 필터링 스킴을 공식화했습니다.
- 또한, 저해상도 그리드에 표현을 레이아웃하기 위한 서브밴드 계수 포장 스킴을 고안하여 디퓨전 모델에 의해 생성 가능한 표현을 만들었습니다.
- 저희는 모델이 거친 및 세부 웨이블릿 계수를 효과적으로 생성하도록 훈련하는 서브밴드 적응 훈련 전략을 파생시켰습니다.
- 마지막으로, 단일/다중 뷰 이미지, 포인트 클라우드, 저해상도 복셀과 같은 다양한 모달리티로부터 형태를 생성하도록 추가 입력 조건을 통제하는 우리의 프레임워크를 확장하였습니다.
- 우리는 조건부 생성, 형태 완성, 무조건부 생성과 같은 다양한 응용 프로그램을 포함하는 광범위한 실험 세트에서 다양한 모달리티에 걸쳐 우리 접근 방식을 입증했습니다.
- 우리의 접근 방법은 고품질 결과를 제공하는 최신 기술을 뛰어넘을 뿐만 아니라 대부분의 조건에서 몇 초 내에 효율적으로 형태를 생성할 수 있으며, 대부분의 조건에서 단 2초 만에 이를 달성합니다.

### [DITTO: Diffusion Inference-Time T-Optimization for Music Generation](https://arxiv.org/abs/2401.12179)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/u0Qsb3ywE6gfKb98ojc4h.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/u0Qsb3ywE6gfKb98ojc4h.mp4" muted="false"></video></div>

Vote: 10

Authors: Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, Nicholas J. Bryan

- 본 논문에서는 사전 훈련된 텍스트-음악 확산 모델을 추론 시간에 제어하는 일반적인 프레임워크인 Diffusion Inference-Time T-Optimization (DITTO)를 제안합니다.
- DITTO는 초기 잡음 잠재 변수를 최적화함으로써, 목표하는 (스타일이 있는) 출력을 달성하기 위해 어떤 차별화 가능한 특징 매칭 손실을 통해서도 최적화할 수 있습니다.
- 메모리 효율성을 높이기 위해 그래디언트 체크포인트를 활용하고, 모델을 미세 조정할 필요 없이 여러 음악 생성 작업에 놀랄 만큼 다양한 적용이 가능함을 보여줍니다.
- 음악의 입체감, 멜로디 및 음악 구조 제어를 포함하여 인페인팅, 아웃페인팅, 루핑 등의 음악 생성에서 광범위한 응용이 가능합니다.
- 관련 훈련, 가이던스, 최적화 기반 방법들과 비교했을 때, DITTO는 거의 모든 작업에서 최신 성능을 달성하며, 특히 제어 가능성, 오디오 품질, 계산 효율성에서 비교가능한 접근법들보다 뛰어난 결과를 보입니다.
- 본 연구는 확산 모델의 높은 품질, 유연성 및 훈련 없는 제어를 가능하게 하는 새로운 길을 열어줍니다.
- 음향 예시는 https://DITTO-Music.github.io/web/에서 확인할 수 있습니다.

### [WARM: On the Benefits of Weight Averaged Reward Models](https://arxiv.org/abs/2401.12187)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/T4eMvoGUt2g-EoLiQRDqP.png)

Vote: 10

Authors: Robert Dadashi, Léonard Hussenot, Alexandre Ramé, Olivier Bachem, Johan Ferret, Geoffrey Cideron, Nino Vieillard

- 대규모 언어 모델(Large Language Models, LLMs)을 인간의 선호에 맞추기 위해 강화 학습(RLHF)을 사용할 때, 보상 모델(Reward Model, RM)의 실패를 이용하는 보상 해킹을 초래할 수 있음을 지적합니다.
- 강화 학습 과정 중의 분포 변화와 인간 선호의 일관성 결여 같은 보상 해킹을 완화하기 위한 RM 설계의 두 가지 주요 도전 과제를 확인합니다.
- 다수의 RM을 미세 조정한 뒤, 가중치 공간에서 평균을 내는 Weight Averaged Reward Models (WARM)을 제안하여 이 문제에 대응합니다.
- 동일한 사전 학습을 공유할 때 미세 조정된 가중치가 선형으로 연결되어있다는 관찰을 바탕으로, WARM은 가중치 평균을 통해 효율성을 향상시키고 분포 변화 하에서의 신뢰도 및 선호 불일치에 대한 강건성을 높입니다.
- 최선의-N(best-of-N) 선택 방법과 강화 학습 방법을 사용한 요약 작업에 대한 실험에서 WARM은 LLM 예측의 전체적인 품질과 정렬을 향상시키는 것으로 나타났습니다.
- 예를 들어, WARM으로 미세 조정된 정책 RL(Policy RL)은 단일 RM으로 미세 조정된 정책 RL에 비해 79.4%의 승률을 보였습니다.

### [Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers](https://arxiv.org/abs/2401.11605)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/sZR76aqnwhbPc3EJDds6f.png)

Vote: 8

Authors: Stefan Andreas Baumann, Enrico Shippole, Alex Birch, Tanishq Mathew Abraham, Katherine Crowson, Daniel Z. Kaplan

- 본 논문은 픽셀 수에 선형적으로 스케일링되는 새로운 이미지 생성 모델인 Hourglass Diffusion Transformer (HDiT)를 제안합니다.
- HDiT는 수십억 개의 파라미터로 확장 가능한 Transformer 아키텍처를 기반으로 하며 고해상도(예: 1024x1024)에서 직접 픽셀 공간에서의 학습을 지원합니다.
- 이 모델은 다단계 아키텍처, 잠재 오토인코더, 자기 조건부 생성과 같은 일반적인 고해상도 학습 기술 없이도 성공적으로 학습됩니다.
- HDiT는 ImageNet 256^2에서 기존 모델들과 경쟁력이 있는 성능을 보여주며, FFHQ-1024^2에서 확산 모델에 대한 새로운 최고 성능을 달성했습니다.

### [OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics](https://arxiv.org/abs/2401.12202)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Lw2SdzIXwCbZZEwTfnzFL.png)

Vote: 7

Authors: Chris Paxton, Lerrel Pinto, Nur Muhammad Mahi Shafiullah, Yaswanth Orru, Peiqi Liu

- 최근 비전, 언어, 로봇 분야에서 눈에 띄는 발전이 있었음에도 불구하고, 일반적인 로봇 응용 프로그램은 인식, 탐색, 그리고 그래핑 같은 기본 능력에 의존함에도 여전히 뒤쳐져 있다.
- 이 논문에서는 시스템 중심 접근을 사용하여 새로운 오픈 지식 기반 로봇 프레임워크인 OK-Robot을 개발했다.
- OK-Robot은 객체 감지를 위한 비전-언어 모델(VLMs), 이동을 위한 탐색 프리미티브(primitives), 그리고 객체 조작을 위한 그래핑 프리미티브를 통합하여, 훈련 없이도 픽앤드롭(pick-and-drop) 작업에 대한 통합 솔루션을 제공한다.
- 10개의 실제 가정 환경에서 OK-Robot의 성능을 평가한 결과, OK-Robot은 오픈 어휘 이동 조작(Open Vocabulary Mobile Manipulation, OVMM)에서 새로운 최신 기술 수준을 달성하여, 이전 작업보다 거의 1.8배 높은 58.5%의 성공률을 기록했다.
- 깨끗하고 정돈된 환경에서는 OK-Robot의 성능이 82%까지 증가한다.
- 그러나 OK-Robot 연구에서 얻은 가장 중요한 통찰은 VLMs와 같은 오픈 지식 시스템을 로봇 모듈과 결합할 때 미묘한 세부 사항들의 결정적인 역할이었다.
- 실험 비디오는 웹사이트 https://ok-robot.github.io 에서 확인 가능하다.

### [StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion](https://arxiv.org/abs/2401.11053)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/4sD3uwJ_rJZWORp031G_S.png)

Vote: 5

Authors: Lei Xie, Yuanzhe Chen, Xinsheng Wang, Yuping Wang, Zhichao Wang, Zhuo Chen, Yuxuan Wang

- 최근 언어 모델(LM)의 발전은 인상적인 제로샷 음성 변환(VC) 성능을 보여주었지만, 기존 LM 기반의 VC 모델들은 오프라인 변환을 적용하여 실시간 응용에는 적합하지 않았다.
- 본 논문에서는 임의의 화자 프롬프트와 원본 음성에 대해 실시간 변환이 가능한 'StreamVoice', 새로운 스트리밍 LM 기반의 제로샷 VC 모델을 소개한다.
- 스트리밍 기능을 가능하게 하기 위해, StreamVoice는 완전한 인과 관계를 가진 컨텍스트-인식 LM과 시간에 독립적인 음향 예측기를 사용하여, 완성된 원본 음성에 대한 의존성을 없앴다.
- 스트리밍 처리 중 불완전한 컨텍스트로 인한 성능 저하를 해결하기 위해 두 가지 전략을 사용하여 LM의 컨텍스트 인식력을 강화했다: 1) 교사-가이드 컨텍스트 예견, 2) 의미 마스킹 전략.
- 특히, StreamVoice는 미래를 예측하지 않고도 스트리밍 제로샷 VC 모델을 구현한 최초의 LM 기반 모델이다.
- 실험 결과는 StreamVoice가 실시간 스트리밍 변환 기능을 유지하면서 비스트리밍 VC 시스템과 비교할만한 제로샷 성능을 유지함을 보여준다.

### [UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures](https://arxiv.org/abs/2401.11078)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/i_1ovEkSCTjalX23CJGDe.png)

Vote: 2

Authors: Mingyuan Zhou, Guojun Qi, Rakib Hyder, Ziwei Xuan

- 최근에 3D 아바타 생성 기술의 발전이 주목받고 있으며, 이러한 혁신은 가상과 실제 세계 경험 사이의 차이를 줄이는 것을 목표로 한다.
- 기존 작업들은 점수 증류 샘플링(SDS) 손실과 차이분화 렌더러 및 텍스트 조건을 결합하여 3D 아바타 생성에 유도하는 확산 모델을 사용한다.
- 하지만 SDS는 종종 세부 면이 결여된 과도하게 부드러운 결과를 만들어내며 선조 샘플링에 비해 다양성이 부족하다.
- 또한 다른 연구들은 단일 이미지에서 3D 아바타를 생성하지만 원치 않는 조명 효과, 관점 뷰, 열화된 이미지 품질 등의 도전 과제로 인해 일관된 질감이 적용된 3D 얼굴 메쉬를 신뢰성 있게 재구성하기 어렵다.
- 본 논문에서는 기하학적 정확성이 향상된 새로운 3D 아바타 생성 접근법인 UltrAvatar를 제안하며, 원하지 않는 조명이 없는 물리기반 렌더링(PBR) 텍스처의 상위 품질을 제공한다.
- 제안된 접근법은 원하지 않는 조명효과를 제거하여 다양한 조명 조건 하에서 렌더링할 수 있는 진짜 산란 색상을 드러내는 산란색 추출 모델과, PBR 텍스처를 생성하기 위한 진위 가이드 확산 모델을 본다.
- 후자는 3D 메쉬 기하학과 더 잘 일치하면서 다양한 얼굴 정체성 특징과 세부 정보를 렌더링하는 데 필요한 두 가지 기울기 기반 안내를 제공한다.
- 실험에서 본 제안된 방법의 효과와 견고함을 확인하였고, 우수한 성능으로 최신 기술 방법들을 크게 능가하였다.

### [Single-View 3D Human Digitalization with Large Reconstruction Models](https://arxiv.org/abs/2401.12175)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/C3zSCCFBEnSazjnDOXB9J.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/C3zSCCFBEnSazjnDOXB9J.mp4" muted="false"></video></div>

Vote: 2

Authors: Jimei Yang, Hao Tan, Zhan Xu, Yang Zhou, Jingyuan Liu, Zhenzhen Weng, Serena Yeung-Levy

- 본 논문에서는 단일 이미지로부터 인간의 Neural Radiance Fields (NeRF)를 추측하는 단일 단계 피드포워드 Large Reconstruction Model인 Human-LRM을 소개한다.
- 본 연구는, 3D 스캔 및 다중 시점 캡처를 포함한 광범위한 데이터셋을 사용하는 훈련에서 주목할 만한 적응성을 보여준다.
- 실외 환경에서의 적용 가능성을 높이기 위해, 특히 가려짐이 있는 경우를 위해, 조건부 트라이플레인 확산 모델을 통해 다중 시점 재구성을 단일 시점으로 압축하는 새로운 전략을 제안한다.
- 이러한 생성적 확장은 단일 시점에서 관찰 시 인간의 신체 형태의 내재된 변화에 대응하고, 가려진 이미지에서도 전체 신체를 재구성할 수 있는 가능성을 제시한다.
- 광범위한 실험을 통해 Human-LRM이 다양한 벤치마크에서 이전 방법들을 상당한 차이로 능가하는 것을 보여준다.

### [Fast Registration of Photorealistic Avatars for VR Facial Animation](https://arxiv.org/abs/2401.11002)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/JeLnWQFP2JaBL3woLeb37.png)

Vote: 1

Authors: Jason Saragih, Te-Li Wang, Shih-En Wei, Chaitanya Patel, Shaojie Bai

- VR에서 실감나는 사회적 상호작용을 위해서는 VR 헤드셋을 착용하고 있는 자신의 사실적 아바타를 정확하게 애니메이션화하는 기술이 중요합니다.
- 오프라인 설정에서는 개인 맞춤형 아바타의 고품질 등록이 가능하지만, 실시간으로 일반 모델을 사용할 때 성능이 크게 저하됩니다.
- 이 연구에서는 아바타와 헤드셋 카메라 이미지 간의 도메인 차이가 주요 어려움 중 하나라는 것을 보여주며, 변환기 기반 아키텍처가 도메인 일관성 데이터에서는 높은 정확성을 달성하지만 도메인 간의 차이가 다시 도입되면 성능이 저하됩니다.
- 이러한 발견을 바탕으로, 문제를 두 부분으로 분리하는 시스템을 개발했습니다: 1) 도메인 내 입력을 받는 반복적 개선 모듈, 2) 현재의 표정과 머리 포즈 추정에 따라 조정되는 일반 아바타 유도 이미지 간 스타일 전송 모듈.
- 두 모듈은 상호 강화되며, 정확한 예제를 보여줄 때 이미지 스타일 전송이 더 쉬워지고 더 나은 도메인 간 차이 제거가 등록을 돕습니다.
- 제안하는 시스템은 개인화된 레이블을 생성하기 위한 비용이 많이 드는 오프라인 등록 없이도 효율적으로 고품질 결과를 생성합니다.
- 표준 VR 헤드셋에서 수행한 광범위한 실험을 통해 접근법의 정확성과 효율성을 검증하고, 직접 회귀 방식과 오프라인 등록보다 상당한 개선을 보여줍니다.

### [Scaling Face Interaction Graph Networks to Real World Scenes](https://arxiv.org/abs/2401.11985)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/JCg4arQ95JzZMsdnGIBrn.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/JCg4arQ95JzZMsdnGIBrn.mp4" muted="false"></video></div>

Vote: 1

Authors: Tatiana Lopez-Guevara, Tobias Pfaff, Yulia Rubanova, Kelsey R. Allen, William F. Whitney, Kimberly Stachenfeld

- 로봇공학, 공학, 그래픽 및 디자인과 같은 다양한 응용 프로그램에서 실제 세계 객체 동적을 정확하게 시뮬레이션하는 것이 중요합니다.
- 복잡한 실제 동작, 예를 들어 접촉과 마찰을 더 잘 포착하기 위해, 최근 그래프 네트워크 기반 학습 시뮬레이터가 큰 가능성을 보여주었습니다.
- 그러나 이러한 학습된 시뮬레이터들을 실제 장면에 적용하는 것은 복잡한 3D 형태의 수백 개의 객체들을 처리하는 확장성과, 3D 상태 정보 대신 인식 정보를 다루는 것의 두 가지 주요 도전 과제가 있습니다.
- 저희는 그래프 기반 학습 시뮬레이터를 실행하는 데 필요한 메모리를 대폭 줄이는 방법을 도입했습니다.
- 이 메모리 효율적인 시뮬레이션 모델을 기반으로, 실제 세계 장면을 그래프 네트워크 시뮬레이터가 처리할 수 있는 구조화된 표현으로 변환할 수 있는 편집 가능한 NeRFs(신경 복셰 함수) 형태의 인식 인터페이스를 제시합니다.
- 우리의 방법은 이전의 그래프 기반 시뮬레이터들보다 훨씬 적은 메모리를 사용하면서도 정확성을 유지하고, 합성 환경에서 학습된 시뮬레이터들이 여러 카메라 각도에서 촬영한 실제 세계 장면에 적용될 수 있음을 보여줍니다.
- 이는 학습된 시뮬레이터를 추론 시간에만 인식 정보가 사용 가능한 설정으로 확장하는 길을 열어줍니다.

