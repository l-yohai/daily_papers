## Daily Papers (2024-01-26)

### [Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try-All](https://arxiv.org/abs/2401.13795)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/trhqMKEd86XcQfsDO8vCj.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/trhqMKEd86XcQfsDO8vCj.mp4" muted="false"></video></div>

Vote: 22

Authors: Karim Bouyarmane, Amir Tavanaei, Ismail B. Tutar, Mehmet Saygin Seyfioglu, Suren Kumar

- 온라인 쇼핑의 성장으로, "가상 트라이-올" 현상, 즉 소비자가 자신의 환경에서 제품을 가상으로 시각화하는 능력이 중요해졌습니다.
- 기존 이미지 조건부 확산 모델은 제품의 세밀한 디테일을 포착하는데 어려움이 있었으나, 개인화 중심의 모델인 'DreamPaint'는 디테일을 잘 보존하지만 실시간 응용에 적합하지 않았습니다.
- 본 연구에서는 "Diffuse to Choose"라는 새로운 확산 기반 이미지 조건부 인페인팅 모델을 제시하는데, 이는 빠른 추론과 주어진 참조 항목의 고화질 디테일 유지, 그리고 주어진 장면 콘텐츠의 정확한 의미 조작을 효과적으로 균형을 맞춥니다.
- 저희 접근법은 참조 이미지의 미세한 특징들을 주요 확산 모델의 잠재적 특징 맵에 직접 통합하고, 참조 항목의 디테일을 더 잘 보존하기 위해 지각 손실도 사용합니다.
- 내부 및 공개 데이터셋에 대한 광범위한 테스트를 수행한 결과, Diffuse to Choose가 기존의 제로샷 확산 인페인팅 방법과 DreamPaint와 같은 소수샷 확산 개인화 알고리즘보다 우수한 성능을 보여줍니다.

### [Rethinking Patch Dependence for Masked Autoencoders](https://arxiv.org/abs/2401.14391)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-5nF2xoe2-zFU8JpJn5n1.png)

Vote: 15

Authors: Letian Fu, Alexei A. Efros, Trevor Darrell, Xudong Wang, Baifeng Shi, Renhao Wang, Ken Goldberg, Adam Yala, Long Lian

- 본 연구는 마스크된 오토인코더(Masked Autoencoders, MAE) 해독 메커니즘의 패치 간 의존성에 대해 재고합니다.
- 마스크된 패치 재구성을 위해 MAE 해독 메커니즘을 셀프 어텐션(self-attention)과 크로스 어텐션(cross-attention)으로 분해합니다.
- 연구 결과, 마스크된 패치 간의 셀프 어텐션은 우수한 표현 학습(representation learning)에 필수적이지 않다는 것을 시사합니다.
- 이에, 새로운 프리트레이닝 프레임워크인 크로스 어텐션 마스크드 오토인코더(Cross-Attention Masked Autoencoders, CrossMAE)를 제안합니다.
- CrossMAE의 디코더는 마스크된 패치와 보이는 패치 간의 크로스 어텐션만을 활용하여, 하류 작업(downstream performance)에서 성능 저하 없이 효율을 높입니다.
- 이 디자인은 마스크 토큰의 소수만 디코딩하여 효율성을 높이는 동시에, 각 디코더 블록이 다른 인코더 특징을 활용할 수 있게 합니다.
- CrossMAE는 MAE와 동등한 성능을 보이며, 디코딩 계산량을 2.5에서 3.7배 감소시킵니다.
- 또한, CrossMAE는 동일한 계산으로 ImageNet 분류 및 COCO 인스턴스 분할에서 MAE를 뛰어넘는 성능을 보여줍니다.
- 코드 및 모델은 https://crossmae.github.io에서 제공됩니다.

### [DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence](https://arxiv.org/abs/2401.14196)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/t3lkOvuYbY7LP6mgUAs7A.png)

Vote: 13

Authors: Fuli Luo, Yingfei Xiong, Kai Dong, Dejian Yang, Wentao Zhang, Xiao Bi, Wenfeng Liang, Zhenda Xie, Guanting Chen, Y. K. Li, Daya Guo, Y. Wu, Qihao Zhu

- 대규모 언어 모델의 빠른 발전이 소프트웨어 개발에서 코드 인텔리전스를 혁신하고 있습니다.
- 클로즈드 소스 모델의 우위로 인해 연구 및 개발이 제한적이었으나, DeepSeek-Coder 시리즈는 이 문제를 해결하고자 합니다.
- DeepSeek-Coder 시리즈는 1.3B부터 33B 사이즈의 오픈 소스 코드 모델로, 2 조 토큰으로부터 스크래치부터 훈련되었습니다.
- 고품질 프로젝트 레벨 코드 코퍼스에 기반한 사전 훈련을 받고, 코드 생성 및 인필링을 강화하기 위해 16K 창으로 fill-in-the-blank 작업을 사용합니다.
- DeepSeek-Coder는 오픈 소스 코드 모델들 중 벤치마크를 통해 최고 성능을 보이며, Codex 및 GPT-3.5와 같은 기존 클로즈드 소스 모델을 뛰어넘습니다.
- DeepSeek-Coder 모델들은 연구와 제한 없는 상업적 사용을 허용하는 관대한 라이선스 하에 있습니다.

### [Deconstructing Denoising Diffusion Models for Self-Supervised Learning](https://arxiv.org/abs/2401.14404)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/HmV9BvJ0no87a3BLAcvCp.png)

Vote: 11

Authors: Zhuang Liu, Kaiming He, Xinlei Chen, Saining Xie

- 본 연구에서는 원래 이미지 생성을 목적으로 했던 Denoising Diffusion Models (DDM)의 표현 학습 능력을 검토합니다.
- 우리의 접근 방법은 DDM을 점진적으로 해체하여 고전적인 Denoising Autoencoder (DAE)로 변환하는 것입니다.
- 이러한 분해 과정을 통해 현대 DDM의 다양한 구성 요소가 자기 지도 학습(self-supervised learning)에 어떻게 영향을 미치는지 탐구합니다.
- 매우 소수의 현대 구성 요소만이 좋은 표현을 학습하는 데 중요하며, 많은 다른 구성 요소는 필수적이지 않다는 것을 관찰합니다.
- 우리의 연구는 궁극적으로 매우 단순화된 접근 방법에 도달하며, 이는 상당 부분 고전적인 DAE와 유사합니다.
- 본 연구는 현대 자기 지도 학습 영역 내에서 고전적인 방법들에 대한 관심을 다시 불러일으키기를 바랍니다.

### [Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI](https://arxiv.org/abs/2401.14019)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/nHTsb6q2z4bktQJvlNGrQ.qt)

Vote: 9

Authors: Elron Bandel, Shachar Don-Yehyia, Ariel Gera, Roni Friedman-Melamed, Elad Venezian, Ofir Arviv, Michal Shmueli-Scheuer, Dafna Sheinwald, Yoav Katz, Matan Orbach, Leshem Choshen, Yotam Perlitz

- 생성적 자연어 처리(NLP)의 빠르게 변화하는 환경에서, Unitxt는 전통적인 텍스트 처리 방식의 한계, 특정 데이터셋, 작업, 모델 조합에 대한 연구 유연성 및 재현성을 제한하는 문제를 해결하기 위해 제시되었습니다.
- Unitxt는 사용자가 맞춤형 텍스트 데이터 준비와 평가를 할 수 있는 혁신적인 라이브러리로, 생성적 언어 모델에 특화되어 있습니다.
- 이 라이브러리는 흔히 사용되는 HuggingFace와 LM-eval-harness 같은 일반 라이브러리와 원활히 연동되며, 모듈형 구성 요소로 처리 흐름을 분해하여 연구자들이 쉽게 사용자 정의 및 공유할 수 있게 합니다.
- 모델 특정 형식, 작업 지시 프롬프트 및 다양한 데이터셋 처리 정의를 포함하는 방대한 구성 요소들이 포함되어 있습니다.
- Unitxt-Catalog는 이러한 구성 요소들을 중앙 집중화하여 현대적인 텍스트 데이터 워크플로우에서의 협업과 탐색을 촉진합니다.
- Unitxt는 단순한 도구를 넘어 사용자가 협력적으로 파이프라인을 구축, 공유, 발전시킬 수 있는 커뮤니티 주도 플랫폼으로 기능합니다.
- Unitxt 커뮤니티에 참여하여, https://github.com/IBM/unitxt에서 협업과 혁신을 경험할 수 있습니다.

### [WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models](https://arxiv.org/abs/2401.13919)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Lm-sOGBerxJu2AF_URTeb.png)

Vote: 8

Authors: Hongliang He, Wenhao Yu, Dong Yu, Zhenzhong Lan, Yong Dai, Kaixin Ma, Hongming Zhang, Wenlin Yao

- 큰 언어 모델의 진보로 인해, 실제 세계에서 자율적인 응용 프로그램 개발이 촉진되고 이에 따라 고급 웹 기반 에이전트의 창조에 혁신이 일어나고 있다.
- 기존의 웹 에이전트들은 하나의 입력 모달리티만을 처리하며, 간단화된 웹 시뮬레이터나 정적 웹 스냅샷에서만 평가되어 실제 세계 시나리오의 적용성이 한정적이다.
- 이러한 격차를 메우기 위해, WebVoyager는 Large Multimodal Model (LMM)을 기반으로 하여 실제 웹사이트와 상호작용하며 사용자의 지시를 완성해낼 수 있는 웹 에이전트를 소개한다.
- 또한, GPT-4V의 강력한 다중모달 이해 능력을 활용하여, 열린형 웹 에이전트 작업의 자동 평가에 대한 도전을 다루는 새로운 평가 프로토콜을 제안한다.
- 15개의 널리 사용되는 웹사이트에서 실제 작업을 수집하여 에이전트를 평가하기 위한 새로운 벤치마크를 만들었다.
- WebVoyager는 GPT-4 (All Tools) 및 WebVoyager (text-only) 설정을 크게 능가하는 55.7%의 작업 성공률을 달성하여 실용적인 응용 프로그램에서의 탁월한 능력을 강조한다.
- 제안된 자동 평가는 인간 판단과 85.3% 합의를 이루어내며 실세계 설정에서 웹 에이전트의 추가 개발을 위한 길을 터주었다.

### [BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models](https://arxiv.org/abs/2401.13974)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/4pM5b_AkB3dQqaNgE4KkG.png)

Vote: 8

Authors: Nikhil Naik, Senthil Purushwalkam, Shafiq Joty, Akash Gokul

- 최근의 텍스트-이미지 생성 모델들은 입력된 문구를 충실히 따르는 이미지를 생성하는 데 있어 놀라운 성공을 거두었지만, 원하는 개념을 설명하기 위해 단어를 사용해야 한다는 요구사항은 생성된 개념의 외모에 대한 제어를 제한한다.
- 본 연구에서는 기존 텍스트-이미지 확산 모델에서 개인화 기능을 가능하게 하는 접근법을 제안하여 이러한 단점을 해결한다.
- BootPIG이라는 새로운 구조를 제안하여 사용자가 참조 이미지를 제공함으로써 생성된 이미지의 개념 외모를 안내하도록 한다.
- BootPIG 구조는 사전 훈련된 텍스트-이미지 확산 모델에 최소한의 수정을 하여 별도의 UNet 모델을 사용하여 생성물을 원하는 외모로 유도한다.
- 사전 훈련된 텍스트-이미지 모델, LLM 챗봇, 이미지 분할 모델로부터 생성된 데이터를 사용하는 교육 절차를 도입하여 BootPIG 구조에서 개인화 기능을 빠르게 구축한다.
- 기존 방법이 수일간의 사전 훈련을 요구하는 반면, BootPIG 구조는 약 1시간 이내에 훈련될 수 있다.
- DreamBooth 데이터셋에 대한 실험을 통해 BootPIG는 기존의 제로샷 방법보다 우수한 성능을 나타내며, 테스트 시간의 미세조정 접근법과 비슷한 수준임을 보여준다.
- 사용자 연구를 통해 참조 객체의 외모에 대한 충실도와 텍스트 프롬프트와의 일치성 측면에서 기존 방법들보다 BootPIG 생성물이 선호됨을 확인한다.

### [pix2gestalt: Amodal Segmentation by Synthesizing Wholes](https://arxiv.org/abs/2401.14398)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/CbIGlUMFhypiPY75z-voY.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/CbIGlUMFhypiPY75z-voY.mp4" muted="false"></video></div>

Vote: 7

Authors: Achal Dave, Dian Chen, Pavel Tokmakov, Ege Ozguroglu, Carl Vondrick, Ruoshi Liu, Dídac Surís

- 'pix2gestalt'라는 새로운 프레임워크를 소개하며, 이는 가려진 물체의 모양과 외관을 추정하는 데 중점을 둔 영역인 제로샷 아모달 세그멘테이션에 적용됩니다.
- 대규모 확산 모델을 활용하여 이러한 표현들을 아모달 세그멘테이션 작업에 전달함으로써, 자연 및 물리적 원칙을 벗어나는 예술 작품과 같은 도전적인 제로샷 사례에서 전체 물체를 재구성하는 조건부 확산 모델을 학습합니다.
- 가려진 물체와 그 완전한 버전이 함께 있는 합성 데이터셋을 사용하여 모델을 훈련시킵니다.
- 실험 결과 우리의 방법이 기존의 감독된 벤치마크에서 우수한 성능을 보이며, 가려짐이 있는 상황에서 기존의 물체 인식과 3D 복원 방법의 성능을 크게 향상시키는 데에도 사용될 수 있음을 보여줍니다.

### [Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities](https://arxiv.org/abs/2401.14405)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/9iOf9qsOWNfGDSYyyL0mE.png)

Vote: 7

Authors: Yixiao Ge, Yiyuan Zhang, Xiaohan Ding, Ying Shan, Kaixiong Gong, Xiangyu Yue

- 다른 모달리티의 관련 없는 데이터를 사용하여 특정 모달리티의 트랜스포머 성능을 향상시키는 새로운 방법을 제안하였습니다.
- 예를 들어, ImageNet 모델을 오디오 또는 포인트 클라우드 데이터셋을 이용하여 개선하는 방법론을 다룹니다.
- Multimodal Pathway라고 명명된 이 방법론은 타겟 모달리티의 트랜스포머 및 다른 모달리티의 데이터로 훈련된 보조 트랜스포머 사이에 경로를 만들어 두 모델 모두가 타겟 모달리티 데이터를 처리할 수 있게 합니다.
- 특히, 모달리티 특화 토크나이저와 태스크 특화 헤드를 사용하면서도 Cross-Modal Re-parameterization이라는 방법을 통해 추가적인 추론 비용 없이 보조 모델의 트랜스포머 블록을 활용합니다.
- 이미지, 포인트 클라우드, 비디오, 오디오 인식 작업에서 타 모달리티의 데이터를 활용해 성능이 대폭 향상됨을 관찰하였습니다.
- 코드와 모델은 https://github.com/AILab-CVC/M2PT에서 제공됩니다.

### [FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design](https://arxiv.org/abs/2401.14112)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kiK0J0qx3TzFSCqqboVWr.png)

Vote: 6

Authors: Haojun Xia, Shuaiwen Leon Song, Olatunji Ruwase, Arash Bakhtiari, Shiyang Chen, Zhen Zheng, Zhewei Yao, Donglin Zhuang, Stephen Youn, Michael Wyatt, Zhongzhu Zhou, Yuxiong He, Xiaoxia Wu

- 6비트 양자화(FP6)는 다양한 애플리케이션에서 대규모 언어 모델(LLMs)의 크기를 효과적으로 줄이고 모델 품질을 일관되게 유지할 수 있습니다.
- 기존 시스템은 FP6 양자화에 대한 텐서 코어 지원이 부족하며 대규모 언어 모델 추론 시 실용적인 성능 향상을 달성하는 데 어려움이 있습니다.
- 이러한 문제를 해결하기 위해, 저희는 다양한 양자화 비트 폭에 대한 통합 텐서 코어 지원을 갖춘 최초의 전체 스택 GPU 커널 설계 방식인 TC-FPx를 제안합니다.
- TC-FPx 커널을 기존 추론 시스템에 통합함으로써, 추론 비용과 모델 품질 사이의 더 나은 균형을 달성하는 새로운 종단 간 지원(이른바 FP6-LLM)을 제공합니다.
- 실험 결과 FP6-LLM은 단일 GPU를 사용하여 LLaMA-70b 모델의 추론을 가능하게 하며, FP16 기준 대비 정규화된 추론 처리량을 1.69배에서 2.65배 향상시켰습니다.
- 곧 공개될 소스 코드를 통해 FP6-LLM의 구현과 성능 개선을 확인할 수 있습니다.

### [Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation](https://arxiv.org/abs/2401.14257)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3ynT-6F5kaP_Yt6pUB7c7.png)

Vote: 5

Authors: Minglin Chen, Yukun Wang, Zilong Dong, Yulan Guo, Liefeng Bo, Longguang Wang, Zhe Sheng, Yisheng He, Weihao Yuan

- 최근 조사에 따르면 텍스트 설명을 이용하여 고품질의 3D 콘텐츠를 생성하는 text-to-3D 기법이 점점 발전하고 있습니다.
- 그러나 생성된 객체는 무작위적이며 세밀한 컨트롤이 부족합니다.
- 스케치는 이러한 세밀한 컨트롤을 도입할 수 있는 저렴한 방법을 제공하지만, 그 추상성과 모호성으로 인해 유연한 컨트롤을 실현하기 어렵습니다.
- 본 논문에서는 스케치로 3D 생성을 제어하기 위한 멀티뷰 스케치 가이드 텍스트-투-3D 생성 프레임워크인 Sketch2NeRF를 제안합니다.
- 우리의 방법은 사전 훈련된 2D 확산 모델(예: Stable Diffusion 및 ControlNet)을 활용하여 신경 반사 필드(NeRF)로 표현된 3D 장면을 최적화하는 것을 지도합니다.
- 본 논문은 NeRF를 효과적으로 최적화하기 위해 새롭고 동기화된 생성 및 재구성 방법을 제안합니다.
- 실험에서는 제안된 방법을 평가하기 위해 두 종류의 멀티뷰 스케치 데이터 세트를 수집하였습니다.
- 우리의 방법은 텍스트 프롬프트에 대한 고신뢰도와 더불어 세밀한 스케치 제어로 3D 일관된 내용을 합성하며 보여줍니다.
- 광범위한 결과는 우리의 방법이 스케치 유사성과 텍스트 정렬 측면에서 최신 성능을 달성함을 보여줍니다.

### [CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion](https://arxiv.org/abs/2401.14066)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/OeAdi_S6tuvTKcWyfYweH.jpeg)

Vote: 5

Authors: Chongyang Ma, Weiming Dong, Nisha Huang, Xiu Li, Yuxin Zhang, Fan Tang, Ronghui Li, Changsheng Xu

- CreativeSynth는 사용자가 입력 이미지의 시각적 요소를 세밀하게 설명하는 텍스트 프롬프트를 만드는 데 어려움을 겪고 특정 영역을 수정할 때 전반적인 예술적 스타일을 방해하는 문제를 해결하기 위한 창의적인 합성과 블렌딩을 기반으로 한 새로운 프레임워크입니다.
- 이 프레임워크는 다양한 모달의 입력을 조정하고 예술적 이미지 생성 분야에서 다양한 작업을 수행할 수 있는 확산 모델에 기반을 두고 있습니다.
- CreativeSynth는 맞춤형 주의 메커니즘과 다모달 특성을 통합하여 미술 영역으로 실제 세계의 의미 있는 콘텐츠를 가져오고, 실시간 스타일 전송을 통해 이미지 스타일과 내용을 정확하게 조작하면서 원본 모델 매개변수의 무결성을 유지합니다.
- 질적 및 양적 평가를 통해 CreativeSynth가 예술 이미지의 충실도를 향상시키고 그들의 본질적인 미적 본성을 보존하는 데 뛰어남을 보여줍니다.
- CreativeSynth는 생성 모델과 예술적 미감 사이의 간극을 메우고 사용자에게 맞춤 디지털 팔레트로 작용합니다.

### [Adaptive Mobile Manipulation for Articulated Objects In the Open World](https://arxiv.org/abs/2401.14403)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gjpbRByQ_LHz_N4VMUTy6.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gjpbRByQ_LHz_N4VMUTy6.mp4" muted="false"></video></div>

Vote: 4

Authors: Kenneth Shaw, Haoyu Xiong, Deepak Pathak, Russell Mendonca

- 본 논문에서는 가정과 같은 구조화되지 않은 환경에서 로봇을 배치하는 오랜 연구 문제를 다루고 있으며, 로봇은 일반적으로 폐쇄된 실험실 설정에서만 연구된다.
- 이 연구는 단순히 물건을 집어 옮기는 것을 넘어서는 실제 세계의 문, 캐비닛, 서랍, 냉장고와 같은 조작 가능한 물체의 작동을 위한 전체적인 접근 방식인 'Open-World Mobile Manipulation System'을 제시한다.
- 로봇은 초기에는 행동 복제를 통해 소량의 데이터에서 학습하고, 이후 연습을 통해 학습 분포 밖에 있는 새로운 물체에 대해 온라인으로 학습하는 적응형 학습 프레임워크를 이용한다.
- 이 시스템은 약 20,000 미국 달러의 비용으로 무구조 환경에서 안전하고 자율적으로 온라인 적응이 가능한 저비용 모바일 조작 하드웨어 플랫폼을 개발하였다.
- 실험에서는 CMU 캠퍼스의 4개 건물에 있는 20개의 관절 객체를 활용하여, 각 객체에 대해 1시간 미만의 온라인 학습을 통해 성공률을 행동 복제 사전 훈련의 50%에서 온라인 적응을 사용하여 95%까지 향상시켰다.
- 연구 결과는 https://open-world-mobilemanip.github.io/ 에서 비디오로 제공된다.

### [Genie: Achieving Human Parity in Content-Grounded Datasets Generation](https://arxiv.org/abs/2401.14367)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/CDpvvF1N2z33v-RSmMtzv.png)

Vote: 4

Authors: Yosi Mass, Asaf Yehudai, Ofir Arviv, Assaf Toledo, Boaz Carmeli, Leshem Choshen, Eyal Shnarch, Nathaniel Mills

- 내용 중심 생성 과제를 진전시키는 데 있어서 고품질 데이터 부족이 주요 장애물로 지적되었고, 이를 해결하기 위해 Genie라는 새로운 방법을 제안하였다.
- Genie는 콘텐츠 준비, 특정 과제 예시들(질문-답변 쌍 혹은 요약문 등) 생성, 그리고 생성된 데이터의 질과 충실도를 보장하는 필터링 메커니즘의 삼 단계로 구성되어 있다.
- 이 방법론을 사용하여 Long-Form 질문-답변(LFQA), 요약, 정보 추출을 위한 세 개의 대규모 인공 데이터 세트를 생성했다.
- 인간 평가에서 생성된 데이터는 자연스럽고 고품질로 평가되었다.
- 사람이 작성한 데이터(ELI5 및 ASQA: LFQA, CNN-DailyMail: 요약)로 학습된 모델과 비교 시, Genie로 생성된 데이터로 학습된 모델이 동등하거나 더 나은 성능을 보였으며, 충실도에서는 일관되게 뛰어난 결과를 보였다.
- 마지막으로, 의료 분야 콘텐츠 중심의 LFQA 데이터를 생성하는 데 Genie 방법을 적용했으며, 이 데이터로 학습된 모델이 다른 분야로 학습된 모델들과 비교되었다.

