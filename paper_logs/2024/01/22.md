## Daily Papers (2024-01-22)

### [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/abs/2401.10891)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xnuiMMUgeI_CHzmmNwR8x.png)

Vote: 26

Authors: Lihe Yang, Lihe Yang, Lihe Yang, Bingyi Kang, Bingyi Kang, Bingyi Kang, Zilong Huang, Zilong Huang, Zilong Huang, Xiaogang Xu, Xiaogang Xu, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao

- 이 연구는 강력한 모노큘러 깊이 추정을 위한 실용적인 해결책인 'Depth Anything'를 제시합니다.
- 본 연구의 주요 목표는 어떠한 이미지 상황에서도 대응할 수 있는 간단하지만 강력한 기반 모델을 구축하는 것입니다.
- 약 6200만 개의 대규모 레이블이 지정되지 않은 데이터를 자동으로 수집하고 주석을 달아, 데이터 세트를 확대하여 데이터 커버리지를 확대하고 일반화 오류를 줄였습니다.
- 데이터 증강 도구를 활용하여 더 도전적인 최적화 대상을 생성하는 전략이 효과적이라는 것을 밝혔습니다.
- 미리 훈련된 인코더로부터 풍부한 의미론적 선행 지식을 계승하도록 하는 보조 감독 법을 개발했습니다.
- 대중적인 여섯 데이터셋과 무작위로 촬영된 사진을 포함해 제로샷 기능이 광범위하게 평가되었으며 놀랍도록 좋은 일반화 능력을 선보였습니다.
- NYUv2와 KITTI의 메트릭 깊이 정보를 이용한 미세조정을 통해 새로운 최첨단 성과를 달성했습니다.
- 더 나은 깊이 모델은 더 나은 깊이 기반 ControlNet 성능을 이끌어 냈습니다.
- 연구에서 사용된 모델들은 https://github.com/LiheYoung/Depth-Anything 에서 공개되어 있습니다.

### [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://arxiv.org/abs/2401.10774)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gxvTu6T6sN1X8MJshLwux.gif)

Vote: 23

Authors: Tianle Cai, Tianle Cai, Tianle Cai, Yuhong Li, Yuhong Li, Yuhong Li, Zhengyang Geng, Zhengyang Geng, Zhengyang Geng, Hongwu Peng, Hongwu Peng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao, Tri Dao, Tri Dao

- 본 논문에서는 대규모 언어 모델(LLM)의 추론 과정을 가속화하기 위해 Medusa라는 효율적인 방법을 제시하고 있다.
- Medusa는 디코딩 헤드를 추가하여 병렬로 여러 후속 토큰을 동시에 예측함으로써 자동 회귀 디코딩 과정의 병렬성 부재를 해결한다.
- 트리 기반의 어텐션 메커니즘을 활용하여, Medusa는 여러 후보 연속 단어들을 생성하고 각 디코딩 단계에서 동시에 검증한다.
- 병렬 처리를 이용하여 최소한의 단일 단계 지연시간을 가지면서도 필요한 디코딩 단계 수를 상당히 줄인다.
- Medusa-1은 기존 LLM의 백본을 고정시키고 위에 직접 파인튜닝을 거쳐 생성 품질을 손상시키지 않으면서 2.2배의 가속을 달성한다.
- Medusa-2는 백본 LLM과 함께 파인튜닝을 통하여 더 나은 예측 정확성과 2.3배에서 3.6배의 속도 향상을 제공하지만, 백본 모델의 능력을 보존하는 특별한 학습 방법이 필요하다.
- 추가적으로 Medusa의 유용성을 개선하거나 확장하기 위한 여러 연장 기법이 제안되었으며, 이는 훈련 데이터가 없는 상황을 처리하는 자체 증류 방법과 생성 품질을 유지하면서 수용률을 높이는 일반 수용 스키마를 포함한다.
- 다양한 크기와 학습 절차의 모델에서 Medusa를 평가한 결과, Medusa-1은 생성 품질을 저하시키지 않으면서 2.2배 이상의 속도 향상을, Medusa-2는 속도 향상을 더욱 개선하여 달성함을 보여준다.

### [Zero Bubble Pipeline Parallelism](https://arxiv.org/abs/2401.10241)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gITPhCb7DEvKRiVeZUuI0.png)

Vote: 15

Authors: Penghui Qi, Penghui Qi, Penghui Qi, Xinyi Wan, Guangxing Huang, Guangxing Huang, Guangxing Huang, Min Lin, Min Lin, Min Lin

- 파이프라인 병렬 처리는 대규모 분산 학습의 핵심 구성 요소 중 하나이지만, 효율성은 파이프라인 버블로 인해 저하되며 이는 불가피하게 여겨졌다.
- 본 연구는 동기화 학습 시맨틱스 하에서 제로 파이프라인 버블을 성공적으로 달성하는 최초의 스케줄링 전략을 소개한다.
- 이 개선의 핵심 아이디어는 역전파 계산을 입력에 대한 기울기를 계산하는 부분과 매개변수에 대한 것을 계산하는 부분으로 분할하는 것에 기반한다.
- 이 아이디어를 바탕으로, 연구진은 기존 방법을 크게 능가하는 새로운 파이프라인 스케줄을 수작업으로 제작했다.
- 더 나아가 연구팀은 특정 모델 구성과 메모리 한계에 기반하여 최적의 스케줄을 자동으로 찾는 알고리즘을 개발하였다.
- 진정한 제로 버블을 달성하기 위해, 연구팀은 최적화 단계 동안 동기화를 우회하는 새로운 기술을 소개한다.
- 실험적 평가 결과, 우리의 방법은 유사한 메모리 한계에서 기존 1F1B 스케줄 대비 최대 23%의 처리량으로 성능이 우수함을 보여줬으며, 메모리 제약이 완화되면 이 수치는 31%까지 더 높아질 수 있다.
- 이 결과는 파이프라인 병렬 처리의 진정한 잠재력을 활용하는 데 중요한 도약이 될 것이라고 생각된다.
- 연구팀은 스케줄링 전략의 구현체를 인기 있는 Megatron-LM 저장소를 기반으로 https://github.com/sail-sg/zero-bubble-pipeline-parallelism 에서 오픈 소스로 공개하였다.

### [Rambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation](https://arxiv.org/abs/2401.10838)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/th8GnobHIcHVFfCzmbbb6.png)

Vote: 7

Authors: Susan Lin, Jeremy Warner, Jeremy Warner, Jeremy Warner, J. D. Zamfirescu-Pereira, J. D. Zamfirescu-Pereira, J. D. Zamfirescu-Pereira, Matthew G. Lee, Matthew G. Lee, Matthew G. Lee, Sauhard Jain, Michael Xuelin Huang, Piyawat Lertvittayakumjorn, Shanqing Cai, Shanqing Cai, Shanqing Cai, Shumin Zhai, Björn Hartmann, Can Liu

- 음성 인식은 모바일 기기에서 효율적인 텍스트 입력을 가능하게 하지만, 음성을 통한 글쓰기는 불분명하고 장황하며 비일관적인 텍스트를 생성할 수 있어, 많은 후처리 작업이 필요합니다.
- 본 논문에서는 Rambler를 소개하는데, 이는 대규모 언어 모델(LLM)을 이용한 그래픽 사용자 인터페이스이며 주요 내용 추출과 매크로 수정이라는 두 가지 주요 기능을 통해 음성으로 작성된 텍스트의 수정을 지원합니다.
- 주요 내용 추출 기능은 키워드 및 요약을 생성하여 사용자가 음성으로 입력한 텍스트를 검토하고 상호 작용하는 데 도움을 줍니다.
- LLM 보조 매크로 수정 기능을 통해 사용자들은 편집 위치를 정확히 지정하지 않고도 음성 다시 말하기, 분할, 합병, 변환과 같은 명령을 통해 음성으로 입력한 텍스트를 조작할 수 있습니다.
- 이러한 기능들은 즉흥적인 말에서 구조적으로 잘 정리된 글쓰기로 전환되는 과정에서 상호작용적인 음성 입력과 수정을 돕습니다.
- 12명의 참가자가 구술 작문 과제를 수행한 비교 연구에서, Rambler는 기존의 음성-텍스트 에디터와 ChatGPT를 조합한 기준선보다 우수한 성능을 보였으며, 내용에 대한 사용자의 제어를 강화하면서 다양한 사용자 전략을 놀랍게도 지원했습니다.

### [ActAnywhere: Subject-Aware Video Background Generation](https://arxiv.org/abs/2401.10822)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vt5f7ZwiqPRMUqh4-IiGF.png)

Vote: 6

Authors: Boxiao Pan, Boxiao Pan, Boxiao Pan, Zhan Xu, Chun-Hao Paul Huang, Chun-Hao Paul Huang, Chun-Hao Paul Huang, Krishna Kumar Singh, Yang Zhou, Leonidas J. Guibas, Jimei Yang, Jimei Yang, Jimei Yang

- 영화 산업 및 시각 효과 커뮤니티에 중요한 문제인 전경 주체 움직임에 맞는 비디오 배경을 생성하는 것에 관한 연구이다.
- 이 작업은 전경 주체의 동작과 외관에 부합하면서도 아티스트의 창의적 의도에 맞는 배경을 합성하는 것을 포함한다.
- 'ActAnywhere'라는 새로운 생성 모델을 소개하며, 이 모델은 수작업으로 했던 과정을 자동화한다.
- 대규모 비디오 확산 모델의 강력함을 활용하여 특히 해당 작업에 맞게 조정되었다.
- 전경 주체 분할 시퀀스와 원하는 장면을 설명하는 이미지를 입력으로 받아 조건 프레임에 따라 현실감 있는 전경-배경 상호작용의 비디오를 생성한다.
- 인간-장면 상호작용 비디오의 대규모 데이터셋으로 모델을 훈련시켰다.
- 광범위한 평가를 통해 기존 기준 모델들을 상당히 능가하는 우수한 성능을 입증했다.
- 또한, ActAnywhere가 비인간 주제를 포함한 다양한 분배 이외의 샘플에도 일반화됨을 보여주었다.
- 해당 프로젝트 웹페이지 https://actanywhere.github.io 에서 더 많은 정보를 확인할 수 있다.

### [Inflation with Diffusion: Efficient Temporal Adaptation for Text-to-Video Super-Resolution](https://arxiv.org/abs/2401.10404)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/q361haSX7HYwayegkL3HW.png)

Vote: 6

Authors: Xin Yuan, Xin Yuan, Xin Yuan, Jinoo Baek, Keyang Xu, Omer Tov, Omer Tov, Omer Tov, Hongliang Fei, Hongliang Fei, Hongliang Fei

- 본 논문에서는 픽셀 수준 이미지 확산 모델이 공간 정보를 포착하는 능력을 활용하는 새로운 효율적인 확산 기반 텍스트에서 비디오로의 초고해상도(SR) 조정 접근법을 제안하였다.
- 이 목표를 달성하기 위해 텍스트에서 이미지로의 SR 모델의 가중치를 비디오 생성 프레임워크로 '확장'하여 효율적인 아키텍처를 설계하였다.
- 비디오 프레임 간의 시간적 일관성을 보장하기 위해 시간 어댑터(temporal adapter)를 추가하였다.
- 우리의 확장된 아키텍처를 기반으로 한 다양한 조정 방법들을 연구하고, 계산 비용과 초고해상도의 품질 사이의 상충 관계를 보고한다.
- 셔터스톡 비디오 데이터셋(Shutterstock video dataset)에서 양적 및 질적으로 평가를 실시한 결과, 우리의 접근법은 좋은 시각적 품질과 시간적 일관성을 가진 텍스트에서 비디오로의 SR 생성을 수행할 수 있음을 나타내었다.
- 시간적 일관성을 평가하기 위해 비디오 형식의 시각화 자료도 제공하며, 이는 https://drive.google.com/drive/folders/1YVc-KMSJqOrEUdQWVaI-Yfu8Vsfu_1aO?usp=sharing 에서 확인할 수 있다.

### [Understanding Video Transformers via Universal Concept Discovery](https://arxiv.org/abs/2401.10831)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/BhQg3EtpWXelFayp8Iaiz.png)

Vote: 5

Authors: Matthew Kowal, Matthew Kowal, Matthew Kowal, Achal Dave, Achal Dave, Achal Dave, Rares Ambrus, Rares Ambrus, Rares Ambrus, Adrien Gaidon, Adrien Gaidon, Adrien Gaidon, Konstantinos G. Derpanis, Pavel Tokmakov, Pavel Tokmakov, Pavel Tokmakov

- 본 논문은 비디오 변환기(transformer) 표현의 개념 기반 해석 가능성 문제를 연구합니다.
- 고차원의 시공간 개념을 자동으로 발견하며, 이를 통해 비디오 변환기의 의사결정 과정을 설명하고자 합니다.
- 기존 연구가 이미지 수준의 작업에만 집중한 반면, 비디오 모델은 시간적 차원이 추가되어 동적 개념을 시간에 걸쳐 식별하는 복잡성과 도전이 증가합니다.
- 이러한 도전을 해결하기 위해, 첫 번째 비디오 변환기 개념 발견(Video Transformer Concept Discovery, VTCD) 알고리즘을 소개합니다.
- 비디오 변환기 표현의 단위 - 개념들을 비지도학습 방식으로 효과적으로 식별하고 모델 출력에 대한 중요도를 순위 지정하는 방법을 제안합니다.
- 결과적으로 도출된 개념들은 시간적 논리 추론 메커니즘과 구조화되지 않은 비디오 모델에서 객체 중심 표현을 드러내는 높은 해석 가능성을 지닙니다.
- 감독된 및 자체 감독된 표현 모두에 대해 이러한 분석을 공동으로 수행함으로써, 비디오 변환기에서 일부 메커니즘이 보편적임을 발견했습니다.
- 마지막으로, VTCD가 세밀한 작업을 위한 모델 성능 향상에 사용될 수 있음을 보여줍니다.

### [Synthesizing Moving People with 3D Control](https://arxiv.org/abs/2401.10889)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/U5nTq8nH2lRpYYR-dBknO.qt)

Vote: 5

Authors: Boyi Li, Boyi Li, Boyi Li, Jathushan Rajasegaran, Jathushan Rajasegaran, Jathushan Rajasegaran, Yossi Gandelsman, Yossi Gandelsman, Yossi Gandelsman, Alexei A. Efros, Jitendra Malik, Jitendra Malik, Jitendra Malik

- 본 논문에서는 단일 이미지로부터 목표 3D 동작 시퀀스가 주어졌을 때 사람을 애니메이팅하는 확산 모델 기반 프레임워크를 제시합니다.
- 학습된 인-필링(in-filling) 확산 모델은 사람의 보이지 않는 부분과 의상에 대한 정보를 추정하여 한 장의 이미지에서 눈에 보이지 않는 부분을 상상력으로 채웁니다.
- 이 모델은 포즈(pose)와 시점(viewpoint)에 불변하는 질감 맵(texture map) 공간에서 훈련되어, 샘플 효율성이 높아집니다.
- 또한, 3D 인간 포즈에 의해 제어되는 확산 기반 렌더링 파이프라인을 개발하여, 사람의 새로운 포즈를 포함한 의상, 머리카락, 그리고 보이지 않는 영역의 합리적인 채우기를 실제같이 생성합니다.
- 이 방법은 3D 포즈의 목표 동작에 충실하고, 시각적 유사성 측면에서 입력 이미지와 비슷한 이미지 시퀀스를 생성할 수 있게 해줍니다.
- 추가적으로 3D 제어 기능은 다양한 합성 카메라 궤적으로 사람을 렌더링할 수 있습니다.
- 실험을 통해 우리의 방법은 지속적인 움직임과 다양하며 복잡한 포즈를 생성함에 있어 이전 방법들에 비해 탄력적임을 보여줍니다.
- 자세한 내용은 우리의 웹사이트 https://boyiliee.github.io/3DHM.github.io/ 에서 확인할 수 있습니다.

