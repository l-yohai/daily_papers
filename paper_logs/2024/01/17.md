## Daily Papers (2024-01-17)

### [Scalable Pre-training of Large Autoregressive Image Models](https://arxiv.org/abs/2401.08541)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/agQVTT_BcFBvRnhUcBL4R.png)

Vote: 21

Authors: Alaaeldin El-Nouby, Alaaeldin El-Nouby, Alaaeldin El-Nouby, Michal Klein, Michal Klein, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Miguel Angel Bautista, Miguel Angel Bautista, Alexander Toshev, Alexander Toshev, Alexander Toshev, Vaishaal Shankar, Vaishaal Shankar, Vaishaal Shankar, Joshua M Susskind, Joshua M Susskind, Joshua M Susskind, Armand Joulin

- 본 논문은 자동회귀 목표를 사용하여 사전 학습된 비전 모델 컬렉션인 AIM을 소개합니다.
- 이 모델들은 대형 언어 모델(LLMs)에서 영감을 받았으며 유사한 확장 성능을 보입니다.
- 모델 성능은 모델 용량과 데이터 양의 증가와 함께 확장되며, 사전 학습된 AIM은 20억 개의 이미지에서 70억 개의 파라미터를 가진 모델로 학습되었습니다.
- 학습된 AIM 모델은 고정된 트렁크로 ImageNet-1k에서 84.0%의 성과를 달성했으며, 이 규모에서도 성능의 포화 현상은 관찰되지 않았습니다.
- AIM의 사전 학습은 언어 모델의 사전 학습과 유사하며, 대규모 학습을 안정화하는 데 이미지 특화 전략이 필요하지 않습니다.
- 또한, 사전 학습 목표치의 가치는 하위 작업에서 모델의 성능과 상관 관계가 있다는 것을 밝혔습니다.

### [InstantID: Zero-shot Identity-Preserving Generation in Seconds](https://arxiv.org/abs/2401.07519)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/hsBzAOuE6olEuzTfBGnax.png)

Vote: 17

Authors: Qixun Wang, Qixun Wang, Qixun Wang, Xu Bai, Haofan Wang, Haofan Wang, Haofan Wang, Zekui Qin, Anthony Chen

- 기존에 개인화된 이미지 합성 방법들인 Textual Inversion, DreamBooth, LoRA는 저장 공간이 많이 필요하고, 긴 튜닝 과정이 필요하며, 여러 참조 이미지가 필요로 하는 등 실제적용에 제한이 많았다.
- 기존 ID 임베딩 기반 방법은 한 번의 전진 추론만 필요하지만, 많은 모델 매개변수에 대한 광범위한 튜닝을 요하거나, 기존의 사전 훈련된 모델과의 호환성이 부족하거나, 높은 얼굴 충실도를 유지하는데 실패하는 문제점이 있었다.
- 이러한 한계점을 해결하기 위해, 우리는 단일 얼굴 이미지만을 사용하여 다양한 스타일로 이미지 개인화를 효과적으로 처리하면서도 높은 충실도를 보장하는 강력한 확산 모델 기반 솔루션인 InstantID를 소개한다.
- InstantID는 강력한 의미 조건과 약한 공간 조건을 적용한 새로운 IdentityNet을 설계하여 얼굴 이미지와 랜드마크 이미지를 텍스트 프롬프트와 함께 결합하여 이미지 생성을 안내한다.
- InstantID는 ID 보존이 중요한 실제 응용 프로그램에서 뛰어난 성능과 효율성을 보여준다.
- 또한, SD1.5와 SDXL과 같은 인기 있는 사전 훈련된 텍스트-이미지 확산 모델들과 원활하게 통합되어 유연한 플러그인으로 작동한다.
- 관련 코드와 사전 훈련된 체크포인트는 https://github.com/InstantID/InstantID 에서 공개될 예정이다.

### [E^2-LLM: Efficient and Extreme Length Extension of Large Language Models](https://arxiv.org/abs/2401.06951)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/r3URA2dtZIqbthOp1EP9Z.png)

Vote: 12

Authors: Jiaheng Liu, Jiaheng Liu, Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu Zhang, Ge Zhang, Ge Zhang, Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Yukang Chen, Yukang Chen, Wenbo Su, Tiezheng Ge, Jie Fu, Jie Fu, Jie Fu, Wenhu Chen, Wenhu Chen, Wenhu Chen, Bo Zheng

- 대규모 언어 모델을 긴 문맥 크기로 훈련하는 것은 계산 비용이 많이 들고 GPU 자원을 대량으로 필요로 한다.
- 기존의 긴 문맥 확장 방법은 긴 문맥 창을 지원하기 위해 추가적인 훈련 절차가 필요하며, 긴 문맥 훈련 데이터(예: 32k)와 높은 GPU 훈련 비용을 가정한다.
- E^2-LLM은 하나의 훈련 절차로 긴 문맥 데이터 수집의 필요성을 없애면서도 계산 비용을 현저하게 줄이는 효율적이고 극대화된 길이 확장 방법을 제안한다.
- E^2-LLM의 훈련 데이터는 짧은 길이(예: 4k)가 필요하여 튜닝 비용을 크게 줄인다.
- 짧은 훈련 문맥 창에 대한 훈련 절차는 단 한 번 수행되며, 추론 시 다른 평가 문맥 창을 지원할 수 있다.
- RoPE 위치 임베딩에 기반하여, 훈련 중 다양한 샘플에 대해 스케일 및 위치 인덱스 파라미터에 대한 두 가지 다른 증강 방법을 도입하여, 추론 시 임의의 문맥 길이를 직접 내삽할 때 모델이 다른 상대적 차이에 더 강건하게 만든다.
- 여러 벤치마크 데이터셋에서 실시한 종합적인 실험 결과는 E^2-LLM이 긴 문맥 과제에서의 효과성을 입증한다.

### [Towards A Better Metric for Text-to-Video Generation](https://arxiv.org/abs/2401.07781)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Er9TRXk_yScajuisjQu55.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Er9TRXk_yScajuisjQu55.mp4" muted="false"></video></div>

Vote: 11

Authors: Jay Zhangjie Wu, Jay Zhangjie Wu, Jay Zhangjie Wu, Guian Fang, Haoning Wu, Haoning Wu, Haoning Wu, Xintao Wang, Xintao Wang, Xintao Wang, Yixiao Ge, Yixiao Ge, Yixiao Ge, Xiaodong Cun, Xiaodong Cun, Xiaodong Cun, David Junhao Zhang, David Junhao Zhang, David Junhao Zhang, Jia-Wei Liu, Jia-Wei Liu, Jia-Wei Liu, Yuchao Gu, Yuchao Gu, Yuchao Gu, Rui Zhao, Rui Zhao, Rui Zhao, Weisi Lin, Wynne Hsu, Ying Shan, Ying Shan, Ying Shan, Mike Zheng Shou, Mike Zheng Shou, Mike Zheng Shou

- 생성 모델은 고품질의 텍스트, 이미지, 비디오 합성에 뛰어난 능력을 보여주고 있으며, 현대의 텍스트-비디오 모델들은 인상적인 시각적 비디오를 만들어 내고 있다.
- 그러나 이러한 비디오 평가는 FVD, IS, CLIP 점수 같은 자동화된 지표를 주로 사용하며 이는 특히 비디오 내용의 시간적 평가에서 불완전한 분석을 제공하여 신뢰할 수 있는 지표가 되지 못한다.
- 인간의 인식을 정확하게 반영할 수 있는 잠재력을 가진 사용자 연구는 시간이 많이 소요되고 노동 집약적인 본질과 주관적 편향에 의해 결과가 오염될 수 있는 문제가 있다.
- 본 논문에서는 기존 측정법의 한계를 조사하고, 텍스트-비디오 정렬(Text-Video Alignment)과 비디오 품질(Video Quality)의 두 가지 중요한 기준을 통합한 새로운 평가 파이프라인인 텍스트-비디오 점수(T2VScore)를 소개한다.
- 제안하는 지표를 평가하고 향후 개선을 촉진하기 위해, T2VScore는 2,543개의 텍스트-비디오 생성 비디오에 대한 인간의 판단을 수집하는 TVGE 데이터셋을 제시한다.
- TVGE 데이터셋에서의 실험은 제안된 T2VScore가 텍스트-비디오 생성을 위한 보다 나은 지표를 제공하는 것을 보여준다.

### [Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation](https://arxiv.org/abs/2401.08417)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8z2LsPN_L9OdoevoTOZiO.png)

Vote: 8

Authors: Haoran Xu, Haoran Xu, Haoran Xu, Amr Sharaf, Amr Sharaf, Amr Sharaf, Yunmo Chen, Yunmo Chen, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, Kenton Murray, Kenton Murray, Young Jin Kim, Young Jin Kim, Young Jin Kim

- 중간 크기의 대규모 언어 모델(LLM)인 7B나 13B 파라미터를 가진 모델들이 기계 번역(MT) 성능 면에서 유망함을 보여준다.
- 그러나 최고의 성능을 내는 13B LLM 기반 번역 모델인 ALMA조차도 최신의 기존 인코더-디코더 번역 모델이나 더 큰 규모의 LLM인 GPT-4와 같은 성능을 내지 못한다.
- 본 연구에서는 이러한 성능 격차를 해소한다. LLM의 MT 과제에서 감독되는 미세 조정에 대한 단점을 평가하며, 인간이 생성한 참조 데이터에서도 품질 문제가 존재함을 강조한다.
- 참조 번역을 단순히 모방하는 SFT와는 달리, 본 연구에서 새롭게 도입된 Contrastive Preference Optimization (CPO)은 만족스러우나 완벽하지 않은 번역의 생성을 피하는 방향으로 모델을 훈련시키는 방법론이다.
- 단 22K의 병렬 문장과 12M의 파라미터만을 가지고 CPO를 ALMA 모델에 적용함으로써 상당한 성능 개선을 달성한다.
- 이렇게 개선된 모델인 ALMA-R은 WMT'21, WMT'22, WMT'23 테스트 데이터셋에서 WMT 대회 우승자들과 GPT-4의 성능에 맞추거나 이를 뛰어넘을 수 있다.

### [Quantum Denoising Diffusion Models](https://arxiv.org/abs/2401.07049)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gNu8XvUl1TTl3e4AzQgua.png)

Vote: 8

Authors: Michael Kölle, Michael Kölle, Michael Kölle, Gerhard Stenzel, Gerhard Stenzel, Gerhard Stenzel, Jonas Stein, Sebastian Zielinski, Björn Ommer, Claudia Linnhoff-Popien

- 최근 기계 학습 모델들은 간결한 설명에서 고해상도 이미지를 생성하는 능력으로 주목을 받고 있으며, 이에 동시에 양자 컴퓨팅은 특히 양자 머신 학습 분야에서 전통적 기계 학습 알고리즘의 증가하는 계산 요구 사항을 충족하기 위한 유망한 발전을 이루고 있다.
- 이 논문은 양자 머신 학습과 변분 양자 회로를 활용하여 확산 기반 이미지 생성 모델의 효과를 증진시키기 위한 통합 방안을 탐구한다.
- 고전적 확산 모델의 두 가지 문제점, 즉 낮은 샘플링 속도와 광범위한 파라미터 요구 사항을 해결한다고 주장한다.
- 논문은 MNIST 숫자, Fashion MNIST, 그리고 CIFAR-10을 사용하여 두 가지 양자 확산 모델을 소개하고 이들의 능력을 고전 모델과 벤치마킹한다.
- 제안된 양자 모델들은 FID, SSIM, 그리고 PSNR과 같은 성능 지표에서 유사한 파라미터 카운트를 가진 고전 모델들을 뛰어넘는 성과를 보인다.
- 또한, 확산 절차를 단일 단계로 결합하고 빠른 한 단계 이미지 생성을 가능하게 하는 일관성 있는 모델 단일 샘플링 아키텍처를 도입한다.

### [Extending LLMs' Context Window with 100 Samples](https://arxiv.org/abs/2401.07004)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Qkvd3tER1WtHEyZa69or6.png)

Vote: 7

Authors: Yikai Zhang, Yikai Zhang, Yikai Zhang, Junlong Li, Junlong Li, Junlong Li, Pengfei Liu, Pengfei Liu, Pengfei Liu

- 대규모 언어모델(LLMs)는 기존의 긴 입력을 다루는 데 한계가 있으나, 최근 연구들은 로터리 위치 인코딩(RoPE) 수정을 통해 이를 확장하려 하고 있다.
- 선행 연구인 위치 보간법(PI)과 YaRN은 자원을 많이 소모하며 비교실험이 부족해 실용성 평가가 어렵다.
- 본 연구에서는 중요한 LLMs의 주의 엔트로피(attention entropy) 안정성을 보존하는 새로운 RoPE 확장 방법을 제시하였다.
- 제안된 방법은 RoPE의 기본 주파수를 조절하고 주의 점수(attention logits)의 크기를 조정하여 효율적으로 넓은 컨텍스트 창에 적응할 수 있다.
- 다양한 긴 입력이 요구되는 태스크들에서 본 방법이 성능과 견고성 측면에서 우수함을 확인하였다.
- 특히 LLaMA-2-7B-Chat 모델을 단 100개의 샘플과 6번의 학습 단계로 설정하여 컨텍스트 창을 16,384까지 확장하는 효율성을 입증하였다.
- 특정 하위 태스크에 대한 컨텍스트 창 확장에 대해 데이터 구성과 학습 커리큘럼이 미치는 영향에 대해서도 탐색하였다.
- 본 논문은 긴 대화를 포함한 데이터로 LLMs을 미세 조정하는 것을 시작점으로 제안한다.
- 연구 코드와 SFT(Sequential Fine-Tuning) 데이터를 https://github.com/GAIR-NLP/Entropy-ABF 에서 공개하였다.

### [Tuning Language Models by Proxy](https://arxiv.org/abs/2401.08565)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6FYDePLkNlKGpx8IsD-Ju.png)

Vote: 7

Authors: Alisa Liu, Alisa Liu, Alisa Liu, Xiaochuang Han, Xiaochuang Han, Xiaochuang Han, Yizhong Wang, Yizhong Wang, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, Yejin Choi, Yejin Choi, Noah A. Smith

- 대규모 사전 훈련된 언어 모델은 원하는 행동을 더 잘 달성하기 위해 추가적인 적응으로 일관되게 이득을 볼 수 있지만, 모델 튜닝은 점점 더 많은 자원을 요구하거나 모델 가중치가 비공개인 경우 불가능합니다.
- 우리는 검은 상자(black-box) 언어 모델(LMs) 위에서 작동하며, 모델의 예측 결과만을 활용하여 직접 튜닝의 결과를 달성하는 경량화된 디코딩 시간(decoding-time) 알고리즘인 '프록시 튜닝(proxy-tuning)'을 소개합니다.
- 이 방법은 더 작은 LM을 튜닝한 다음, 튜닝되지 않은 작은 LM의 예측과의 차이를 이용하여 기본 모델의 원래 예측을 튜닝 방향으로 이동시켜 대규모 사전 훈련의 이점을 유지합니다.
- 실험에서, 우리는 프록시 튜닝을 적용하여 Llama2-70B에 88%의 성능 향상을 도모했고, 7B 크기의 프록시를 사용하여 진실된 챗 버전과의 성능 격차를 줄일 수 있었습니다.
- 또한, 프록시 튜닝된 모델이 TruthfulQA에 대해 실제로 직접 튜닝한 모델보다 더 진실된 응답을 제공할 수 있음을 보여주며, 이는 디코딩 시간의 가이드가 모델의 사실적 지식을 더 잘 유지하기 때문일 수 있습니다.
- 우리는 코드의 도메인 적응 및 질문-응답과 수학 문제를 위한 특정한 미세조정 작업에 프록시 튜닝을 적용함으로써 그 범용성을 입증하였습니다.
- 이 연구는 디코딩 시간 가이드를 통해 크고, 가능성 있는 독점적인 LMs를 효율적으로 맞춤 설정하기 위한 작은 튜닝된 LMs의 이용 가능성을 보여줍니다.

### [HexaGen3D: StableDiffusion is just one step away from Fast and Diverse Text-to-3D Generation](https://arxiv.org/abs/2401.07727)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/zZaNBA_hs0EQK9LwuIBic.png)

Vote: 2

Authors: Antoine Mercier, Ramin Nakhli, Ramin Nakhli, Ramin Nakhli, Mahesh Reddy, Rajeev Yasarla, Hong Cai, Fatih Porikli, Guillaume Berger

- 최신 생성 모델링 기술이 발전했음에도 불구하고, 텍스트 명령에서 고화질 3D 자산을 효율적으로 생성하는 것은 여전히 어려운 과제입니다.
- 데이터 부족이 주요 문제로, 가장 큰 3D 데이터셋이 수백만 자산을 포함하는 반면 그 2D 대응물은 수십억의 텍스트-이미지 쌍을 포함합니다.
- 이를 해결하기 위해, 우리는 대규모 사전 훈련된 2D 확산 모델의 힘을 활용하는 새로운 접근법을 제안합니다.
- 우리의 접근법인 HexaGen3D는 사전 훈련된 텍스트-이미지 모델을 미세 조정하여 6개의 직교 투영을 동시에 예측하고 해당하는 잠재 삼평면을 예측합니다.
- 그 다음 이 잠재 변수들을 디코드하여 텍스처가 있는 메시를 생성합니다.
- HexaGen3D는 샘플별 최적화가 필요 없으며, 텍스트 명령에서 고품질이고 다양한 객체를 7초 안에 추론할 수 있으며, 기존 접근법에 비해 현저하게 우수한 품질 대 대기 시간 트레이드 오프를 제공합니다.
- 또한 HexaGen3D는 새로운 객체나 구성에 강한 일반화 능력을 보여줍니다.

