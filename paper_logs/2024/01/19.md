## Daily Papers (2024-01-19)

### [Self-Rewarding Language Models](https://arxiv.org/abs/2401.10020)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GvhF-FArhZFDSQJirlPSv.png)

Vote: 42

Authors: Weizhe Yuan, Weizhe Yuan, Weizhe Yuan, Richard Yuanzhe Pang, Richard Yuanzhe Pang, Richard Yuanzhe Pang, Kyunghyun Cho, Kyunghyun Cho, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, Jason Weston, Jason Weston

- 이 연구는 초인적 에이전트를 달성하기 위해서는 적절한 훈련 신호를 제공하기 위한 초인적 피드백이 미래 모델에 필요하다고 주장합니다.
- 현재 접근 방식은 인간의 선호도에 기반한 보상 모델을 훈련하는데, 이는 인간의 성능 수준에 의해 한계가 있을 수 있으며, 별도로 고정된 보상 모델은 LLM 훈련 중에 개선을 학습할 수 없습니다.
- 본 연구에서는 언어 모델 자체가 LLM-as-a-Judge 프롬프팅을 통해 훈련 중에 자체 보상을 제공하는 자기 보상 언어 모델(Self-Rewarding Language Models)에 대해 연구했습니다.
- 반복적인 DPO 훈련 중에는 지시에 따른 능력뿐만 아니라 고품질 자기 보상을 제공하는 능력도 향상됨을 보여줍니다.
- Llama 2 70B를 우리의 접근 방식 세 번의 반복으로 미세 조정한 모델은 AlpacaEval 2.0 리더보드에서 Claude 2, Gemini Pro, GPT-4 0613을 포함한 많은 기존 시스템들을 능가합니다.
- 이 초기 연구는 두 갈래 모두에서 지속적으로 개선될 수 있는 모델의 가능성에 대한 문을 열어줍니다.

### [DiffusionGPT: LLM-Driven Text-to-Image Generation System](https://arxiv.org/abs/2401.10061)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/S7274X86qLOt_miW5W9qL.png)

Vote: 17

Authors: Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Xuefeng Xiao, Xuefeng Xiao, Rui Wang, Shilei Wen, Shilei Wen, Shilei Wen

- 확산 모델은 이미지 생성 분야에 새로운 가능성을 열어 고품질의 모델들이 오픈소스 플랫폼에서 공유되고 있으나, 현재의 텍스트-이미지 시스템은 다양한 입력 처리에 한계가 있습니다.
- 기존의 통합 시도는 입력 단계에서 다양한 프롬프트를 해석하고(expert model을 활성화하는 두 가지 상반된 접근법에 머물러 있습니다.
- 이 두 세계의 장점을 결합하기 위해, 다양한 유형의 프롬프트를 수용하고 도메인 전문 모델을 통합할 수 있는 통합 생성 시스템인 DiffusionGPT를 제안합니다.
- DiffusionGPT는 이전 지식에 기반하여 다양한 생성 모델에 대한 도메인별 트리를 구성합니다.
- 입력이 주어지면 LLM은 프롬프트를 해석하고 사고의 트리(Tree-of-Thought)를 사용하여 적절한 모델을 선택함으로써 입력 제약을 완화하고 다양한 도메인에서 탁월한 성능을 보장합니다.
- 또한, Advantage Databases를 도입하여 사고의 트리를 인간의 피드백으로 풍부하게 하여 모델 선택 과정을 인간의 선호도와 일치시킵니다.
- 광범위한 실험과 비교를 통해 DiffusionGPT의 효과성을 입증하며, 다양한 도메인에서의 이미지 합성의 경계를 확장할 수 있는 잠재력을 보여줍니다.

### [ChatQA: Building GPT-4 Level Conversational QA Models](https://arxiv.org/abs/2401.10225)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/UkABNPp5_cGu24Z_PYrnA.png)

Vote: 16

Authors: Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, Bryan Catanzaro, Bryan Catanzaro

- 본 연구에서는 GPT-4 수준의 정확도를 가진 대화형 질문 응답(QA) 모델인 ChatQA를 소개합니다.
- 연구팀은 대규모 언어 모델(LLMs)로부터의 제로샷 대화형 QA 결과를 상당히 개선하는 두 단계 지시 튜닝 방법을 제안합니다.
- 대화형 QA에서 검색 문제를 처리하기 위해, 연구팀은 다차례 QA 데이터셋을 활용하여 밀집 검색기를 미세 조정하였는데, 이는 배포 비용을 크게 절감하면서 최신의 질의 문장 재작성 모델과 비슷한 결과를 제공합니다.
- 특히, 연구팀의 ChatQA-70B 모델은 10개의 대화형 QA 데이터셋에서 평균 점수 면에서 GPT-4 모델을 능가합니다(54.14 대 53.90), 이는 OpenAI GPT 모델로부터 어떤 합성 데이터에도 의존하지 않는 결과입니다.

### [VMamba: Visual State Space Model](https://arxiv.org/abs/2401.10166)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/MMWId1L6muy8vFuad1NO2.png)

Vote: 15

Authors: Yue Liu, Yunjie Tian, Yunjie Tian, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Yaowei Wang, Yaowei Wang, Qixiang Ye, Yunfan Liu

- 컨볼루션 네트워크(CNNs)와 비전 트랜스포머(ViTs)는 시각적 표현 학습을 위한 두 가장 인기 있는 기본 모델로 자리 잡고 있습니다.
- CNN은 선형 복잡도로 이미지 해상도에 대한 뛰어난 확장성을 보이는 반면, ViTs는 제곱 복잡도에도 더 나은 적합 능력을 제공합니다.
- ViTs가 전역 수용 영역(global receptive fields)과 동적 가중치(dynamic weights)를 통합함으로써 우수한 시각 모델링 성능을 달성한다는 것을 발견했습니다.
- 이러한 관찰을 통해, 우리는 전역 수용 영역을 유지하면서 계산 효율성을 향상시키는 새로운 아키텍처인 Visual State Space Model(VMamba)을 제안합니다.
- 방향에 민감한 문제를 해결하기 위해 공간 영역을 횡단하고 비인과적인 시각 이미지를 순서있는 패치 시퀀스로 변환하는 Cross-Scan Module(CSM)을 도입했습니다.
- 대규모 실험 결과는 VMamba가 다양한 시각 인식 작업에서 유망한 능력을 나타내는 것뿐만 아니라 이미지 해상도가 증가함에 따라 기존 벤치마크보다 더 두드러진 장점을 보여줌을 입증합니다.
- 소스 코드는 https://github.com/MzeroMiko/VMamba 에서 확인할 수 있습니다.

### [WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens](https://arxiv.org/abs/2401.09985)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/R3qk6IqP6WI_X4W9_ykJA.png)

Vote: 11

Authors: Xiaofeng Wang, Zheng Zhu, Zheng Zhu, Zheng Zhu, Guan Huang, Boyuan Wang, Boyuan Wang, Boyuan Wang, Xinze Chen, Jiwen Lu

- WorldDreamer는 비디오 생성을 위해 일반적인 세계 동적 환경의 복잡성을 포착할 수 있는 선도적인 세계 모델을 도입합니다.
- 이 모델은 대규모 언어 모델의 성공에 영감을 받아 시각적 입력을 이산 토큰으로 매핑하고 가려진 토큰을 예측함으로써 비감독 시각 시퀀스 모델링 문제로 세계 모델링을 구성합니다.
- 다중 모달 프롬프트를 도입하여 세계 모델 내에서의 상호작용을 촉진합니다.
- WorldDreamer는 자연 풍경과 운전 환경을 포함한 다양한 시나리오에서 비디오 생성에 뛰어난 성능을 보여줍니다.
- 텍스트-비디오 변환, 이미지-비디오 합성, 비디오 편집과 같은 작업을 수행하는 데 있어 다재다능함을 과시합니다.
- 이러한 결과는 여러 일반적인 세계 환경 내에서 동적 요소를 포착하는 WorldDreamer의 효과성을 강조합니다.

### [Improving fine-grained understanding in image-text pre-training](https://arxiv.org/abs/2401.09865)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/S6rsTD3mFqV4MVXbH3YUK.png)

Vote: 9

Authors: Ioana Bica, Anastasija Ilić, Matthias Bauer, Goker Erdogan, Matko Bošnjak, Christos Kaplanis, Alexey A. Gritsenko, Matthias Minderer, Matthias Minderer, Matthias Minderer, Charles Blundell, Razvan Pascanu, Razvan Pascanu, Razvan Pascanu, Jovana Mitrović

- 본 논문은 이미지-텍스트 쌍으로부터 보다 세밀한 멀티모달 표현을 사전 학습하기 위한 단순한 방법인 SPARse Fine-grained Contrastive Alignment (SPARC)를 소개합니다.
- SPARC는 캡션 내의 각 토큰에 대하여 이미지 패치들을 그룹화하는 방법을 제안하며, 이미지 패치와 언어 토큰 간의 희소 유사성 지표를 사용하여 언어 기반 시각 임베딩을 계산합니다.
- 토큰과 언어 기반 시각 임베딩은, 배치 샘플 간의 부정적 예제를 요구하지 않는 샘플 단위의 세밀한 시퀀스별 손실 함수를 통해 대조됩니다.
- 이 방법은 전체 이미지와 텍스트 임베딩 사이의 대조적 손실과 함께 결합되어, 전역적 및 지역적 정보를 동시에 인코딩하는 표현을 학습합니다.
- SPARC는 분류와 같은 이미지-수준 작업 뿐만 아니라 검색, 객체 감지, 세분화와 같은 지역-수준 작업에서 경쟁 방법론들을 능가하는 개선된 성능을 보여줍니다.
- SPARC는 기초 시각-언어 모델에서 모델 신뢰도와 캡션 생성을 개선합니다.

### [SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild](https://arxiv.org/abs/2401.10171)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/fAlm1hGW2xn1lpnYOi6RH.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/fAlm1hGW2xn1lpnYOi6RH.mp4" muted="false"></video></div>

Vote: 7

Authors: Andreas Engelhardt, Andreas Engelhardt, Andreas Engelhardt, Amit Raj, Mark Boss, Mark Boss, Mark Boss, Yunzhi Zhang, Yunzhi Zhang, Yunzhi Zhang, Abhishek Kar, Yuanzhen Li, Yuanzhen Li, Yuanzhen Li, Deqing Sun, Ricardo Martin Brualla, Jonathan T. Barron, Hendrik P. A. Lensch, Varun Jampani, Varun Jampani, Varun Jampani

- 다양한 조명, 자세, 배경이 있는 객체 이미지에서 형태, 재질 및 조명을 재구성하기 위한 SHINOBI라는 end-to-end 프레임워크를 소개합니다.
- 제약이 없는 이미지 컬렉션을 기반으로 한 객체의 역렌더링은 컴퓨터 비전 및 그래픽 분야의 오랜 도전 과제이며, 형태, 광선 및 자세에 대한 공동 최적화가 필요합니다.
- 다중 해상도 해시 인코딩을 기반으로 하는 암묵적 형태 표현은 이전 연구보다 빠르고 안정적인 형태 재구성과 공동 카메라 정렬 최적화를 가능하게 합니다.
- 조명 및 객체 반사도(재질)의 편집을 가능하게 하기 위해, 객체의 형태와 함께 BRDF 및 조명을 공동으로 최적화합니다.
- 이 방법은 클래스를 구분하지 않고 자연적 상황에서 취득한 객체의 이미지 컬렉션에서 조명이 가능한 3D 자산을 생성하며, AR/VR, 영화, 게임 등 여러 용도로 활용됩니다.
- 프로젝트 페이지 및 동영상 링크를 통해 이 연구의 자세한 정보와 결과물을 볼 수 있습니다.

### [FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder](https://arxiv.org/abs/2401.10032)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/BE4b_4cz9zQl5idby3k_B.png)

Vote: 7

Authors: Tan Dat Nguyen, Tan Dat Nguyen, Tan Dat Nguyen, Ji-Hoon Kim, Youngjoon Jang, Youngjoon Jang, Youngjoon Jang, Jaehun Kim, Jaehun Kim, Jaehun Kim, Joon Son Chung, Joon Son Chung, Joon Son Chung

- 이 논문은 가볍고 빠른 확산 기반 보코더인 FreGrad를 사용하여 현실적인 오디오를 생성하는 것을 목표로 합니다.
- 첫째, 복잡한 파형을 간단하고 명료한 특징 공간으로 분해하는 이산 웨이블릿 변환을 채택하여 FreGrad가 작업하도록 해줍니다.
- 둘째, 정확한 주파수 정보를 가진 음성을 생성하기 위해 주파수 인식이 향상된 주파수 인식 확장 컨볼루션을 설계하였습니다.
- 셋째, 제안하는 모델의 생성 품질을 향상시키는 여러 가지 방법들을 소개합니다.
- 실험에서 FreGrad는 기준 모델에 비해 3.7배 더 빠른 학습 시간과 2.2배 더 빠른 추론 속도를 달성하면서 모델 크기도 0.6배 줄였습니다(단 1.78M 매개변수).
- 오디오 샘플은 온라인에서 확인할 수 있습니다: https://mm.kaist.ac.kr/projects/FreGrad.

### [CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects](https://arxiv.org/abs/2401.09962)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GFvdOMUcvcL_cSHtfo0BG.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GFvdOMUcvcL_cSHtfo0BG.mp4" muted="false"></video></div>

Vote: 5

Authors: Zhao Wang, Aoxue Li, Enze Xie, Enze Xie, Enze Xie, Lingting Zhu, Lingting Zhu, Lingting Zhu, Yong Guo, Qi Dou, Zhenguo Li, Zhenguo Li, Zhenguo Li

- 맞춤형 텍스트-비디오 생성은 텍스트 프롬프트와 주제 참조에 의해 제어되는 고품질 비디오를 생성하는 것을 목표로 합니다.
- 기존 접근 방식은 단일 주제 설계에 집중되어 있어 다양한 주체가 포함된 보다 복잡하고 실용적인 시나리오를 다루는 데 어려움이 있습니다.
- 본 연구에서는 다중 주제를 가이드로 하는 텍스트-비디오 맞춤형 생성을 촉진하기 위해 'CustomVideo'라는 새로운 프레임워크를 제안합니다.
- 우선, 단일 이미지 내에서 여러 주제의 공존을 촉진하고, 기본 텍스트-비디오 확산 모델에서, 다른 주제를 확산 모델의 잠재 공간 안에서 분리할 수 있는 간단하지만 효과적인 주의 제어 전략을 설계합니다.
- 또한 모델이 특정 객체 영역에 집중할 수 있도록 주어진 참조 이미지에서 객체를 분리하고 해당 객체 마스크를 주의 학습에 제공합니다.
- 본 연구진은 69개의 개별 주제와 57개의 의미 있는 쌍을 포함하여, 종합적인 벤치마크로서 다중 주제 텍스트-비디오 생성 데이터셋도 수집했습니다.
- 광범위한 질적, 양적 및 사용자 연구 결과는 이전 최신 방법들에 비해 우리의 방법의 우수성을 입증합니다.

### [Rethinking FID: Towards a Better Evaluation Metric for Image Generation](https://arxiv.org/abs/2401.09603)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Ik64AlizVmcMBdGMZpzk8.png)

Vote: 5

Authors: Sadeep Jayasumana, Sadeep Jayasumana, Sadeep Jayasumana, Srikumar Ramalingam, Srikumar Ramalingam, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, Sanjiv Kumar

- 이미지 생성 분야의 발전은 좋은 평가 지표에 의존하는데, 가장 인기 있는 지표 중 하나인 FID(Frechet Inception Distance)는 여러 단점이 있음을 지적합니다.
- FID는 실제 이미지의 Inception-v3 특징과 알고리즘이 생성한 이미지의 분포 간의 거리를 측정하지만, 현대의 텍스트-이미지 모델이 생성하는 풍부하고 다양한 내용을 제대로 표현하지 못한다는 문제가 있습니다.
- FID는 정규성 가정이 부정확하며, 샘플 복잡성이 낮다는 등의 중대한 문제점을 가지고 있으며, 이에 따라 생성된 이미지의 주요 품질 메트릭으로서의 FID 사용에 대한 재평가가 필요함을 주장합니다.
- 연구는 FID가 사람들의 평가와 모순되며, 텍스트-이미지 모델의 점진적 개선을 반영하지 못하고, 왜곡 수준을 포착하지 못하며, 샘플 크기가 달라질 때 일관성 없는 결과를 낸다는 것을 실증적으로 보여줍니다.
- 대안으로서 풍부한 CLIP 임베딩과 Gaussian RBF 커널을 사용하는 최대 평균 차이 거리(Maximum Mean Discrepancy Distance, MMD)에 기반한 새로운 메트릭 CMMD를 제안합니다.
- CMMD는 임베딩의 확률 분포에 대한 가정 없이 편향되지 않은 추정치를 제공하고 표본 효율성이 높습니다.
- 광범위한 실험과 분석을 통해 텍스트-이미지 모델의 FID 기반 평가가 신뢰할 수 없을 수 있으며, CMMD가 이미지 품질의 보다 강인하고 신뢰할 수 있는 평가를 제공한다는 것을 입증합니다.

