## Daily Papers (2024-01-01)

### [FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis](https://arxiv.org/abs/2312.17681)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Yt9oyILX23Wi_j4hHVTYC.png)

Vote: 10

Authors: Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda, Diana Marculescu

- 본 논문은 이미지 간 합성(Image-to-Image, I2I) 분야에 혁신을 가져온 확산 모델을 비디오에 적용하여 시간적 일관성 문제를 해결하는 새로운 비디오 간 합성(Video-to-Video, V2V) 프레임워크를 제안합니다.
- 기존 방법과 달리, 본 연구는 광학 흐름(optical flow)의 불완전성을 다룰 수 있으면서도 그 이점을 활용하여 소스 비디오 내에서 공간 조건과 시간적 정보를 함께 이용합니다.
- 연구진은 첫 번째 프레임에서 와핑을 통해 광학 흐름을 인코딩하고, 확산 모델에서 보조 참조로 활용함으로써 첫 프레임을 수정한 후 연속하는 프레임에 수정 사항을 전파하여 비디오 합성을 가능하게 합니다.
- FlowVid 모델은 기존 I2I 모델과 원활하게 작동하여 스타일 변화, 객체 교체, 지역 편집 등 다양한 수정을 용이하게 합니다.
- 30 FPS의 512x512 해상도를 가진 4초 분량 비디오 생성에 단 1.5분이 소요되어 CoDeF, Rerender, TokenFlow에 비해 각각 3.1배, 7.2배, 10.5배 빠른 효율성을 보입니다.
- 사용자 연구에서 FlowVid는 CoDeF(3.5%), Rerender(10.2%), TokenFlow(40.4%)를 능가하는 45.7%의 선호도로 뛰어난 품질의 비디오 합성 결과를 보여줍니다.

### [LARP: Language-Agent Role Play for Open-World Games](https://arxiv.org/abs/2312.17653)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/aoH7aWnExG9TuOby-2y5W.png)

Vote: 9

Authors: Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, Ji Yan

- 정의된 환경과 짧은 시간대에서 언어 에이전트들이 인상적인 문제 해결 능력을 보여주었지만, 계속 발전하는 오픈-월드 시뮬레이션의 복잡성을 고려할 때, 복잡한 환경에 유연하게 적응하고 일관된 장기 기억을 유지하여 일관된 행동을 보장할 수 있는 에이전트의 필요성이 증가하고 있다.
- 오픈-월드 게임과 언어 에이전트 사이의 간극을 메우기 위해, 기억 처리와 의사 결정 보조를 포함하는 인지구조, 피드백 기반 학습 가능한 행동 공간을 가진 환경 상호작용 모듈, 그리고 다양한 개성의 조화를 촉진하는 후처리 방법을 포함하는 Language Agent for Role-Playing (LARP)를 소개한다.
- LARP 프레임워크는 사용자와 고유한 배경과 개성을 가진 에이전트 간의 상호작용을 개선하여 오픈-월드 상황에서 게임 경험을 향상시킨다.
- 또한, LARP는 엔터테인먼트, 교육 및 다양한 시뮬레이션 시나리오 등 다양한 분야에서 언어 모델의 사용을 강조한다.
- 해당 프로젝트 페이지는 https://miao-ai-lab.github.io/LARP/ 에서 확인할 수 있다.

### [PanGu-$π$: Enhancing Language Model Architectures via Nonlinearity Compensation](https://arxiv.org/abs/2312.17276)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-IsluqfKHhsJ4EzNKTIQR.png)

Vote: 6

Authors: Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao Wang, Hailin Hu, Zheyuan Bai, Yun Wang, Fangcheng Liu, Zhicheng Liu, Jianyuan Guo, Sinan Zeng, Yinchen Zhang, Qinghua Xu, Qun Liu, Jun Yao, Chao Xu, Dacheng Tao

- 최근 대규모 언어 모델(LLMs)의 추세는 더 나은 생성 능력을 달성하기 위해 모델 크기(즉, 매개변수의 수)와 데이터 세트의 규모를 증가시키는 것인데, 이는 GPT 및 Llama와 같은 많은 작업에서 입증되었습니다.
- 그러나 큰 모델은 종종 방대한 계산 비용을 수반하며, 실제 응용 분야에서는 이러한 높은 비용을 감당하기 어렵습니다.
- 연구자들은 최신 언어 모델 아키텍처를 분석하고 특징 충돌 문제를 관찰한 후 언어 모델에 대해서도 비선형성이 중요하다고 주장합니다.
- 이론적 분석을 바탕으로, 비전과제에 주로 연구된 컨볼루션 신경망에서 영감을 받은 정보가 풍부한 활성화 함수를 소개하고, 모델 비선형성을 향상시키기 위해 확장된 단축 경로를 추가 사용합니다.
- 이러한 접근 방식이 모델 비선형성을 향상시키는 데 상당히 효과적임을 신중하게 설계된 소거법을 통해 입증하였으며, 이를 통해 현대적이고 효율적인 새로운 모델 아키텍처인 PanGu-$π$를 제시합니다.
- 같은 데이터 세트와 훈련 전략을 사용하여 PanGu-$π$를 최신 LLM들과 비교한 실험을 수행하였고, 결과적으로 PanGu-$π$-7B는 약 10%의 추론 속도 향상으로 벤치마크와 비슷한 성능을 달성할 수 있었습니다.
- 또한, PanGu-$π$-1B는 정확도와 효율성 측면에서 최첨단 성능을 달성할 수 있었습니다.
- 추가적으로, PanGu-$π$-7B를 금융 및 법률과 같은 고부가가치 분야에 배치하여 실제 어플리케이션으로 사용될 수 있는 언어 모델인 YunShan을 개발하였고, 이 모델은 유사한 규모의 다른 모델들을 벤치마크에서 능가하는 성능을 보여주었습니다.

### [Learning Vision from Models Rivals Learning Vision from Data](https://arxiv.org/abs/2312.17742)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/qCU4_xj-0LtM8Y-2_M2Z7.png)

Vote: 5

Authors: Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, Phillip Isola

- 본 논문에서는 SynCLR이라는 새로운 접근법을 소개하며, 이 방법은 어떤 실제 데이터 없이도 합성된 이미지와 합성된 캡션만을 사용해 시각적 표현을 학습합니다.
- 이미지 설명을 생성하기 위해 대규모의 데이터셋을 인공지능 언어모델을 이용해 합성하고, 각 합성된 캡션에 해당하는 다수의 이미지를 텍스트-이미지 모델을 사용해 생성합니다.
- 같은 캡션을 공유하는 이미지들을 양성 쌍으로 취급하는 대조 학습을 통해 이러한 합성 이미지들로부터 시각적 표현을 학습합니다.
- 생성된 시각적 표현은 많은 다운스트림 작업으로 전이되며, 이미지 분류 작업에서 CLIP, DINO v2와 같은 다른 일반 목적의 시각 표현 학습 기법들과 비교해 유리하게 경쟁합니다.
- 특히, 의미론적 세분화 같은 밀집 예측 작업에서 SynCLR은 이전 자기감독 학습 방법들을 큰 폭으로 능가하며 예를 들어, ADE20k에서 ViT-B/16에 대해 MAE 및 iBOT 보다 6.2와 4.3 mIoU 향상을 이루었습니다.

### [Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models](https://arxiv.org/abs/2312.17661)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Epzw8VFxfmNIblolMUJom.png)

Vote: 3

Authors: Yuqing Wang, Yun Zhao

- 다중모드 대규모 언어모델(MLLM)에 대한 관심이 급증하고 있으며, 이는 OpenAI의 GPT-4V(ision)와 같은 모델들이 학계 및 산업계에 큰 영향을 미치고 있다.
- 최근 구글은 다중모드 통합에 특화된 첨단 MLLM인 Gemini를 소개했으나, 상식 추론 과제에서 GPT 모델에 뒤쳐지는 것으로 초기 벤치마크가 가리키고 있다.
- 이러한 평가는 한정된 데이터셋(HellaSWAG)을 사용하여 Gemini의 진정한 상식 추론 능력을 전부 파악하지 못했을 가능성이 있다.
- 본 연구는 언어만을 대상으로 하는 11개 데이터셋과 다중모드 요소를 포함하는 1개 데이터셋을 포함한 12개의 상식 추론 데이터셋을 통한 Gemini의 성능을 철저히 평가했다.
- 네 개의 LLM과 두 개의 MLLM에 걸친 실험을 통해 Gemini의 경쟁력 있는 상식 추론 능력을 입증하였다.
- 또한, 현재 LLM과 MLLM이 상식 문제를 해결함에 있어 직면하는 공통적인 도전과제들을 식별하고, 이러한 모델들의 상식 추론 능력을 증진시키기 위한 추가적인 발전이 필요함을 강조하였다.

