## Daily Papers (2024-01-02)

### [Improving Text Embeddings with Large Language Models](https://arxiv.org/abs/2401.00368)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/2-XVqmOkOi8qp3JX74X3g.png)

Vote: 20

Authors: Liang Wang, Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei

- 이 논문에서는 1k 미만의 학습 단계만을 사용하여 합성 데이터만으로 고품질 텍스트 임베딩을 얻는 새롭고 간단한 방법을 소개합니다.
- 기존 방법들은 종종 수십억 개의 약한 지도를 받은 텍스트 쌍에 대한 다단계 중간 사전 학습, 그리고 소수의 레이블이 있는 데이터셋으로의 미세 조정에 의존하지만, 제안된 방법은 복잡한 학습 파이프라인 구축이나 수작업으로 수집된 데이터셋에 의존하지 않습니다.
- 연구자들은 거의 100개 언어에 대하여 수십만 개의 텍스트 임베딩 작업을 위한 다양한 합성 데이터 생성을 위해 독자적인 대규모 언어 모델(Large Language Models, LLMs) 활용했습니다.
- 이후에, 연구진은 표준 대조 손실을 사용하여 합성 데이터로 오픈소스 디코더-전용 LLM들에 미세 조정을 수행했습니다.
- 실험 결과, 우리의 방법은 레이블이 표시되지 않은 데이터 없이도 경쟁력 있는 텍스트 임베딩 벤치마크에서 강력한 성능을 달성한다는 것을 보여줍니다.
- 또한, 합성 데이터와 레이블이 있는 데이터의 혼합으로 미세 조정을 할 때, 우리의 모델은 BEIR 및 MTEB 벤치마크에서 새로운 최고 성능을 달성하였습니다.

### [Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models](https://arxiv.org/abs/2401.00788)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/PuoMQK6jqYgZR3-0S-OXh.png)

Vote: 12

Authors: Terry Yue Zhuo, Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, Qian Liu, Niklas Muennighoff

- 큰 언어 모델의 전체 파라메터 미세 조정(FFT) 비용이 높기 때문에, 여러 가지 파라메터 효율적 미세 조정(PEFT) 방법들이 등장하였습니다.
- 본 논문에서는 다양한 모델 스케일에서 최상의 비용-성능 균형을 제공하는 방법이 무엇인지 명확하지 않다는 문제점을 다루었습니다.
- 저자들은 'Astraios'를 소개했으며, 이는 7가지 튜닝 방법과 4가지 모델 크기에 걸쳐 28개의 지시문-튜닝 OctoCoder 모델을 포함합니다.
- 코드 이해 및 생성 작업을 포함한 5가지 작업과 8가지 다른 데이터 세트에서 알아본 결과, FFT가 모든 규모에서 가장 좋은 하류 성능을 나타내었습니다.
- 모델 스케일에 따라 PEFT 방법의 효과는 크게 달라졌으며, LoRA가 일반적으로 비용 대비 성능면에서 가장 유리한 균형을 제시했습니다.
- 또한, 이러한 방법이 모델의 견고성과 코드 보안에 미치는 영향에 대해 조사했고, 큰 모델이 더 낮은 견고성과 보안성을 보였음을 발견했습니다.
- 마지막으로, 업데이트된 파라미터, 교차 엔트로피 손실, 및 작업 성능 간의 관계를 탐구했으며, 지시문 튜닝에서의 검증 손실이 전체 하류 성능의 신뢰할 수 있는 지표가 될 수 있음을 발견했습니다.

### [Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws](https://arxiv.org/abs/2401.00448)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/lMvgQ8nsAViGR-vMXEUzq.png)

Vote: 12

Authors: Nikhil Sardana, Jonathan Frankle, Jonathan Frankle

- 대형 언어 모델(LLM)의 스케일링 법칙은 모델 품질이 매개변수 수와 훈련 데이터 크기 증가에 따라 어떻게 변하는지를 추정하는 경험적 공식입니다.
- 인기있는 DeepMind의 Chinchilla 스케일링 법칙을 포함하여 기존의 공식들은 추론 비용을 고려하지 않고 있습니다.
- 저자들은 주어진 품질과 추론 요구 사항을 가진 모델을 훈련하고 배포하기 위해 최적의 LLM 매개변수 수와 사전 훈련 데이터 크기를 계산하기 위해 Chinchilla 스케일링 법칙을 수정했습니다.
- 이 분석은 컴퓨팅 예산뿐만 아니라 실제 비용 측면에서도 이루어졌습니다.
- 연구원들은 상당한 추론 요구 사항(~10억 요청)을 기대하는 경우, Chinchilla-최적보다 더 작고 오랜 기간 훈련시킬 모델을 훈련해야 한다는 결과를 발견했습니다.

### [COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training](https://arxiv.org/abs/2401.00849)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/oxN6cPCC0peOut77IN8Hi.png)

Vote: 7

Authors: Alex Jinpeng Wang, Linjie Li, Kevin Qinghong Lin, Jianfeng Wang, Kevin Lin, Zhengyuan Yang, Lijuan Wang, Mike Zheng Shou

- 시각-언어 사전학습(Vision-Language Pre-training)의 진화에서 긴 텍스트 이해를 포함하는 것이 중요하며, 최근의 고성능 언어모델을 이용한 자동회귀 시각-언어 모델들이 짧은 텍스트 생성 작업에는 성공하였지만, 정렬 작업에서는 도전에 직면해있다.
- 이러한 과제를 해결하기 위해 본 논문에서는 대비 손실(contrastive loss)을 텍스트 생성 모델에 도입하여 맞춤형 단일 모드 텍스트 처리와 다중 모드 데이터 처리 부문으로 언어 모델을 전략적으로 분할하는 COSMO(Contrastive-Streamlined Multimodal) 프레임워크를 제안한다.
- COSMO는 단일 모드와 다중 모드 요소를 결합하여 시각적 및 문헌 데이터를 포함하는 작업에서 모델 성능을 향상시키면서 학습 가능한 매개변수를 상당히 줄인다.
- 고품질의 긴 텍스트 비디오 데이터셋 부족 문제를 해결하기 위해, 본 연구에서는 종합적인 캡션을 특징으로 하는 처음으로 서술된 비디오-텍스트 데이터셋을 소개한다.
- 이 데이터셋을 통해 이미지-텍스트 작업의 모델 성능을 향상시키는 방법을 보여준다.
- COSMO는 학습 가능한 매개변수를 34% 줄이고 사용 가능한 데이터의 72%를 활용하면서도 OpenFlamingo 모델에 비해 현저한 우수성을 보인다.
- 예를 들어, 4-shot flickr 캡셔닝 작업에서 성능이 57.2%에서 65.1%로 크게 향상되었다.
- 아네 모델과 데이터셋은 이미지-텍스트 및 비디오-텍스트 작업을 포함한 14가지 다양한 다운스트림 데이터셋에서 뛰어난 성능 향상을 통해 그 중요성을 입증한다.

### [GeoGalactica: A Scientific Large Language Model in Geoscience](https://arxiv.org/abs/2401.00434)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/9sAlkPt701RLqXsroyBju.png)

Vote: 5

Authors: Zhouhan Lin, Cheng Deng, Le Zhou, Tianhang Zhang, Yi Xu, Yutong Xu, Zhongmou He, Yuanyuan Shi, Beiya Dai, Yunchong Song, Boyi Zeng, Qiyuan Chen, Tao Shi, Tianyu Huang, Yiwei Xu, Shu Wang, Luoyi Fu, Weinan Zhang, Junxian He, Chao Ma, Yunqiang Zhu, Xinbing Wang, +

- 대규모 언어 모델(LLM)은 자연어 처리(NLP)에서 다양한 작업을 해결할 수 있는 일반적 지식을 통해 큰 성공을 거두었습니다.
- 이러한 LLM의 능력은 인공지능으로 특정 분야의 과학 발견을 촉진하는 잠재적인 학제 간 응용 프로그램에 빛을 비추고 있습니다.
- 지질과학 연구와 실무에서 NLP 기술을 활용하는 일은 지식 추출, 문서 분류, 질문 응답 및 지식 발견에 이르기까지 광범위하고 복잡합니다.
- 본 연구에서는 지질과학 텍스트 대량을 통한 모델의 추가 사전 훈련 및 사용자 지정 수집 지시 튜닝 데이터 세트로의 감독된 미세 조정을 통해 LLM을 지질과학 분야에 특화시키려는 초기 단계를 밟습니다.
- 이러한 노력을 통해 약 30억 개의 파라미터를 갖는 GeoGalactica 모델이 탄생했습니다.
- GeoGalactica는 지질과학 관련 텍스트 코퍼스를 통해 추가 사전 훈련된 Galactica에서 나온 것으로, 650억 토큰을 포함하는 지질과학 특정 텍스트 코퍼스로 구성되어 있으며 이는 현재까지 가장 큰 지질과학 텍스트 코퍼스입니다.
- 그 후 전문 지질과학 지식이 필요한 질문으로 구성된 100만 쌍의 지시-튜닝 데이터로 모델을 미세 조정했습니다.
- 본 기술 보고서에서는 GeoGalactica의 모든 측면, 즉 데이터 수집, 데이터 정제, 기본 모델 선택, 사전 훈련, SFT 및 평가에 대해 자세히 설명할 것입니다.
- 우리는 사전 훈련의 처음 3/4 동안의 GeoGalactica 체크포인트와 데이터 큐레이션 도구를 오픈소스로 제공합니다.

### [Boosting Large Language Model for Speech Synthesis: An Empirical Study](https://arxiv.org/abs/2401.00246)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SG4npWu-QA28qSUDo6ULQ.png)

Vote: 4

Authors: Hongkun Hao, Long Zhou, Shujie Liu, Jinyu Li, Shujie Hu, Rui Wang, Furu Wei

- 대규모 언어 모델(Large language models, LLMs)은 자연어 처리 분야에서 중요한 진보를 이루었으며, 음성 및 시각과 같은 다른 모달리티에 대한 언어 능력을 확장하고 있다.
- 그러나 이전 연구들은 대부분 LLMs의 청각 이해 같은 인식 능력에 중점을 두었고, LLMs에 음성 합성 능력을 향상시키는 효과적인 방법은 아직 명확하지 않다.
- 본 논문에서는 사전 훈련된 LLM인 LLaMA/OPT와 텍스트-음성 변환 모델인 VALL-E를 결합하여, 음성을 생성하는 능력을 갖춘 LLMs를 향상시키기 위한 종합적인 실증적 탐구를 수행한다.
- LLMs와 음성 합성 모델 간의 세 가지 통합 방법을 비교했으며, 이에는 직접 세부 조정된 LLMs, LLMs와 VALL-E의 겹친 레이어, 강력한 텍스트 인코더로 사용된 LLMs와 결합된 VALL-E가 포함된다.
- 실험 결과 LoRA 방법을 사용하여 LLMs를 직접 미세 조정하는 것은 음성 합성 능력을 향상시키지 못하는 것으로 나타났으며, 겹친 LLMs와 VALL-E는 발화자 유사성과 단어 오류율(Word Error Rate, WER) 모두에서 생성된 음성의 품질을 향상시킬 수 있다.
- 이 세 가지 방법 중, LLMs를 텍스트 인코더로 활용한 결합 방법이 가장 좋은 성능을 달성하며, 일관되게 더 나은 발화자 유사성과 뚜렷한 WER 감소(10.9%)를 통해 기존의 음성 합성 모델을 능가하는 결과를 제시한다.

### [Unicron: Economizing Self-Healing LLM Training at Scale](https://arxiv.org/abs/2401.00134)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/YQMlJIsQMTgBygoREOBTw.png)

Vote: 4

Authors: Tao He, Xue Li, Zhibin Wang, Kun Qian, Jingbo Xu, Wenyuan Yu, Jingren Zhou

- 대규모 언어 모델의 트레이닝은 다양한 도메인에서 점점 중요해지고 있으나, 자주 발생하는 실패로 인해 상당한 시간과 경제적 비용이 소요되고 있다.
- 기존 클라우드 기반의 실패 복구 방법들은 다양하고 복잡한 시나리오에 대응하지 못하고, 개별 작업의 다운타임만을 간과하는 데 집중함으로써 클러스터 전체의 비용 영향을 고려하지 않는다.
- 'Unicron'은 대규모 언어 모델 트레이닝의 효율적인 자가 치유를 위해 설계된 워크로드 관리자로, 클러스터 내 여러 동시 작업의 실패 관련 비용을 최소화하도록 최적화한다.
- Unicron의 주요 기능은 추가 비용 없이 실시간으로 오류를 식별할 수 있는 인밴드 오류 감지, 최적 재구성을 위한 동적 비용 인식 계획 생성 메커니즘, 상태 변경 중 다운타임을 줄이는 효율적인 전환 전략을 포함한다.
- 128-GPU 분산 클러스터에 배포된 Unicron은 최신 방법론에 비해 훈련 효율성을 최대 1.9배 향상시켜, 대규모 언어 모델 트레이닝의 실패 복구 비용을 크게 줄이고 신뢰성을 강화한다.

### [SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity](https://arxiv.org/abs/2401.00604)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/93yL_RLv87F_ymXLSNHrP.png)

Vote: 1

Authors: Peihao Wang, Zhiwen Fan, Dejia Xu, Dilin Wang, Sreyas Mohan, Forrest Iandola, Rakesh Ranjan, Yilei Li, Qiang Liu, Zhangyang Wang, Vikas Chandra

- 텍스트-3D 자산 합성을 위한 점수 증류법이 유망한 방법으로 부상하였으나 고변동성으로 인한 경사 추정 문제가 드러났습니다.
- 점수 증류의 효과를 확대시키기 위해 다양한 제어 변량을 몬테카를로 추정기에 적용하는 SDS와 VSD의 유용성이 해석됩니다.
- 이러한 재고찰과 스타인 항등식(Stein's identity)에 기반하여, 저희는 점수 증류를 위한 보다 일반적인 분산 감소 솔루션인 스타인 점수 증류(Stein Score Distillation, SSD)를 제안합니다.
- SSD는 임의의 기준 함수를 허용하는 스타인 항등식에 의해 구축된 제어 변량을 통합함으로써, 분산 감소를 명시적으로 최적화할 수 있는 유연한 안내 선도 및 네트워크 구조를 포함할 수 있습니다.
- 실험에서, SteinDreamer라고 명명된 전체 파이프라인은 단안 깊이 추정기를 사용하여 제어 변량을 구현하였습니다.
- SSD는 증류 분산을 효과적으로 감소시키고, 객체 및 장면 수준 생성 모두에 대해 시각적 품질을 일관되게 향상시킬 수 있는 것으로 나타났습니다.
- 더 안정적인 경사 업데이트로 인해 SteinDreamer는 기존 방법보다 빠르게 수렴하는 성과를 보여줍니다.

