## Daily Papers (2024-01-04)

### [From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations](https://arxiv.org/abs/2401.01885)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/orTXtyJk4x4s9Elb8xz-6.png)

Vote: 14

Authors: Evonne Ng, Javier Romero, Timur Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, Alexander Richard

- 이 연구는 대화 상호작용의 동적인 제스처에 따라 움직이는 사진처럼 생생한 아바타를 생성하는 프레임워크를 제시합니다.
- 주어진 음성 오디오로부터 얼굴, 몸, 손을 포함해 개인별로 제스처 모션의 다양한 가능성을 출력할 수 있습니다.
- 벡터 양자화의 샘플 다양성과 디퓨전을 통한 고주파 세부 정보 활용의 장점을 결합하여, 더 역동적이고 표현력 있는 모션을 생성하는 것이 우리 방법의 핵심입니다.
- 우리는 실제와 같은 싱크대 속의 아바타를 사용하여, 제스처의 중요한 미묘함(예: 쓴웃음과 미소)을 표현할 수 있는 모션을 시각화합니다.
- 본 연구를 촉진하기 위해, 사진처럼 생생한 재구성을 허용하는 최초의 다시점 대화 데이터셋을 소개합니다.
- 실험 결과, 우리의 모델은 적절하고 다양한 제스처를 생성하며, 디퓨전만 사용한 방법이나 VQ만 사용한 방법보다 우수함을 보여줍니다.
- 또한, 대화 제스처에서 미묘한 모션 세부 사항을 정확하게 평가하는데 있어 사실주의(메시 대비)의 중요성을 강조하는 인지 평가가 이루어졌습니다.
- 관련 코드와 데이터셋은 온라인에서 사용할 수 있습니다.

### [aMUSEd: An Open MUSE Reproduction](https://arxiv.org/abs/2401.01808)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/AMTbamtulOuaLWYAvkj9J.png)

Vote: 13

Authors: Suraj Patil, Suraj Patil, William Berman, Robin Rombach, Patrick von Platen, Patrick von Platen

- 본 논문에서는 MUSE를 바탕으로 한 오픈 소스 경량 마스크 이미지 모델(MIM)인 aMUSEd를 소개하며, 이는 MUSE의 매개변수의 10퍼센트만을 이용하여 빠른 이미지 생성에 중점을 둡니다. 
- MIM이 텍스트-이미지 생성의 주류 방법인 잠재적 확산(latent diffusion)에 비해 탐험되지 않았다고 보고 이는 추론 단계가 적고 더 해석 가능하다는 장점이 있습니다.
- 추가적으로, MIM은 단일 이미지만을 사용하여 추가 스타일을 학습하는 데 미세 조정될 수 있습니다.
- 대규모 텍스트-이미지 생성 작업에서 MIM의 효과성을 입증하고 재현 가능한 훈련 코드를 공개함으로써 MIM에 대한 추가적인 탐구를 장려하기를 희망합니다.
- 저자들은 또한 직접적으로 256x256 및 512x512 해상도에서 이미지를 생성하는 두 모델의 체크포인트를 공개합니다.

### [GPT-4V(ision) is a Generalist Web Agent, if Grounded](https://arxiv.org/abs/2401.01614)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/e3P73-qOL1Zb3PYiNhpDs.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/e3P73-qOL1Zb3PYiNhpDs.mp4" muted="false"></video></div>

Vote: 11

Authors: Boyuan Zheng, Boyu Gou, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su, Yu Su

- 최근 대규모 다중모달 모델(LMM)의 개발을 통해, 특히 GPT-4V(ision) 및 Gemini와 같은 모델들이 이미지 캡셔닝이나 시각적 질문 답변과 같은 전통적 작업을 넘어서 다양한 기능으로 발전되고 있습니다.
- 본 연구에서는 GPT-4V와 같은 LMM이 자연어 명령을 따라 주어진 웹 사이트에서 작업을 완수할 수 있는 범용 웹 에이전트의 가능성을 탐구합니다.
- 우리는 LMM의 시각적 이해 및 웹상의 행동 통합 기능을 활용하는 범용 웹 에이전트인 SEEACT를 제안합니다.
- 최근 MIND2WEB 벤치마크에서 평가를 진행하고, 캐시된 웹사이트에 대한 표준 오프라인 평가 외에도, 실제 웹사이트에서 웹 에이전트를 실행할 수 있는 도구를 개발하여 새로운 온라인 평가 설정을 가능하게 합니다.
- GPT-4V는 웹 사이트에서 작업을 수행할 수 있는 큰 잠재력을 보여줍니다 - 웹 사이트에서 텍스트 계획을 수동으로 행동으로 옮기면 50%의 작업을 성공적으로 완료할 수 있습니다.
- 이는 GPT-4 같은 텍스트 전용 LLM이나 웹 에이전트에 특화되어 미세 조정된 작은 모델들(FLAN-T5 및 BLIP-2)보다 월등한 성능을 나타냅니다.
- 그러나, 기반을 마련하는 일은 여전히 큰 도전입니다. 웹 에이전트에 대해 효과적이지 않은 것으로 나타난 기존 LMM 기반 전략을 참고하여, 본 논문에서 개발한 가장 효과적인 전략은 HTML 텍스트와 시각을 모두 활용합니다.
- 그럼에도 불구하고 오라클 기반 방식과 여전히 큰 격차가 있어, 앞으로 더 많은 개선이 필요함을 나타냅니다.

### [Image Sculpting: Precise Object Editing with 3D Geometry Control](https://arxiv.org/abs/2401.01702)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/b3tvdZ4fR3ZwKIvW3D0fp.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/b3tvdZ4fR3ZwKIvW3D0fp.mp4" muted="false"></video></div>

Vote: 10

Authors: Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, Saining Xie

- 이 연구는 3D 기하학 및 그래픽 도구를 사용하여 2D 이미지를 편집하는 새로운 프레임워크인 Image Sculpting을 소개합니다.
- 기존의 2차원 공간에 제한되고 텍스트 지시에 의존하는 방법과는 차별화되며, 모호성을 줄이고 제어력을 강화합니다.
- Image Sculpting은 2D 객체를 3D로 변환하여 3D 기하학적 구조와의 직접적 상호작용을 가능하게 합니다.
- 편집된 객체는 다시 2D로 렌더링되어 원본 이미지와 합쳐지며, 정밀한 단계별 향상 과정을 통해 고품질의 결과물을 생성합니다.
- 자세 조정, 회전, 이동, 3D 구성, 조각, 연속 추가 등 정밀하고 측정 가능하며 물리적으로 타당한 편집 옵션을 지원합니다.
- 이 프레임워크는 생성 모델의 창의적 자유와 그래픽 파이프라인의 정밀성을 결합하는 방향으로의 초기 단계를 나타냅니다.

### [SIGNeRF: Scene Integrated Generation for Neural Radiance Fields](https://arxiv.org/abs/2401.01647)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/IyHDI5dN9nyZ9BNuMBH3N.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/IyHDI5dN9nyZ9BNuMBH3N.mp4" muted="false"></video></div>

Vote: 7

Authors: Jan-Niklas Dihlmann, Jan-Niklas Dihlmann, Andreas Engelhardt, Hendrik Lensch

- 최신 이미지 확산 모델 발전은 고품질 이미지 생성에서 주목할 만한 향상을 이끌었으며, 이를 Neural Radiance Fields(NeRFs)와 결합하여 3D 생성에 새로운 기회를 제공했습니다.
- 대부분의 생성적 3D 접근법은 객체 중심이며, 실제와 같은 장면을 편집하는 데 직관적이지 않습니다.
- SIGNeRF라는 새로운 방법을 제안하여 NeRF 장면 편집과 장면 통합 객체 생성을 빠르고 조절 가능하게 합니다.
- 새로운 생성 업데이트 전략은 이미지 간의 3D 일관성을 확보하면서 반복 최적화를 필요로 하지 않습니다.
- 깊이 조건부 확산 모델은 단일 뷰 대신 이미지 그리드를 요청함으로써 3D 일관된 뷰를 생성할 수 있는 능력이 내재되어 있음을 발견했습니다.
- 이러한 통찰을 바탕으로 수정된 이미지의 다중 뷰 참조 시트를 소개합니다.
- 우리의 방법은 참조 시트에 기반하여 이미지 컬렉션을 일관되게 업데이트하고 새로 생성된 이미지 세트로 원래의 NeRF를 한 번에 정제합니다.
- 이미지 확산 모델의 깊이 조절 메커니즘을 활용하여 편집의 공간 위치를 정밀하게 제어하고, 선택된 영역이나 외부 메시에 의한 형태 가이드를 적용할 수 있습니다.

### [Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions](https://arxiv.org/abs/2401.01827)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ovYO8mstj9VO9hD315y2z.png)

Vote: 7

Authors: David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, Doyen Sahoo

- 본 연구에서는 이미지와 텍스트라는 다양한 모달 입력에 동시에 조건을 부여한 새로운 비디오 생성 모델인 Moonshot를 제시하였습니다.
- Moonshot는 비디오 특성을 표현하기 위한 전통적인 공간시간 계층과 외모 조절을 위한 이미지와 텍스트 입력을 처리하는 분리된 교차 주의 계층이 포함된 멀티모달 비디오 블록(MVB)이라는 핵심 모듈을 기반으로 구축되었습니다.
- 이 모델은 기존 방법들과 달리 추가적인 훈련 부하 없이도 기하학적 시각 조건을 위한 사전 훈련된 이미지 ControlNet 모듈과 선택적으로 통합될 수 있도록 설계되었습니다.
- 실험 결과, Moonshot는 다양한 멀티모달 조건화 메커니즘을 통해 기존 모델들에 비해 시각적 품질과 시간적 일관성이 현저히 향상되었음을 보여주었습니다.
- 또한, 이 모델은 개인화된 비디오 생성, 이미지 애니메이션, 비디오 편집과 같은 다양한 생성 애플리케이션에 쉽게 재목적화될 수 있으며, 제어 가능한 비디오 생성을 위한 기본 아키텍처로서의 잠재력을 드러냈습니다.
- 제공되는 모델과 관련 정보는 https://github.com/salesforce/LAVIS 에서 공개될 예정입니다.

### [Incremental FastPitch: Chunk-based High Quality Text to Speech](https://arxiv.org/abs/2401.01755)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ngf8eBahxAD0Bo4HDd5u6.png)

Vote: 5

Authors: Muyang Du, Chuan Liu, Junjie Lai

- 실시간 음성 합성용 병렬 텍스트-투-스피치 모델은 통제 가능하고 합성 속도가 빠르지만, 트랜스포머와 같은 완전히 병렬 구조로 인해 증분 합성에는 부적합하다.
- 이 연구에서는 FFT 블록을 기반으로 하는 새로운 FastPitch 변형 모델인 Incremental FastPitch를 제안하여, 고품질 멜(Mel) 청크를 점진적으로 생성할 수 있게 한다.
- Incremental FastPitch는 청크 기반의 훈련에 제한된 수신 필드를 가진 청크 주의 마스크와 고정된 크기의 과거 모델 상태를 사용하여 추론을 개선함으로써, 실시간 응용 프로그램에 대한 응답 시간을 크게 단축시킨다.
- 실험 결과는 제안된 Incremental FastPitch가 병렬 FastPitch와 비슷한 음질을 제공하면서도 훨씬 낮은 지연 시간을 달성, 실시간 음성 응용 프로그램에 더 낮은 응답 시간을 제공한다는 것을 보여준다.

### [Efficient Hybrid Zoom using Camera Fusion on Mobile Phones](https://arxiv.org/abs/2401.01461)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/z3W6T6L3URAXwVoXLuHHH.png)

Vote: 4

Authors: Xiaotong Wu, Wei-Sheng Lai, YiChang Shih, Charles Herrmann, Michael Krainin, Deqing Sun, Chia-Kai Liang

- DSLR 카메라는 렌즈 간격을 조정하거나 렌즈 유형을 교체하여 다양한 줌 수준을 달성할 수 있지만, 스마트폰은 공간 제약으로 인해 이러한 기술을 적용할 수 없습니다.
- 대부분의 스마트폰 제조업체는 하이브리드 줌 시스템을 채택하여, 저 줌 수준에서는 'Wide(W)' 카메라를, 고 줌 수준에서는 'Telephoto(T)' 카메라를 사용합니다.
- W와 T 사이의 줌 수준을 시뮬레이션하기 위해 W에서 이미지를 자르고 디지털로 업샘플링하는데, 이 과정에서 세부 정보가 크게 손실됩니다.
- 본 논문에서는 W와 T 이미지를 동시에 촬영하고 기계 학습 모델을 이용하여 두 이미지를 정렬하고 T에서 W로 세부 정보를 전송하는 효율적인 모바일 기기용 하이브리드 줌 슈퍼해상도 시스템을 제안합니다.
- 피사계 심도 불일치, 장면 가리개, 흐름 불확실성, 정렬 오류를 고려하는 적응형 블렌딩 방법을 추가로 개발했습니다.
- 도메인 간 격차를 최소화하기 위해 실제 입력과 감독 학습을 위한 지상 진실 학습 데이터를 캡쳐하는 이중 폰 카메라 장치를 설계했습니다.
- 우리의 방법은 모바일 플랫폼에서 12-메가픽셀 이미지를 500ms 내에 생성하며, 실제 상황에서 광범위한 평가하에 최신 방법과 비교할 때 유리합니다.

### [CoMoSVC: Consistency Model-based Singing Voice Conversion](https://arxiv.org/abs/2401.01792)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/KXUi9H_kyqD3796PW0yjg.png)

Vote: 4

Authors: Yiwen Lu, Zhen Ye, Wei Xue, Xu Tan, Qifeng Liu, Yike Guo

- 확산 기반 가창 음성 변환(Singing Voice Conversion, SVC) 방법은 타깃 음색과 높은 유사성을 가진 자연스러운 오디오 생성에서 뛰어난 성능을 달성하였지만, 반복적인 샘플링 과정으로 인해 추론 속도가 느려져 가속이 중요해졌습니다.
- 본 논문에서는 CoMoSVC라는 일관성 모델 기반의 SVC 방법을 제안하여 고품질 생성과 고속 샘플링을 동시에 달성하고자 합니다.
- 특별히 SVC를 위해 설계된 확산 기반 교사 모델이 먼저 제안되며, 자기 일관성 특성에 따라 학생 모델을 가르쳐 한 단계 샘플링을 실현합니다.
- 단일 NVIDIA GTX4090 GPU에서 진행된 실험 결과는 CoMoSVC가 기존의 가장 성능이 좋은 확산 기반 SVC 시스템보다 추론 속도가 현저히 빠름에도 불구하고 주관적 및 객관적 지표를 기반으로 비슷하거나 우수한 변환 성능을 달성했음을 보여줍니다.
- 오디오 샘플과 코드는 https://comosvc.github.io/에서 제공됩니다.

### [A Vision Check-up for Language Models](https://arxiv.org/abs/2401.01862)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/7ls49JA20azGxbksMr-ND.png)

Vote: 4

Authors: Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, Antonio Torralba

- 대규모 언어 모델(Large Language Models, LLMs)이 문자 사이의 관계 모델링 학습을 통해 시각 세계에 대해 어떤 지식을 얻는지에 대해 체계적으로 평가하였습니다.
- 연구에서는 다양한 복잡성을 가진 시각 개념을 생성하고 인식하는 LLMs의 능력을 검토하고, 텍스트 모델을 이용해 시각 표현 학습 시스템을 훈련시키는 방법을 보여주었습니다.
- 언어 모델이 픽셀로 시각 정보를 소비하거나 출력할 수 없기 때문에, 이미지를 표현하기 위해 코드를 사용하는 연구 방법론을 적용하였습니다.
- LLM이 생성한 이미지는 자연 이미지와 다르게 보이지만, 이러한 이미지 생성과 모델이 생성한 이미지를 수정하는 능력을 통해 정밀한 문자열 모델링이 시각 세계에 관한 다양한 측면을 언어 모델에 가르칠 수 있음을 나타냅니다.
- 텍스트 모델로 생성된 이미지를 활용한 자기 지도 학습(self-supervised visual representation learning) 실험은 언어 모델만을 사용하여 자연 이미지에 대한 의미론적 평가를 수행할 수 있는 시각 모델을 훈련시킬 수 있는 잠재력을 강조합니다.

### [Multilingual Instruction Tuning With Just a Pinch of Multilinguality](https://arxiv.org/abs/2401.01854)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wG58InkU8XI0eQPtAmkfJ.png)

Vote: 4

Authors: Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, Matan Eyal

- 대규모 언어 모델(Large Language Models, LLMs)이 세계적으로 사용됨에 따라 다양한 언어로 지시를 따르는 능력이 중요해지고 있다.
- 본 연구에서는 다국어로 지시를 따르는 능력이 언어 간 이전을 통해 달성될 수 있는지를 조사한다.
- 연구 결과, 일부 언어는 단일 언어 튜닝으로도 다른 언어로의 지시 실행 능력을 어느 정도 전달한다는 것을 보여준다.
- 또한, 영어로 튜닝된 세트에 다국어 예제가 단 40개만 포함되어 있어도 다국어로 지시를 따르는 능력이 크게 향상됨을 발견했다.
- 일반적으로, 다양한 언어로 조합된 다국어 튜닝 모델은 단일 언어로 튜닝된 모델에 비해 비슷하거나 우수한 성능을 보여주며, 해당 언어에서의 예제 수는 10배 적다.
- 지시 튜닝 세트에 포함된 언어의 수를 1개에서 2, 3, 4개로 늘리면 교차 언어 일반화가 향상된다는 것을 발견하였다.
- 연구 결과는 광범위한 다국어 지시 기반 튜닝 모델을 매우 소량의 다국어 지시-응답 세트와 함께 구축할 수 있다는 것을 시사한다.

### [WordArt Designer API: User-Driven Artistic Typography Synthesis with Large Language Models on ModelScope](https://arxiv.org/abs/2401.01699)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/TTsJmRsurIA70oM9p4Wq8.png)

Vote: 2

Authors: Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Yusen Hu, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Bin Luo, Yifeng Geng, Xuansong Xie, Jingren Zhou

- 이 논문은 사용자 주도의 예술적 타이포그래피 합성을 위한 새로운 프레임워크인 WordArt Designer API를 소개합니다.
- 이 시스템은 비전문가들도 복잡한 타이포그래피 작업을 간소화할 수 있도록, 모델스코프 상의 대형 언어 모델을 활용합니다.
- 대형 언어 모델을 사용함으로써 사용자 입력을 이해하고 해석하는 기능을 통해 디자인 과정을 더 직관적으로 만듭니다.
- 여러 사례 연구를 통해 사용자가 그들의 미적 선호도와 기능 요구 사항을 표현하고 시스템이 독특하고 창의적인 타이포그래피 디자인으로 변환하는 과정을 시연했습니다.
- 이 평가들은 기존 시스템 대비 사용자 만족도, 디자인 유연성, 창의적 표현력에서 상당한 향상을 나타내었습니다.
- WordArt Designer API는 타이포그래피 예술을 대중화하고 개인화된 디지털 커뮤니케이션 및 디자인을 위한 새로운 가능성을 제시합니다.

