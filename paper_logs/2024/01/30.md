## Daily Papers (2024-01-30)

### [InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model](https://arxiv.org/abs/2401.16420)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/YtF3UCeotLE5b7BIHMrdw.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/YtF3UCeotLE5b7BIHMrdw.mp4" muted="false"></video></div>

Vote: 42

Authors: Haodong Duan, Linke Ouyang, Wenwei Zhang, Xiaoyi Dong, Jingwen Li, Yining Li, Xingcheng Zhang, Bin Wang, Yang Gao, Maosong Cao, Wei Li, Yuhang Cao, Yuhang Zang, Pan Zhang, Xinyue Zhang, Yu Qiao, Conghui He, Xilin Wei, +, Kai Chen, Hang Yan, Dahua Lin, Songyang Zhang

- InternLM-XComposer2는 개요, 상세한 텍스트 명세 및 참조 이미지와 같은 다양한 입력에서 교차 텍스트-이미지 콘텐츠를 능숙하게 제작하여 맞춤형 콘텐츠 생성을 가능하게 하는 최첨단 비전-언어 모델을 소개합니다.
- 이 모델은 사전 학습된 언어 지식의 무결성을 보존하기 위해 이미지 토큰에만 추가 LoRA 파라미터를 적용하는 Partial LoRA (PLoRA) 접근 방식을 제안하여, 정밀한 비전 이해와 문학적 재능을 갖춘 텍스트 구성 사이의 균형을 맞추고 있습니다.
- 실험 결과 InternLM-XComposer2는 다양한 벤치마크에서 기존 멀티모달 모델들을 크게 능가하며, GPT-4V 및 Gemini Pro와 대등하거나 뛰어난 시각-언어 이해 성능을 보여주면서 고품질의 긴 텍스트 멀티모달 콘텐츠 생성에서 우수함을 입증했습니다.
- InternLM-XComposer2 모델 시리즈는 7B 매개변수로 구성되어 있으며, https://github.com/InternLM/InternLM-XComposer 에서 공개적으로 이용 가능합니다.

### [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models](https://arxiv.org/abs/2401.15947)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3ta3Rnuv4Syt6mKIu_8BE.png)

Vote: 32

Authors: Bin Lin, Li Yuan, Jiaxi Cui, Peng Jin, Yang Ye, Munan Ning, Zhenyu Tang, Junwu Zhang, Bin Zhu

- 이 연구에서는 대규모 시각-언어 모델(LVLMs)을 위하여, 모델의 크기를 확장함으로써 성능을 효과적으로 개선하는 MoE-tuning이라는 새로운 훈련 전략을 제안합니다.
- MoE-tuning은 상수의 계산 비용으로 막대한 수의 모델 매개 변수를 가진 희소 모델을 구성하여 다중 모달 학습과 모델 희소성과 관련된 성능 저하를 효과적으로 해결합니다.
- 또한, MoE 기반의 희소 LVLM 아키텍처인 MoE-LLaVA 프레임워크를 소개합니다; 이 프레임워크는 배포 중에 라우터를 통해서만 상위 k 전문가를 활성화시키고 나머지 전문가들은 비활성 상태로 유지합니다.
- 광범위한 실험을 통해 MoE-LLaVA가 시각적 이해에서 우수한 능력을 보이고 모델 출력에서 환상을 줄일 가능성을 보여주었습니다.
- 특히, MoE-LLaVA는 30억 개의 희박하게 활성화된 매개 변수로 다양한 시각적 이해 데이터 세트에서 LLaVA-1.5-7B와 비교할 수 있는 성능을 보여주며 객체 환상 벤치마크에서는 LLaVA-1.5-13B를 능가합니다.
- MoE-LLaVA를 통해 연구자들은 희소 LVLMs에 대한 기준을 설정하고 미래의 더 효율적이고 효과적인 다중 모달 학습 시스템 개발을 위한 중요한 통찰을 제공하기를 목표로 합니다.
- 관련 코드는 https://github.com/PKU-YuanGroup/MoE-LLaVA 에서 공개되었습니다.

### [Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling](https://arxiv.org/abs/2401.16380)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WApmi8a7YcGCsMoVlFfXu.png)

Vote: 30

Authors: Skyler Seto, He Bai, Pratyush Maini, Navdeep Jaitly, David Grangier, Yizhe Zhang

- 웹에서 추출한 대규모 자료는 종종 구조가 없고, 잡음이 많으며, 문장 구성이 나쁘기 때문에, 크기가 큰 언어 모델을 훈련하는 데 많은 계산 능력과 데이터가 필요하다.
- 여기서 제안하는 '웹 재구성 증강 사전 훈련(WRAP)'은 기존 모델을 이용해 웹 문서를 "위키피디아처럼" 또는 "질문-응답 형식으로" 재구성하여, 실제 데이터와 합성 재구성 데이터에 대한 언어 모델들을 동시에 사전 훈련한다.
- WRAP을 C4 데이터셋에 적용했을 때 사전 훈련이 3배 빨라지고, 같은 사전 훈련 예산으로 Pile의 다양한 부분집합에서 평균 10% 이상의 언어 모델 복잡도를 낮출 수 있었다.
- 또한, WRAP은 13가지 작업에서 제로샷 질문 응답 정확도를 2% 이상 향상시켰으며, 다양한 재구성 스타일이 언어 모델 성능에 미치는 영향에 대해 조사하여 교차 도메인 설정에서의 성능에 대한 통찰을 제공한다.
- 합성 재구성 데이터는 하위 작업 평가 스타일을 반영하는 스타일 다양성을 포함하고, 웹 스크랩된 데이터보다 '품질'이 더 높기 때문에 실제 데이터만을 사용할 때보다 더 높은 효용성을 가진다고 결론지었다.

### [Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling](https://arxiv.org/abs/2401.15977)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/DFR3GpMI4021qU9o36Vhk.png)

Vote: 17

Authors: Yi Zhang, Jifeng Da, Dasong Li, Ka Chun Cheung, Simon See, Hongsheng Li, Xiaoyu Shi, Weikang Bian, Zhaoyang Huang, Hongwei Qin, Manyuan Zhang, Fu-Yun Wang

- Motion-I2V는 두 단계로 나뉘며 명확한 동작 모델링을 통해 일관되고 제어할 수 있는 새로운 이미지에서 비디오로 전환하는(I2V) 프레임워크를 도입합니다.
- 첫 번째 단계에서는 기준 이미지 픽셀의 궤도를 추론하는 데 중점을 둔 확산 기반의 동작 필드 예측기를 제안합니다.
- 두 번째 단계에는 제한된 1-D 시간적 주의력을 강화하기 위해 동작-증강 시간적 주의 모듈을 제안합니다.
- 이 모듈은 첫 번째 단계에서 예측된 궤도의 지도를 받아 기준 이미지의 특징을 합성 프레임에 효과적으로 전파할 수 있습니다.
- 대규모 동작 및 시점 변화가 있는 경우에도 Motion-I2V는 기존 방법들보다 더 일관된 비디오를 생성할 수 있습니다.
- 첫 번째 단계에 대해 교육된 희소 궤도 제어넷(ControlNet)을 통해 사용자는 궤도와 지역 주석으로 동작 궤도와 동작 영역을 정확하게 제어할 수 있습니다.
- 이는 텍스트 지시문에만 의존하는 것보다 이미지에서 비디오로의 전환(I2V) 과정을 더욱 제어할 수 있게 합니다.
- 또한, Motion-I2V의 두 번째 단계는 자연스럽게 제로샷 비디오에서 비디오로의 번역을 지원합니다.
- 질적 및 양적 비교는 이전 접근 방식들에 비해 Motion-I2V가 일관되고 조절 가능한 이미지에서 비디오 생성에서의 이점을 보여줍니다.

### [StableIdentity: Inserting Anybody into Anywhere at First Sight](https://arxiv.org/abs/2401.15975)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SfUWrc0CWvO0fFkNSpK9m.png)

Vote: 15

Authors: Taiqing Li, Xu Jia, Huchuan Lu, Qinghe Wang, Xiaomin Li, Liqian Ma, Yunzhi Zhuge

- 이 연구에서는 단 한 장의 얼굴 이미지만으로도 정체성을 일관되게 유지하며 다양한 맥락에 그 인물을 삽입할 수 있는 StableIdentity를 제안합니다.
- 얼굴 인코더와 정체성 사전을 사용하여 입력받은 얼굴을 인코딩하고, 유명인의 이름으로 구성된 편집 가능한 사전(prior) 공간에 이를 매핑합니다.
- 정체성 사전과 편집 가능성 사전을 결합함으로써, 배우를 다양한 상황에 맞게 주입할 수 있습니다.
- 또한, 입력 얼굴의 픽셀 수준 인식을 강화하고 생성의 다양성을 유지하기 위해 마스크 처리된 2단계 확산 손실을 설계했습니다.
- 광범위한 실험을 통해 이 방법이 기존의 맞춤형 생성 방법보다 우수함을 입증하였습니다.
- 이미지, 비디오, 그리고 3D 맞춤형 생성 모델을 통합하는 데 중요한 단계인 StableIdentity를 통해 단일 이미지에서 학습한 정체성을 마이크로튜닝 없이 비디오/3D 생성에 직접 주입할 수 있는 첫 사례를 보여주고 있습니다.

### [Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception](https://arxiv.org/abs/2401.16158)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ZZp9zr-4JeFSR234q6ucp.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ZZp9zr-4JeFSR234q6ucp.mp4" muted="false"></video></div>

Vote: 13

Authors: Ming Yan, Fei Huang, Jitao Sang, Jiabo Ye, Haiyang Xu, Ji Zhang, Weizhou Shen, Junyang Wang

- 이 논문은 다중 모드 대형 언어 모델(Multimodal Large Language Models, MLLM)을 기반으로 하는 모바일 디바이스 에이전트인 Mobile-Agent를 소개합니다.
- Mobile-Agent는 앱의 프론트엔드 인터페이스 내의 시각적 및 텍스트 요소를 정확히 식별하고 위치를 파악하기 위해 시각 인식 도구를 활용합니다.
- 인식된 시각적 맥락에 기반하여, Mobile-Agent는 복잡한 조작 작업을 자율적으로 계획하고 분해하며, 단계별로 모바일 앱을 조작합니다.
- 기존 솔루션들이 앱의 XML 파일이나 모바일 시스템 메타데이터에 의존하는 것과 달리, Mobile-Agent는 시각 중심적 방식으로 다양한 모바일 운영 환경에서 더 큰 적응성을 제공하며 시스템 특화 설정의 필요성을 없앱니다.
- Mobile-Agent의 성능을 평가하기 위해, 모바일 디바이스 조작을 평가하는 벤치마크인 Mobile-Eval을 도입하였습니다.
- Mobile-Eval을 기반으로 종합적인 평가를 실시한 결과, Mobile-Agent는 높은 정확도와 완료율을 달성하는 것으로 나타났습니다.
- 심지어 여러 앱을 조작하는 등의 복잡한 지시사항을 가진 경우에도 Mobile-Agent는 요구 사항을 성공적으로 완수할 수 있습니다.
- 해당 코드와 모델은 https://github.com/X-PLUG/MobileAgent에서 오픈소스로 공개될 예정입니다.

### [Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance](https://arxiv.org/abs/2401.15687)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/6258561f4d4291e8e63d8ae6/bD020RRfSYX4URGMY68P9.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/6258561f4d4291e8e63d8ae6/bD020RRfSYX4URGMY68P9.mp4" muted="false"></video></div>

Vote: 10

Authors: Han Liang, Longwen Zhang, Lan Xu, Pengyu Long, Dafei Qin, Jingyi Yu, Yingliang Zhang, Qixuan Zhang, Qingcheng Zhao

- 음성으로부터 3D 얼굴 애니메이션 합성에 관한 연구가 주목을 받고 있으며, 높은 품질의 4D 얼굴 데이터와 다양한 모드의 풍부한 레이블의 부족으로 이전 방법들은 현실감이 부족하고 유연한 조건 설정에 한계가 있었다.
- 이 도전을 해결하기 위해, 저자들은 첫째로, Generalized Neural Parametric Facial Asset (GNPFA)을 소개하였는데, 이는 얼굴 기하학과 이미지를 매우 일반화된 표현 잠재 공간에 매핑하는 효율적인 변이형 오토인코더로서, 표현과 정체성을 분리한다.
- 둘째로, GNPFA를 사용하여 다양한 비디오로부터 고품질의 표현과 정확한 머리 자세를 추출하였고, 이는 M2F-D 데이터셋, 즉 감정과 스타일 레이블이 잘 분류된 대규모, 다양하고 스캔 수준의 공동 발음 3D 얼굴 애니메이션 데이터셋을 제시한다.
- 마지막으로, 저자들은 오디오, 텍스트, 이미지로부터 풍부한 다중 모드 가이던스를 수용하는 GNPFA 잠재 공간에서의 확산 모델인 Media2Face를 제안하여, 이를 이용한 공동 발음 얼굴 애니메이션 생성 방법을 제안한다.
- 광범위한 실험을 통해 해당 모델은 얼굴 애니메이션 합성에서 높은 충실도를 달성할 뿐만 아니라 3D 얼굴 애니메이션의 표현성과 스타일 적응성 범위를 넓히는 것을 확인할 수 있었다.

### [SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning](https://arxiv.org/abs/2401.16013)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/uxKWerYgEAvqMBKZLxE92.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/uxKWerYgEAvqMBKZLxE92.mp4" muted="false"></video></div>

Vote: 8

Authors: Zheyuan Hu, Jacob Berg, Archit Sharma, Stefan Schaal, Sergey Levine, Abhishek Gupta, Chelsea Finn, Jianlan Luo, You Liang Tan, Charles Xu

- 로봇 강화 학습(RL) 분야에서 최근 복잡한 이미지 관찰, 실세계 훈련, 시연 및 이전 경험과 같은 보조 데이터를 통합하는 방법이 향상되었다.
- 이러한 발전에도 불구하고 로봇 RL은 여전히 사용하기 어렵고, 알고리즘 선택만큼 알고리즘의 구현 세부사항이 성능에 중요한 것으로 인식되고 있다.
- 로봇 RL의 널리 채택과 방법론의 추가 개발을 제약하는 주된 도전 과제 중 하나는 이러한 방법의 비교적 접근성이 낮다는 것이다.
- 이 도전을 해결하기 위해, 저자들은 샘플 효율적인 오프-폴리시(off-policy) 딥 RL 방법, 보상 계산 및 환경 초기화 방법, 널리 사용되는 로봇을 위한 고품질 컨트롤러, 다양한 도전적 예제 태스크가 포함된 정교하게 구현된 라이브러리를 개발하였다.
- 이 라이브러리는 커뮤니티를 위한 자원으로 제공되며, 설계 선택, 실험 결과에 대해 기술한다.
- 놀랍게도, 이 구현은 PCB 보드 조립, 케이블 라우팅, 물체 이동 등의 정책을 평균적으로 25~50분의 훈련만으로 매우 효율적으로 학습할 수 있으며, 유사한 작업에 대해 문헌에 보고된 최신 결과보다 개선된다.
- 학습된 정책들은 완벽하거나 거의 완벽한 성공률을 달성하고, 심지어 방해 요소가 있을 때도 매우 강건하며, 회복 및 수정 행동이 자연스럽게 발생한다.
- 개발자들은 이 라이브러리가 로봇 RL의 추가 개발을 촉진하는 로봇공학 커뮤니티에 유용한 도구를 제공할 것이라 기대하며, 코드, 문서, 동영상은 https://serl-robot.github.io/ 에서 확인할 수 있다.

### [Object-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with Prototypical Embedding](https://arxiv.org/abs/2401.15708)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/O9rAhpoGNtYTlmnkuivK9.png)

Vote: 7

Authors: Hui Guo, Jianxiang Lu, Cong Xie

- 대규모 텍스트-이미지 생성 모델의 발전으로 많은 파인튜닝 방법이 제안되었으나, 새로운 객체에 대해 일회성 시나리오에서 어려움을 겪는 문제가 있습니다.
- 본 연구에서는 객체 주도 방식을 사용하여 단일 입력 이미지와 관심 객체 특정 영역만을 이용하여 일반화 및 고선명도 과제에 접근하고자 합니다.
- 객체의 외관과 클래스를 기반으로 프로토타입 임베딩을 초기화 한 다음, 확산 모델을 파인튜닝하여 일반화를 개선하고 과적합을 완화합니다.
- 또한, 객체 클래스의 이전 지식을 유지하기 위해 클래스 특성화 정규화를 도입하는 방법을 제안합니다.
- 객체 특정 손실을 도입하여 충실도를 향상시키고, 여러 객체를 식별하는데도 사용할 수 있습니다.
- 제안된 메소드는 기존 개념과의 통합은 물론, 높은 충실도와 일반화를 이룰 수 있으며, 여러 기존 작업보다 우수한 성능을 보입니다.
- 연구 코드는 공개될 예정입니다.

### [Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation](https://arxiv.org/abs/2401.15688)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Vjh-Fdss7FTOfryl0Bx2t.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Vjh-Fdss7FTOfryl0Bx2t.mp4" muted="false"></video></div>

Vote: 6

Authors: Zhongdao Wang, Aoxue Li, Zhenyu Wang, Zhenguo Li, Enze Xie, Xihui Liu

- 본 논문은 복잡한 텍스트 프롬프트에 대한 이미지의 제어성을 보장하는 데 어려움을 겪고 있는 텍스트-이미지 모델의 발전에도 불구하고, 특히 객체 속성과 관계를 유지하는 데 중점을 둔 CompAgent라는 훈련이 필요 없는 새로운 방식을 제안합니다.
- 대형 언어 모델(LLM) 에이전트를 핵심으로 하여 복합 텍스트 프롬프트를 분리하고 연속적인 장면 레이아웃을 예측하는 '나누고 정복하라'는 방법론을 적용합니다.
- LLM 에이전트는 각 객체를 독립적으로 구성하고, 텍스트를 분석하여 계획을 수립한 뒤, 이러한 객체들을 조합하기 위한 도구들을 사용합니다.
- 생성된 이미지를 수정하고 속성 오류를 정정하기 위해 검증과 인간의 피드백 메커니즘을 에이전트에 통합하여 활용합니다.
- 본 연구는 여러 객체 사이의 혼동을 방지하기 위해 장면 레이아웃이 이미지 생성 과정을 제어하는 다중 개념 맞춤 모델과 레이아웃-이미지 생성 모델, 그리고 로컬 이미지 편집 방법을 제안합니다.
- CompAgent는 오픈월드 복합 T2I 생성을 위한 포괄적인 벤치마크인 T2I-CompBench에서 10% 이상의 개선을 달성함으로써, 조합적 텍스트-이미지 생성을 위한 우리의 접근법의 우수성을 입증합니다.
- 다양한 관련 작업으로의 확장은 CompAgent의 잠재적인 응용 분야에 대한 유연성을 보여줍니다.

### [Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization](https://arxiv.org/abs/2401.15914)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/uX3fS402Lq94MfLNLJVWJ.png)

Vote: 4

Authors: Hanlin Goh, Josh Susskind, Chen Huang, Yuhang Zang

- 시각-언어 모델이 다양한 시각적 도메인 및 작업에서 일반화 능력을 발휘하지만, 닫힌 세트 방식의 제로샷 인식에 집중되어 있어 개방형 시각 개념을 처리하는 데 한계가 있습니다.
- 최근 프롬프트 학습과 같은 미세조정 방법이 배포 내(ID)와 배포 외(OOD) 샘플 간 구별을 연구하며 ID 및 OOD 정확도 모두 향상시키는 데 도움을 주었습니다.
- 본 논문에서는 적절한 규제 없이 오래 조정된 후의 시각-언어 모델이 주어진 데이터셋의 알려진 클래스에 과적합되는 경향이 있으며, 미지의 클래스에서의 성능 저하 문제를 보여줍니다.
- 이러한 문제를 다루기 위해, 알려지지 않은 클래스의 이름만을 사용하여 OOD 특징을 생성하는 클래스 조건부 특징 생성기를 도입하는 새로운 접근법 OGEN을 제안합니다.
- 생성된 특징들은 미지의 것에 대한 유용한 지식을 제공하며, ID와 OOD 데이터 사이의 결정 경계를 최적화하는 과정에서 규제하는 데 도움을 줍니다.
- 모델 상태 간 지식을 적응적으로 전달하여 과적합을 더욱 방지하는 자체 적응형 자기교육(distillation) 메커니즘도 중요한 역할을 합니다.
- 실험을 통해 OOD 일반화 성능을 향상시키는 영역에서 우리의 방법이 설득력 있는 성과를 거두고 있음을 확인하였습니다.

