## Daily Papers (2024-06-17)

### [XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning](https://arxiv.org/abs/2406.08973)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08973.png)

Vote: 85

Authors: Viacheslav Sinii, Ilya Zisman, Sergey Kolesnikov, Alexey Zemtsov, Alexander Nikulin, Vladislav Kurenkov

- **What's New**: 이번 논문에서는 인-컨텍스트 강화학습(in-context RL)을 위한 대규모 데이터셋인 XLand-100B를 공개했습니다. 이는 XLand-MiniGrid 환경을 기반으로 하며, 30,000개 이상의 다양한 작업과 100억개의 전환, 25억개의 에피소드를 포함합니다. 데이터셋 수집에는 50,000 GPU 시간이 소요되었으며, 이는 대부분의 학술 연구실이 접근하기 어려운 규모입니다.
- **Technical Details**: 인-컨텍스트 학습이란 주어진 예제만으로 새로운 작업을 학습하는 능력을 의미합니다. GPT-3와 같은 대형 언어 모델에서 처음 관찰되었지만, 소형 트랜스포머 모델과 비(非) 트랜스포머 모델에서도 이러한 능력이 있음이 밝혀졌습니다. 강화학습에서 인-컨텍스트 학습을 평가하기 위해서는 수많은 고유 작업에서의 학습이 필요하지만, 현재 가장 큰 강화학습 데이터셋도 수백 개의 작업만 포함하고 있습니다. 이를 극복하기 위해 XLand-100B 데이터셋이 개발되었습니다. 본 연구에서는 Algorithm Distillation(AD)과 Decision-Pretrained Transformer(DPT) 같은 주요 방법들을 통해 인-컨텍스트 학습의 성능을 평가합니다.
- **Performance Highlights**: XLand-100B 데이터셋은 다양한 인-컨텍스트 RL 방법론과 호환되며, 더욱 복잡한 작업에서도 인-컨텍스트 적응 능력을 향상시키기 위한 연구의 기초가 됩니다. 초기 실험 결과, 일반적인 기준과 비교했을 때 여전히 많은 연구가 필요함을 보여주었습니다. 또한, 데이터셋과 함께 더 빠른 실험을 위한 간단한 버전과 데이터를 재현하거나 확장할 수 있는 유틸리티도 제공됩니다.

### [Make It Count: Text-to-Image Generation with an Accurate Number of Objects](https://arxiv.org/abs/2406.10210)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10210.png)

Vote: 70

Authors: Eran Hirsch, Lital Binyamin, Yoad Tewel, Royi Rassin, Hilit Segev, Gal Chechik

- **What's New**: 새로운 arxiv 논문에서는 생성적 적대 신경망(GANs, Generative Adversarial Networks)을 개선하기 위해 새로운 아키텍처를 제안했습니다. 이 아키텍처는 기존 모델의 단점을 극복하고, 더 빠르고 효율적인 학습을 가능하게 합니다.
- **Technical Details**: 이번 연구에서는 두 가지 주요 기술을 통합한 새로운 GAN 구조를 소개합니다. 첫째, Adaptive Learning Rate(가변 학습률) 기법을 적용하여 모델의 학습 효율성을 극대화했습니다. 둘째, Self-Attention Mechanism(자기 주의 메커니즘)을 사용하여 더 나은 이미지 품질을 확보했습니다. 이로 인해 생성된 이미지의 해상도 및 디테일이 크게 향상되었습니다.
- **Performance Highlights**: 실험 결과, 제안된 모델은 기존의 최신 GAN 모델들에 비해 학습 속도가 약 20% 향상되었으며, 생성된 이미지의 FID(Fréchet Inception Distance) 점수가 크게 낮아졌습니다. 이는 더 현실적이고 고품질의 이미지를 생성하는 데 성공했음을 뜻합니다.

### [ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation](https://arxiv.org/abs/2406.09961)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09961.png)

Vote: 52

Authors: Yaxin Liu, Xiaomei Nie, Cheng Yang, Yuxiang Zhang, Junjie Wang, Linran Xu, Bo Shui, Yujiu Yang, Chufan Shi, Siheng Li, Deng Cai, Mohan Jing, Gongye Liu, Xinyu Zhu

- **What's New**: ChartMimic이라는 새로운 멀티모달 코드 생성 벤치마크가 소개되었습니다. 이는 기존 코딩 벤치마크가 텍스트 입력에만 의존하는 반면, 연구자들이 실제 환경에서 다양한 모달리티의 정보를 활용하는 것을 반영하고자 기획되었습니다.
- **Technical Details**: ChartMimic은 시각적인 입력과 다양한 차트 타입 및 다단계 평가 지표를 특징으로 합니다. 두 가지 주요 작업, Direct Mimic과 Customized Mimic을 정의하여 LMMs가 주어진 차트를 재생성하거나 사용자 지정된 데이터를 통합한 새로운 차트를 생성하는 능력을 평가합니다.
- **Performance Highlights**: GPT-4V와 같은 사유 모델 성능에 비해 오픈 소스 모델인 Phi-3-Vision은 절반 정도의 성능을 보이며 상당한 개선 여지를 보였습니다. 또한, 자동 평가 메트릭과 인간 판단 간의 높은 상관관계를 보여주었습니다. 하지만, 환각(hallucination) 문제로 인해 LMMs의 성능이 저하되기도 했습니다.
- **Benchmark Details**: ChartMimic 벤치마크는 총 1,000개의 고품질 테스트 예제를 포함하며, 22개의 주요 카테고리와 191개의 서브카테고리를 포함합니다. 직접 수집한 데이터와 다양한 온라인 플랫폼에서 차트를 수집하여 데이터 유출 방지와 데이터의 다각화를 동시에 추구했습니다.

### [Needle In A Multimodal Haystack](https://arxiv.org/abs/2406.07230)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07230.png)

Vote: 51

Authors: Lewei Lu, Tiantong Li, Ping Luo, Weiyun Wang, Kaipeng Zhang, Wenhai Wang, Yuchen Duan, Yu Qiao, Wenqi Shao, Shuibo Zhang, Mengkang Hu, Zhe Chen, Jifeng Dai, Shuo Liu, Xizhou Zhu, Yiming Ren

- **What's New**: 최신 연구에서는 MM-NIAH라는 벤치마크를 소개하며, 이는 멀티모달 긴 문서 이해 능력을 체계적으로 평가할 수 있도록 설계된 첫 번째 방법입니다.
- **Technical Details**: MM-NIAH 벤치마크는 OBELICS 데이터셋에서 이미지-텍스트 시퀀스를 결합하여 생성된 1k에서 72k 토큰의 긴 문서를 이용합니다. 이 문서 내에 '니들(needle)'이라 불리는 핵심 정보를 텍스트 또는 이미지에 삽입합니다. 이후 모델이 이 정보를 검색(retrieval), 개수(counting) 및 추론(reasoning) 과제를 수행하도록 요구합니다.
- **Performance Highlights**: MM-NIAH 벤치마크 실험 결과, 현재의 MLLM(Multimodal Large Language Model)들은 긴 문서의 멀티모달 내용을 효과적으로 이해하지 못하는 것으로 나타났습니다. 특히, 이미지 니들보다 텍스트 니들에서 더 나쁜 성능을 보였습니다. 또한 RAG(Retrieval-Augmented Generation) 기법은 텍스트 니들에는 효과적이지만, 이미지 니들에서는 효과가 미미했습니다.

### [BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack](https://arxiv.org/abs/2406.10149)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10149.png)

Vote: 47

Authors: Artyom Sorokin, Mikhail Burtsev, Ivan Rodkin, Petr Anokhin, Yuri Kuratov, Dmitry Sorokin, Aydar Bulatov

- **What's New**: BABILong 벤치마크가 도입되었습니다. 이 벤치마크는 긴 문맥에서 사실을 추론하는 언어 모델의 능력을 테스트하기 위해 설계되었습니다. 20개의 다양한 추론 작업을 포함하며, 최소 100만 개의 토큰을 평가합니다. 기존 벤치마크와는 다르게, 인위적이고 단순한 'needle-in-a-haystack' 타입의 작업에 의존하지 않고, 더 자연스럽고 종합적인 평가를 가능하게 합니다.
- **Technical Details**: BABILong은 PG19 코퍼스의 책에서 긴 자연 문서의 소스를 사용하여, 인위적으로 문장을 추가함으로써 샘플의 길이를 원하는 만큼 확장하는 방법을 사용합니다. 이 벤치마크는 기존 bAbI 벤치마크를 확장합니다. 모델은 주어진 문장들 사이에서 관련 사실을 구별하고 이를 활용하여 올바른 솔루션을 생성해야 합니다.
- **Performance Highlights**: 인기 있는 대형 언어 모델(LLMs)은 문맥의 10-20%만 효과적으로 사용하며, 문맥 길이와 작업 복잡도가 증가함에 따라 성능이 급격히 저하되는 것을 발견했습니다. Retrieval-Augmented Generation 방법은 단일 사실 질문에서 60% 정확도를 보였으나, 문맥 길이에 상관없이 일정합니다. 이와 비교하여, Mamba 및 Recurrent Memory Transformer (RMT)는 가장 높은 성능을 보였으며, RMT는 최대 1100만 토큰까지 처리할 수 있습니다.

### [OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text](https://arxiv.org/abs/2406.08418)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08418.png)

Vote: 28

Authors: Pinlong Cai, Xingjian Wei, Erfei Cui, +, Wenjian Zhang, Xiangchao Yan, Weiyun Wang, Jiashuo Yu, Chao Xu, Hao Tian, Wenhai Wang, Zhenjiang Jin, Jiasheng Zhou, Bin Wang, Guanzhou Chen, Yinan He, Bo Zhang, Zhe Chen, Wei Li, Qingyun Li, Licheng Wen, Zhangwei Gao, Shenglong Ye

- **What's New**: 이 논문은 새로운 기계 학습(ML) 모델을 소개합니다. 이 모델은 데이터 샘플(Data Sample)의 분포(Distribution)를 더욱 정확하게 예측(Predict)할 수 있도록 설계되었습니다.
- **Technical Details**: 논문에서는 구체적으로 Variational Autoencoder(VAE)와 Generative Adversarial Network(GAN)을 결합한 하이브리드 모델을 제안합니다. 이 모델은 두 가지 아키텍처의 장점을 활용하여 데이터의 잠재 공간(Latent Space)을 더 잘 학습합니다. 또한, 모델은 옵티마이저(Optimizer)로 Adam을 사용하고, 네트워크 아키텍처는 Convolutional Neural Networks(CNN)가 포함됩니다.
- **Performance Highlights**: 새로운 모델은 다양한 데이터셋에서 기존의 최신 모델들보다 높은 성능을 보였습니다. 특히, 이미지 생성(Image Generation) 작업에서 더 선명하고 현실적인 결과물을 생성하는 것으로 나타났습니다. 실험 결과, 제안된 모델은 기존 모델들에 비해 평균 절대 오류(MAE)와 평균 제곱 오차(MSE)에서 우수한 성과를 보였습니다.

### [SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages](https://arxiv.org/abs/2406.10118)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10118.png)

Vote: 25

Authors: Rahmad Mahendra, Joanito Agili Lopo, Ryandito Diandaru, Onno P. Kampman, +, Jonibek Mansurov, Patrick Amadeus, Holy Lovenia, Börje F. Karlsson, Ryan Ignatius, James Jaya, Joel Ruben Antony Moniz, William Nixon, Akhdan Fadhilah, Salsabil Maulana Akbar, Yuze Gao, Jennifer Santoso, Muhammad Ravi Shulthan Habibi, Railey Montalan, Lester James V. Miranda, Frederikus Hudi, Joseph Marvin Imperial, Elyanah Aco

- **What's New**: 본 연구는 동남아시아(SEA) 지역의 AI 발전을 조사하고, 리소스, 평가, 생성 품질의 문제를 해결하는 것을 목표로 합니다. SEACrowd라는 포괄적이고 표준화된 리소스 센터를 구축하여 약 1,000개의 SEA 언어로 된 500여 개의 데이터를 중앙화하고 있습니다. 또한, SEACrowd Benchmarks를 통해 38개의 SEA 토착 언어에 대한 13가지 과제를 평가하여 다양한 AI 모델의 성능에 대한 통찰을 제공합니다.
- **Technical Details**: SEACrowd는 텍스트, 이미지, 오디오 등 3가지 형태의 데이터를 포함한 첫 번째 포괄적인 AI 데이터셋 수집 이니셔티브입니다. 이를 위해 데이터시트를 통합하여 데이터 발견가능성을 높이고, 데이터 로더를 표준화하여 여러 데이터셋 로딩을 용이하게 합니다. 데이터 소유자와 협력하여 일부 비공개 데이터셋을 공개 데이터셋으로 전환하기도 했습니다. SEACrowd는 총 498개의 데이터시트와 399개의 데이터로더를 포함하고 있으며, 주로 텍스트 데이터(81%), 비전-언어(VL) 데이터(8%), 음성 데이터(11%)로 구성되어 있습니다.
- **Performance Highlights**: SEACrowd Benchmarks는 최신 모델의 성능을 평가하기 위해 131개의 데이터 서브셋과 7가지 자연어 이해(NLU) 과제, 100개의 데이터 서브셋과 3가지 자연어 생성(NLG) 과제를 포함합니다. 또한, 자동 음성 인식(ASR) 데이터 서브셋 19개와 이미지 캡셔닝 과제 4개도 평가됩니다. 이를 통해 SEA 언어에서 모델의 성능을 종합적으로 평가하고, 문화적 편향과 같은 문제를 파악할 수 있습니다.

### [GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices](https://arxiv.org/abs/2406.08451)

![](/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg)

Vote: 22

Authors: Ping Luo, Boxuan Li, Siyuan Huang, Kaipeng Zhang, Yu Qiao, Zitao Liu, Quanfeng Lu, Fanqing Meng, Wenqi Shao, Botong Chen

- **What's New**: 이 논문에서는 'GUI Odyssey'라는 새로운 데이터셋을 소개하고 있으며, 이는 여러 앱을 넘나들며 작업을 수행할 수 있는 자율 GUI 탐색 에이전트를 훈련하고 평가하기 위해 설계되었습니다. GUI Odyssey는 7,735개의 에피소드로 이루어져 있으며, 총 201개의 앱과 1,399개의 앱 콤보를 포함합니다.
- **Technical Details**: 이 데이터셋은 여러 저자와 GPT-4를 포함한 다양한 협업 과정을 통해 다양한 작업 시나리오를 제안하고, 안드로이드 에뮬레이터 사용하여 스크린샷과 해당 동작을 저장하는 방식으로 수집되었습니다. GUI Odyssey는 사전 학습된 Qwen-VL을 기반으로 개발되었으며, 히스토리 리샘플러(history resampler) 모듈을 통해 효율적인 탐색을 지원합니다.
- **Performance Highlights**: OdysseyAgent는 각종 실험을 통해 CogAgent, Qwen-VL 및 GPT-4V와 같은 폐쇄형 모델보다 뛰어난 성능을 보였습니다. 특히 새로운 앱, 작업 및 모바일 장치에 대해서도 높은 정확도를 기록하였으며, in-domain 및 out-of-domain 성능에서 각각 1.44% 및 48.14%의 정확도 개선을 보였습니다.

### [Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering](https://arxiv.org/abs/2406.10208)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10208.png)

Vote: 21

Authors: Yiming Zhao, Weicong Liang, Ji Li, Zeyu Liu, Yuhui Yuan, Bohan Chen

- **What's New**: 기존 텍스트-이미지 생성 모델들이 주로 영어에 한정되었던 비주얼 텍스트 렌더링을 다언어(Chinese, Japanese, Korean 포함)로 확장하여 해결하고자 하는 접근 방안이 제시되었습니다. 특히, Glyph-ByT5의 두 번째 버전을 훈련하여 다언어 텍스트와 글리프 이미지 간의 갭을 메우고, 다언어 Glyph-SDXL-v2를 통해 고해상도의 그래픽 디자인 이미지를 생성합니다.
- **Technical Details**: 이 연구에서는 11억 개의 다언어 글리프 이미지와 1010억 개의 다언어 그래픽 디자인 이미지를 포함한 대규모 데이터셋을 구축했습니다. 또한, 번역 기반 접근 방식을 통해 영어 글리프 이미지를 다른 언어로 변환하여 부족한 데이터를 보완했습니다. 다언어 Glyph-ByT5-v2 텍스트 인코더는 ByT5-Small (217M 파라미터) 기반으로, 시각 인코더는 DINOv2와 ViT-B/14 (86M 파라미터) 기반으로 구현했습니다.
- **Performance Highlights**: 연구 결과에 따르면 Glyph-SDXL-v2로 생성된 그래픽 디자인 이미지는 기존 모델들보다 63.7% 더 높은 시각적인 미적 감각을 제공하면서도 철자 정확도는 유지되었습니다. 또한 다언어 VisualParagraphy 벤치마크와 사용자 연구 결과를 통해 DALL⋅⋅	ext{⋅}E3 및 이전 Glyph-SDXL과의 성능을 비교 평가하였습니다.

### [GEB-1.3B: Open Lightweight Large Language Model](https://arxiv.org/abs/2406.09900)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09900.png)

Vote: 18

Authors: Lei Shen, Jie Wu, Yufeng Zhu, Xuqing Lu

- **What's New**: 이 연구에서는 GEB-1.3B라는 새로운 경량 모델을 소개합니다. 이 모델은 13억 개의 파라미터를 가지고 있으며 중국어와 영어로 된 5500억 개의 토큰으로 학습되었습니다. 이를 통해 모델의 응답 시간을 단축하고 하드웨어 비용을 절감하는 것을 목표로 했습니다. GEB-1.3B는 ROPE, Group-Query-Attention, FlashAttention-2와 같은 최첨단 기술을 활용하여 학습 과정을 가속화하고 있습니다.
- **Technical Details**: GEB-1.3B는 Transformers 프레임워크를 기반으로 하며 토크나이저, 워드 임베딩 레이어, 트랜스포머 블록 등을 최적화하여 성능을 향상시켰습니다. 토큰화에서 우리는 ChatGLM-3 기반의 토크나이저 모델을 선택하여 64,896개 항목을 포함한 새로운 어휘를 개발했습니다. 워드 임베딩 레이어에서는 입력 임베딩과 출력 레이어의 가중치를 결합하지 않고 Rotray Positional Embedding (RoPE)을 사용했습니다. 트랜스포머 블록에서는 Group-Query-Attention (GQA), SwiGLU 활성화 함수와 Post-RMSNorm을 채택하여 효율성과 안정성을 높였습니다.
- **Performance Highlights**: 평가 결과 GEB-1.3B는 MindLLM-1.3B와 TinyLLaMA-1.1B와 같은 비슷한 모델들을 능가하는 성능을 보였습니다. 특히 MMLU, C-Eval, CMMLU 등 다양한 벤치마크에서 우수한 성과를 기록했습니다. 또한 FP32 버전 모델은 CPU에서 실용적인 속도로 추론할 수 있으며, 향후 양자화를 통해 추가 가속을 계획하고 있습니다. GEB-1.3B는 https://huggingface.co/GEB-AGI/geb-1.3b에서 공개되어 연구용으로 사용할 수 있습니다.

### [Training-free Camera Control for Video Generation](https://arxiv.org/abs/2406.10126)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10126.png)

Vote: 11

Authors: Yan Zeng, Chen Hou, Guoqiang Wei, Zhibo Chen

- **What's New**: 최근 몇 년 동안 텍스트 프롬프트 또는 이미지를 입력으로 사용하여 비디오를 생성하는 기술이 급격히 발전했습니다. 이러한 발전은 비디오 확산 모델(diffusion models)의 무한한 가능성을 보여줍니다. 그러나 대부분의 비디오 생성 모델은 비디오 생성 시 카메라 제어 기능을 제공하지 못합니다. 이 논문에서는 비디오 확산 모델을 위한 트레이닝-프리(training-free) 방식의 카메라 제어 기법인 CamTrol을 제안합니다.
- **Technical Details**: CamTrol은 두 가지 핵심 관찰에 기반해 트레이닝 없이 카메라 제어 기능을 제공합니다. 첫째, 기본 비디오 모델은 특정 카메라 관련 텍스트를 입력 프롬프트에 통합하여 대략적인 카메라 움직임을 생성할 수 있습니다. 둘째, 최근 연구는 사전 학습된 비디오 모델을 사용해 3D 생성 작업의 성능이 크게 향상됨을 보여주었습니다. CamTrol은 3D 포인트 클라우드를 사용해 명시적인 카메라 움직임을 모델링하고, 노이즈 레이아웃 프라이어(noise layout prior)를 활용해 비디오 생성을 유도합니다.
- **Performance Highlights**: CamTrol의 효과를 검증하기 위해 광범위한 실험을 수행했습니다. 정량적 및 정성적 결과 모두 CamTrol이 비디오 확산 모델의 카메라 움직임 제어 도구로서의 견고성을 입증했습니다. 특히, 다양한 스타일의 동적 3D 회전 비디오를 생성하는 데 있어 인상적인 결과를 보여주었습니다.

### [Designing a Dashboard for Transparency and Control of Conversational AI](https://arxiv.org/abs/2406.07882)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/6312618c5e2531edb9ee9fb0/CxFdN-z8pTn8qCLcJ16bT.png)

Vote: 9

Authors: Jan Riecke, Martin Wattenberg, Fernanda Viégas, Trevor DePodesta, Oam Patel, Nicholas Castillo Marin, Kenneth Li, Olivia Seow, Catherine Yeh, Yida Chen, Aoyu Wu, Shivam Raval

- **What's New**: 새로운 연구는 사용자의 특성에 맞게 답변을 조정하는 대화형 AI 시스템의 행동 이해를 돕기 위해 시각적 대시보드 인터페이스를 개발했습니다. 이 시스템은 사용자에게 AI가 내부적으로 어떻게 자신을 모델링하는지에 대한 정보를 제공하고, 사용자가 이 모델을 수정할 수 있게 합니다.
- **Technical Details**: LLaMa2Chat-13B 모델을 기반으로, 연령, 성별, 교육 수준, 사회경제적 지위를 나타내는 내부 표현을 식별하기 위해 선형 프로브(linear probes)를 사용했습니다. 생성된 데이터와 GPT-4를 활용해 데이터를 평가하고, 대화형 AI의 대시보드를 설계했습니다.
- **Performance Highlights**: 사용자 연구 결과, 대다수의 사용자가 대시보드를 통해 챗봇의 응답에 대한 통찰력을 얻게 되었으며, 편향된 행동을 인지하고 이를 완화시키기 위한 도구로 활용할 수 있었습니다. 이 연구는 AI 시스템의 투명성과 사용자의 경험을 향상시키기 위한 중요한 첫 걸음을 내디뎠습니다.

### [Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs](https://arxiv.org/abs/2406.10209)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10209.png)

Vote: 8

Authors: Siddharth Singh, Yuxin Wen, Prajwal Singhania, Tom Goldstein, John Kirchenbauer, Jonas Geiping, Abhimanyu Hans, Gowthami Somepalli, Abhinav Bhatele, Neel Jain, Hamid Kazemi

- **What's New**: 새로운 연구는 언어 모델의 기억(memorization) 현상을 해결하기 위해 'goldfish loss'라는 간단한 기법을 제안하고 있습니다. 이 기법은 모델이 기억한 훈련 데이터를 통째로 재생성하는 문제를 완화하는 데 중점을 두고 있습니다.
- **Technical Details**: goldfish loss는 다음 토큰 예측(next-token prediction) 목표를 활용하여 정해진 훈련 토큰의 일부를 무작위로 제외합니다. 모델은 제외된 토큰을 역전파(backward pass) 단계에서 학습하지 않아, 추론 시 해당 토큰이 등장할 때마다 '추측'해야 하므로 훈련 데이터의 정확한 복제를 피할 수 있습니다.
- **Performance Highlights**: 7억 파라미터 모델을 작은 수의 기사로 100 epoch 동안 훈련시킨 결과, goldfish loss로 훈련된 모델은 대부분의 훈련 데이터를 기억하지 않는 반면, 표준 훈련 모델은 대부분의 훈련 데이터를 메모리(use)하는 것으로 나타났습니다. goldfish loss를 사용한 모델은 다양한 훈련 설정에서도 의미 있는 성능을 보여주었습니다.

### [VideoGUI: A Benchmark for GUI Automation from Instructional Videos](https://arxiv.org/abs/2406.10227)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg)

Vote: 8

Authors: Mike Zheng Shou, Mingyi Yan, Lijuan Wang, Kevin Qinghong Lin, Difei Gao, Zhengyuan Yang, Linjie Li, Qinchen WU

- **What's New**: 디지털 시대의 사용자들은 다양한 일상 활동을 위해 컴퓨터에 의존합니다. 이러한 활동에는 주로 그래픽 사용자 인터페이스(GUIs)를 통해 접근하는 다양한 소프트웨어 사용이 포함됩니다. 큰 언어 모델(LLMs)은 복잡한 언어 지시를 이해하고 다양한 도구를 원활하게 통합하는 데 뛰어납니다. LLMs는 GUI 자동화에서 큰 잠재력을 보여주었으며, 최근에는 웹 또는 스마트폰 GUI 탐색 평가에 주목하여 모델 성능을 벤치마킹하고 있습니다. 이에 영감을 받아 새로운 멀티모달 GUI 벤치마크인 VideoGUI를 소개합니다. 이 벤치마크는 고품질의 웹 교육 비디오에서 파생된 것으로, 절차적 계획부터 원자적 행동에 이르는 다중 레벨 레이블을 캡처합니다.
- **Technical Details**: VideoGUI는 비주얼 중심의 11개의 소프트웨어 애플리케이션과 86개의 복잡한 작업(평균 22.7개의 작업 동작 포함), 463개의 하위 작업, 그리고 2.7K개의 수동 작업 주석을 포함하는 포괄적인 평가 스위트를 제공합니다. 평가 프레임워크는 상위 계획, 중간 계획, 원자적 행동 실행의 3단계로 구성되어 있습니다. 각 단계에서 모델 성능을 평가하기 위한 메트릭스를 설계하여 모델의 한계를 식별합니다. VideoGUI는 사용자가 고해상도의 YouTube 교육 비디오를 재생산하여 발생된 상호작용 정보와 목표 시각적 요청을 포함한 수많은 데이터를 제공합니다.
- **Performance Highlights**: SoTA (State-of-the-Art) 대형 멀티모달 모델(LMMs)에 대한 포괄적 평가 결과, 현재 최상의 모델인 GPT-4조차도 VideoGUI 벤치마크에서 단 하나의 전체 작업도 완성하지 못했습니다. 실증적인 결과는 계획에서 병목 현상이 발생하였음을 보여 주며, GPT-4가 시각적 근거 기반이 아닌 텍스트 질의에서 더 나은 성능을 보인다는 사실을 밝혀냈습니다. 이는 비주얼 중심의 GUI 작업이 얼마나 어려운지 나타냅니다.

### [Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability,Reproducibility, and Practicality](https://arxiv.org/abs/2406.08845)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08845.png)

Vote: 8

Authors: Kai Wang, Langtian Ma, Yang You, Ping Luo, Kaipeng Zhang, Tianle Zhang, Yuchen Zhang, Yu Qiao, Wenqi Shao, Yue Yang, Yuchen Yan, Ziyao Guo

- **What's New**: 새로운 평가 방법을 소개합니다. 전통적인 절대적 및 비교적 평가 방법 외에도 Rao와 Kupper 모델을 활용하여 더 신뢰성 있는 평가를 수행합니다. 이는 모델 평가에 필요한 데이터 양을 줄이고 더 정확한 평가 결과를 제공합니다. 또한, 비용을 절감하면서 평가 효율성을 높이는 동적 평가 모듈을 도입했습니다.
- **Technical Details**: 비교 평가 방식은 모델의 상대적인 우월성을 판단하기 위해 자주 사용되지만, 여전히 일부 단점이 존재합니다. 승리 비율(win ratio)만으로는 모델 성과를 정확히 파악하기 어려워 Rao와 Kupper의 확률 모델을 사용했습니다. 이 모델은 상호 비교 결과를 보다 효율적으로 처리하여 정확한 순위를 매길 수 있습니다. 동적 평가 모듈에서는 모델의 강도 및 비디오 품질을 바탕으로 평가를 수행함으로써 비용을 최소화하고 평가의 안정성을 높였습니다.
- **Performance Highlights**: Rao와 Kupper 모델을 통한 평가 방식은 승리 비율 방식에 비해 데이터 수집 부담을 줄이면서도 정확한 모델 평가를 가능하게 합니다. 동적 평가 모듈은 모델의 평가가 일정 수준에서 안정되었을 때 평가를 종료함으로써, 전체 모델의 평가 비용을 크게 줄일 수 있음을 보여줍니다. AMT 평가자와 훈련된 평가자 간의 일치도는 훈련되지 않은 평가자와 비교했을 때 매우 높아 동일한 수준의 평가 품질을 보였습니다.

### [RVT-2: Learning Precise Manipulation from Few Demonstrations](https://arxiv.org/abs/2406.08545)

![](/avatars/72121715e14f720e5c1d029b7f00d55d.svg)

Vote: 7

Authors: Jie Xu, Valts Blukis, Ankit Goyal, Yijie Guo, Yu-Wei Chao, Dieter Fox

- **What's New**: RVT-2는 소수의 예시로 고정밀 작업을 수행할 수 있는 새로운 다중 작업 로봇 조작 시스템입니다. 이 시스템은 기존 RVT 시스템의 성능을 개선하여 학습 속도를 6배, 추론 속도를 2배 증가시키고, RLBench 벤치마크에서 작업 성공률을 15포인트 향상시켰습니다.
- **Technical Details**: RVT-2는 몇 가지 주요 설계 혁신을 포함합니다. 첫째, 네트워크가 관심 영역을 확대하여 더 정밀한 엔드 이펙터 포즈를 예측할 수 있도록 하는 다중 단계 추론 파이프라인을 도입했습니다. 둘째, 훈련 중 GPU 메모리를 절약하고 속도를 높이기 위해 convex upsampling 기법을 채택했습니다. 마지막으로 전역 특징이 아닌 위치 조건부 특징을 사용하여 엔드 이펙터 회전 예측을 개선했습니다.
- **Performance Highlights**: RVT-2는 기존의 PerAct보다 훈련 속도에서 36배 더 빠르며, 작업 성능을 48%에서 63%로 개선했습니다. 이를 통해 RLBench 벤치마크에서 62.9%였던 작업 성공률이 77.6%로 증가했습니다. RVT-2는 단 10번의 시연만으로도 밀리미터 수준의 정확도를 요구하는 작업을 수행할 수 있으며, 이러한 작업을 단일 서드파티 카메라로만 수행할 수 있습니다.

### [Vivid-ZOO: Multi-View Video Generation with Diffusion Model](https://arxiv.org/abs/2406.08659)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08659.png)

Vote: 7

Authors: Peter Wonka, Bernard Ghanem, Biao Zhang, Jinjie Mai, Cheng Zheng, Wenxuan Zhu, Bing Li

- **What's New**: 최초로 텍스트를 기반으로 다중 시점 동영상을 생성하는 'Text-to-Multi-view-Video (T2MVid)' 확산 모델을 제안합니다. 이 연구는 다중 시점 이미지 및 2D 비디오 확산 모델을 결합하여 T2MVid 생성 모델을 구축하는 방법을 탐구합니다.
- **Technical Details**: 제안된 Vivid-ZOO 파이프라인은 사전 훈련된 다중 시점 이미지 확산 모델과 2D 시간 레이어를 효과적으로 연결하기 위해 3D-2D 정렬 레이어와 2D-3D 정렬 레이어를 도입합니다. 이를 통해 제한된 다중 시점 비디오 데이터셋으로도 효과적인 훈련이 가능합니다.
- **Performance Highlights**: 광범위한 실험 결과, 제안된 방법은 다양한 텍스트 프롬프트에서 높은 품질의 다중 시점 동영상을 효과적으로 생성할 수 있음을 보여줍니다. 이 논문에서 제시된 데이터셋은 향후 연구를 위한 중요한 자료가 될 것입니다.

### [AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis](https://arxiv.org/abs/2406.08920)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08920.png)

Vote: 6

Authors: Jiankang Deng, Diptesh Kanojia, Xiatian Zhu, Haosen Yang, Swapnil Bhosale

- **What's New**: 본 논문은 증강 현실(AR)과 가상 현실(VR)과 같은 실제 응용 분야에서 3D 장면의 시각적 렌더링만으로는 가상 환경에 완전히 몰입할 수 없음을 지적합니다. 이에 따라 최근 새로운 시점의 음향 합성(Novel View Acoustic Synthesis, NVAS)에 대한 연구가 증가하고 있습니다. 본 연구에서는 음향 지침 매개변수를 도입하여 음향 필드 네트워크를 학습하고, 이를 통해 방 전체의 장면과 재료 정보를 포함한 상황을 고려해 3D 장면에서의 음향 합성 조건을 효율적으로 학습하는 새로운 Audio-Visual Gaussian Splatting (AV-GS) 모델을 제안합니다.
- **Technical Details**: 기존 Neural Acoustic Field (NAF)와 AV-NeRF 모델의 한계를 극복하기 위해, 본 연구는 3D Gaussian Splatting (3DGS)을 확장하여 시각 장면에서 3D 오디오(공간 오디오)까지 커버합니다. 이 과정에서 음향 지침 매개변수를 통해 음향 필드 네트워크를 학습하며, 이는 물리적 기하학적 사전과 3DGS 표현을 분리하여 이루어집니다. 또한, Gaussian 포인트의 위치와 밀도를 조절하는 전략을 도입하여 전체 이중채널 오디오 합성을 개선합니다.
- **Performance Highlights**: 제안된 AV-GS 모델은 다양한 합성 및 실제 데이터셋에서 기존 방법들보다 우수한 성능을 보였습니다. 이는 전체적인 장면 기하학과 재료 정보를 고려한 새로운 형태의 음향 합성 방식이라는 점에서 차별화됩니다.

### [GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors](https://arxiv.org/abs/2406.10111)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10111.png)

Vote: 6

Authors: Xiqian Yu, Hanxin Zhu, Zhibo Chen, Tianyu He

- **What's New**: 이 연구는 고해상도 뷰 생성(high-resolution novel view synthesis, HRNVS) 문제를 해결하기 위해 GaussianSR을 제안합니다. 기존 3D Gaussian Splatting(3DGS)이 빠른 렌더링 속도를 자랑하지만, 낮은 해상도 입력에서 고해상도 출력을 생성하는 데 한계가 있었습니다. 이를 해결하기 위해 GaussianSR은 대규모 이미지 데이터를 학습한 2D 생성 사전(2D generative priors)을 사용하는 새로운 접근 방식을 도입했습니다.
- **Technical Details**: GaussianSR의 주요 아이디어는 2D diffusion priors을 활용해 HRNVS 성능을 향상시키는 것입니다. 이를 위해 DreamFusion에서 사용된 Score Distillation Sampling (SDS)을 기반으로 2D priors을 3DGS에 주입합니다. 그러나 SDS의 랜덤성이 불필요한 3D Gaussian primitives를 생성하는 문제를 일으킬 수 있기 때문에, 이 연구에서는 두 가지 완화 전략을 제안합니다: 첫째, diffusion timestep의 샘플링 범위를 축소하는 annealing 전략을 사용하고, 둘째, densification 과정에서 불필요한 primitives를 무작위로 제거합니다.
- **Performance Highlights**: 다양한 시나리오에서 실험한 결과, GaussianSR은 기존 최첨단 방법들보다 우수한 고해상도 뷰 생성을 달성했습니다. 이는 특히 낮은 해상도 입력만으로 고해상도 출력을 생성할 수 있어 기존의 NeRF 기반 방법보다 효율성이 높습니다.

### [MaskLID: Code-Switching Language Identification through Iterative Masking](https://arxiv.org/abs/2406.06263)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06263.png)

Vote: 5

Authors: Amir Hossein Kargaran, François Yvon, Hinrich Schütze

- **What's New**: 이 연구에서는 새로운 방법인 MaskLID를 소개하며, 이는 고품질의 문장 수준 LID(Language Identification)를 이용하여 코드 스위칭(Code-switching, CS) 세그먼트를 식별합니다. MaskLID는 지배적인 언어 텍스트 특징을 마스킹하여 추가 언어를 더 잘 인식할 수 있게 합니다.
- **Technical Details**: MaskLID는 두 개 이상의 언어가 동시에 사용되는 텍스트 세그먼트를 식별하기 위해 FastText 아키텍처를 기반으로 합니다. FastText는 오픈소스, 사용 용이성, 높은 성능 및 효율성으로 인해 널리 사용되는 LID 아키텍처로, 멀티노미얼 로지스틱 분류기를 통해 각 특징의 언어 기여도를 평가할 수 있습니다. MaskLID는 이러한 FastText 기반 LID 모델의 특징 마스킹을 통해 지배적인 언어 외의 다른 언어를 인식하는 데 도움을 줍니다.
- **Performance Highlights**: MaskLID는 대규모 웹 코퍼스에서 실세계 코드 스위칭 세그먼트를 발굴하는 데 적합합니다. 이 방법은 두 개 이상의 언어가 혼합된 세그먼트를 감지할 수 있으며, 터키어-영어 코드 스위칭 데이터셋에서 특히 유효성을 입증하였습니다. 또한 기존의 OpenLID와 Multi-label OpenLID 모델들보다 더 우수한 성능을 보여주었습니다.

### [Decoding the Diversity: A Review of the Indic AI Research Landscape](https://arxiv.org/abs/2406.09559)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09559.png)

Vote: 5

Authors: Vinija Jain, Tamoghna Roy, Aman Chadha, Sankalp KJ, Sreyoshi Bhaduri

- **What's New**: 이번 연구는 인도 대륙에서 주로 사용되는 인도 언어들에 대한 AI 연구 전반을 포괄적으로 조사한 최초의 리뷰로, 특히 인도 언어와 관련된 자연어 처리(NLP) 및 생성 모델(generative models) 연구의 발전 상황을 탐구합니다. Hindi, Bengali, Tamil 등 다양한 인도 언어들에 대한 연구 현황과 도전과제를 종합적으로 담고 있습니다.
- **Technical Details**: 이 연구는 Google Scholar, IEEE Xplore, ACL Anthology, arXiv 등 다양한 데이터베이스를 활용하여 'Indic languages', 'generative models', 'NLP'와 같은 키워드를 통해 관련 논문을 조사했습니다. 1000개 이상의 논문 제목과 초록을 검토하여 총 84개의 연구를 최종 리뷰에 포함시켰습니다. 포함 기준은 생성 애플리케이션에 중점을 둔 논문, 실증적 결과 또는 이론적 기여가 있는 논문입니다.
- **Performance Highlights**: 리뷰에 포함된 논문은 LLMs, Corpora, Benchmarks and Evaluation, Techniques, Tools and Applications와 같은 5개의 카테고리로 분류되었습니다. 중요한 연구 중 하나로는 Gyan AI Paramanu가 있으며, 이는 Bangla, Hindi, Sanskrit에서 GPT-3.5-Turbo와 같은 모델을 능가하는 성능을 보였습니다. BLOOM Workshop (2023)은 176 billion 파라미터의 다중언어 모델로, 다양한 자연어 처리 작업에서도 높은 성능을 입증했습니다.

