## Daily Papers (2024-06-18)

### [MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs](https://arxiv.org/abs/2406.11833)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11833.png)

Vote: 56

Authors: Zijian Liang, Yuhang Zang, Xilin Wei, Jiaqi Wang, Tao Chu, Xiaoyi Dong, Yu Qiao, Ziyu Liu, Yuanjun Xiong, Pan Zhang, Dahua Lin

- **What's New**: MMDU는 다중 이미지와 다중 턴 대화를 이해하는 능력을 평가하기 위해 고안된 종합적인 벤치마크입니다. 오픈 소스 Wikipedia에서 관련 이미지를 자동으로 선택하고 텍스트 설명을 이용해 구성된 데이터 파이프라인을 사용합니다.
- **Technical Details**: MMDU 벤치마크는 최대 20개의 이미지와 17개의 대화 턴을 포함하여, LVLMs가 장기간의 문맥 정보를 처리하고 이해할 수 있는 능력을 평가합니다. 복잡한 질문과 답변을 포함한 자유 형태의 멀티 턴 대화를 통해 LVLM의 성능을 평가합니다. 데이터를 수집할 때 클러스터링 알고리즘을 사용해 고품질의 이미지와 텍스트 세트를 구성합니다. GPT-4o를 사용해 멀티 턴 질문과 답변을 생성하고 사람 검토자들이 이를 평가하여 최종 데이터를 확보합니다.
- **Performance Highlights**: MMDU 벤치마크를 통해 15개의 LVLM을 평가한 결과, 오픈 소스 모델과 독점 모델 사이에 큰 성능 격차가 있음을 확인했습니다. 최고 오픈 소스 모델은 42.8%의 성능을 보인 반면, GPT-4o는 70.2%를 기록했습니다. 성능 격차를 줄이기 위해 MMDU-45k라는 대규모 튜닝 데이터셋을 추가로 선보였습니다. 이 데이터셋으로 InternLM-XC2의 성능이 14.5% 향상됨을 확인했습니다.

### [DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/abs/2406.11794)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11794.png)

Vote: 38

Authors: Mitchell Wortsman, Jeffrey Li, Georgios Smyrnis, Sedrick Keh, Alex Fang, Samir Gadre, Hritik Bansal, Niklas Muennighoff, Rui Xin, Jean Mercat, Alon Albalak, Marianna Nezhurina, Mayee Chen, Reinhard Heckel, Suchin Gururangan, +, Saurabh Garg, Maor Ivgi, Etash Guha, Yonatan Bitton, Matt Jordan, Kushal Arora, Amro Abbas

- **What's New**: 이 논문은 데이터 큐레이션(data curation)의 방법론에 대해 깊이 있는 분석을 제공하고 있습니다. 데이터 큐레이션은 성능 향상을 목표로 하는 방법과 비성능 관련 목표를 가진 방법으로 나뉩니다. 최근의 접근 방식은 사전 훈련된 언어 모델을 사용하여 다양한 차원의 품질을 판단하고 큐레이션하는 방식을 포함합니다.
- **Technical Details**: 언어 감지(language detection) 방법은 주로 fastText 분류기를 사용하며, 일부 방법은 나라 도메인 기반 웹 페이지 필터링을 사용합니다. 품질 필터링은 고품질 데이터셋(예: Wikipedia)에 대한 이진 분류기(binary classifier) 훈련 후 낮은 점수를 받은 데이터를 필터링하는 방식이 일반적입니다. 데이터 중복 제거(deduplication)는 URL, 해시 기반 또는 모델 기반 표현법을 사용하여 수행됩니다. 데이터 혼합(data mixing)은 데이터 출처별로 최적의 비율을 결정하는 것이 주요 과제입니다.
- **Performance Highlights**: 최근에는 사전 훈련된 언어 모델을 활용하여 높은 품질의 데이터를 큐레이션 하는 방법이 제안되어 주목받고 있습니다. Ankner 등은 125M 파라미터 모델을 사용하여 3B 모델의 훈련 데이터를 정제할 수 있음을 발견했습니다. MiniPile 연구는 클러스터링을 통해 저품질 클러스터를 제거하고도 작은 언어 모델이 GLUE 성능을 유지할 수 있음을 보여주었습니다.

### [mDPO: Conditional Preference Optimization for Multimodal Large Language Models](https://arxiv.org/abs/2406.11839)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11839.png)

Vote: 35

Authors: Sheng Zhang, Nan Xu, James Y. Huang, Wenxuan Zhou, Fei Wang, Muhao Chen, Hoifung Poon

- **What's New**: 최신 연구에 따르면, Direct Preference Optimization (DPO)이라는 방법이 대형 언어 모델(LLMs)을 인간의 선호도에 맞추는 데 있어 주도적인 방법으로 자리잡았습니다. 이러한 성공을 바탕으로 최근 연구들은 DPO를 멀티모달 시나리오로 확장하고자 시도하고 있지만, 이미지와 언어 데이터를 결합한 DPO는 일관된 성과를 보이지 않고 헛소리(hallucination) 문제를 악화시키기도 한다는 사실을 발견했습니다.
- **Technical Details**: 기존의 DPO는 주어진 질문 q에 대해 평가자가 선택한 응답이 거부된 응답보다 선호된다는 것을 학습하도록 모델을 최적화합니다. 멀티모달 DPO에서는 각 인스턴스에 이미지 m이 추가되며, 최적화 목표는 이미지와 질문에 기반하여 응답의 선호도를 최대화하는 것입니다. 그러나 이 연구는 멀티모달 DPO가 이론적 기대와 실질적 구현 사이에 체계적인 간격이 존재한다고 지적합니다.
- **Performance Highlights**: 제안된 mDPO는 기존 DPO의 한계를 극복하기 위해 이미지에 조건부 선호 최적화(conditional preference optimization)를 도입하고, 선택된 응답의 가능성을 유지하기 위해 리워드 앵커(reward anchor)를 포함합니다. 실험 결과, mDPO는 멀티모달 시나리오에서 표준 DPO보다 뛰어난 성과를 보였으며, 다양한 모델과 데이터 스케일에서 헛소리 문제를 효과적으로 줄였습니다. 상세한 분석을 통해 조건부 선호가 멀티모달 LLMs의 효과성을 높이는 데 중요한 역할을 한다는 것을 확인했습니다.

### [THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation](https://arxiv.org/abs/2406.10996)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10996.png)

Vote: 31

Authors: Keummin Ka, Seung-won Hwang, SeongHyeon Bae, Seo Hyun Kim, Taeyoon Kwon, Kai Tzu-iunn Ong, Namyoung Kim, Yohan Jo, Jinyoung Yeo, Dongha Lee

- **What's New**: 최근 대형 언어 모델(Large Language Models, LLMs)의 인상적인 성능으로 인해 여러 세션에 걸쳐 더 긴 대화를 이어가는 추세가 나타나고 있습니다. 이러한 장기적인 상호작용은 이전 이벤트나 화자 정보를 기억하고 이를 바탕으로 맞춤형 응답을 생성해야 합니다. 하지만 이는 메모리 회수 과정에서 정보 손실을 초래할 수 있습니다. 이를 해결하기 위해 본 연구에서는 장기 대화 응답 생성을 위한 'Theanine'이라는 시간대 기반 체계적 사고(chain-of-thought) 프레임워크를 제안합니다.
- **Technical Details**: Theanine은 메모리를 그래프로 관리하여 메모리 간의 연관성을 기반으로 연결하는 방식으로 작동합니다. 먼저, 최신 대화 세션에서 요약된 새로운 메모리를 기존 메모리 그래프에 연결합니다. 단순히 텍스트 유사성에 따라 메모리를 연결하는 대신, 대형 언어 모델을 활용하여 시간적 및 원인-결과 상관관계를 기반으로 메모리를 동적으로 연결합니다. 그리고 새로운 응답을 생성할 때, 현재 대화를 쿼리로 사용하여 상위-케이(top-k) 메모리만 회수하는 대신, 연관 이벤트의 발전을 나타내는 전체 메모리 타임라인을 회수합니다.
- **Performance Highlights**: Theanine 시스템은 LLM 기반(G-Eval) 및 인간 평가에서 보다 상세하고 비일반적인 응답을 생성함으로써 더 나은 성능을 입증했습니다. 또한, 과거 대화를 제대로 참조하는 효율성도 보여주었습니다. 평가를 위한 TeaFarm 및 TeaBag 데이터는 인간 개입 없이 시스템의 성능을 평가할 수 있도록 설계되었습니다.

### [How Do Large Language Models Acquire Factual Knowledge During Pretraining?](https://arxiv.org/abs/2406.11813)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11813.png)

Vote: 27

Authors: Sohee Yang, Seonghyeon Ye, Hoyeon Chang, Du-Seong Chang, Minjoon Seo, Youngkyung Seo, Jinho Park

- **What's New**: 이 연구는 대형 언어 모델(LLM)의 사전 학습 중에서 사실 지식(factual knowledge)을 어떻게 획득하고 잊어버리는지를 분석하려는 초기 시도입니다. 기존 연구는 LLM이 사전 학습 데이터로부터 상당한 양의 사실 지식을 포착할 수 있다는 것을 보여주었지만, 그 메커니즘에 대한 이해는 부족했습니다. 이 연구는 지식을 주입하는 시나리오, 사전 학습 단계, 모델 크기 및 배치 크기를 변화시키면서 LLM이 사실 지식을 어떻게 기억하고 일반화하는지를 분석합니다.
- **Technical Details**: 이 연구에서는 중간 사전 학습 체크포인트를 사용하여 목표 지식을 이전에 접한 적 없는 LLM에 주입하고, 다양한 조건 하에서 사실 지식 획득의 단계별 진행 상황을 모니터링했습니다. Fictional Knowledge 데이터셋을 사용하여 현실적이지만 허구적인 엔터티 내용을 포함하는 문장을 주입했고, 이를 통해 기억과 일반화의 역학을 조사했습니다. 세 가지 깊이 수준에서 지식 획득을 분류했으며, 각 주입된 지식에 대해 클로즈 태스크(cloze task)를 사용하여 분석했습니다.
- **Performance Highlights**: 첫째, 모델 업데이트에 의해 유도된 작은 확률 증가를 통해 사실 지식 획득이 발생한다는 것을 발견했습니다. 둘째, 나중 단계의 체크포인트는 초기 단계와 비교하여 효과성에 큰 차이를 보이지 않았고, 7B 모델이 1B 모델보다 더 높은 효과성을 보였습니다. 셋째, 학습 단계 또는 토큰과 사실 지식의 망각 사이에 멱법칙 관계가 있음을 발견했습니다. 추가적으로, 훈련 데이터의 중복 제거와 큰 배치 크기를 사용하면 사실 지식 획득이 강화되어 잊어버리는 것에 대해 더 견고해진다는 것을 발견했습니다.

### [MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers](https://arxiv.org/abs/2406.10163)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10163.png)

Vote: 23

Authors: Gang Yu, Chi Zhang, Zhongang Cai, Weicai Ye, Sijin Chen, Guosheng Lin, Di Huang, Tong He, Yiwen Chen, Lei Yang, Xin Chen, Jiaxiang Tang

- **What's New**: 최근 몇 년간 3D 커뮤니티에서는 자동으로 고품질 3D 자산을 생성하는 다양한 방법이 개발되며 급격한 발전을 이루었습니다. 다양한 3D 재현 및 생성 기법들은 손으로 만든 모델에 필적하는 품질을 보이나, 효율성과 제어 가능성면에서 우수한 메쉬 기반 파이프라인에 크게 의존하는 현재의 3D 산업에서 완전한 잠재력을 발휘하지는 못하고 있습니다. 이에 따라 기존의 3D 재현을 메쉬로 변환하는 데 많은 노력과 일부 성공이 있었으나, 생성된 메쉬의 토폴로지 품질이 여전히 부족합니다. 본 연구는 이러한 문제를 해결하기 위해 자동 생성된 3D 자산이 산업에 적용될 수 있도록 지원하는 것을 목표로 합니다.
- **Technical Details**: 기존의 방법들은 지나치게 조밀한 면을 가진 메쉬를 재구성하는 방식으로 여러 문제를 해결하지 못했기 때문에, 우리는 메쉬 추출을 생성 문제로 처음으로 규정하였습니다. 본 연구에서는 Shape-Conditioned AM Generation이라는 새로운 설정을 도입하여, 주어진 3D 자산과 정렬된 Artist-Created Meshes (AMs)를 생성하는 모델을 학습시킵니다. 모델 학습을 위해 다양한 3D 표현에서 파생된 형상 조건과 AMs를 포함하는 데이터셋을 구축하고, VQ-VAE를 사용한 메쉬 보캐블러리 학습 및 Transformer 기반 디코더를 통한 메쉬 생성 등 여러 최신 기법을 적용하였습니다. 또한, 노이즈 저항성이 있는 디코더를 개발하여 메쉬 생성 품질을 향상시켰습니다.
- **Performance Highlights**: 우리의 모델인 MeshAnything은 다양한 3D 표현에서 AMs로 변환할 수 있으며, 이를 통해 3D 산업에서의 적용을 촉진합니다. 우리의 광범위한 실험 결과, MeshAnything은 기존 방법들에 비해 훨씬 적은 면과 보다 정교한 토폴로지를 가진 AMs를 생성하며, 정확성 측면에서도 유사한 개선을 이루었습니다.

### [A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache Compression](https://arxiv.org/abs/2406.11430)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11430.png)

Vote: 21

Authors: Yu Zhao, Alessio Devoto, Simone Scardapane, Pasquale Minervini

- **What's New**: 로그 언어 모델(large language models, LLMs)의 긴 문맥 관리를 위해 새로운 KV 캐시(KV cache) 압축 방안이 제안되었습니다. 이는 특히 기계 학습에서 키-값 쌍의 관리를 용이하게 합니다.
- **Technical Details**: 제안된 방법은 캐시된 키(key)의 L2 정규화 값(L2 subscript norm)과 주의(attention) 점수와의 높은 상관관계를 이용해 단순하고 효과적으로 KV 캐시를 압축합니다. L2 정규화 값이 낮은 키와 해당하는 값을 메모리에 유지하는 방식을 사용하여 모델 구조나 추가 훈련 없이도 적용할 수 있습니다.
- **Performance Highlights**: 실험 결과, 이 휴리스틱 방법은 언어 모델링 작업에서 모델 성능을 유지하면서 가장 중요한 정보를 저장하고 검색하는 작업에서도 효율적으로 작동함을 보였습니다. 예를 들어, 주목받는 토큰들이 적은 L2 정규화 값과 높은 주의 점수를 가지는 것이 관찰되었습니다.

### [VideoLLM-online: Online Video Large Language Model for Streaming Video](https://arxiv.org/abs/2406.11816)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/642435a1a3adbc7142c3b0a6/EBmQ7LnfdTdyuhSUti0-d.png)

Vote: 20

Authors: Mike Zheng Shou, Chenan Song, Joya Chen, Jia-Wei Liu, Ziteng Gao, Shiwei Wu, Kevin Qinghong Lin, Dongxing Mao, Difei Gao, Zhaoyang Lv

- **What's New**: 항상 켜져 있는 상황별 AI 비서의 미래를 구축하는 것은 AI 연구의 '성배' 중 하나입니다. 최신 대형 언어 모델(LLMs)과 대형 멀티모달 모델(LMMs)의 발전은 비전-언어 대화, 공간 이해, 다양한 형태의 입력 처리 등과 같은 놀라운 기능을 보여주고 있습니다. 그러나, 현재의 최첨단 AI 어시스턴트는 여전히 실시간 비디오 스트리밍 환경에서 효과적인 도움을 제공하는 데 한계가 있습니다.
- **Technical Details**: LIVE (Learning-In-Video-strEam)라는 새로운 프레임워크를 제시하여, LMMs가 실시간 비디오 스트리밍에서 일관된 응답을 제공하고, 장시간의 컨텍스트를 다룰 수 있으며, 실시간 응용 프로그램에서 효율적으로 실행될 수 있도록 합니다. 이는 실시간 비디오 스트리밍 대화를 가능하게 하는 Streaming EOS (End-Of-Sequence) 예측을 도입함으로써 달성합니다. 또한, 빠른 영상 인코딩과 느린 언어 디코딩을 병렬 처리하여 병목 현상을 방지하고, 연속적인 키-값 캐싱을 통해 추론 효율성을 향상시킵니다.
- **Performance Highlights**: LIVE 프레임워크를 통해 개발된 모델은 CLIP 비전 인코더와 Llama-2/Llama-3 언어 모델을 기반으로 구축되었습니다. 실험 결과, 동시 프레임 당 10 FPS 이상의 속도와 20GB 이하의 메모리 비용으로 5분 이상의 비디오 스트리밍 대화를 지속할 수 있는 높은 효율성을 보여주었습니다. 또한, COIN과 Ego4D LTA 벤치마크에서의 활동 인식 및 예측 등 여러 오프라인 벤치마크에서 최첨단 결과를 달성하여 미래의 실세계를 위한 활용 가능성을 입증했습니다.

### [Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models](https://arxiv.org/abs/2406.11831)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11831.png)

Vote: 18

Authors: Yu Liu, Bingqi Ma, Guanglu Song, Hongsheng Li, Zhuofan Zong

- **What's New**: diffusion 확률 모델은 이미지 합성에서 큰 발전을 이루었으며, CLIP 텍스트 인코더 및 T5 시리즈와 같은 강력한 프롬프트 인코더의 도움으로 DALL-E 3와 Stable Diffusion 3 모델은 텍스트-이미지 프롬프트 이해 능력을 크게 향상시켰습니다. 최근에는 GPT의 성공에 고무된 디코더-온리(Decoder-Only) 대형 언어 모델(LLM)이 등장하여 더 강력한 텍스트 이해 능력을 입증했습니다. 하지만 이런 LLM을 효과적으로 diffusion 모델에 활용하는 방법은 아직 탐구 중입니다.
- **Technical Details**: 본 연구는 transformer 기반 diffusion 모델(DiT)을 이용하여 Diffusion 모델에서 LLM의 고유 특성을 실험적으로 분석합니다. LLaMA3-8B와 T5-XXL 등의 모델을 비교 평가한 결과, 디코더-온리 LLM은 프롬프트 인코딩에서 상위 모델들에 비해 성능이 떨어지는 것으로 나타났습니다. 이를 해결하기 위해 LLM이 프롬프트를 정확히 이해하고 효과적으로 활용할 수 있도록 LLM-infused Diffuser라는 새로운 프레임워크를 제안했습니다. 이 프레임워크는 명시적인 지시(instruction)를 추가하고, 인적 지시를 통한 LLM의 언어 모델을 활용하며, 언어 토큰 리파이너(linguistic token refiner)를 제안하여 위치적 편향을 보정합니다.
- **Performance Highlights**: LLM-infused Diffuser는 다양한 모델 크기와 데이터 크기에서 탁월한 성능을 보였습니다. Transformer 아키텍처의 탁월한 성능을 기반으로 LLM-infused Diffusion Transformer (LI-DiT)를 설계하여, Stable Diffusion 3, DALL-E 3, Midjourney V6 등 최신 오픈 소스 및 상용 모델을 능가하는 프롬프트 이해 성능을 보여주었습니다.

### [GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities](https://arxiv.org/abs/2406.11768)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11768.png)

Vote: 18

Authors: Utkarsh Tyagi, S Sakshi, Chandra Kiran Reddy Evuru, Ashish Seth, Dinesh Manocha, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, Sonal Kumar

- **What's New**: 이번 연구에서는 GAMA라는 새로운 Large Audio Language Model (LALM)을 소개합니다. 이 모델은 향상된 오디오 이해 및 복합적 추론 능력을 갖추고 있으며, 다양한 오디오 특성을 인코딩하는 여러 유형의 오디오 특징을 통합합니다.
- **Technical Details**: GAMA는 Audio Q-Former와 Audio Spectrogram Transformer (AST)를 결합하여 다양한 오디오 정보 측면을 인코딩합니다. AST는 추가적으로 집합 모듈이 장착되어 있으며, 이는 여러 층에서 추출한 특징들을 통합하여 모델에 다양한 지식을 전달합니다. 이러한 특징들은 MLP 레이어를 통해 워드 임베딩 공간으로 연결되며, 모델의 전반적인 오디오 이해 능력을 향상시킵니다.
- **Performance Highlights**: GAMA는 대규모 오디오-언어 코퍼스에서 처음으로 미세 조정되었으며, 표준 오디오 및 음악 이해 벤치마크에서 다른 모든 모델을 능가하는 성능을 보여줍니다. 또한, CompA-R-test에서 복합적 추론 능력을 평가한 결과, 기존의 여러 기준 모델들보다 뛰어난 성능을 보였습니다.

### [LLaNA: Large Language and NeRF Assistant](https://arxiv.org/abs/2406.11840)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11840.png)

Vote: 17

Authors: Luigi Di Stefano, Samuele Salti, Giuseppe Lisanti, Andrea Amaduzzi, Pierluigi Zama Ramirez

- **What's New**: 최근 자연어 처리(Natural Language Processing) 분야에서 놀라운 성과를 보여준 대형 언어 모델(LLM, Large Language Models)이 여러 모드를 포함하는 멀티모달 대형 언어 모델(MLLM, Multimodal Large Language Models)로 발전하고 있습니다. 본 연구에서는 처음으로 Neural Radiance Fields(NeRFs)를 직접 받아들이는 MLLM인 LLaNA(Large Language and NeRF Assistant)을 소개합니다.
- **Technical Details**: NeRFs는 좌표 기반 신경망으로, 주로 MLP(Multi-Layer Perceptrons)를 사용하여 3D 공간의 각 위치에서 연속적인 방사장(radiance field)를 학습해 객체의 기하학 및 사진 실감 외형을 포착합니다. 본 연구에서는 기존의 이미지나 3D 점 클라우드로 변환하지 않고, NeRF의 가중치를 메타-네트워크(meta-network) 인코더로 직접 처리해 LLM의 임베딩 공간으로 투영하는 방법을 사용했습니다. 이를 통해 NeRF 캡션 생성, Q&A, 제로샷 분류 등의 작업을 수행할 수 있습니다.
- **Performance Highlights**: 새로운 NeRF-언어 데이터세트를 만들어 LLaNA의 성능을 벤치마크했으며, 기존의 MLLM들이 이미지나 3D 데이터를 사용하는 방법에 비해 높은 성능을 입증했습니다. 특히, 잘못된 시점에서 렌더링하는 경우 중요 정보가 손실되는 문제를 해결하며, MLP 가중치를 직접 처리함으로써 더 많은 정보를 유지하고 빠르게 작업을 수행할 수 있습니다.

### [From Pixels to Prose: A Large Dataset of Dense Image Captions](https://arxiv.org/abs/2406.10328)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10328.png)

Vote: 16

Authors: Heng Huang, Tom Goldstein, Abhinav Bhatele, Mayuka Jayawardhana, Reza Shirkavand, Gowthami Somepalli, Kaiyu Yue, Alireza Ganjdanesh, Vasu Singla, Sukriti Paul

- **What's New**: PixelProse는 기존의 비전-언어 데이터셋의 한계를 해결하기 위해 고안된 새로운 데이터셋입니다. 이 데이터셋은 alt-text 대신 고도로 상세한 설명을 포함하며, 다목적 이미지 설명을 제공합니다. 또한, 생성적 대규모 언어 모델(LLM)을 사용하여 텍스트 라벨의 빠른 재구성이 가능합니다.
- **Technical Details**: PixelProse 데이터셋은 Google Gemini 1.0 Pro Vision Model을 사용하여 생성된 캡션과 함께 제공되는 16M 이상의 다양한 이미지를 포함합니다. 데이터 소스에는 CommonPool, CC12M, RedCaps 등이 있으며, 이들 이미지는 웹 크롤링을 통해 수집되었습니다. 각 데이터 소스는 이미지 품질 및 큐레이팅 정도가 다릅니다. 예를 들어, CommonPool은 다양성이 높지만 품질이 낮은 반면, RedCaps는 인간이 엄격하게 큐레이팅한 고품질 이미지로 구성됩니다. 캡션 생성에는 다섯 가지 고유한 프롬프트를 사용하여 다양성을 확보하며, 부정적 설명을 포함하여 특정 객체의 부재를 명확히 설명합니다. 텍스트 인식 정확도를 보장하기 위해 일부 이미지를 수동으로 점검하였습니다.
- **Performance Highlights**: PixelProse는 이미지 속 텍스트를 식별하는 캡션 요소를 포함하여 비전-언어 모델(예: VLM 및 확산 모델)의 성능 향상을 지원합니다. 운이 좋게도 76%의 이미지에서 캡션 내 텍스트를 정확히 인식하였습니다. 그러나 예술적 글꼴이나 매우 임의적인 형태와 같은 도전적인 경우에서는 인식 실패가 발생했습니다. 또한, CSAM(아동 성 학대 자료) 등의 유해 콘텐츠 필터링을 위해 Microsoft의 PhotoDNA API를 비롯한 다양한 상업용 API를 사용하여 데이터셋의 안전성과 무결성을 보장하였습니다.

### [In-Context Editing: Learning Knowledge from Self-Induced Distributions](https://arxiv.org/abs/2406.11194)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11194.png)

Vote: 14

Authors: Kailin Jiang, Bangcheng Yang, Yifan Zhong, Zilong Zheng, Yaodong Yang, Jiaqi Li, Xiaobo Wang, Siyuan Qi

- **What's New**: LLM의 최신 정보 반영 문제를 해결하기 위해 새로운 접근법인 Consistent In-Context Editing (ICE)를 제안합니다. ICE는 모델을 다시 학습시키는 대신, 특정 질의에 대한 컨텍스트를 활용해 자연스럽게 모델의 지식을 업데이트합니다.
- **Technical Details**: ICE는 기존의 파인 튜닝(fine-tuning) 방식과 달리 단일 결과물이 아닌 분포(distribution)를 학습합니다. 모델이 특정 컨텍스트를 포함할 때와 포함하지 않을 때의 출력 차이를 최소화하는 방식으로 동작하며, 상위 분포를 유지하면서도 새로운 지식이 효과적으로 반영되도록 합니다. 이를 통해 오버피팅을 방지하고 모델의 기존 지식 정확도를 유지합니다.
- **Performance Highlights**: 다섯 가지 데이터셋을 대상으로 한 실험에서 ICE는 전통적인 방법보다 높은 정확도, 지역성, 일반화, 언어 품질 면에서 우수한 성능을 보였습니다. 특히 지속적인 편집 시나리오에서도 우수한 성능을 나타내, 기존 지식의 보존과 새로운 정보의 원활한 통합이 가능함을 입증했습니다.

### [WPO: Enhancing RLHF with Weighted Preference Optimization](https://arxiv.org/abs/2406.11827)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11827.png)

Vote: 13

Authors: Sathish Reddy Indurthi, Sanqiang Zhao, Wenxuan Zhou, Kaiqiang Song, Shujian Zhang, Chenguang Zhu, Ravi Agrawal, Silei Xu

- **What's New**: 대형 언어 모델(LLMs)은 사람과 유사한 응답을 생성하는 뛰어난 능력을 보여주었으나, 높은 신뢰성, 안전성, 윤리적 기준이 요구되는 상황에서는 여전히 도전에 직면하고 있습니다. 이러한 문제를 해결하기 위해 인간 피드백을 통한 강화 학습(RLHF) 방법이 제안되었습니다. 이번 논문에서는 오프-폴리시(off-policy) 데이터의 효율성을 활용하며 온-폴리시(on-policy) 성능을 모방하는 새로운 방법을 제안합니다.
- **Technical Details**: RLHF는 현재의 정책 모델과 다른 모델로부터 출력된 데이터를 사용해서 정책 모델을 최적화합니다. 제안된 방법은 기존의 선호 데이터셋을 부트스트래핑하여 새로운 선호 데이터셋으로 재샘플링하고, 이를 통해 'Weighted Policy Optimization(WPO)' 목표를 사용하여 최적화합니다. WPO는 출력의 확률에 따라 선호 쌍을 재가중치하여 분포 차이를 줄이고 최적화 효과를 높입니다.
- **Performance Highlights**: WPO는 Alpaca Eval 2와 MT-bench 같은 지시 사항 준수 벤치마크에서 평가되었습니다. 오프-폴리시 설정에서 GPT-4-turbo에 비해 최대 14.9%의 길이 제어 승률 향상을 달성했으며, DPO를 최대 5.6% 능가했습니다. 특히, 하이브리드 RL 설정에서는 Alpaca Eval 2에서 48.6%의 새로운 SOTA(length-controlled winning rate)를 달성하여 현재까지 가장 강력한 8B 모델이 되었습니다.

### [Pandora: Towards General World Model with Natural Language Actions and Video States](https://arxiv.org/abs/2406.09455)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09455.png)

Vote: 12

Authors: Guangyi Liu, Tianhua Tao, Eric P. Xing, Yi Gu, Qiyue Gao, Yuheng Zha, Yemin Shi, Zhengzhong Liu, Zhiting Hu, Shibo Hao, Zeyu Feng, Yuting Ning, Jiannan Xiang

- **What's New**: 이 연구는 다양한 도메인에서 비디오 생성과 실시간 제어를 가능하게 하는 일반화된 월드 모델(World Model; WM)인 \\(*X\\)를 소개합니다. 이 모델은 자연어로 표현된 임의의 행동(action)을 입력받아 다음 세계 상태를 예측하는 자가회귀 모델(autoregressive model)입니다.
- **Technical Details**: \\(*X\\)는 대규모 비디오 및 텍스트 데이터로 사전 훈련된 후, 고품질 텍스트-비디오 시퀀스 데이터로 훈련된 자가회귀 모델입니다. 이를 통해 다양한 도메인의 비디오와 자연어 행동을 학습함으로써 언제든지 제어가 가능한 비디오 생성이 가능합니다. 모델의 백본(backbone)으로 Vicuna-7B-v1.5 언어 모델과 DynamiCrafter 텍스트-비디오 모델을 사용합니다. 훈련 중 추가된 비전 인코더(vision encoder)와 질의 임베딩(query embeddings)은 모델이 비디오 생성 과정을 시작하도록 자극합니다.
- **Performance Highlights**: 모델은 실시간 제어와 긴 비디오 생성을 가능하게 하며, 행동의 학습 후 도메인 전이(domain transfer)가 원활합니다. \\(*X\\)는 기존 비디오 생성 모델이 가진 제한을 넘어 더 긴 비디오를 고품질로 생성할 수 있습니다.

### [WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences](https://arxiv.org/abs/2406.11069)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11069.png)

Vote: 11

Authors: Yejin Choi, William Yang Wang, Bill Yuchen Lin, Dongfu Jiang, Wenhu Chen, Yujie Lu

- **What's New**: WildVision-Arena의 사용자 인터페이스가 공개되었습니다. 이 플랫폼은 멀티모달 대형 언어 모델을 평가할 수 있는 인터랙티브 환경을 제공하며, 사용자는 질문을 입력하고 여러 모델의 답변을 동시에 비교할 수 있습니다.
- **Technical Details**: WildVision-Arena의 인터페이스는 질문과 관련된 특정 이미지나 작업에 대해 각 모델의 답변을 나란히 표시합니다. 사용자 인터페이스는 모델의 성능과 기능을 간편하게 비교할 수 있도록 설계되었으며, 답변 선택 및 투표 기능을 통해 사용자들이 모델의 출력을 효과적으로 판단하고 개선할 수 있습니다.
- **Performance Highlights**: 표 6에서 8에는 WildVision-Arena 사용자들이 각 이미지 도메인과 질문 카테고리별로 예제 데이터를 제시하고 있습니다.

### [L4GM: Large 4D Gaussian Reconstruction Model](https://arxiv.org/abs/2406.10324)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10324.png)

Vote: 11

Authors: Xiaohui Zeng, Karsten Kreis, Jiawei Ren, Sanja Fidler, Ziwei Liu, Antonio Torralba, Seung Wook Kim, Ashkan Mirzaei, Huan Ling, Kevin Xie, Hanxue Liang

- **What's New**: 3D 가상 세계에 생명을 불어넣기 위한 필수 요소인 애니메이션 3D 자산을 생성하는 새로운 접근법으로, 단안 비디오(monocular video)나 텍스트로부터 애니메이션을 자동으로 생성하는 L4GM 모델을 선보입니다. 이는 단일 이미지 3D 복원(Single-image 3D reconstruction)을 기반으로 한 빠르고 높은 품질의 4D 모델링을 목표로 합니다. L4GM 모델은 새로운 대규모 데이터셋을 활용하여 다중 보기 비디오(multiview video)로부터 일관된 3D 표현을 학습합니다.
- **Technical Details**: L4GM은 3D Gaussians의 시퀀스를 단안 비디오 입력으로부터 피드포워드(feed-forward) 방식으로 재구성하는 데 중점을 둡니다. 이는 LGM 모델을 확장하여 다중 프레임 입력을 받아 각 프레임에 대하여 3D Gaussian 표현을 출력하도록 개선되었습니다. 중요한 요소는 개별 프레임 사이에 시간적 자기 주의(temporal self-attention) 레이어를 추가하여 일관된 3D 표현을 학습하는 것입니다. 덧붙여, 1fps로 훈련된 모델을 더 높은 fps로 업샘플링(up-sampling)하기 위해 보간(interpolation) 모델을 사용합니다. 우리의 대규모 데이터셋은 Objaverse 1.0에서 생성된 1200만 개의 렌더된 애니메이션 3D 오브젝트의 다중 뷰 비디오로 구성되어 있습니다.
- **Performance Highlights**: L4GM은 합성 데이터에서만 훈련되었음에도 불구하고 실제 월드 비디오에 대해 뛰어난 일반화 성능을 보입니다. 예를 들어, Sora가 생성한 비디오와 ActivityNet의 실제 비디오에서도 높은 품질을 유지합니다. 단안 비디오를 4D로 변환하는 벤치마크에서, L4GM은 다른 접근법에 비해 질적으로 우수하면서도 100배에서 1,000배 빠릅니다.

### [MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens](https://arxiv.org/abs/2406.11271)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11271.png)

Vote: 10

Authors: Ludwig Schmidt, Oscar Lo, Yejin Choi, Sheng Shen, Le Xue, Mohamed Awadalla, Silvio Savarese, Anas Awadalla, Caiming Xiong, Ran Xu, Manli Shu, Hannah Lee, Matt Jordan, Etash Kumar Guha

- **What's New**: MINT-1T (Multimodal INTerleaved) 데이터셋이 공개되었습니다. 이는 현재까지 가장 큰 오픈소스 멀티모달 인터리브드 데이터셋으로, 1조 개의 텍스트 토큰과 30억 개의 이미지를 포함하고 있습니다.
- **Technical Details**: MINT-1T 데이터셋은 HTML, PDF, ArXiv와 같은 다양한 소스로부터 데이터를 수집했습니다. PDF와 HTML 데이터의 경우, PDF 링크 추출, URL 중복 제거, 텍스트 블록 바운딩 박스 기반 클러스터링을 통한 읽기 순서 추출 등의 방법을 사용했습니다. 또한, 기존 OBELICS 방식처럼WARC 파일에서 DOM 트리를 파싱한 후, 이미지를 너무 많이 포함하거나 부적절한 URL을 가진 도큐먼트를 필터링했습니다. NSFW(성인용) 콘텐츠 및 중복 데이터를 걸러내기 위해 다양한 필터링 기법이 적용되었습니다.
- **Performance Highlights**: MINT-1T를 활용한 LMM(대형 멀티모달 모델)은 기존의 최고의 오픈소스 데이터셋인 OBELICS를 학습한 모델과 비교했을 때 성능 면에서 동일하거나 그 이상을 보여주었습니다. 또한 MINT-1T는 데이터의 스케일면에서 10배의 증가를 제공하며, 이는 더 다양한 고품질 데이터셋을 확보했음을 의미합니다.

### [Vid3D: Synthesis of Dynamic 3D Scenes using 2D Video Diffusion](https://arxiv.org/abs/2406.11196)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11196.png)

Vote: 8

Authors: Rishab Parthasarathy, Zack Ankner, Aaron Gokaslan

- **What's New**: 이번 연구에서는 고품질 3D 비디오를 효율적으로 생성하는 도전에 대해 다룹니다. Vid3D라는 새로운 엔드 투 엔드 파이프라인을 제안하며, 이는 단일 참조 이미지에서 동적 3D 장면을 합성할 수 있습니다.
- **Technical Details**: Vid3D는 다음과 같은 세 가지 단계로 3D 비디오를 생성합니다: (1) 3D 비디오의 2D 비디오 윤곽 생성, (2) 시드 비디오의 각 시간 단계별로 다중 뷰 생성 (multiview generation), (3) 각 프레임의 3D 표현 생성. Vid3D의 주요 기술적 특징은 3D 시간적 일관성을 명시적으로 모델링하지 않으며, Gaussian Splat 기법과 Stable Video Diffusion 모델을 활용합니다. 또한, 다중 뷰 생성을 위해 Objaverse 데이터셋을 활용한 모델을 사용합니다.
- **Performance Highlights**: Vid3D는 2D 비디오 모델 프라이어(prior)를 통해 시간적 일관성을 확보하면서도 높은 품질의 3D 비디오를 생성할 수 있음을 보여줍니다. 비록 Vid3D가 DreamGaussian4D보다는 낮은 CLIP-I 점수를 기록하였지만, Animate124보다 높은 성능을 보였습니다. 이는 3D 시간적 역학을 명시적으로 모델링하지 않더라도 경쟁력 있는 성능을 발휘할 수 있음을 시사합니다. 추가적으로, 다양한 시점에서의 뷰 수에 따른 클립 I 점수 변화를 실험하였고, 뷰 수가 증가할수록 점수가 높아짐을 확인했습니다.

### [Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning](https://arxiv.org/abs/2406.10522)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10522.png)

Vote: 7

Authors: Timothy Rogers, Lalit Jain, Jifan Zhang, Jiayi Chen, Siddharth Suresh, Scott Sievert, Kuan Lok Zhou, Kevin Jamieson, Yang Guo, Andrew Wagenmaker, Robert Nowak, Robert Mankoff

- **What's New**: 이 논문은 대규모 언어 모델(Large Language Models, LLMs)에서의 정렬 문제를 탐구하기 위해 설계된 데이터셋과 벤치마크를 소개합니다. 이 데이터셋은 뉴요커(The New Yorker)의 만화 캡션 콘테스트에서 수집된 2억 5천만 개 이상의 인간 평가를 포함하고 있습니다. 유머 감지 및 생성에서 LLMs의 성능을 평가하기 위해 고안된 이 벤치마크는 AI 정렬 문제를 해결하는 데 중요한 일조를 합니다.
- **Technical Details**: 데이터셋은 뉴요커의 만화 캡션 콘테스트에서 인간이 생성한 캡션에 대해 매주 수집된 대규모 크라우드소싱 평가 점수로 구성되어 있으며, 유머 생성의 복잡성을 파고드는 기회를 제공합니다. 또한, GPT와 같은 첨단 시스템을 활용하여 유머의 품질을 평가하는 새로운 메트릭스를 도입했습니다. 이를 통해 AI가 생성한 유머 콘텐츠를 평가하는 표준화된 프레임워크를 제공합니다.
- **Performance Highlights**: 새로운 벤치마크를 사용하여 GPT4o와 Claude와 같은 SOTA 모델들이 인간이 생성한 유머와 비교하여 얼마나 잘 수행하는지를 평가했습니다. 이를 통해 LLMs의 현재 성능과 제한점을 이해하고, AI 시스템이 유머를 생성하는 영역에서 우수한 성능을 발휘할 수 있는지 평가합니다. 또한, 강화 학습(인간 피드백을 통한 강화학습, RLHF), 직접 선호 최적화(DPO) 및 Best-of-N 샘플링(BoN) 등의 정렬 전략의 효과를 평가하여 모델 정렬에 대한 통찰을 제공했습니다.

### [Task Me Anything](https://arxiv.org/abs/2406.11775)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11775.png)

Vote: 7

Authors: Oscar Michel, Zixian Ma, Aniruddha Kembhavi, Tanmay Gupta, Ranjay Krishna, Dong He, Wei-Chiu Ma, Ali Farhadi, Weikai Huang, Jieyu Zhang

- **What's New**: Task-Me-Anything(맞춤형 벤치마크 생성 엔진)을 도입합니다. 이는 사용자의 질문에 따라 맞춤형 벤치마크를 큐레이션합니다.
- **Technical Details**: Task-Me-Anything는 확장 가능한 분류 체계(Taxonomy)와 관련된 시각 재산(이미지, 3D 객체, 비디오 등)을 사용합니다. 벤치마크 생성 프로세스를 통해 프로그램적 생성(Task Generation)이 가능하며, 이는 기존의 CLEVR나 GQA와 유사합니다. 사용자는 연산 예산에 따라 모델 성능을 예측할 수 있습니다.
- **Performance Highlights**: Task-Me-Anything는 1백만 개 이상의 작업 인스턴스에서 13개의 오픈소스 MLM을, 8,400개 작업 인스턴스에서 18개의 오픈소스/독점 MLM을 평가했습니다. 대표적인 결과는 오픈소스 MLM이 객체 및 속성 인식에서 강한 성능을 보이며, 개별 모델은 공간적 관계 이해 등 특정 강점을 지닌다는 것입니다. 일부 대형 MLM이 소형 MLM보다 성능이 우수하지만, 특정 상황에서는 예외가 존재했습니다.

### [Unifying Multimodal Retrieval via Document Screenshot Embedding](https://arxiv.org/abs/2406.11251)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11251.png)

Vote: 6

Authors: Xueguang Ma, Minghan Li, Jimmy Lin, Wenhu Chen, Sheng-Chieh Lin

- **What's New**: 문서 검색 시스템에서 서로 다른 형식과 모달리티를 통합하여 처리하는 새로운 패러다임인 'Document Screenshot Embedding(DSE)'를 도입했습니다. 이 새로운 패러다임은 문서의 스크린샷을 사용하여 일관된 문서 인코딩과 인덱싱을 제공하며, 이를 통해 더 정확한 검색 결과를 도출합니다.
- **Technical Details**: DSE는 문서의 스크린샷을 직접 대형 비전-언어 모델을 사용하여 밀집 표현(dense representation)으로 인코딩합니다. 검색 시 사용자의 쿼리는 언어 모델을 통해 인코딩되며, 가장 가까운 문서 임베딩을 찾습니다. 우리는 텍스트 중심 및 텍스트-이미지 혼합 된 문서 검색 환경에서 실험을 진행했습니다. 텍스트 중심 실험에서 130만 개의 위키피디아 페이지 스크린샷을 사용했고, NQ 데이터셋의 질문에 대해 모델을 세밀하게 조정했습니다. 텍스트-이미지 혼합 실험에서는 SlideVQA 데이터셋을 사용했습니다.
- **Performance Highlights**: 실험 결과, DSE는 전통적인 텍스트 기반 검색 방법인 BM25보다 상위 1 검색 정확도에서 17포인트 높은 성능을 보였습니다. SlideVQA 데이터셋 기반 실험에서는 OCR을 사용하는 모든 텍스트 기반 검색 방법보다 nDCG@10에서 15포인트 이상 뛰어난 성능을 보였습니다.

### [Just How Flexible are Neural Networks in Practice?](https://arxiv.org/abs/2406.11463)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11463.png)

Vote: 6

Authors: Micah Goldblum, Yann LeCun, Arpit Bansal, Andrew Gordon Wilson, Ravid Shwartz-Ziv, C. Bayan Bruss

- **What's New**: 이 논문은 실제 훈련 루프에서 모델이 데이터를 맞출 수 있는 능력을 측정하고, 네트워크 아키텍처와 최적화기(optimizers)의 효과를 분석합니다. 기존의 이론은 뉴럴 네트워크가 무한 도메인에서 특정 함수 클래스를 근사하기 위해 필요한 파라미터나 숨겨진 유닛 수를 제한하는 것을 다루었지만, 본 연구는 실제 데이터셋에서 네트워크의 유연성을 측정합니다.
- **Technical Details**: 논문에서는 Effective Model Complexity (EMC) 메트릭을 사용하여 모델이 완벽하게 맞출 수 있는 최대 샘플 크기를 측정합니다. 이 과정은 다양한 데이터 유형에서 수행되며, 특정한 라벨링 특성을 가진 데이터의 적합성을 반영해야 하고, 실제 훈련 데이터셋에 민감해야 하며, 계산 가능해야 한다는 세 가지 기준을 충족합니다.
- **Performance Highlights**: 실험 결과에 따르면, CNN(Convolutional Neural Networks)은 컴퓨터 비전 문제에서 다층 퍼셉트론보다 공간적 관계(spatial relationships)와 국소성(locality)에 강한 귀납적 편향을 갖고 있음에도 불구하고, 랜덤 레이블 데이터에서도 훨씬 더 효율적인 파라미터 사용을 보여줍니다. 또한, ReLU 활성화 함수는 시그모이달 활성화 함수보다 더 많은 훈련 샘플을 맞출 수 있으며, SGD(확률적 경사 하강법)는 풀 배치 경사 하강법보다 더 많은 훈련 샘플을 맞추는 데 도움이 됩니다.

### [Evaluating Open Language Models Across Task Types, Application Domains, and Reasoning Types: An In-Depth Experimental Analysis](https://arxiv.org/abs/2406.11402)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11402.png)

Vote: 6

Authors: Neelabh Sinha, Vinija Jain, Aman Chadha

- **What's New**: 최근 NLP(자연어 처리) 분야에서 Language Models(LMs)의 발전과 활용이 크게 개선되었습니다. 이 논문에서는 다양한 task types, application domains, reasoning types에 대해 10개의 작은 LMs(2B-11B 파라미터)의 성능을 심층적으로 분석합니다. 주요 목표는 각각의 요구사항에 맞는 최적의 LM과 prompt 스타일을 식별하는 것입니다.
- **Technical Details**: 실험 데이터셋은 ‘Super-Natural Instructions’에서 추출되었으며, 각 task는 정의, in-context 예제, 다중 인스턴스를 포함합니다. 영어 input/output을 기준으로 119개의 task, 11810개의 task 인스턴스를 선정하여 12 task types, 36 domains 및 18 reasoning types로 분류했습니다. prompt styles는 다양한 예제들을 포함하거나 제외하는 방식으로 구성했으며, 총 8가지 스타일로 제공합니다. 실험 대상은 2-11B 파라미터의 Gemma-2B, Gemma-7B, Llama-3-8B, Mistral-7B-v0.3, Falcon-2-11B와 그들의 instruction-tuned(IT) 버전입니다.
- **Performance Highlights**: 이번 연구는 10개의 공개된 LMs의 semantic correctness를 평가하고, 일부 모델이 GPT-3.5-Turbo, GPT-4o, DeepSeek-v2와 같은 대형 SOTA 모델과 경쟁할 수 있음을 보여줍니다. 실험 결과, 다양한 요구 조건에 따라 어떤 LMs와 prompt 스타일이 최적의 성능을 발휘하는지를 밝힙니다.

### [HiddenTables & PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies](https://arxiv.org/abs/2406.10803)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10803.png)

Vote: 4

Authors: William Watson, Nicole Cho, Manuela Veloso, Tucker Balch

- **What's New**: 본 논문에서는 테이블 기반 질문-응답 작업에 대한 언어 모델 접근 방식으로 'HiddenTables'라는 협력 게임을 제안합니다. HiddenTables는 'Oracle(신탁)'과 'Solver(해결자)'라는 두 에이전트로 구성되며, 솔버는 오라클의 지침과 스키마 전달에만 의존하여 사용자 질문에 답하는 코드를 생성합니다. 솔버는 테이블 콘텐츠를 모른 채로 게임을 진행하고, 오라클이 보안 환경에서 코드를 평가하여 답변을 전달하거나 팔로우업 질문을 합니다.
- **Technical Details**: HiddenTables는 '중국어 방 논증'에 영감을 받아 설계되었습니다. 오라클은 사용자 질문을 받아 적절한 프롬프트를 솔버에게 전달하며, 데이터 항목은 드러나지 않습니다. 솔버는 이러한 프롬프트를 기반으로 실행 가능한 코드를 생성하며, 이 과정에서 일곱 번의 라운드를 통해 개선하거나 수정할 기회를 얻습니다. 솔버는 데이터베이스에 접근하지 않고 스키마와 지시사항만으로 코드를 작성해야 합니다. 오라클은 프롬프트 부담을 관리하며, 보안 통역사를 통해 코드의 실행 및 데이터 노출을 방지합니다. 주로 Python 언어를 사용하여, 절차 지향적 명령을 통해 문제 해결 과정을 명확하게 표현합니다.
- **Performance Highlights**: HiddenTables는 토큰 수를 줄이고 데이터 프라이버시를 강화하면서 gpt-3.5-turbo의 정확도가 감소하는 것으로 나타났습니다. 그러나 피드백 라운드를 거치며 정확도가 향상됩니다. 이 연구는 언어 모델의 테이블 질문-응답 작업에서 단순 데이터 이용을 넘어 사고 체인을 통해 문제를 해결할 수 있음을 보여줍니다. 또한, PyQTax라는 새로운 데이터셋을 생성하여 향후 학술 실험에 기여할 수 있는 잠재력을 제공합니다.

### [CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training](https://arxiv.org/abs/2406.10670)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10670.png)

Vote: 4

Authors: Andreas Kirsch, Jonathan Richard Schwarz, David Brandfonbrener, Sham Kakade, Hanlin Zhang

- **What's New**: 이 논문에서는 언어 모델의 사전 학습 데이터 셋을 선택하는 방법에 대한 새로운 접근법을 제안합니다. 저자들은 기존의 손실 기반 접근법을 분석하고, 더 나은 성능을 제공하는 단순한 접근법인 CoLoR-Filter (Conditional Loss Reduction Filtering)를 제안합니다. 이 방법은 특정 다운스트림 작업에서의 성능을 최대화하기 위해 데이터를 선택하는 문제를 해결합니다.
- **Technical Details**: CoLoR-Filter 알고리즘은 베이즈 정리를 응용하여 사후 확률을 최대화하는 방식으로 데이터를 선택합니다. 구체적으로는, 'prior' 모델과 다운스트림 데이터를 통해 미세 조정된 'conditional' 모델 사이의 가능도 차이를 계산하여 각 시퀀스를 점수화합니다. 이는 데이터 선택 과정에서 필요한 계산 비용을 줄여주는 장점을 가지고 있습니다.
- **Performance Highlights**: 저자들은 두 가지 작업에서 CoLoR-Filter 알고리즘을 평가했습니다. 첫 번째는 Books에 대한 언어 모델링이고, 두 번째는 8가지 다운스트림 다중 선택 작업입니다. 두 작업 모두에서 CoLoR-Filter를 사용하여 선택된 데이터가 무작위로 선택된 데이터보다 8배 더 많은 양의 데이터를 사용한 모델보다 성능이 우수함을 보였습니다. 또한, 선택된 데이터를 작은 모델에서 대형 모델로 전이하는 것이 효율적임을 확인했습니다. CoLoR-Filter는 병렬 처리 가능하여 계산 비용 측면에서도 유리하다는 점을 강조합니다.

### [Breaking the Attention Bottleneck](https://arxiv.org/abs/2406.10906)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10906.png)

Vote: 4

Authors: Kalle Hilsenbek

- **What's New**: 최신 논문 'Transformers are Multi-State RNNs'에서는 Transformer 모델을 다중 상태 재귀 신경망(RNN)처럼 볼 수 있다는 새로운 관점을 제시합니다. 이러한 접근 방식은 Transformer와 RNN 간의 전통적인 경계를 허물고, 각 레이어가 RNN의 숨겨진 상태를 관리하는 것처럼 여러 상태 표현을 갖도록 합니다.
- **Technical Details**: 이 논문은 기존의 Transformer 모델이 처리 시퀀스를 더 효율적으로 처리할 수 있도록 'attention bottleneck' 문제를 해결하는 방법을 탐구합니다. 제안된 방법은 코사인 유사성 기반 정규화를 도입해 레이어 선형성(linearity)을 줄이고 성능을 향상시키는 것입니다. 이 접근 방법은 Tiny Stories와 SuperGLUE 같은 벤치마크에서 성능 지표를 향상시킵니다.
- **Performance Highlights**: 논문에서 제안한 방법은 연산 비용을 줄이고, 더 작은 모델 크기로 동일한 성능을 유지하며, 학습 초기에는 성능이 현저히 향상되는 결과를 보여주었습니다. 예를 들어, 평가 손실(validation loss)이 주류 attention 메커니즘보다 0.135 정도 낮은 1.555로 개선되었습니다. 또한, 추가적인 벡터 평균화 방법을 활용해 성능이 더욱 향상되었습니다.

### [Consistency^2: Consistent and Fast 3D Painting with Latent Consistency Models](https://arxiv.org/abs/2406.11202)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11202.png)

Vote: 3

Authors: Konrad Schindler, Tianfu Wang, Anton Obukhov

- **What's New**: Consistency2는 새로운 라텐트 일관성 모델(Latent Consistency Models, LCMs)을 활용해 빠르고 일관된 멀티뷰 텍스처 페인팅을 가능하게 합니다. 이는 콘텐츠 제작자들이 간단한 텍스트 프롬프트만으로도 3D 메시 텍스처를 설계할 수 있는 효율적인 방법을 제공합니다.
- **Technical Details**: 기존의 라텐트 확산 모델(Latent Diffusion Models, LDMs)보다 적은 스텝으로 고품질 텍스처 페인팅을 수행할 수 있습니다. Consistency2는 뷰 일관성을 유지하면서 빠른 속도로 작동하도록 설계되었습니다. 각 뷰 당 4 스텝 내에 메시 페인팅을 완료할 수 있으며, 일반 소비자 GPU에서 메시 하나를 페인팅하는 데 2분이 채 걸리지 않습니다. 주요 기술적 기여는 멀티뷰 노이즈와 색상 텍스처를 이용한 독창적인 페인팅 표현을 통해 텍스처 해상도와 카메라 뷰 샘플링의 유연성을 증대시킨 점입니다.
- **Performance Highlights**: Consistency2는 다양한 3D 메시에서 고품질의 텍스처 페인팅 성능을 입증했습니다. 이는 기존의 여러 멀티뷰 디노이징 방법보다 훨씬 빠르며, 특히 뷰 일관성을 유지하면서도 빠른 생성을 제공하는 LCMs의 장점을 최대한으로 활용하였습니다.

### [Deep Bayesian Active Learning for Preference Modeling in Large Language Models](https://arxiv.org/abs/2406.10023)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10023.png)

Vote: 2

Authors: Panagiotis Tigas, Alessandro Abate, Luckeciano C. Melo, Yarin Gal

- **What's New**: 새로운 연구는 LLM(대규모 언어 모델)을 인간의 선호도에 맞춰 조정하는데 중요한 역할을 하는 Preference Modeling을 다룹니다. 기존의 방법들이 주로 무작위 선택이나 단일 포인트 확보 방식에 의존했지만, 이번 연구는 보다 효율적인 데이터 선택을 위해 Bayesian Active Learning을 활용한 새로운 방법을 제안합니다.
- **Technical Details**: 본 연구에서는 Bayesian Active Learner for Preference Modeling (BAL-PM)이라는 새로운 확률적 데이터 확보 정책을 제안합니다. 이 방법은 선호 모델의 에피스테믹 불확실성이 높은 지점을 타겟으로 하면서, 획득한 프롬프트 분포의 엔트로피를 최대화하는 방식으로 동작합니다. 이를 통해 저밀도 지역에서 프롬프트를 선택하여 피처 공간의 불확실성을 줄이고, 중복 샘플 선택을 방지합니다.
- **Performance Highlights**: Reddit와 CNN/DM 선호 데이터셋을 사용한 실험에서, BAL-PM은 무작위 샘플링에 비해 거의 33% 및 68% 피드백 볼륨을 줄이는 데 성공했습니다. 또한, 다른 강력한 Bayesian 확보 정책보다 일관되게 뛰어난 성능을 보였습니다. 이는 다양한 샘플을 획득함으로써 더 나은 Bayesian 선호 모델 학습과 후속 획득 시 정확한 에피스테믹 불확실성 추정에 도움이 되었습니다.

