## Daily Papers (2024-06-20)

### [Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models](https://arxiv.org/abs/2406.11230)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11230.png)

Vote: 29

Authors: Haizhou Shi, Tunyu Zhang, Hengyi Wang, Wenyuan Wang, Weiyi Qin, Hao Wang, Shiwei Tan, Akshay Nambi, Tanuja Ganu

- **What's New**: 최신 연구에서 시각적 질문 응답과 크로스 모달 검색(cross-modal retrieval) 등 다양한 응용 분야를 가능하게 한 멀티모달 대형 언어 모델(MLLMs)의 진보를 다루고 있습니다. 이 연구는 MLLMs의 장기 문맥 이해(long-context understanding) 평가의 부족함을 해결하고자 합니다. 이를 위해 새로운 벤치마크인 MultiModal Needle-in-a-haystack(MMNeedle)를 도입하여 장기 문맥 능력을 종합적으로 평가합니다.
- **Technical Details**: MMNeedle 벤치마크는 MLLMs가 큰 이미지 집합(haystack)에서 텍스트 지시(instructions)에 따라 목표 하위 이미지(needle)를 찾는 능력을 평가합니다. 이를 위해 고급 기술(예: 이미지 스티칭)을 사용하여 입력 문맥 길이를 늘립니다. 이 과정에서 데이터셋에는 40,000개의 이미지, 560,000개의 캡션, 280,000쌍의 needle-haystack가 포함되어 있습니다. 다양한 문맥 길이, 단일 및 다중 needle, 긍정 및 부정 샘플과 같은 다양한 설정을 포함합니다. 또한, '존재 정확도(existence accuracy)', '인덱스 정확도(index accuracy)', '정확 정확도(exact accuracy)' 등 다양한 평가 지표를 사용합니다.
- **Performance Highlights**: 주요 발견으로는 모델 간의 상당한 성능 격차 및 부정 샘플을 통한 최신 MLLMs의 환각(hallucination) 문제를 확인했습니다. 예를 들어, 최신 API 기반 모델(Claude 3 Opus 및 Gemini 1.0 Pro)조차 많은 이미지와 이미지 내 하위 이미지가 있을 때 정확도가 크게 떨어집니다. 제시된 데이터에 따르면, GPT-4가 포함된 모델은 10개의 이미지에서는 97%의 정확도를 보였으나, 하위 이미지가 긴 문맥을 포함한 경우(160개의 이미지)에는 26.9%로 떨어집니다.

### [Long Code Arena: a Set of Benchmarks for Long-Context Code Models](https://arxiv.org/abs/2406.11612)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11612.png)

Vote: 19

Authors: Aleksandra Eliseeva, Maliheh Izadi, Evgeniy Glukhov, Anton Shapkin, Egor Bogomolov, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Timur Galimzyanov, Arie van Deursen, Timofey Bryksin

- **What's New**: Machine Learning for Software Engineering (ML4SE) 분야에서 새롭게 발표된 'Long Code Arena'는 코드 생성, 수리, 완성, 요약 및 코드 차이 분석 등 다양한 코드를 처리하는 여섯 가지 벤치마크 모음을 소개합니다. 이는 기존 벤치마크의 한계를 보완하며, 프로젝트 모듈 또는 전체 프로젝트 수준에서의 평가를 가능하게 합니다.
- **Technical Details**: Long Code Arena는 오픈 소스 GitHub 리포지토리에서 수집된 데이터를 바탕으로 작성되었으며, 주로 Python 코드를 사용합니다. 각 작업별 데이터 수집, 평가 및 베이스라인 모델 구현 방법론을 상세히 설명합니다. 예를 들어, 라이브러리 기반 코드 생성 작업에서는 GPT-4 및 CodeLlama-70B 등을 활용하여 모델을 평가합니다. 모든 데이터는 엄격히 검증되어 최고의 데이터 품질을 보장하며, 오픈소스 리포지토리와 GitHub Actions 로그에서 추출됩니다.
- **Performance Highlights**: 기본 베이스라인 모델로 평가한 결과, GPT-4는 가장 높은 품질을 보여주었고, API Recall 37%를 달성했습니다. 반면, 오픈 소스 모델들은 라이브러리 맥락을 고려하지 않은 경우 7-11%의 API Recall을 달성했습니다. BM-25 검색을 통해 일부 모델에서는 API Recall이 최대 6% 개선되었으나, 여전히 개선의 여지가 많습니다.

### [Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations for Vision Foundation Models](https://arxiv.org/abs/2406.12649)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12649.png)

Vote: 14

Authors: Hao Wang, Shiwei Tan, Hengyi Wang

- **What's New**: 최근 비전 트랜스포머(ViT, Vision Transformers) 모델은 자율 주행과 같은 고위험 도메인에서 많이 사용되고 있습니다. 이에 따라 설명 가능성이 중요한 문제로 떠오르고 있습니다. 이 논문에서는 ViT 모델의 설명 가능성을 높이기 위해 확률적 개념 설명자(PACE, Probabilistic Concept Explainers)를 제안합니다.
- **Technical Details**: 이 논문은 ViT 모델을 설명하는 데 필요한 다섯 가지 조건(신뢰성, 안정성, 희소성, 다중 레벨 구조, 간결성)을 정의하고, 이를 충족시키기 위해 PACE라는 방법을 개발했습니다. PACE는 계층적 베이지안 모델을 사용하여 데이터셋 수준에서 패치 수준까지의 개념을 추론합니다. 이를 위해 K개의 가우시안 패치 임베딩 분포를 혼합하여 이미지 수준, 패치 수준에서 ViT의 패치 임베딩을 관찰된 변수로 처리합니다.
- **Performance Highlights**: 정성적 및 정량적 평가를 통해 PACE는 합성 및 실제 데이터셋 모두에서 최첨단 방법들보다 우수한 성능을 보였습니다. 특히, PACE는 ViT 예측을 설명하는 데 있어 신뢰성과 안정성을 보장하며, 희소한 개념을 통해 설명 효과를 높였습니다.

### [Breaking Boundaries: Investigating the Effects of Model Editing on Cross-linguistic Performance](https://arxiv.org/abs/2406.11139)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11139.png)

Vote: 12

Authors: Somnath Banerjee, Avik Halder, Animesh Mukherjee, Sayan Layek, Rima Hazra, Rajarshi Mandal, Ian Soboroff

- **What's New**: 모델 편집 기술의 등장으로 LLM(Local Language Models)의 특정 입력에 대한 응답을 개선할 수 있는 혁신적인 방법이 제시되었습니다. 하지만 이러한 기술을 다국어 프레임워크에 적용하는 것은 독특한 장애물을 수반하며, 이러한 도전 과제를 해결하기 위해 창의적인 전략이 필요합니다. 본 연구는 다국어 PLM(Pre-trained Language Models)의 전 세계 언어 사용에서 효과적이고 공정한 운영을 보장하기 위한 전략을 개발하는 데 중점을 두고 있습니다.
- **Technical Details**: 우리는 '각 언어별로 자신을 위해' (ELFI) 및 '각 언어별로 다른 언어를 위해' (ELFO)라는 두 가지 접근 방식을 사용하여 다국어 모델이 여덟 가지 다른 언어(영어, 독일어, 프랑스어, 이탈리아어, 스페인어, 힌디어, 타밀어, 칸나다어)에서 지식을 얼마나 효율적으로 전이하는지에 대한 실험을 진행했습니다.
- **Performance Highlights**: 이 연구는 Mistral, TowerInstruct, OpenHathi, Tamil-Llama, Kan-Llama와 같은 최신 상태의 디코더 전용 모델들을 연구하였으며, ROME 및 MEMIT과 같은 잘 알려진 편집 방법과 결합하여 사용했습니다. 초기 연구에서 모델 병합은 능력을 향상시키지만, 편집 후 다국어 일관성을 유지하는 데에는 여전히 한계를 드러냈습니다. 우리의 포괄적인 오류 분석은 다양한 언어 간의 언어적 불일치가 모델의 다양한 해석과 의미를 초래할 수 있음을 밝혀냈습니다.

### [Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts](https://arxiv.org/abs/2406.12034)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12034.png)

Vote: 12

Authors: David Cox, Rogerio Feris, Hongyin Luo, Jacob Hansen, Leonid Karlinsky, Rameswar Panda, Alan Ritter, Junmo Kang, Zhen Wang, James Glass

- **What's New**: 최근 논문에서 소개된 Self-MoE 기법은 기존의 대형 언어 모델(LLM)의 단점인 비효율성, 망각 문제, 불투명성을 극복하기 위한 새로운 접근법을 제시합니다. Self-MoE는 모델을 모놀리식(monolithic) 구조에서 MiXSE(Mixture of Self-specialized Experts)라는 조합형 시스템으로 전환시키는 방법입니다.
- **Technical Details**: Self-MoE는 합성 데이터(synthetic data)를 기반으로 가벼운 전문가 모듈(expert modules)을 구성하고, 이를 기본 LLM과 통합하는 방식입니다. 이 방법은 인간 주석 데이터(human-labeled data)나 기존의 학습된 모듈에 의존하지 않으며, 각 전문가의 의미와 정합성을 유지하면서 다양한 도메인 작업을 동적으로 처리할 수 있습니다. 특히,(Self-MoE는 자가 최적화 길찾기 메커니즘(self-optimized routing mechanism)을 통해 모듈간 동적인 상호작용을 촉진합니다.
- **Performance Highlights**: 다양한 도메인(지식, 추론, 수학, 코딩)에서의 실험 결과, Self-MoE는 대상 도메인에서 성능을 향상시키면서도 비대상 도메인에서의 성능 저하를 방지한다는 점에서 우수한 결과를 나타냈습니다. 또한, MiXSE는 인스턴스 병합(instance merging) 및 가중치 병합(weight merging) 등의 기존 기법보다 뛰어난 유연성과 해석 가능성을 제공합니다. 마지막으로, Self-MoE 접근법은 단일 모델에서 발생하는 망각 문제를 해결했으며, 다양한 모델 계열과 크기에 적용 가능함을 확인했습니다.

### [Interface Design for Self-Supervised Speech Models](https://arxiv.org/abs/2406.12209)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12209.png)

Vote: 6

Authors: David Harwath, Yi-Jen Shih

- **What's New**: 최근 자가 지도 학습(self-supervised learning, SSL)에 기반한 음성 모델들이 큰 인기를 끌고 있습니다. 이 논문에서는 SSL 음성 모델들을 활용하기 위한 업데이트된 프레임워크를 제안하며, 다양한 인터페이스 모듈(interface module) 설계를 제시하고 있습니다. 특히, Hierarchical Convolution이 여러 음성 처리 작업에서 가장 높은 성능을 나타냈음을 보였습니다.
- **Technical Details**: 제안된 프레임워크는 세 가지 주요 컴포넌트로 구성됩니다: 상류 모델(Upstream model), 하류 예측 헤드(Downstream prediction head), 그리고 이들 사이 정보를 집계하는 인터페이스(Interface). 특히, Hierarchical Convolution을 사용하여 층(layer) 간 정보를 집계하는 방식이 성능 면에서 우수하다고 주장합니다.
- **Performance Highlights**: Hierarchical Convolution 인터페이스는 ML-SUPERB 및 SUPERB 벤치마크에서 최상의 성능을 보였습니다. 추가 실험을 통해 다른 인터페이스 설계들 간의 성능 차이가 학습 가능한 매개변수 개수의 차이 때문이 아니라 설계 자체의 차이 때문임을 확인했습니다. 코드 역시 공개되어 있습니다.

### [Measuring memorization in RLHF for code completion](https://arxiv.org/abs/2406.11715)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11715.png)

Vote: 5

Authors: Ilia Shumailov, Jamie Hayes, Aneesh Pappu, Billy Porter

- **What's New**: 코드 자동 완성 도우미가 개발자 환경에서 점점 더 중요한 도구로 자리잡고 있습니다. 이들은 개발자가 작성 중인 코드의 문맥 정보를 이용하여 다음에 적합한 코드를 제안합니다. Github CoPilot, Google Colab의 Gemini, TabNine, Sourcegraph의 Cody와 같은 여러 시스템들이 인기를 끌고 있으며, 모두 대형 언어 모델을 코드 데이터셋에 맞추어 튜닝(fine-tuning)하고 정렬시키는 방식을 사용하고 있습니다.
- **Technical Details**: 이 논문에서는 강화 학습(복원학습, RLHF)을 통해 사용자 의도와 선호도에 맞춰 대형 모델을 정렬시키는 과정을 설명합니다. 대표적인 것으로 Codex와 같이 자율학습으로 기본적인 코딩 문법 및 스타일을 이해하고, 보상 모델(Reward Model, RM)을 통해 사용자가 좋다고 평가한 코드는 긍정적인 점수를, 나쁜 코드는 부정적인 점수를 부여하는 식으로 인간의 선호도를 근사화하는 과정을 거칩니다. 그리고 최종적으로 RLTF(RL fine-tuning)을 통해 모델의 완성도를 높입니다.
- **Performance Highlights**: 강화 학습의 긍정적 예제와 부정적 예제를 학습하며, 민감한 사용자 데이터를 활용할 때는 상업적 및 프라이버시 측면에서 주의가 필요합니다. 중요한 발견 중 하나는 보상 모델 훈련 단계의 데이터가 RLFT 모델에 기억됨으로써 발생할 수 있는 민감한 정보 누출 가능성이 낮다는 점입니다. 이러한 발견은 고품질 데이터를 보상 모델 훈련에 사용하는 것을 좀 더 안전하게 할 수 있습니다.

### [Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces](https://arxiv.org/abs/2406.11614)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11614.png)

Vote: 3

Authors: Shauli Ravfogel, Haiqin Yang, Mor Geva, Lei Yu, Yihuai Hong

- **What's New**: 최근 대형 언어 모델(LLMs)에서 민감하거나 유해한 정보, 편향, 오래된 사실을 제거하기 위한 'Unlearning' 방법에 대한 관심이 증가하고 있습니다. 기존의 평가 프로토콜은 질문에 답하거나 제거된 정보에 대한 쿼리를 완료할 수 있는 능력과 같은 행동 테스트에 크게 의존하고 있습니다. 그러나 이 연구에서는 모델이 언러닝 이후에도 제거된 정보를 생성하는 것을 조작할 수 있다는 증거가 증가하고 있음을 강조하고 있습니다. 이를 해결하기 위해 내부적으로 언러닝 방법을 평가하는 새로운 벤치마크 ConceptVectors를 제안하고 있습니다.
- **Technical Details**: 본 연구는 'parametric knowledge traces'라는 특정 매개변수 집합에 초점을 맞추어 제거할 정보와 강한 상관관계가 있음을 제안합니다. 이 접근법은 최근 모델 매개변수를 어휘로 투영하여 조사하는 방법을 사용하여 내부 지식을 추적합니다. 이 방법을 활용하여 LLaMA와 OLMo 두 개의 오픈 소스 LLM에 적용하고, 다층 퍼셉트론(MLP) 층에 위치한 개념 벡터를 식별하여 언러닝 테스트를 위한 적합한 파라메트릭 개념 벡터를 생성했습니다.
- **Performance Highlights**: 연구 결과, 기존 언러닝 방법은 모델이 언러닝된 개념 정보를 생성하지 못하게 하지만, 그 파라메트릭 지식 흔적에는 미미한 변화를 주는 것으로 나타났습니다. 반면 특정 개념 벡터에 직접 개입하는 방식은 해당 개념에 대한 정보를 효과적으로 지우는 동시에 모델의 세대에 매우 큰 영향을 미쳤습니다. 그리고 적대적인 프롬프트를 사용하여 모델을 탈옥 시도한 결과, 세밀 조정 방법으로 언러닝된 경우 모델이 추정된 언러닝 지식을 훨씬 더 자주 생성하는 것을 발견했습니다. 반면 개념 벡터를 제거하는 방법으로 언러닝된 경우 이러한 현상이 상당히 줄어듭니다.

