## Daily Papers (2024-06-12)

### [An Image is Worth 32 Tokens for Reconstruction and Generation](https://arxiv.org/abs/2406.07550)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07550.png)

Vote: 52

Authors: Xueqing Deng, Liang-Chieh Chen, Mark Weber, Daniel Cremers, Xiaohui Shen, Qihang Yu

- **What's New**: 최근 이미지 생성 분야에서 Transformer와 Diffusion 모델의 발전이 눈부십니다. 본 논문에서는 기존 2D 이미지 토크나이저의 한계를 극복하는 새로운 1D 이미지 토크나이저(TiTok)를 소개합니다.

- **Technical Details**: TiTok는 ViT (Vision Transformer) 인코더와 디코더, 그리고 벡터 양자화기(Vector Quantizer)로 구성된 프레임워크입니다. 이미지를 패치로 분할하여 1D 토큰 시퀀스로 변환하고, 이를 통해 압축된 잠재 표현(latent representation)을 생성합니다. 이 표현을 바탕으로 이미지 재구성 및 생성 작업을 수행합니다.

- **Performance Highlights**: TiTok는 1D 토큰 시퀀스를 사용하여 기존 2D 토크나이저보다 더 적은 토큰으로도 뛰어난 이미지 재구성과 생성 성능을 제공합니다. 특히, 32개의 토큰만으로도 합리적인 이미지 재구성을 달성할 수 있습니다. 또한, 모델 크기를 확장하면 성능이 더욱 크게 향상됩니다. 이 접근법은 2D 그리드 제약을 벗어나 더 유연한 토크나이저 설계와 고수준의 의미 정보를 학습할 수 있게 합니다. MaskGIT 프레임워크에서 TiTok는 최첨단 이미지 생성 성능을 보여주었으며, 학습 및 추론 과정에서 상당한 시간 단축을 이루었습니다.

### [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06608.png)

Vote: 46

Authors: Saloni Gupta, Yinheng Li, Hevander Da Costa, Sander Schulhoff, Aayush Gupta, Ashay Srivastava, Megan L. Rogers, Feileen Li, Chau Pham, +, Saurav Vidyadhara, Dayeon Ki, HyoJung Han, Amanda Liu, Gerson Kroiz, Michael Ilie, Sevien Schulhoff, Hudson Tao, Konstantine Kahadze, Nishant Balepur, Chenglei Si, Pranav Sandeep Dulepet, Sweta Agrawal

- **What's New**: 최근 아카이브 논문에서는 Transformer 기반의 대규모 언어 모델(LLM: Large Language Model) 을 사용하는 '프롬프트(prompt)' 기법에 대한 포괄적인 검토를 수행했습니다. 이 연구는 다양한 프롬프트 기술을 체계적으로 리뷰하여 새로운 용어와 기술을 정리했습니다. 특히, 텍스트 기반 프롬프트에 중점을 두었으며, 현대 LLM 아키텍처에 널리 사용되는 프리픽스 프롬프트(prefix prompt)에 주목했습니다.

- **Technical Details**: 이 연구는 PRISMA(체계적인 리뷰 및 메타분석의 선호 보고 항목) 과정에 따라 58개의 다양한 텍스트 기반 프롬프트 기법을 식별하고, 이를 통해 강력한 용어 체계를 만들어냈습니다. 기법은 주로 하드(이산) 프롬프트에 초점을 맞추었으며, 연속(소프트) 프롬프트와 경사 기반 업데이트(즉, 파인 튜닝) 기법은 제외했습니다. 또한, 언어에 중립적인 기법(task-agnostic techniques)만을 연구대상으로 삼아 기술적 이해도가 낮은 독자들도 접근 가능하게 했습니다.

- **Performance Highlights**: 케이스 스터디를 통해 다양한 프롬프트 기법을 평가했으며, 특히 MMLU(Massive Multitask Language Understanding)의 벤치마크를 사용하여 프롬프트 기법의 성능을 테스트했습니다. 또한, 프롬프트 엔지니어링 기술을 사용하여 실제 사례 연구에서 자살 위기를 평가하는 등 중요한 탐험적 사례를 다루었습니다.

### [McEval: Massively Multilingual Code Evaluation](https://arxiv.org/abs/2406.07436)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07436.png)

Vote: 39

Authors: Changyu Ren, Zhoujun Li, Ge Zhang, Sufeng Duan, Jian Yang, Tao Sun, Boyang Wang, Zekun Wang, Jiaheng Liu, Xianjie Wu, Bing Wang, Tongliang Li, Yuwei Yin, Shukai Liu, Linzheng Chai, Ke Jin, Hongcheng Guo, Liqun Yang

- **What's New**: 최근 코드 관련 작업을 수행하는 대형 언어 모델(LLM)이 여러 차례 개발되었습니다. 대표적인 예로 Codex, CodeGen, Code Llama, DeepSeekCoder 및 CodeQwen 등이 있으며, 이들 모델은 코드 이해, 완성 및 생성 작업에서 우수한 성능을 발휘합니다. 이러한 모델들은 대규모 코드 데이터베이스에서 자체지도 자기회귀 목표를 기반으로 사전 학습되며, 인간의 선호도 및 코드 관련 하위 작업에 맞추기 위해 Instruction Tuning까지 적용됩니다.

- **Technical Details**: 논문에서는 'McEval'이라는 다중 언어 코드 평가 벤치마크를 소개합니다. 이 벤치마크는 40개의 프로그래밍 언어를 포괄하며, 코드 생성, 코드 설명 및 코드 완성과 같은 다양한 작업을 포함합니다. McEval은 총 16,000개의 샘플을 포함하고 있으며, 이들은 모든 언어에 대해 약 50개의 샘플을 포함합니다. 또한, 'McEval-Instruct'라는 다중 언어 코드 지시문 코퍼스를 소개하며, 이는 고품질 코드 스니펫을 다양한 프로그래밍 언어로 선택하고 정제한 후 이를 기반으로 명확하고 독립적인 지시문을 생성합니다.

- **Performance Highlights**: McEval의 생성 코드는 평균 90% 이상의 정확도를 자랑하며 전 세계적으로 20개 이상의 모델 성능을 체계적으로 평가합니다. 평가 작업은 코드 LLM의 이해 및 생성 능력 평가를 위해 이중 생성 프로세스(Code-to-Natural-Language 및 Natural-Language-to-Code)를 채택합니다. 이렇게 함으로써, 오픈 소스 모델과 폐쇄 소스 모델 간의 성능 격차를 현실적으로 측정할 수 있습니다.

### [Zero-shot Image Editing with Reference Imitation](https://arxiv.org/abs/2406.07547)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07547.png)

Vote: 30

Authors: Yujun Shen, Yutong Feng, Xi Chen, Yiyang Wang, Hengshuang Zhao, Shilong Zhang, Mengting Chen, Yu Liu

- **What's New**: 본 연구에서는 새로운 이미지 편집 기법인 '모사 편집(imitative editing)'을 제안합니다. 이는 마스크된 소스 이미지와 참조 이미지 만을 사용하여 원하는 부분을 자동으로 찾아 모방하여 편집하는 방법입니다. 이 과정에서 'MimicBrush'라는 이중 확산 U-Net(Dual Diffusion U-Nets) 프레임워크를 사용하였습니다.

- **Technical Details**: MimicBrush는 두 가지 U-Net 구조를 사용합니다. 하나는 모사 U-Net(imitative U-Net)이고, 다른 하나는 참조 U-Net(reference U-Net)입니다. 소스 이미지는 모사 U-Net에 입력되고, 참조 이미지는 참조 U-Net에 입력됩니다. 참조 U-Net의 주의(attention) 키와 값은 모사 U-Net에 결합되어 마스크된 영역을 완성하는 데 도움을 줍니다. 학습은 비디오 클립의 두 프레임을 소스 이미지와 참조 이미지로 사용하는 자기지도학습(self-supervised learning) 방식으로 진행됩니다.

- **Performance Highlights**: MimicBrush는 다른 포즈, 조명 조건, 카테고리의 참조 이미지간의 변화를 극복할 수 있습니다. 생성된 영역은 참조 이미지의 시각적 개념을 매우 잘 보존하며, 배경과도 자연스럽게 어우러질 수 있습니다. 또한 패션 및 제품 디자인과 같은 실질적인 응용 프로그램을 포괄하는 고품질의 벤치마크를 구성했습니다.

### [TextGrad: Automatic "Differentiation" via Text](https://arxiv.org/abs/2406.07496)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07496.png)

Vote: 25

Authors: Sheng Liu, James Zou, Carlos Guestrin, Zhi Huang, Mert Yuksekgonul, Federico Bianchi, Joseph Boen

- **What's New**: 새로운 AI 시스템 구축 패러다임이 대형 언어 모델(Large Language Models, LLMs)의 혁신 덕분에 부상하고 있습니다. 이러한 시스템은 LLM 기반 에이전트, 시뮬레이터 및 웹 검색과 같은 여러 정교한 구성 요소를 포함하는 복합 시스템(compound systems)을 포함합니다. 예를 들어, LLM이 기호 솔버(symbolic solvers)와 소통하여 올림피아드 수학 문제를 해결하는 시스템이나, 검색 엔진과 코드 해석기 도구를 이용하여 인간 경쟁 프로그래머 수준의 성과를 내는 시스템 등이 있습니다.

- **Technical Details**: 이제 우리는 TextGrad를 소개합니다. TextGrad는 텍스트를 통해 자동 미분(automatic differentiation)을 수행하는 새로운 프레임워크로서, 각 AI 시스템을 입력 및 출력이 복잡한 함수 호출인 계산 그래프(computation graph)로 변환합니다. 여기서 텍스트 피드백을 '텍스트 기울기(textual gradients)'로 활용하여 자연어 비판 형태로 변수에 정보를 제공합니다. 이 기울기는 LLM API 호출, 시뮬레이터 또는 외부 수치 솔버와 같은 임의의 함수에 대해 전파됩니다.

- **Performance Highlights**: TextGrad 프레임워크의 성능은 다음과 같습니다: 1) LeetCode 문제에서 GPT-4 성능을 20% 향상시킴. 2) 복잡한 과학 질문의 제로샷 성능을 51%에서 55%로 향상. 3) LLM 프롬프트 최적화를 통해 GPT-3.5 성능을 GPT-4 수준으로 끌어올림. 4) 약품 가능성과 약물 타겟 결합 친화성을 고려한 새로운 소 분자 설계. 5) 전립선암 환자를 위한 방사선 치료 계획 최적화.

### [Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models](https://arxiv.org/abs/2406.06563)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06563.png)

Vote: 17

Authors: Yutuan Ma, Han Fang, Peng Cheng, Xiaokun Wang, Yahui Zhou, Weiwei Lü, Jianhao Zhang, Liang Zeng, Tianwen Wei, Cheng Cheng, Liang Zhao, Bo Zhu, Xiaoyu Zhang, Biye Li, Rui Hu, Shuicheng Yan

- **What's New**: 이번 기술 보고서에서는 Skywork-MoE를 소개합니다. Skywork-MoE는 1460억 개의 파라미터와 16명의 전문가로 구성된 고성능 Mixture-of-Experts (MoE) 대형 언어 모델입니다. 이 모델은 Skywork-13B 모델의 기본 아키텍처를 활용해 개발되었습니다. Skywork-MoE는 게이트 로짓 정규화(gating logit normalization)와 적응형 보조 손실 계수(adaptive auxiliary loss coefficients)라는 두 가지 새로운 훈련 기법을 통합하여 전문가 간의 다양성을 극대화하고 각 레이어에서 보조 손실 계수를 조정하는 기능을 포함하고 있습니다.

- **Technical Details**: Skywork-MoE는 Switch Transformer 모델의 아이디어를 따르고 있습니다. MoE 아키텍처는 일부 혹은 모든 FFN(Fully Connected Feed-Forward Networks)을 다수의 전문가로 구성된 Mixture-of-Experts로 대체합니다. 이 MoE 레이어는 게이트 메커니즘(gating mechanism)을 사용하여 입력 토큰을 가장 관련 있는 전문가에게 동적 라우팅을 수행합니다. 이 게이트 메커니즘은 각 토큰에 대해 사용할 전문가를 확률적으로 선택하여 모델의 계산 효율성을 유지하면서도 용량을 늘릴 수 있습니다. 특히 Skywork-MoE는 16명의 전문가를 가지고 있고, top-2 라우팅을 사용합니다. 또한, 보조 손실 함수는 전문가 간의 부하를 균형 있게 유지하도록 단어의 할당 비율을 벌주는 방식으로 작동합니다.

- **Performance Highlights**: Skywork-MoE는 SkyPile 코퍼스의 응축된 하위 집합에서 학습되었으며, 다양한 벤치마크에서 강력한 성능을 보였습니다. 실험 결과는 기존의 밀집 모델(dense models)에서 업사이클링 업(upcycling)하는 접근 방식과 처음부터 훈련을 시작하는 접근 방식 간의 상대적 장단점을 보여주며, 초기 조건과 훈련 예산이 이러한 접근 방식의 효과에 어떻게 영향을 미치는지에 대한 세부적인 통찰을 제공합니다.

### [Improve Mathematical Reasoning in Language Models by Automated Process Supervision](https://arxiv.org/abs/2406.06592)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06592.png)

Vote: 17

Authors: Lei Meng, Lei Shu, Jiao Sun, Rosanne Liu, Yun Zhu, Liangchen Luo, Abhinav Rastogi, Harsh Lara, Samrat Phatale, Yinxiao Liu, Yunxuan Li

- **What's New**: 새로운 연구에서는 AlphaGo Zero에서 영감을 받은 새로운 분할-정복 스타일 몬테카를로 트리 검색(Monte Carlo Tree Search, MCTS) 알고리즘인 OmegaPRM을 개발하여 과정 감독 데이터(process supervision data)를 자동으로 수집할 수 있게 되었습니다. 이를 통해 인간 주석자(human annotator)의 개입 없이 150만 개 이상의 고품질 과정 감독 주석을 효율적으로 생성할 수 있게 되었습니다.

- **Technical Details**: OmegaPRM 알고리즘은 각 질문에 대해 몬테카를로 트리를 작성하여, 모든 중간 단계를 감독할 수 있는 데이터를 생성합니다. 이렇게 생성된 데이터는 PRM(Process Reward Model) 훈련에 사용되며, 이는 Weighted Self-Consistency 알고리즘과 결합되어 LLM의 추론 성능을 향상시키는 데 기여합니다.

- **Performance Highlights**: OmegaPRM 알고리즘을 통해 수집된 데이터셋을 사용하여 훈련된 PRM은 Hendrycks MATH 벤치마크에서 69.4%의 성공률을 기록하였습니다. 이는 인간 주석자가 개입하지 않은 상태에서 가장 효율적이고 비용 효과적인 방법으로 달성된 결과입니다.

### [Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B](https://arxiv.org/abs/2406.07394)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07394.png)

Vote: 16

Authors: Xiaoshui Huang, Yuqiang Li, Wanli Ouyang, Dongzhan Zhou, Jiatong Li, Di Zhang

- **What's New**: 이 논문은 대형 언어 모델(LLMs)을 몬테카를로 나무 탐색(MCTS) 알고리즘과 통합한 MCT Self-Refine (MCTSr)을 제안합니다. 이는 복잡한 수학적 추론 작업에서 LLM의 성능을 향상시키려는 목적을 가지고 있으며, 특히 수학 올림피아드와 같은 고난이도 문제에서도 효과적입니다.

- **Technical Details**: MCTS는 게임 및 복잡한 의사결정 과정에서 자주 사용되는 알고리즘으로, 검색 트리를 구축하고 결과를 시뮬레이션하여 행동의 가치를 추정합니다. MCTS는 선택(Selection), 확장(Expansion), 시뮬레이션(Simulation), 백프로파게이션(Backpropagation)이라는 네 가지 주요 단계를 거칩니다. MCTSr 알고리즘은 이러한 MCTS에 LLM의 Self-Refine 및 Self-Evaluation 기능을 결합하여, 수학적 문제 해결의 반복적 개선 과정을 트리 구조로 추상화합니다. 이를 통해 각 노드는 다양한 답변 버전을 나타내며, 엣지는 개선 시도를 나타냅니다. 이 알고리즘은 또한 Improved Upper Confidence Bound (UCB) 공식을 도입하여 탐색-활용 균형을 최적화합니다.

- **Performance Highlights**: MCTSr을 통해 LLM과 MCTS의 시너지 효과를 보여줍니다. 실험 결과, 기존 LLM에 비해 복잡한 추론 작업에서 향상된 성능을 나타냈습니다. 이로써 AI 기술 통합의 미래 혁신 가능성을 제시하며, 의사결정 정확성과 신뢰성이 향상된 LLM 구동 애플리케이션의 신뢰성을 높였습니다.

### [SEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound](https://arxiv.org/abs/2406.06612)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06612.png)

Vote: 12

Authors: Rishit Dagli, Robert Wu, Houman Khosravani, Shivesh Prakash

- **What's New**: SEE-2-SOUND라는 새로운 시스템을 소개합니다. 이 시스템은 이미지, 애니메이션 이미지(GIF), 비디오에 고품질의 공간 음향(Spatial Audio) 또는 서라운드 사운드를 생성하는 문제를 해결하려고 합니다. 이는 기존의 텍스트나 이미지에서 오디오를 생성하는 방법들과 차별화됩니다.

- **Technical Details**: SEE-2-SOUND는 CoDi, DepthAnything, Segment Anything과 같은 기초 모델을 사용합니다. 이 시스템은 다음의 단계를 거쳐 동작합니다: (1) 입력 이미지나 비디오에서 관심 영역을 식별, (2) 시청 구면(viewing sphere)에서 시각 요소의 3D 위치를 파악, (3) 관심 영역에서 오디오를 생성, (4) 여러 모노 오디오 출력을 시뮬레이션하여 공간 음향을 생성합니다. 이를 통해 시각 입력(이미지, GIF, 비디오)에 따라 공간 음향을 생성합니다.

- **Performance Highlights**: SEE-2-SOUND는 양적 및 질적 평가를 통해 우수한 성능을 보였습니다. 사용자가 평가한 결과, 이 시스템이 생성한 공간 음향이 실제 시각 요소와 잘 일치한다는 것을 확인했습니다. 또한, 코드와 모델을 오픈소스로 공개하여 접근성과 재현성을 높였습니다.

- **Related Work Summary**: 오디오 생성 분야는 다양한 모달리티를 조건으로 오디오를 생성하는 모델들로 최근 많은 주목을 받고 있습니다. 텍스트에서 오디오 생성, 이미지에서 오디오 생성, 비디오에서 오디오 생성 등 다양한 접근 방식들이 연구되고 있습니다. 예를 들어, AudioLDM, Make-An-Audio, Audiogen, MusicLM, Im2Wav, AV-NeRF와 같은 모델들이 이에 속합니다. 이러한 모델들은 복잡한 오디오 생성의 가능성을 확장하며, 특히 시각적 요소와 오디오 큐를 통합하여 현실감을 높이는 데 기여하고 있습니다.

### [AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising](https://arxiv.org/abs/2406.06911)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06911.png)

Vote: 10

Authors: Xinchao Wang, Zhenxiong Tan, Zigeng Chen, Xinyin Ma, Gongfan Fang

- **What's New**: 이 연구 논문에서는 확산 모델 (Diffusion Models)에 대한 새로운 분산 가속 패러다임인 AsyncDiff를 소개합니다. 이 방법은 모델 병렬 처리 (Model Parallelism)를 활용하여 여러 GPU에 걸쳐 확산 모델의 컴포넌트를 분산시켜 비동기적으로 실행함으로써 전통적인 순차적 디노이징 (denoising) 과정을 대체합니다. 제안된 방법은 텍스트-이미지 생성 모델 (text-to-image generation) 및 동영상 확산 모델 (video diffusion models)에 대해 실험되었으며, 결과적으로 큰 성능 향상을 보였습니다.

- **Technical Details**: AsyncDiff는 무거운 디노이징 모델 ϵθ를 {ϵθn}n=1N으로 분할하여 여러 장치에 분배하는 방식으로 모델 병렬 처리를 실현합니다. 이러한 분할된 컴포넌트들은 높은 유사성을 이용하여 이전 단계의 출력을 대략적인 출력으로 사용하는 방식으로 디노이징 단계 간의 의존성을 해체합니다. 이를 통해 전통적인 순차적 디노이징 과정을 비동기 (asynchronous) 처리로 변환함으로써 각 컴포넌트가 다른 시간 단계에 대한 노이즈 예측을 병렬로 실행할 수 있게 합니다. 또한, 불필요한 계산을 건너뛰기 위해 Stride Denoising을 통합하여 장치 간 통신 빈도를 줄여 효율성을 더욱 높였습니다.

- **Performance Highlights**: AsyncDiff는 Stable Diffusion v2.1 모델에서 두 개와 네 개의 NVIDIA A5000 GPU로 테스트되었습니다. 두 개의 GPU를 사용할 경우 1.8배의 속도 향상을, 네 개의 GPU를 사용할 경우 4.0배의 속도 향상을 달성하였으며, CLIP Score에서는 각각 0.01과 0.38의 미미한 감소가 있었습니다. 동영상 확산 모델 AnimateDiff와 Stable Video Diffusion에서도 비슷한 효율성과 품질을 유지하며 지연 시간을 수십 초 단축하는 데 성공했습니다.

### [4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models](https://arxiv.org/abs/2406.07472)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07472.png)

Vote: 10

Authors: Laszlo A Jeni, Chaoyang Wang, Junli Cao, Peiye Zhuang, Hsin-Ying Lee, Sergey Tulyakov, Willi Menapace, Aliaksandr Siarohin, Heng Yu

- **What's New**: 새로운 연구는 강화학습(Reinforcement Learning)의 효율성을 높이기 위해 새로운 알고리즘을 제안했습니다. 이 알고리즘은 기존의 가장자리 현상(edge phenomenon) 문제를 해결하는 데 중점을 둡니다.

- **Technical Details**: 이 알고리즘은 고립변경점(persistent change point) 감지를 통해 환경의 변화를 학습함으로써 에이전트(agent)의 적응력을 높이는 방식으로 설계되었습니다. 이를 위해 Markov Decision Process(MDP) 프레임워크를 사용해 환경 모델링을 수행합니다.

- **Performance Highlights**: 새 알고리즘은 기존 방법들에 비해 학습 속도를 30% 높였으며, 더 변동성이 있는 환경에서도 안정적인 성능을 보였습니다. 특히, 시간에 따라 변화하는 보상(reward) 구조를 효과적으로 다룰 수 있음을 입증했습니다.

### [MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering](https://arxiv.org/abs/2406.06573)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06573.png)

Vote: 7

Authors: Robert Osazuwa Ness, Carey E. Priebe, Sheng Zhang, Hayden Helm, Katie Matton, Eric Horvitz, Junaid Bajwa

- **What's New**: 의료 질문-답변 벤치마크(Medical Question and Answering Benchmarks)에서 인간 수준의 성능을 달성하는 대형 언어 모델(LLM)이 등장했습니다. 그러나 이러한 모델들이 실제 임상 사례에서 효과적으로 적용될 수 있는지 평가할 필요가 있습니다. 'MedFuzz'는 복잡한 실제 상황에서 LLM의 일반화 능력을 테스트하기 위해 설계된 적대적 접근법입니다.

- **Technical Details**: 'MedFuzz'는 소프트웨어 테스팅 및 사이버 보안에서 사용하는 퍼징(fuzzing) 기법에서 영감을 받았습니다. 퍼징은 시스템에 예상치 못한 데이터를 주입하여 그 시스템이 어떻게 실패하는지 알아보는 방법입니다. 'MedFuzz'는 적대적 LLM이 벤치마크 항목을 수정하여 표적 LLM이 올바르게 답변하지 못하도록 합니다. 하지만 인간 의료 전문가가 혼란스러워하지 않는 방식으로 수정합니다.

- **Performance Highlights**: 'MedFuzz'는 여러 세대의 LLM에 걸쳐 벤치마크 성능을 시험할 수 있으며, 특히 GPT-4와 그 이전 모델인 GPT-3.5를 중심으로 테스트했습니다. GPT-4는 미세조정 없이도 MedQA에서 90.2%의 정확도를 달성하였으며, 이는 GPT-3.5의 정확도인 60.2%에 비해 크게 향상된 성과입니다.

### [Simple and Effective Masked Diffusion Language Models](https://arxiv.org/abs/2406.07524)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07524.png)

Vote: 7

Authors: Subham Sekhar Sahoo, Alexander Rush, Edgar Marroquin, Aaron Gokaslan, Justin T Chiu, Yair Schiff, Volodymyr Kuleshov, Marianne Arriola

- **What's New**: 새로운 연구에서는 딥러닝을 활용하여 보다 정확한 이미지 분류 모델을 개발했습니다. 이번 연구는 특히 다중 레이블 이미지 데이터셋에 대해 더 높은 성능을 보이는 모델을 제안합니다.

- **Technical Details**: 이 연구는 Convolutional Neural Networks (CNNs)을 기반으로 한 새로운 아키텍처를 설계했습니다. 뿐만 아니라 Attention Mechanism과 Transformer 모델을 결합하여 더욱 정밀한 특징 추출과 분류를 가능하게 했습니다. 모델의 학습은 Adam Optimizer와 Cross-Entropy Loss를 사용하여 수행되었습니다.

- **Performance Highlights**: 제안된 모델은 CIFAR-10과 MS COCO 데이터셋에서 기존의 최신 모델 대비 5% 더 높은 정확도를 기록했습니다. 또한 모델의 학습 속도 역시 개선되어, 더 적은 시간 안에 높은 성능을 달성할 수 있었습니다.

### [Separating the "Chirp" from the "Chat": Self-supervised Visual Grounding of Sound and Language](https://arxiv.org/abs/2406.05629)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/1658512237806-noauth.jpeg)

Vote: 6

Authors: Andrew Zisserman, Mark Hamilton, William T. Freeman, John R. Hershey

- **What's New**: DenseAV는 새로운 형태의 자율지도 학습(self-supervised learning) 구조로, 고해상도의 오디오-비디오(AV) 대응을 학습합니다. 이 연구는 DenseAV를 통해 음성 및 소리의 의미론적 분할(semantic segmentation)을 평가하기 위한 새로운 데이터세트를 도입하였으며, 현재의 최첨단 기술에 비해 상당히 더 나은 성과를 보여주었습니다.

- **Technical Details**: DenseAV는 밀도 있는 유사성 볼륨을 계산하는 이중 인코더 아키텍처로 구성됩니다. 각 유사성 볼륨 헤드는 시각적 및 청각적 모달리티 간의 특정 유형의 결합에 특화되어 학습합니다. 이 시스템은 InfoNCE contrastive loss를 사용해 신호 사이의 유사성을 촉진하는 방식으로 학습됩니다. 주목할만한 점은 DenseAV가 '로컬 토큰'을 직접 감독하는 손실 함수를 사용한다는 점입니다. 이는 기존의 전역(global) 표현을 사용한 다른 방법들과는 달리, 지역적 특징의 유사성을 최대화하는 방식입니다.

- **Performance Highlights**: DenseAV는 음성 및 소리의 의미론적 분할 작업에서 현존하는 최고 수준의 모델보다 뛰어난 성능을 보였습니다. 또한, 크로스-모달 검색(cross-modal retrieval) 성능에서도 우수한 결과를 나타냈으며, 자연스럽게 오디오와 비디오 간의 대응을 소리 및 언어 성분으로 분리하는 데 성공했습니다.

### [Merging Improves Self-Critique Against Jailbreak Attacks](https://arxiv.org/abs/2406.07188)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07188.png)

Vote: 3

Authors: Victor Gallego

- **What's New**: 대규모 언어 모델 (LLM)은 다양한 텍스트 생성 작업에서 놀라운 성과를 거두었지만, 적대적 조작, 특히 'jailbreak 공격'에 취약한 문제는 여전히 큰 도전 과제입니다. jailbreak 공격은 적대적인 프롬프트가 LLM을 조작하여 해롭거나 부적절한 출력을 생성하도록 유도하는 경우를 말합니다. 본 연구에서는 LLM을 이러한 공격으로부터 방어하기 위한 새로운 프레임워크를 소개하며, 이는 기본 모델의 출력 작성 능력을 향상시키고 인조 데이터로 추가 미세조정을 통해 모델의 강인성을 개선하는 방법을 포함합니다.

- **Technical Details**: 제안된 프레임워크에서 먼저 기본 모델이 출력물을 철저히 검사하고 수정하는 능력을 개선한 후, 정제된 인조 데이터를 생성 및 활용하여 모델을 미세 조정합니다. 최근의 자기 비평 기법(Self-Critique, Madaan et al., 2023)을 확장하여 외부 비평 모델(External Critic Model)을 도입하고, 이를 원래 모델과 병합하여 자기 비평 기능을 더욱 강화합니다. 이는 수작업으로 라벨링된 데이터 없이도 적용 가능하여 널리 유용할 수 있습니다.

- **Performance Highlights**: 본 연구에서는 '모델 병합' 기법을 활용하여 LLM을 개선하는 방법을 탐구합니다. 이를 통해 하나의 모델에 여러 모델의 가중치를 혼합하여 단일 추론 패스로 성능을 향상시킬 수 있습니다. 이 방법은 확률적 가중치 평균화(Stochastic Weight Averaging)와 선형 모드 연결성(Linear Mode Connectivity)을 기반으로 합니다. 본 연구에서 제안된 기법은 모델의 강인성을 현저히 향상시킬 수 있음을 실험적으로 입증하였습니다.

### [Neural Gaffer: Relighting Any Object via Diffusion](https://arxiv.org/abs/2406.07520)

![](/avatars/e4f630616a16a7fc691b5275bb62f2aa.svg)

Vote: 3

Authors: Yuanbo Xiangli, Noah Snavely, Jin Sun, Sai Bi, Fujun Luan, Yuan Li, Kai Zhang, Zexiang Xu, Haian Jin

- **What's New**: 새로운 논문에서는 Neural Gaffer라는 카테고리 무관의 단일 이미지 재조명(single-image relighting) 모델을 제안합니다. 이 모델은 단일 2D 이미지와 고역동범위(HDR) 환경 맵을 입력으로 받아 다양한 조명 조건에서의 재조명 이미지를 생성할 수 있습니다. 이는 기존 단일 이미지 재조명 모델들이 특정 카테고리에서만 작동하는 것과 다르게, Neural Gaffer는 다양한 객체와 장면에서 일반화가 가능합니다.

- **Technical Details**: Neural Gaffer는 강력한 확산 모델(diffusion model)을 활용하며, 다양한 데이터셋에 훈련되어 물리적 사전 정보(physical priors)를 학습합니다. 이 모델은 고품질의 합성 데이터셋으로 훈련되었으며, High Dynamic Range (HDR) 환경 맵과 물리 기반 물질을 포함하고 있습니다. Neural Gaffer는 2D 이미지의 재조명 뿐만 아니라, 객체 삽입(object insertion)과 같은 2D 이미지 편집 작업과 신경 방사장(neural radiance fields)을 활용한 3D 재조명에도 적합합니다.

- **Performance Highlights**: Neural Gaffer의 성능은 합성 이미지와 실제 이미지를 모두 포함한 테스트에서 뛰어난 일반화와 정확성을 보여주었습니다. 특히, 모델은 물리 기반의 재조명 효과를 보다 정확하게 반영할 수 있으며, 이전의 모델 기반 접근법이 겪던 한계들을 극복했습니다. 또한, Neural Gaffer는 다른 생성 방법들과 통합하여 다양한 2D 이미지 편집 작업을 쉽게 수행할 수 있는 기능도 가지고 있습니다.

