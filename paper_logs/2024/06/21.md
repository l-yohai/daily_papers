## Daily Papers (2024-06-21)

### [$\nabla^2$DFT: A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Network Potentials](https://arxiv.org/abs/2406.14347)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14347.png)

Vote: 93

Authors: Dmitry Protasov, Artem Tsypin, Anton Ber, Artur Kadurin, Mikhail Shirokikh, Egor Rumiantsev, Kuzma Khrabrov, Alexander Telepov, Ilya Shenbin, Anton Alekseev, Sergey Nikolenko, Elena Tutubalina, Konstantin Ushenin

- **What's New**: 이번 연구에서는 새로운 딥러닝 모델을 소개합니다. 이 모델은 기존 모델 대비 향상된 성능을 보여주며, 특정 어플리케이션에서의 응용 가능성을 검토합니다.
- **Technical Details**: 제안된 모델은 Transformer 아키텍처를 기반으로 하며, self-attention 메커니즘을 통해 데이터의 연관성을 효과적으로 학습합니다. 또한, 이 모델은 데이터 전처리 단계에서 새로운 기술을 도입하여 입력 데이터를 효율적으로 변환합니다. 추가적으로, 모델 훈련 과정에서도 hyperparameter tuning 기법을 통해 최적의 성능을 도출하였습니다.
- **Performance Highlights**: 실험 결과, 제안된 모델은 여러 벤치마크 데이터셋에서 최첨단 기술 대비 우수한 성능을 보였습니다. 특히, IMDB 리뷰 데이터셋에서 정확도(accuracy)가 기존 모델 대비 5% 이상 개선되었으며, 다른 자연어 처리(NLP) 작업에서도 일관되게 높은 성능을 보였습니다.

### [Instruction Pre-Training: Language Models are Supervised Multitask Learners](https://arxiv.org/abs/2406.14491)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14491.png)

Vote: 66

Authors: Shaohan Huang, Minlie Huang, Daixuan Cheng, Furu Wei, Junyu Bi, Yuxian Gu

- **What's New**: 최근 논문에서는 일반 인공지능을 향한 새로운 접근법으로 Instruction Pre-Training을 소개하였습니다. 이는 기존의 원시 코퍼스를 직접적으로 학습하는 방식 대신, 방대한 원시 코퍼스의 내용을 기반으로 한 instruction-response 쌍을 생성하여 해당 쌍을 포함한 코퍼스를 학습하는 새로운 방법입니다.
- **Technical Details**: Instruction Pre-Training은 instruction synthesizer를 사용하여 원시 텍스트에서 다양한 instruction-response 쌍을 만들어 내는 방법입니다. 이 쌍들은 고유한 작업 입력과 작업 출력을 나타내며, 원시 코퍼스 데이터에 기반하여 생성됩니다. 이는 OpenAI 등의 폐쇄형 모델 대신, 공개된 대규모 모델(Mistral-7B-v0.1)을 활용하여 비용 효율성을 높였습니다.
- **Performance Highlights**: ['500M 모델을 100B 토큰으로 Pre-Training한 결과는 300B 토큰으로 학습한 1B 모델의 성능에 도달.', 'Instruction Pre-Training을 거친 모델은 추가적인 instruction tuning에서 더욱 큰 성능 향상을 보임.', '금융 및 생의학 분야에서 Llama3-70B 모델과 비교할 때 Llama3-8B 모델의 성능을 향상시킴.']

### [The Devil is in the Details: StyleFeatureEditor for Detail-Rich StyleGAN Inversion and High Quality Image Editing](https://arxiv.org/abs/2406.10601)

![](/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg)

Vote: 63

Authors: Aibek Alanov, Denis Bobkov, Dmitry Vetrov, Vadim Titov

- **What's New**: 최근 몇 년 동안, GANs는 이미지 생성 분야에서 인상적인 성과를 보였으며, StyleGAN 모델이 특히 뛰어난 효과를 보여줍니다. 본 논문은 StyleGAN의 잠재 공간(Latent space)에서 실제 이미지의 내부 표현을 찾아야 하는 문제인 GAN Inversion을 다루고 있습니다. 기존 최적화 및 인코더 기반 방법의 한계를 넘어, 높은 품질의 재구성과 편집 가능성을 동시에 지니며, 빠른 추론 속도를 가지는 새 방법을 제안합니다.
- **Technical Details**: 기존 GAN Inversion 방법은 최적화 기반과 인코더 기반으로 나뉩니다. 최적화 기반 방법은 각 입력 이미지에 대한 잠재 코드를 학습하여 높은 품질의 재구성을 하지만, 잠재 공간의 원래 분포로부터 벗어나 편집 품질이 떨어지는 단점이 있습니다. 인코더 기반 방법은 단일 네트워크 패스로 잠재 표현을 얻을 수 있지만, 동시에 높은 품질과 편집 가능성을 달성하기 어렵습니다. 저자들은 고차원 ℱ_𝑘 공간에서 인코더를 학습하는 두 단계 프레임워크를 제안하여 이 문제를 해결하고자 합니다.
- **Performance Highlights**: 제안하는 방법은 기존 최첨단 방법들보다 훨씬 높은 재구성 품질과 우수한 편집 가능성을 보입니다. 특히, StyleRes[28] 대비 LPIPS 및 L2 지표에서 4배 이상의 개선을 달성하였으며, 실행 시간도 기존 인코더 기반 방법과 유사합니다.

### [HARE: HumAn pRiors, a key to small language model Efficiency](https://arxiv.org/abs/2406.11410)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11410.png)

Vote: 35

Authors: Xuewen Shen, Shiqi Chen, Lingyun Zhang, Houqian Zhang, Yongneng Jiang, Gaojian Ge, Bin jin, Mingyong Wu, Shi Pu, Lunhui Liu

- **What's New**: 이 논문에서는 작은 언어 모델(Small Language Models, SLMs)이 대형 모델의 스케일링 법칙에 따른 제약을 극복하기 위해 새로운 데이터 구축 원리를 제안합니다. HARE-1.1B라는 SLM을 교육하여 기존 최고 성능의 SLM과 비교했을 때 우수한 성능을 보였습니다.
- **Technical Details**: 데이터 구축 파이프라인은 크게 네 단계로 구성됩니다. 첫째, 대규모 웹 스크랩 데이터에서 고품질의 범주화된 데이터를 추출하여 의미론적 다양성과 일관성을 유지합니다. 둘째, Mixtral-8×7B 모델을 사용해 주제별 데이터와 다양한 프롬프트를 결합하여 합성 데이터를 생성합니다. 셋째, 자연 언어 형태로 많은 NLP 작업 데이터를 구성합니다. 넷째, 각 데이터의 엄격한 정화를 통해 벤치마크 데이터의 누출을 방지합니다.
- **Performance Highlights**: HARE-1.1B는 Mistral 아키텍처를 사용하며, 16개의 Nvidia-H800 GPU에서 30일 동안 약 600억 토큰을 처리하며 교육되었습니다. 이 모델은 대규모 벤치마크 데이터셋에서 우수한 성능을 발휘하며, 데이터 다양성과 품질 일관성을 모두 확보함으로써 SOTA SLM들과 경쟁할 수 있는 수준의 성능을 입증했습니다.

### [Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs](https://arxiv.org/abs/2406.14544)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14544.png)

Vote: 32

Authors: Lin Chen, Yuxuan Qiao, Songyang Zhang, Haodong Duan, Kai Chen, Jiaqi Wang, Dahua Lin, Junming Yang, Xinyu Fang

- **What's New**: Prism이라는 프레임워크가 소개되었습니다. 이 프레임워크는 VLMs (Vision Language Models)의 지각(perception)과 추론(reasoning) 과정을 분리하여 성능을 평가할 수 있도록 설계되었습니다. Prism은 시각 정보 추출 및 텍스트로 변환하는 지각 단계와, 이를 바탕으로 질문에 답하는 대규모 언어 모델(LLM)의 추론 단계를 포함합니다.
- **Technical Details**: Prism은 두 가지 모듈, 즉 지각 모듈과 추론 모듈로 구성됩니다. 지각 모듈은 VLM으로부터 시각 정보를 추출하고 텍스트로 표현하며, 추론 모듈은 이 텍스트 정보를 기반으로 질문에 대한 답변을 생성합니다. 이 프레임워크를 통해 다양한 VLM의 지각 능력과 추론 능력을 개별적으로 평가할 수 있습니다. 지각 능력 평가를 위해 일정한 LLM(예: ChatGPT 3.5)을 사용하고, 다양한 VLM을 테스트합니다. 반대로, 고정된 VLM을 사용하고 다른 LLM을 변경하여 추론 능력을 평가합니다.
- **Performance Highlights**: 프리즘을 활용한 분석 결과 주목할 만한 몇 가지 사실이 밝혀졌습니다. 독점 VLM (예: GPT-4v)들은 지각 능력에서 선두를 달리고 있으며, 오픈 소스 VLM들은 언어 모델 크기와 관계없이 비교적 일관된 지각 성능을 보였습니다. 작은 규모의 언어 모델(7B variants)을 사용하는 오픈 소스 VLM의 성능은 제한된 추론 능력으로 인해 제약을 받았습니다. 또한, Prism을 사용하여 비교한 결과, 약 2B-파라미터의 LLaVA와 강력한 LLM을 결합시켰을 때, 더 큰 모델들과 유사한 성능을 보여주었습니다.

### [MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding](https://arxiv.org/abs/2406.14515)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14515.png)

Vote: 27

Authors: Yining Li, Kangrui Mao, Haodong Duan, Kai Chen, Dahua Lin, Xiangyu Zhao, Xinyu Fang

- **What's New**: 대규모 비디오 랭귀지 모델(Large Vision-Language Models, Video-LLMs)을 보다 정밀하게 평가하기 위해 새로운 비디오QA 벤치마크인 MMBench-Video를 개발했습니다. 이 벤치마크는 뉴스, 스포츠 등 16개의 주요 카테고리에 걸쳐 약 600개의 웹 비디오를 포함하며, 각 비디오는 30초에서 6분 사이의 길이입니다.
- **Technical Details**: MMBench-Video는 2,000개의 고유 질문-답변 쌍을 포함하며, 이는 자원봉사자들에 의해 기여되었습니다. 이러한 질문들은 26개의 세분화된 능력을 평가할 수 있도록 설계되었습니다. 또한, 이전에 GPT-3.5를 사용한 평가의 한계를 극복하기 위해 GPT-4를 사용한 평가 방식을 도입하여, 정확성, 일관성 및 인간 판단과의 정렬 면에서 개선된 평가 품질을 제공합니다.
- **Performance Highlights**: MMBench-Video에서 주류 LVLMs, 특히 오픈소스 및 상용 LVLMs의 성능을 평가했습니다. 놀랍게도, 기존의 Video-LLMs는 상용 LVLMs뿐만 아니라 Idefics2 및 InternVL-Chat-v1.5와 같은 오픈소스 LVLMs에도 크게 뒤처지는 성과를 보였습니다. 이는 Video-LLMs가 공간적 및 시간적 이해에서 현재의 한계를 명확히 드러내며, 향후 연구와 개발에 중요한 시사점을 제공합니다.

### [Model Merging and Safety Alignment: One Bad Model Spoils the Bunch](https://arxiv.org/abs/2406.14563)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14563.png)

Vote: 26

Authors: Mete Ozay, Philip Torr, Fabio Pizzati, Umberto Michieli, Bernard Ghanem, Adel Bibi, Hasan Abed Al Kader Hammoud

- **What's New**: 이 논문에서는 대규모 언어 모델 (LLM, Large Language Models)을 병합하는 기술이 모델의 안전성 정렬(safety alignment)에 어떤 영향을 미치는지 탐구합니다. 기존의 모델 병합 기법에서는 이러한 안전성 문제가 다루어지지 않았기에, 저자들은 안전성을 고려한 모델 병합 기법을 제안합니다.
- **Technical Details**: 저자들은 안전성 정렬을 별도의 작업으로 간주하고 이를 최적화하는 간단하지만 효과적인 방법을 설계했습니다. 먼저 합성을 위한 합성 데이터를 생성한 후, 기존 기법을 활용하여 데이터 기반 병합 최적화 절차를 수행합니다. 두 개의 데이터 세트를 생성하여, 하나는 안전성을 유지하고 다른 하나는 도메인별 지식을 전이하는 데 사용됩니다. 이렇게 생성된 데이터는 데이터 기반 병합 접근법을 사용하여 병합 시 손실을 최소화하도록 최적화합니다.
- **Performance Highlights**: 안전성을 고려한 병합 파이프라인을 통해 병합된 모델이 더 높은 안전성을 유지하면서도 정확도를 떨어뜨리지 않는다는 것을 입증했습니다. 실험 결과, 제안된 방식이 기존 방법보다 더욱 안전하고 정확하게 모델을 병합할 수 있음을 확인했습니다.

### [Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities](https://arxiv.org/abs/2406.14562)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14562.png)

Vote: 26

Authors: Carl Vondrick, Sachit Menon, Richard Zemel

- **What's New**: 새로운 연구에서는 멀티모달 대형 언어 모델(Multimodal Large Language Models, MLLMs)을 활용한 '화이트보드 방식의 사고(Whiteboard-of-Thought, WoT)' 기법을 소개합니다. 이 기법은 시각적 추론을 요구하는 문제를 해결하기 위해 이미지와 텍스트 입력을 모두 수용하는 모델의 능력을 활용합니다. 이를 통해 MLLMs는 중간 추론 과정을 시각화하고, 자신이 생성한 이미지를 기반으로 답을 도출할 수 있습니다.
- **Technical Details**: WoT 기법은 MLLMs가 주어진 문제에 대한 시각화를 생성할 수 있도록 파이썬의 Matplotlib 및 Turtle 라이브러리를 활용하여 코드를 작성하도록 유도합니다. 생성된 코드는 실행되어 이미지로 렌더링되며, 생성된 이미지는 모델이 최종 답을 도출하기 위한 추가적인 시각적 추론 단계에서 사용됩니다. 이를 통해 기존의 텍스트 기반 중간 추론 기술(Chain-of-Thought, CoT)과의 성능 격차를 좁히고자 합니다.
- **Performance Highlights**: 연구진은 BIG-Bench의 ASCII 아트 이해와 최근의 공간 추론 능력을 평가하는 벤치마크에서 WoT 방식의 큰 성능 격차를 입증했습니다. 이들은 텍스트 토큰 대신 시각적 토큰을 활용한 추론이 특정 문제 유형에 더 적합함을 발견했습니다. 실험 결과, WoT는 텍스트 기반 접근방식인 Direct와 CoT보다 나은 성능을 보였습니다.

### [Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps](https://arxiv.org/abs/2406.14539)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14539.png)

Vote: 24

Authors: Mikhail Khoroshikh, Dmitry Baranchuk, Nikita Starodubcev, Artem Babenko

- **What's New**: 최근 텍스트-이미지 변환 확산 모델(text-to-image diffusion models)이 사용자 제공 텍스트 프롬프트(prompt)로부터 이미지 생성을 주도하는 패러다임으로 자리 잡고 있습니다. 이러한 모델은 그래픽 편집 도구로서 높은 품질의 이미지를 제공하지만, 순차적 샘플링 절차(sequential sampling procedure)로 인해 추론 속도가 느려 실용적인 적용에 한계가 있습니다. 이를 해결하기 위해 여러 연구에서는 확산 디스틸레이션(diffusion distillation)을 통해 확산 단계를 줄여 빠른 고품질 생성을 목표로 하고 있습니다. 특히, 역전 가능한 일관성 증류(invertible Consistency Distillation, iCD) 프레임워크를 제안하여 텍스트-이미지 생성의 높은 정확도와 이미지 코딩의 정밀도를 확보했습니다. 또한, 동적 클래스프리 가이던스(dynamic Classifier-Free Guidance)를 도입하여 이미지 반전 품질을 크게 향상시켰습니다.
- **Technical Details**: 텍스트-이미지 모델에서 높은 충실도의 텍스트-이미지 생성을 위해 클래스프리 가이던스(classifier-free guidance, CFG)를 활용합니다. 그러나 기존 디스틸레이션 방식은 몇 단계 추론을 위해 모드 커버리지(mode coverage)나 이미지 품질을 희생하는 경향이 있었으나, iCD 프레임워크는 이러한 문제를 해결하고자 합니다. iCD는 고품질 이미지 생성을 가능하게 하면서 적은 샘플링 단계에서 정확한 반전을 지원하는 일반화된 일관성 모델링 프레임워크입니다. 또한, 동적 CFG는 초반 샘플링 단계에서 탐색을 촉진하고 반전기반 편집을 보다 효율적으로 구현할 수 있게 합니다.
- **Performance Highlights**: iCD는 Stable Diffusion 1.5 및 XL과 같은 대규모 텍스트-이미지 모델에 적용되었으며, 이는 이미지 편집 문제에 대해 광범위하게 평가되었습니다. 자동 및 인간 연구에 따르면, iCD는 기존 텍스트 기반 이미지 편집 방법과 비교해 여러 배 빠르게 6−8 샘플링 단계에서 충실도 높은 텍스트 지향 이미지 편집을 가능하게 합니다.

### [PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents](https://arxiv.org/abs/2406.13923)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.13923.png)

Vote: 20

Authors: Ge Zhang, Qunshu Lin, Yin Zhang, Bei Chen, Wenhu Chen, Tiezhen Wang, Zekun Wang, Wenhao Huang, Jie Fu, Minghao Liu, Yubo Wang, Junjie Wang, Chunyang Jiang, Kang Zhu, Yatai Ji, Yuxiang Zhang

- **What's New**: 최근의 대규모 멀티모달 모델(Large Multimodal Models, LMM)의 발전은 차트 추론 및 현상 이해와 같은 다양한 지식 기반 작업에 성공적으로 응용되고 있습니다. 그러나, 최신 벤치마크 연구는 지각적 오류와 추론 오류라는 두 가지 주요 오류 유형을 강조하였습니다. 이를 해결하고 지식 집약적인 LMM을 훈련하기 위해, 우리는 PIN(Paired and Interleaved multimodal documents)이라는 새로운 데이터 형식을 제안합니다.
- **Technical Details**: PIN 형식은 두 가지 주요 구성 요소로 구성됩니다: (1) Markdown 파일: 지식 집약적인 문서를 포함하며 간단한 마크업 구문을 사용해 이해를 돕습니다. (2) 전체 이미지: 레이아웃과 텍스트-이미지 간의 연결을 학습할 수 있게 합니다. 또한, PIN 형식은 이미지-텍스트 쌍 훈련과 교차 멀티모달 훈련과 같은 여러 학습 전략을 지원하며, 기존 데이터세트를 PIN 형식으로 변환할 수 있는 확장성을 가집니다.
- **Performance Highlights**: 우리는 PIN-14M이라는 1414만 개 샘플의 오픈 소스 데이터세트를 출시하였습니다. 이 데이터에는 웹 페이지뿐만 아니라 다이어그램과 차트와 같은 과학 문서도 포함되어 있습니다. 초기 데이터세트로서 PIN-14M은 다양한 요구를 충족시키도록 설계되었습니다. 향후 더 큰 규모의 데이터세트를 제공하고, PIN 형식의 유효성을 탐구하기 위한 추가 실험을 계획하고 있습니다.

### [GLiNER multi-task: Generalist Lightweight Model for Various Information Extraction Tasks](https://arxiv.org/abs/2406.12925)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12925.png)

Vote: 17

Authors: Mykhailo Shtopko, Ihor Stepanov

- **What's New**: 이 논문에서는 GLiNER 기반 새로운 정보 추출(Information Extraction, IE) 접근법을 제안합니다. 이는 과거의 제한사항을 극복하고, 제로샷 네임드 엔티티 인식(NER)과 다른 정보 추출 벤치마크에서 최고 성능을 실현합니다.
- **Technical Details**: 본 모델은 GLiNER 토큰 분류 아키텍처를 기반으로 하며, DeBERTA v3 large를 사용합니다. 이 모델은 트랜스포머(Transformer) 메커니즘을 통해 라벨과 텍스트 사이의 정보 교환을 가능하게 하여 라벨과 토큰 임베딩(embedding)을 추출합니다. 최종 스코어 예측을 위한 MLP 모듈을 거쳐 출력을 생성합니다. Synthetic 데이터셋과 고퀄리티 데이터셋을 사용하여 2단계 학습을 실시했습니다.
- **Performance Highlights**: 제안된 접근법은 기존의 제약을 극복하며, 정확성 및 효율성을 크게 향상시킵니다. 특히, 다양한 정보 추출 작업에서 높은 적응성과 성능을 보여줍니다.

### [DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning](https://arxiv.org/abs/2406.11896)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/61568f37272f2d87a99ba884/lgvkl5f0rEyiQRVU5FE32.png)

Vote: 16

Authors: Jiayi Pan, Mert Cemri, Alane Suhr, Hao Bai, Aviral Kumar, Sergey Levine, Yifei Zhou

- **What's New**: Android in the Wild (AiTW)라는 새로운 대규모 데이터셋이 소개되었습니다. 이 데이터셋은 Android 디바이스 제어를 위한 여러 하위 집합들로 구성되어 있으며, 이 논문에서는 General과 Web Shopping 하위 집합을 집중적으로 다룹니다. 이 두 섹션은 다단계 작업을 평가하기에 적합한 데이터셋입니다.
- **Technical Details**: General 하위 집합은 정보를 검색하고 기본 애플리케이션을 사용하는 작업에 중점을 둡니다. Web Shopping 하위 집합은 다양한 쇼핑 웹사이트에서의 검색 지침을 포함하며, 캡차 검증 등을 피하기 위해 일부 작업이 후처리되었습니다. Offline data (오프라인 데이터)는 초기 AutoUI 정책을 통해 수집되었으며, Offline-to-Online (오프라인-온라인) 설정과 Offline (오프라인) 설정으로 구분하여 사용되었습니다. 필터된 BC와 DigiRL의 효율성을 비교하기 위해 두 가지 horizon limits (H=10, H=20)로 설정하여 배운 결과를 분석하였습니다.
- **Performance Highlights**: DigiRL은 필터된 BC에 비해 더 짧은 average rollout lengths (평균 롤아웃 길이)를 일관되게 달성하였습니다. 특히, 잘못된 화면에서 복구하는 능력이 뛰어나며, GPT-4V와 AutoUI보다 더 효율적으로 작업을 수행할 수 있습니다. GPT-4V는 높은 수준에서 계획을 수립하지만, 구체적인 제어 동작에서 실수가 발생하면 복구하지 못하는 문제가 있습니다. 주요 실패 모드로는 실수에서의 복구 실패, 잘못된 링크 클릭, 완전한 시도 실패 등이 있습니다.

### [Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models](https://arxiv.org/abs/2406.13542)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.13542.png)

Vote: 14

Authors: Keming Lu, Jingren Zhou, Tingyu Xia, Chengpeng Li, Guanting Dong, Bowen Yu, Chang Zhou

- **What's New**: AutoIF는 LLM(Large Language Models)의 명령 수행 능력을 향상시키기 위한 자동, 확장 가능한 신뢰성 높은 방법을 소개합니다. AutoIF는 SFT(Supervised Finetuning)와 RLHF(Reinforcement Learning from Human Feedback) 훈련 데이터를 자동으로 생성합니다.
- **Technical Details**: AutoIF의 핵심 아이디어는 코드로 명령 수행의 정확성을 검증하는 것입니다. 이를 위해 (1) 코드로 검증할 수 있는 명령을 자동으로 생성하고, (2) 이 명령에 대한 검증 코드를 자동으로 생성하며, (3) 첫 두 단계를 신뢰성 있게 유지합니다. 간단한 작업을 위해 LLM이 시드 명령을 생성하고, 검증 코드를 작성한 후 컴파일과 테스트케이스를 통해 검증합니다.
- **Performance Highlights**: AutoIF는 Qwen2-72B와 LLaMA3-70B 모델을 SFT, Offline DPO, Online DPO로 훈련했을 때 IFEval 및 FollowBench 벤치마크에서 90%를 넘는 성과를 기록했습니다. 특히 FollowBench 벤치마크에서 평균 성능이 4% 이상 향상되었습니다. 또한, 이러한 데이터를 기반으로 첫 번째로 대규모 오픈소스 복잡 명령 수행 데이터셋을 공개할 예정입니다.

### [Improving Visual Commonsense in Language Models via Multiple Image Generation](https://arxiv.org/abs/2406.13621)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.13621.png)

Vote: 13

Authors: Yossi Adi, Idan Schwartz, Sagie Benaim, Guy Yariv

- **What's New**: 이번에 제안된 방법은 VLMs(Vision Language Models)가 비주얼 정보에 지나치게 의존하면서 기본적인 상식적 추론에서 성능 저하를 겪는 문제를 해결하려고 시도합니다. 이를 위해 텍스트와 이미지를 늦은 단계에서 융합(fusion)하는 새로운 아키텍처를 개발하였으며, 입력 텍스트를 기반으로 여러 이미지를 생성하고 이를 통합하는 추론 기반 절차를 제안합니다.
- **Technical Details**: 제안된 방법은 두 가지 주요 구성 요소로 나뉩니다. 첫 번째는 텍스트와 이미지의 늦은 단계 융합을 허용하는 새로운 아키텍처를 도입하는 것입니다. 훈련 과정에서는 사전 훈련된 멀티모달 인코더(multimodal encoder)를 통해 이미지를 인코딩하고, 이를 프로젝트를 통해 가상 텍스트 토큰(pseudo-text tokens) 임베딩으로 변환합니다. 동시에 입력 텍스트는 사전 훈련된 LLM을 통해 인코딩되며, 마지막 단계에서 텍스트 토큰과 가상 텍스트 토큰을 융합하는 메커니즘을 사용해 예측을 수행합니다. 두 번째는 추론 과정에서 입력 텍스트를 조건으로 사전 훈련된 텍스트-이미지 모델을 사용해 여러 이미지를 생성하고, 이를 통해 다양한 예측을 통합하는 것입니다. 이 과정에서 각 이미지 변형(version)으로부터 생성된 확률 벡터를 가중 평균하여 최종 출력을 도출합니다.
- **Performance Highlights**: 제안된 접근 방식을 사용하면 객체 및 시각적 상식(Object and visual commonsense) 작업에서 평가된 다양한 베이스라인을 크게 능가하는 성능을 보였습니다. 또한, 텍스트 기반 상식적 추론(text-based commonsense reasoning) 작업에서도 성능이 조금 향상되었습니다. 이 방법은 객체 상식 작업에서는 색상, 모양, 크기와 관련된 질문을 다루는 zero-shot 벤치마크에서, 시각적 상식 작업에서는 다양한 도메인에 걸친 고품질 질문-답변 쌍으로 구성된 ImageNetVC 데이터셋을 사용해 평가되었습니다. 실험 결과, 제안된 방법의 각 구성 요소의 중요성을 분석하는 ablation 연구 결과도 포함됩니다.

### [LiveMind: Low-latency Large Language Models with Simultaneous Inference](https://arxiv.org/abs/2406.14319)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14319.png)

Vote: 13

Authors: Cheng Zhuo, Grace Li Zhang, Bing Li, Chuangtao Chen, Ulf Schlichtmann, Xunzhao Yin

- **What's New**: 최근 연구에서는, 대형 언어 모델(LLMs)이 자연어 처리(NLP)에서 뛰어난 성능을 보이고 있지만, 이러한 모델의 자가 회귀적 디코딩(nature)과 커다란 모델 크기로 인해 응답 생성 속도가 느려지는 문제가 발생하고 있다. 이를 해결하기 위해 새로운 접근 방식인 LiveMind 프레임워크가 제안되었다. 이 프레임워크는 사용자의 입력이 완료되기 전에 불완전한 프롬프트를 통해 실시간으로 추론을 시작하여 최종 출력 속도를 크게 향상시키는 것을 목표로 한다.
- **Technical Details**: LiveMind 프레임워크는 사용자가 입력하는 동안 LLM이 불완전한 프롬프트를 통해 추론을 수행할 수 있도록 설계되었다. 이를 통해 사용자가 입력을 완료한 후 최종 출력 단계에서 필요한 추론 단계를 줄일 수 있다. 이 프레임워크는 초기 추론 단계에서 대형 언어 모델(LLM)을 사용하고 최종 출력 단계에서는 소형 언어 모델(SLM)을 사용하여 응답 시간을 더욱 단축시키면서도 높은 추론 정확도를 유지한다. 또한, 이 프레임워크는 실시간 상호작용 시나리오를 위해 설계되었으며, 기존의 머신번역과 같은 순차적(seq-to-seq) 작업과는 다른 일반적인 상호작용 작업에 초점을 맞추고 있다.
- **Performance Highlights**: LiveMind 프레임워크는 다양한 도메인에 걸친 Llama-3-8B 및 Llama-3-70B 모델을 활용한 평가를 통해 그 효과성을 입증했다. 예를 들어, MMLU-Pro 데이터셋에서 Llama-3-70B 모델을 사용할 경우, 기존의 완전한 프롬프트를 사용하는 추론 방법에 비해 59.2% 더 빠른 응답 속도를 보여주면서 유사한 정확도를 유지했다. 또한, Llama-3-70B와 Llama-3-8B 모델을 결합할 경우, 지연 시간이 평균 93% 감소하며 정확도는 5.5% 증가했다. 이러한 결과는 일반적인 상호작용 시나리오에서 대형 언어 모델의 실시간 추론에 대한 가능성을 제시한다.

### [Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level](https://arxiv.org/abs/2406.11817)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11817.png)

Vote: 13

Authors: Chao Yang, Wanli Ouyang, Jie Liu, Jiaheng Liu, Zhanhui Zhou, Han-Sen Zhong, Xingyuan Bu

- **What's New**: 새로운 연구인 'Iterative Direct Preference Optimization' (iDPO)가 소개되었습니다. 이 연구는 인간 피드백을 통한 학습 방법을 제시하며, 모델을 GPT-4 수준으로 향상시킬 수 있다는 것을 보여줍니다. 핵심 기여는 모델의 반복 학습에서 발생하는 'verbose' 문제를 해결하기 위해 길이 페널티를 추가한 'iLR-DPO' 방식을 도입했다는 점입니다.
- **Technical Details**: 논문은 기본 언어 모델을 주어진 보상 모델에 대해 최적화하는 간단한 방법인 iterative length-regularized DPO (iLR-DPO)를 제안합니다. 이 방법은 두 단계로 반복적으로 진행됩니다: (1) 보상 모델로부터 합성 선호도를 수집하고 (2) 합성 선호도에 길이 페널티를 추가하여 언어 모델을 최적화합니다. 이 과정은 매 반복마다 최신 모델 체크포인트에서 시작하며, DPO 방식에 길이 페널티를 추가함으로써 응답의 장황함을 줄입니다.
- **Performance Highlights**: iLR-DPO는 강력한 베이스라인들을 능가하여 언어 모델을 인간의 가치에 맞게 정렬하는 데 성공적임을 보여주었습니다. 특히, iLR-DPO는 AlpacaEval 2.0에서 GPT-4 Preview를 상대로 50.5%의 길이 제어된 승률을 기록했으며, MT-Bench, Arena-Hard, Open LLM Leaderboard 등 표준 벤치마크에서도 뛰어난 성과를 기록했습니다. 이 결과는 인간의 선호도에 맞춘 언어 모델을 정렬하는 데 있어 iLR-DPO의 효과성을 강조합니다.

### [ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning](https://arxiv.org/abs/2406.14130)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14130.png)

Vote: 10

Authors: Wenmeng Zhou, Zhongjie Duan, Yaliang Li, Weining Qian, Cen Chen

- **What's New**: 최근 몇 년 동안 이미지 합성 분야에서 대단한 성과를 냈던 확산 모형(diffusion models)이 비디오 합성으로도 도입되었습니다. 특히, SORA(Liu et al., 2024)의 혁신적인 결과 이후, 비디오 합성 연구가 다시 주목받고 있습니다. 현재 비디오 합성 모델이 높은 품질의 영상 클립을 생성할 수 있음에도 불구하고, 긴 영상을 생성하는 데 있어 많은 도전과제가 남아 있습니다.
- **Technical Details**: 비디오 합성 모델의 긴 영상 생성을 위한 해결책을 세 가지로 나눌 수 있습니다: 1) 긴 영상 데이터셋으로 훈련하기, 2) 스트리밍 또는 슬라이딩 윈도우 방식으로 생성하기, 3) 프레임 보간(Frame Interpolation) 사용하기. 이들은 모두 각각의 한계점을 가지고 있습니다. 이에 영감을 받아, 우리는 ExVideo라는 새로운 포스트 튜닝(post-tuning) 전략을 제안합니다. 이 전략은 기존 비디오 합성 모델이 제한된 계산 자원으로 긴 영상을 생성할 수 있도록 합니다.
- **Performance Highlights**: 이론적으로 ExVideo는 대부분의 기존 비디오 합성 모델과 호환 가능합니다. 우리는 인기 있는 오픈 소스 이미지-비디오 모델인 Stable Video Diffusion(Blattmann et al., 2023)을 사용하여 ExVideo의 효능을 실증적으로 검증했습니다. ExVideo를 통해 원래 25프레임의 한계를 128프레임까지 확장할 수 있었습니다. 이 확장은 모델의 우수한 생성 능력을 저해하지 않았으며, 텍스트-이미지 모델과도 원활하게 통합될 수 있음을 보여주었습니다. 이 연구의 기여는 긴 영상을 생성할 수 있는 비디오 합성 모델의 포스트-튜닝 기술 제안과 이를 기반으로 한 새로운 확장된 모델의 공개에 있습니다.

### [Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation](https://arxiv.org/abs/2406.13663)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.13663.png)

Vote: 7

Authors: Arianna Bisazza, Raquel Fernández, Jirui Qi, Gabriele Sarti

- **What's New**: 새로운 연구에서 mirage라는 모델 내장 기반 답변 출처 프레임워크를 도입했습니다. 이 프레임워크는 RAG(Retrieval-augmented generation) 응용 프로그램을 최적화하고자 합니다.
- **Technical Details**: mirage는 PECoRe(Context-reliance evaluation) 프레임워크의 확장을 이용합니다. 이 방법은 입력 컨텍스트가 추가됨에 따라 발생하는 LM(언어 모델)의 예측 분포 변화를 측정하여 생성된 문장에서 컨텍스트에 민감한 토큰을 식별한 후, 이런 변화를 조정하는 특정 중요한 토큰들을 기울기 기반의 saliency나 다른 피처 속성 기법(feature attribution techniques)을 이용해 출처를 추적합니다.
- **Performance Highlights**: mirage는 XOR-AttriQA 및 ELI5 데이터셋에서 높은 성능을 입증했습니다. 특히 인간 주석과 높은 일치도를 보였고, NLI(Natural Language Inference) 및 self-citation 방법보다도 더 효율적이고 제어할 수 있다는 것을 입증하였습니다.

### [REPOEXEC: Evaluate Code Generation with a Repository-Level Executable Benchmark](https://arxiv.org/abs/2406.11927)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11927.png)

Vote: 7

Authors: Nam Le Hai, Dung Manh Nguyen, Nghi D. Q. Bui

- **What's New**: RepoExec라는 새로운 벤치마크가 공개되었습니다. 이 벤치마크는 저장소 수준에서의 코드 생성을 평가하기 위해 설계되었으며, 현실 세계의 적용 가능성과 코드 기능에 대한 포괄적인 평가에 초점을 맞추고 있습니다.
- **Technical Details**: RepoExec는 다음과 같은 주요 기능을 갖추고 있습니다: 1) 향상된 실행 가능성: 생성된 코드가 실제 환경에서 실행 가능하도록 자동 시스템을 통해 설치 및 런타임 요구 사항을 확인합니다. 2) 동적 테스트 케이스 생성: 새로운 코드의 기능에 맞춘 높은 범위의 테스트 케이스를 자동으로 생성합니다. 3) 의존성 사용 평가: 코드 의존성을 얼마나 잘 활용하는지 평가할 수 있는 파이프라인을 제공하며, 이에 따라 의존성 호출률(Dependency Invocation Rate, DIR)이라는 새로운 메트릭을 도입하였습니다.
- **Performance Highlights**: RepoExec를 사용한 실험에서, 사전 훈련된 모델들(Codellama-34b-Python 등)은 높은 초기 정확도를 보였으나, 명령어 튜닝된(instruction-tuned) 모델들은 의존성 관리에 뛰어났습니다. 특히, 다중 라운드 디버깅 테스트는 pass@1과 DIR 모두에 개선을 보여주었습니다. 우리의 데이터셋은 모델 성능을 향상시키는 동시에 계산 비용을 줄이는 데 기여했습니다.

### [StableSemantics: A Synthetic Language-Vision Dataset of Semantic Representations in Naturalistic Images](https://arxiv.org/abs/2406.13735)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.13735.png)

Vote: 5

Authors: Margaret M. Henderson, Andrew F. Luo, Shaurya Dewan, Leila Wehbe, Michael J. Tarr, Rushikesh Zawar

- **What's New**: 우리의 연구팀은 StableSemantics라는 새로운 데이터세트를 소개합니다. 이 데이터세트는 사람이 생성하고 큐레이션한 프롬프트(prompt), 자연어 캡션(caption), 캡션으로부터 생성된 이미지, 그리고 캡션의 객체에 대응하는 주의 속성 맵(attention attribution map)을 포함하고 있습니다. StableSemantics는 사람들에게 시각적으로 매력적이고 흥미로운 이미지를 포함하며, 이러한 프롬프트를 자연스러운 언어로 변환하여 자연어 캡션을 생성합니다.
- **Technical Details**: StableSemantics 데이터세트는 Stable Diffusion XL 모델을 사용하여 고해상도 이미지를 생성합니다. 텍스트-이미지 합성(text-to-image synthesis) 모델에서 교차 주의(cross-attention) 메커니즘을 사용하여 텍스트 입력과 시각적 표현을 연결하고 있습니다. 또한, 우리는 주어진 텍스트와 시각적 표현 간의 연관성을 기록하여 캡션의 각각의 명사 덩어리(nominal chunk)에 대한 교차 주의 활성화 맵(cross-attention activation map)을 체계적으로 기록했습니다.
- **Performance Highlights**: 우리 데이터세트는 기존의 텍스트-이미지 합성 모델이 얼마나 텍스트와 시각적 개념 간의 연관성을 잘 이해하고 있는지를 평가합니다. StableSemantics는 텍스트와 시각적 개념의 시각적 분포(semantic distribution)의 공간적 분포를 시각화하고, 캡션 모델과 개방형 세그멘테이션 모델의 성능 정렬을 평가합니다.

### [Sampling 3D Gaussian Scenes in Seconds with Latent Diffusion Models](https://arxiv.org/abs/2406.13099)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.13099.png)

Vote: 4

Authors: Titas Anciukevičius, Melonie de Almeida, Daniela Ivanova, Paul Henderson

- **What's New**: 이번 연구에서는 3D 세상을 포착하는 생성 모델을 효율적으로 학습하는 방법을 제안합니다. 특히, 다중 뷰 이미지를 이용한 3D 생성 모델을 구축하여, 낮은 차원의 잠재 공간(latent space)에서 디퓨전 모델(diffusion model)을 활용해 빠르고 정확하게 3D 장면을 샘플링하는 접근법을 다룹니다. 이를 통해 기존 3D 인식 모델 대비 20배 이상 빠른 성능을 보여줍니다.
- **Technical Details**: 본 연구의 핵심 아이디어는 다중 뷰 이미지를 통해 autoencoder를 학습하여 구조화된 3D 표현을 구축함과 동시에 이를 저차원 잠재 공간으로 압축하는 것입니다. Gaussian Splats를 3D 표현으로 채택하여 효율적인 렌더링 및 최적화를 도모하였으며, 잠재 표현 상에서 디퓨전 모델을 학습하여 다양한 3D 장면을 샘플링할 수 있습니다. 제안된 모델은 고가의 볼류메트릭 렌더링(volumetric rendering)을 반복적인 샘플링 과정에서 제거함으로써 속도를 대폭 향상시켰습니다.
- **Performance Highlights**: 제안된 모델은 8개의 3D 장면을 1.6초 만에 샘플링할 수 있어 기존 3D-aware 디퓨전 모델보다 20배 이상 빠릅니다. 또한, 입력 이미지에 불구하고 무조건적 생성, 단일 이미지 기반 3D 재구성, 희박 뷰 기반 3D 재구성 등 다양한 태스크를 지원하며, 깊이(depth)나 세그멘테이션(분할) 정보 없이 다중 뷰 이미지로만 학습 가능합니다.

### [A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models](https://arxiv.org/abs/2406.11289)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11289.png)

Vote: 4

Authors: Philip S. Yu, Jiawei Zhang, Haopeng Zhang

- **What's New**: 자연언어처리(NLP) 분야에서 텍스트 요약이 필수적이고 도전적인 작업으로 꼽히며, 특히 인터넷의 발달로 인해 온라인상에 방대한 텍스트 정보가 제공됨에 따라 요약 연구가 큰 주목을 받고 있습니다. 최근에는 대형 언어 모델(LLMs, Large Language Models)의 등장으로 요약 연구와 산업 제품이 급격히 진화하고 있으며, 요약 성능이 인간에 맞먹는 수준으로 향상되었습니다.
- **Technical Details**: 요약 접근 방식은 네 가지 주요 단계로 구분할 수 있습니다: 통계적 방법, 딥러닝 방식, 사전 훈련 언어 모델(PLM) 미세 조정(fine-tuning) 방식, 현재의 대형 언어 모델 단계입니다. 최근 LLM 시대에는 대규모 텍스트 데이터 코퍼스를 활용하여 복잡한 언어 패턴과 의미 관계, 문맥 단서를 포착하여 고품질의 요약을 생성할 수 있습니다. 이 조사 논문은 대표적인 요약 방법을 체계적으로 분석하며, 요약 연구의 발전 상황과 앞으로의 방향성을 제안합니다.
- **Performance Highlights**: LLM의 등장으로 요약 성능이 크게 향상되었으며, 기존의 통계적 방법과 딥러닝 기반 방식, PLM 미세 조정 방식보다 뛰어난 결과를 보였습니다. 특히, LLM 기반 요약 연구들이 어떠한 벤치마킹 관행, 모델링 연구, 요약 평가 연구에서 탁월한 성과를 나타내었으며, 이는 CNN/DM 데이터셋에서의 정량적 결과 비교에서도 입증되었습니다.

### [From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP](https://arxiv.org/abs/2406.12618)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12618.png)

Vote: 4

Authors: Vagrant Gautam, Marius Mosbach, Tomás Vergara-Browne, Dietrich Klakow, Mor Geva

- **What's New**: 이 논문은 거대 언어 모델(LLMs)의 발전이 자연어 처리(NLP) 분야에 미친 영향을 검토합니다. LLMs는 뛰어난 성능과 새로운 기능을 실현했지만, 공정성, 신뢰성, 책임성, 설명 가능성과 같은 기대를 충족시키지 못하는 '블랙박스'로 여겨지는 문제를 안고 있습니다. 이에 따라 설명 가능성과 분석(IA) 연구가 중요하게 떠오르고 있으며, 이는 LLMs의 효율성, 견고성 및 신뢰성을 향상시키는 것을 목표로 하고 있습니다.
- **Technical Details**: 이 연구는 ACL과 EMNLP에서 발표된 185,384개의 논문에 대한 서지 분석(bibliometric analysis)과 NLP 커뮤니티의 138명에게 설문조사를 실시하여 분석을 진행합니다. 연구는 IA 연구가 NLP 연구에 미치는 영향을 평가하기 위해 복합 방법론을 사용하며, 서지 분석과 설문조사를 통해 더 넓은 범위에서 IA 연구의 영향력을 측정합니다.
- **Performance Highlights**: 주요 발견 사항은 다음과 같습니다: (1) NLP 연구자들은 IA 연구 결과를 기반으로 새로운 연구를 진행하고 있으며, (2) IA 연구는 다양한 이유로 NLP의 여러 하위 분야 및 연구자 개인에게 중요하게 인식되고 있으며, (3) 많은 새로운 비-IA 방법들이 IA 연구 결과에 강하게 영향을 받고 있다는 것입니다.

### [$τ$-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains](https://arxiv.org/abs/2406.12045)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.12045.png)

Vote: 4

Authors: Noah Shinn, Shunyu Yao, Karthik Narasimhan, Pedram Razavi

- **What's New**: 새로운 arXiv 논문은 인공지능(AI) 및 기계 학습(ML) 분야에서 중요한 발전을 소개하고 있습니다. 이 논문은 최신 기본 모델(vertical model) 또는 알고리즘을 이용해 기존의 문제를 해결하거나 새로운 응용 프로그램에 적용한 것입니다.
- **Technical Details**: 이 연구는 Transformer architecture와 같이 AI 모델 개발에서 주요한 기술을 사용합니다. 추가로, 논문에서는 강화 학습(reinforcement learning), 자연어 처리(NLP), 또는 컴퓨터 비전(computer vision) 등의 기술적 영역에서도 다루고 있습니다.
- **Performance Highlights**: 테스트 결과, 이 모델 혹은 알고리즘은 기존의 모델 대비 성능 향상을 보였습니다. 특히 특정 데이터셋에서의 정확도(accuracy), F1 score, 또는 처리 속도(inference time)에서 우수한 결과를 나타냈습니다. 이러한 성과는 실질적인 응용 프로그램에서의 잠재력을 시사합니다.

