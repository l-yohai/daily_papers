## Daily Papers (2024-06-11)

### [Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation](https://arxiv.org/abs/2406.06525)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06525.png)

Vote: 38

Authors: Shilong Zhang, Peize Sun, Yi Jiang, Ping Luo, Zehuan Yuan, Shoufa Chen, Bingyue Peng

- LlamaGen은 비주얼 생성 영역에서 대형 언어 모델의 '다음 토큰 예측' 패러다임을 적용한 새로운 이미지 생성 모델입니다.
- 이 모델은 시각 신호에 대한 귀납적 편향 없이도 적절한 규모로 확장되면 최첨단 이미지 생성 성능을 달성할 수 있음을 보여줍니다.
- 이미지 토크나이저의 디자인 영역, 이미지 생성 모델의 확장 가능성, 그리고 훈련 데이터 품질을 재검토하였습니다.
- 16배 다운샘플링 비율, 0.94 rFID 재구성 품질, 그리고 97%의 코드북 사용률을 가진 이미지 토크나이저를 개발했습니다.
- 111M에서 3.1B 파라미터까지의 클래스 조건부 이미지 생성 모델이 ImageNet 256x256 벤치마크에서 2.18 FID를 달성하며, LDM 및 DiT와 같은 인기 있는 확산 모델을 능가했습니다.
- 775M 파라미터의 텍스트 조건부 이미지 생성 모델은 LAION-COCO와 고품질 미학 이미지에서 2단계 훈련을 통해 시각적 품질과 텍스트 정렬 면에서 경쟁력 있는 성능을 보여주었습니다.
- LLM 서빙 프레임워크의 효과성을 검증하여 이미지 생성 모델의 추론 속도를 326% - 414% 향상시켰습니다.
- 모든 모델과 코드를 공개하여 비주얼 생성 및 멀티모달 기본 모델 관련 오픈소스 커뮤니티에 기여합니다.

### [Vript: A Video Is Worth Thousands of Words](https://arxiv.org/abs/2406.06040)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06040.png)

Vote: 14

Authors: Xiaodong Han, Suyuan Huang, Yao Hu, Haoxin Zhang, Dongjie Yang, Yan Gao, Hai Zhao, Chengqiang Lu

- Vript는 동영상 이해와 제작에서 고성능 모델 성능을 향상시키기 위한 고품질 비디오-텍스트 데이터셋의 필요성에 대응합니다.
- 이 데이터셋은 12K 고해상도 동영상으로 구성된 42만 개 이상의 클립에 상세하고 밀집된 스크립트형 캡션을 제공합니다.
- 각 클립은 약 145단어로 이루어진 캡션을 가지며, 이는 대부분의 기존 비디오-텍스트 데이터셋보다 10배 이상 깁니다.
- 이전 데이터셋이 정적인 콘텐츠만 문서화한 것과 달리, Vript는 콘텐츠뿐만 아니라 카메라 작업, 샷 유형, 카메라 움직임 등을 포함해 비디오 스크립트를 문서화합니다.
- Vript를 이용하여 클립-캡션 쌍보다 더 많은 텍스트와 비디오 모달리티를 맞추는 세 가지 훈련 패러다임을 탐구합니다.
- 이로 인해 Vriptor라는 모델이 개발되었으며, 이는 성능 면에서 공개된 비디오 캡션 모델 중 최고 수준이며, GPT-4V에 비견될 수 있습니다.
- 또한 Vriptor는 긴 동영상에 대한 밀도 높고 상세한 캡션을 엔드 투 엔드로 생성할 수 있는 강력한 모델입니다.
- Vript-Hard는 현재의 벤치마크보다 어려운 세 가지 비디오 이해 작업을 포함하는 벤치마크를 도입합니다.
- Vript-HAL은 비디오 LLM에서의 동작 및 객체 헛구역질을 평가하는 최초의 벤치마크입니다.
- Vript-RR은 긴 비디오 QA에서 질문의 모호성을 해결하는 추론과 검색을 결합합니다.
- Vript-ERO는 짧은 비디오의 동작이 아닌 긴 비디오의 사건에 대한 시간적 이해도를 평가하는 새로운 작업입니다.
- 모든 코드, 모델 및 데이터셋은 https://github.com/mutonix/Vript에서 이용할 수 있습니다.

### [Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning](https://arxiv.org/abs/2406.06469)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06469.png)

Vote: 9

Authors: Joongwon Kim, Bhargavi Paranjape, Tushar Khot, Hannaneh Hajishirzi

- Husky는 다양한 복잡한 작업을 수행하기 위해 도구를 사용하는 개방형 언어 에이전트입니다.
- 수학이나 멀티홉 질문 응답과 같은 특정 작업을 목표로 하는 기존의 대부분의 에이전트와 달리, Husky는 숫자, 표형, 지식 기반의 복잡한 추론 작업을 해결하기 위해 통합된 행동 공간에서 학습합니다.
- Husky는 다음 행동을 생성하고 전문가 모델을 사용하여 행동을 실행하며 현재 해결 상태를 업데이트하는 두 단계를 반복합니다.
- 복잡한 작업 해결을 위한 포괄적인 행동 온톨로지를 도출하고, 이러한 행동을 실행하기 위한 고품질 데이터를 구축합니다.
- 14개의 평가 데이터셋에서 Husky는 이전의 언어 에이전트를 능가하는 성과를 보였습니다.
- HuskyQA라는 평가 셋을 도입하여 혼합 도구 추론 능력과 누락된 지식 검색 및 숫자 추론에 중점을 두어 언어 에이전트를 테스트합니다.
- 7B 모델을 사용함에도 불구하고 Husky는 GPT-4와 같은 최첨단 언어 모델과 동등하거나 능가하는 성과를 보여줍니다.
- Husky의 코드와 모델은 https://github.com/agent-husky/Husky-v1에서 이용 가능합니다.

### [MLCM: Multistep Consistency Distillation of Latent Diffusion Model](https://arxiv.org/abs/2406.05768)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.05768.png)

Vote: 6

Authors: Zhijie Deng, Haonan Lu, Shixiang Tang, Qingsong Xie, Zhenyi Liao, Chen chen

- 큰 잠재 확산 모델(LDM)을 샘플링 속도가 빠른 모델로 증류하는 연구가 증가하고 있음.
- 기존 방법들은 여러 개의 개별 증류 모델이 필요하거나, 샘플링 단계가 제한되며 생성 품질을 희생하는 문제에 직면.
- 멀티 스텝 일관성 증류(MCD) 전략을 LDM에 적용하여 고품질 이미지 합성을 위한 Multistep Latent Consistency Models (MLCMs) 접근법을 제안.
- MLCM은 다양한 샘플링 단계를 위한 통합 모델로 작동하며, 진행형 훈련 전략으로 적은 스텝에서도 높은 품질의 이미지를 생성.
- 교사 모델의 샘플링 경로 상태를 MLCM 훈련 데이터로 활용, 고품질 훈련 데이터셋 요구 사항을 완화하고 훈련과 추론의 격차를 줄임.
- 시각적 품질과 미적 매력을 향상시키기 위해 선호 학습 전략과 호환 가능.
- MSCOCO-2017 5K 벤치마크에서 MLCM은 4 스텝으로 CLIP 점수 33.30, 미학 점수 6.19, 이미지 보상 1.20을 기록하며 기존 모델들을 크게 능가.
- MLCM의 다재다능함을 이용한 통제 가능한 생성, 이미지 스타일 전환, 중국어-이미지 생성 등의 응용을 입증.

### [ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization](https://arxiv.org/abs/2406.05981)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.05981.png)

Vote: 5

Authors: Haoran You, Xiaofan Zhang, Wei Zhou, Yichao Fu, Huihong Shi, Yipin Guo, Amir Yazdanbakhsh, Yingyan Lin, Souvik Kundu

- 대형 언어 모델(LLM)은 뛰어난 성능을 보이나, 자원 제한 장치에서의 배포에는 많은 파라미터와 밀집 곱셈 의존 때문에 높은 메모리 요구와 지연 병목 현상을 겪습니다.
- Shift-and-add 재구성은 주의(aAttention) 및 다층 퍼셉트론(MLP) 층에서의 비싼 곱셈을 하드웨어 친화적인 기본 연산으로 대체하여 이러한 문제를 해결할 수 있는 유망한 솔루션을 제공합니다.
- 현재의 재구성 기법은 정확도를 회복하기 위해 처음부터 다시 훈련하거나 모든 파라미터를 미세 조정해야 하며, 이는 LLM에 대해 자원 집약적입니다.
- 이를 해결하기 위해, 우리는 사후 훈련 후 shift-and-add 재구성을 통해 효율적인 곱셈 없는 모델(ShiftAddLLM)을 만들어 사전 훈련된 LLM을 가속화할 것을 제안합니다.
- 구체적으로 우리는 각 가중치 행렬을 이진 행렬과 그룹 단위 스케일링 인자로 양자화하고, 관련된 곱셈을 이진 행렬에 따라 (1) 활성화와 스케일링 인자 간의 shifts와 (2) 쿼리 및 추가 연산으로 재구성합니다.
- 정확도 손실을 줄이기 위해, 우리는 가중치 및 출력 활성화 재구성 오류를 최소화하는 다목적 최적화 방법을 제시합니다.
- 다양한 층의 재구성 민감도에 기반하여, 우리는 메모리 사용량 및 지연 시간을 줄이기 위한 자동 비트 할당 전략도 개발합니다.
- 5개의 LLM 패밀리와 8개의 작업에 대한 실험 결과, ShiftAddLLM은 경쟁력 있는 양자화 LLM에 비해 평균 복잡도 개선을 3비트와 2비트 수준에서 각각 5.6 및 22.7 포인트로 달성하며, 원래의 LLM 대비 메모리와 에너지를 80% 이상 절감하는 등 효율성을 입증합니다.
- 코드와 모델은 https://github.com/GATECH-EIC/ShiftAddLLM에서 이용 가능합니다.

### [Margin-aware Preference Optimization for Aligning Diffusion Models without Reference](https://arxiv.org/abs/2406.06424)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06424.png)

Vote: 4

Authors: Jongheon Jeong, Kashif Rasul, Sayak Paul, James Thorne, Jiwoo Hong, Noah Lee

- 최신 인공지능 모델 최적화 기술인 RLHF 및 DPO는 훈련 안정성을 위해 기준 모델에 대한 발산 규제를 사용하지만, 이는 정렬 과정에서 모델의 유연성을 제한할 수 있다.
- 특히, 텍스트-이미지 확산 모델을 정렬할 때, 기준 모델과의 분포 차이가 문제가 된다.
- 이 논문에서는 Stable Diffusion XL(SDXL)과 같은 최신 텍스트-이미지 확산 모델의 정렬에서 이러한 "참조 불일치" 문제를 해결하기 위한 새로운 방법을 제안한다.
- 제안하는 방법인 margin-aware preference optimization(MaPO)은 기준 모델에 의존하지 않고 메모리 친화적인 선호도 정렬 방법을 제공한다.
- MaPO는 선호 이미지 세트와 비선호 이미지 세트 간의 가능성 차이와 선호 이미지 세트의 가능성을 동시에 최대화 하여 일반적인 스타일 기능과 선호도를 학습한다.
- 평가를 위해 SDXL에서 생성한 이미지 쌍을 포함하는 새로운 페어와이즈 선호도 데이터셋 두 가지를 도입하여 다양한 참조 불일치 시나리오를 시뮬레이션 한다.
- 실험 결과, MaPO는 Pick-Style과 Pick-Safety에서 정렬 성능을 크게 향상시키고, Pick-a-Pic v2를 사용했을 때보다도 우수한 성능을 보였다.
- 코드, 모델 및 데이터셋은 https://mapo-t2i.github.io에서 공개되어 있다.

### [Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis](https://arxiv.org/abs/2406.06216)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06216.png)

Vote: 4

Authors: Xin Jin, Chongyi Li, Pengyi Jiao, Zheng-Peng Duan, Xingchao Yang, Chun-Le Guo, Bo Ren

- NeRF와 같은 볼륨 렌더링 기반 기법들은 밤 장면에서 RAW 이미지를 이용한 HDR 뷰 합성에 뛰어나지만, 긴 훈련 시간과 실시간 렌더링 불가능성의 문제가 있다.
- 3D Gaussian Splatting (3DGS) 기술의 출현은 실시간 렌더링과 빠른 훈련을 가능하게 하지만, RAW 이미지 기반 뷰 합성에서는 몇 가지 고유한 단점이 있다.
- 밤 장면에서는 매우 낮은 신호 대 잡음비(SNR)로 인해 먼 거리에서의 구조 상태 추정(SfM)이 나쁘다.
- 구형 고조파(SH) 함수의 제한된 표현 능력은 RAW 선형 색 공간에 적합하지 않다.
- 부정확한 장면 구조는 초점 재조정과 같은 후속 작업을 방해한다.
- 이러한 문제를 해결하기 위해, LE3D(3DGS로 모든 어둠을 밝히기) 방법을 제안하며, Cone Scatter Initialization을 통해 SfM 추정을 풍부하게 하고, SH를 RAW 선형 색 공간을 표현하기 위해 Color MLP로 대체한다.
- 깊이 왜곡과 근-원거리 규제를 도입하여 후속 작업을 위한 장면 구조의 정확성을 개선한다.
- 이러한 설계는 LE3D가 실시간 새로운 보기 합성, HDR 렌더링, 초점 재조정 및 톤 매핑 변화를 수행할 수 있게 해준다.
- LE3D는 기존의 볼륨 렌더링 기반 기법들과 비교하여 훈련 시간을 1%로 단축하고, 2K 해상도 이미지의 경우 FPS 기준으로 렌더링 속도를 최대 4,000배 향상시킨다.
- 코드와 뷰어는 https://github.com/Srameo/LE3D 에서 확인할 수 있다.

### [Tx-LLM: A Large Language Model for Therapeutics](https://arxiv.org/abs/2406.06316)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06316.png)

Vote: 4

Authors: Shekoofeh Azizi, Eeshit Dhaval Vaishnav, S. Sara Mahdavi, Vivek Natarajan, Eric Wang, Byron Lee, Christopher Semturs, David Fleet, Tao Tu, Juan Manuel Zambrano Chaves

- 신약 개발은 오래 걸리고 비용이 많이 들며 다양한 기준을 충족해야 하는 과정이다.
- 현재의 대부분의 AI 접근 방식은 특정 영역 내의 좁은 범위의 작업만 처리한다.
- 이를 해결하기 위해, 다양한 치료 방법에 대한 지식을 암호화한 대형 언어 모델(Large Language Model, LLM)인 Tx-LLM을 소개한다.
- Tx-LLM은 PaLM-2에서 파인튜닝 되었으며, 신약 발견 파이프라인의 여러 단계를 아우르는 66가지 작업을 타겟으로 한 709개의 데이터셋을 사용해 훈련되었다.
- Tx-LLM은 하나의 가중치 세트로 다양한 화학적 또는 생물학적 엔티티(작은 분자, 단백질, 핵산, 세포주, 질병)를 처리하며, 자유로운 텍스트와 혼합해 다양한 관련 속성을 예측할 수 있다.
- 66개 작업 중 43개에서 최첨단(SOTA) 성과와 경쟁하며, 22개에서는 SOTA를 능가하는 성과를 보였다.
- 특히 분자 SMILES 표현과 텍스트(예: 세포주명 또는 질병명)를 결합한 작업에서 뛰어난 성능을 발휘했다.
- 다양한 약물 유형(예: 작은 분자와 단백질을 포함하는 작업) 간의 긍정적 전이 현상을 관찰했다.
- 모델 크기, 도메인 파인튜닝 및 프롬프트 전략이 성능에 미치는 영향을 연구했다.
- Tx-LLM은 생화학 지식을 암호화한 LLM으로 중요한 진전을 이루었으며, 앞으로 신약 발견 개발 파이프라인 전반에 걸쳐 종합적인 도구로서 역할을 할 수 있을 것으로 기대된다.

### [IllumiNeRF: 3D Relighting without Inverse Rendering](https://arxiv.org/abs/2406.06527)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06527.png)

Vote: 4

Authors: Dor Verbin, Ricardo Martin Brualla, Philipp Henzler, Pratul P. Srinivasan, Xiaoming Zhao, Keunhong Park

- 기존의 3차원 리라이팅 재구성 방법은 역 렌더링에 기반하여 물체의 기하학, 재질, 조명을 분리하려 시도하며, 이는 계산 비용이 많이 들고 취약하다.
- 본 연구에서는 더 간단한 접근 방식을 제안하는데, 먼저 조명 조건을 고려한 이미지 확산 모델을 사용하여 입력 이미지를 재조명한 후 이를 이용해 NeRF(Neural Radiance Field)를 재구성한다.
- 이 결과로 얻어진 NeRF를 통해 목표 조명 아래에서 새로운 뷰포인트로 렌더링할 수 있다.
- 이 전략은 놀랄 만큼 경쟁력이 있으며, 여러 리라이팅 벤치마크에서 최첨단 성과를 달성한다.
- 프로젝트 페이지는 https://illuminerf.github.io/에서 확인할 수 있다.

### [Towards a Personal Health Large Language Model](https://arxiv.org/abs/2406.06474)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06474.png)

Vote: 4

Authors: Jian Cui, Jameson K. Rogers, Zhun Yang, Cathy Speed, Roy Lee, Robby Bryant, Megan Walker, Erik Schenck, Allen Jiang, Ryan G. Gomes, Chace Lee, Javier Perez, Justin Cosentino, Tim Althoff, Xin Liu, Yojan Patel, +, Shyam Tailor, Yun Liu, Jeffrey Yu, Logan Douglas Schneider, Nicholas A. Furlotte, Anastasiya Belyaeva

- 건강 분야에서 대부분의 대형 언어 모델(LLM) 연구는 임상 작업에 집중되어 있다.
- 모바일 및 웨어러블 기기는 풍부한 종단 데이터로 개인 건강 모니터링에 기여하지만, 이러한 작업에 거의 통합되지 않는다.
- Gemini 모델을 미세 조정하여 수치 시계열 개인 건강 데이터를 이해하고 추론할 수 있는 Personal Health Large Language Model (PH-LLM)를 소개한다.
- PH-LLM은 세 가지 데이터셋을 만들어 수면 패턴, 신체 활동, 생리학적 반응에서 개인 맞춤형 통찰과 추천을 제공하며, 전문가 도메인 지식 및 자가 보고된 수면 결과 예측을 테스트했다.
- 첫 번째 작업에서 도메인 전문가와 협력하여 실제 수면 및 피트니스 시나리오를 평가하기 위해 857개의 사례 연구를 설계했다.
- 도메인별 평가 기준을 통해 Gemini Ultra 1.0과 PH-LLM이 피트니스에서 전문가 성능과 통계적으로 차이가 없음을 관찰했으며, 수면에서는 전문가가 여전히 우세했으나 PH-LLM의 미세 조정이 관련 도메인 지식 사용과 개인 맞춤형 정보 제공에서 유의미한 개선을 보였다.
- PH-LLM의 도메인 지식을 다지선다형 수면 의학 및 피트니스 시험으로 평가했으며, PH-LLM은 수면에서 79%, 피트니스에서 88%를 획득하며 인간 전문가 평균 점수를 초과했다.
- 마지막으로 PH-LLM을 훈련시켜 웨어러블 데이터의 텍스트 및 다중 모달 인코딩 표현에서 자가 보고된 수면 품질 결과를 예측하고, 이는 특화된 판별 모델 성능과 맞먹기 위해 다중 모달 인코딩이 필요함을 입증했다.
- 안전에 중요한 개인 건강 영역에서 추가 개발 및 평가가 필요하지만, 이러한 결과는 Gemini 모델의 폭넓은 지식과 역량 및 PH-LLM을 통한 생리 데이터를 개인 건강 응용 프로그램에 맥락화하는 이점을 보여준다.

### [Unified Text-to-Image Generation and Retrieval](https://arxiv.org/abs/2406.05814)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.05814.png)

Vote: 4

Authors: Tan Wang, Wenjie Wang, Liqiang Nie, Haochuan Li, Yongqi Li, Tat-Seng Chua, Leigang Qu

- 인간이 효율적이고 효과적으로 이미지를 획득하는 방법에 대해 탐구하는 것은 오랜 질문이다.
- 텍스트 쿼리에 기반하여 기존 데이터베이스에서 이미지를 검색하는 텍스트-이미지 검색이 일반적인 해결책이지만, 창의성이 부족하다.
- 최근 텍스트-이미지 생성 기술의 발전으로 멋지고 다양한 시각 콘텐츠를 생성할 수 있게 되었으나, 지식 집약적인 이미지 합성에는 한계가 있다.
- 본 연구에서는 텍스트-이미지 생성과 검색의 관계를 재고하고, 이를 멀티모달 대형 언어 모델(MLLM) 맥락에서 통합된 프레임워크를 제안한다.
- 먼저 MLLM의 본질적인 판별 능력을 탐구하고, 훈련이 필요 없는 생성 검색 방법을 도입하여 검색을 수행한다.
- 이후 생성과 검색을 자기회귀적 생성 방식으로 통합하고, 생성된 이미지와 검색된 이미지 중 가장 적합한 것을 선택하는 자율 결정 모듈을 제안한다.
- 창의적이고 지식 집약적인 도메인을 포함한 TIGeR-Bench라는 벤치마크를 구축하여 통합된 텍스트-이미지 생성 및 검색의 평가를 표준화한다.
- TIGeR-Bench와 Flickr30K 및 MS-COCO 두 가지 검색 벤치마크에서의 광범위한 실험 결과는 제안된 방법의 우수성과 효과를 보여준다.

### [VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers](https://arxiv.org/abs/2406.05370)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.05370.png)

Vote: 4

Authors: Shujie Liu, Sheng Zhao, Long Zhou, Yao Qian, Furu Wei, Yanqing Liu, Sanyuan Chen, Jinyu Li, Xu Tan

- 이 논문은 최신 신경 코덱 언어 모델 VALL-E 2를 소개하며, 최초로 인간 수준의 제로샷 텍스트 음성 변환(TTS)을 달성함을 보고합니다.
- VALL-E 2는 이전 버전인 VALL-E를 기반으로 두 가지 주요 개선 사항을 도입했습니다:
- LibriSpeech와 VCTK 데이터셋에서의 실험 결과 VALL-E 2는 이전 시스템을 능가하는 음성 견고성, 자연스러움, 화자 유사성을 보여줍니다.
- VALL-E 2는 이러한 벤치마크에서 최초로 인간 수준에 도달했으며, 복잡한 또는 반복적인 문장에서도 일관된 고품질 음성을 합성하는 능력으로 주목받고 있습니다.
- 이 연구의 이점은 실어증 환자나 근육위축측삭경화증 환자 등을 위한 음성 생성 같은 가치 있는 노력에 기여할 수 있습니다.

### [GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement](https://arxiv.org/abs/2406.05649)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.05649.png)

Vote: 3

Authors: Chaoyang Wang, Vladislav Shakhrai, Aliaksandr Siarohin, Songfang Han, Peiye Zhuang, Sergey Korolev, Michael Vasilkovsky, Hsin-Ying Lee, Jiaxu Zou, Sergey Tulyakov

- 우리는 다중 뷰 이미지를 통한 3D 메쉬 복원을 위한 새로운 접근 방식을 제안한다.
- 이 방법은 트랜스포머 기반 트라이플레인 생성기와 다중 뷰 이미지로 학습된 NeRF 모델을 사용하는 대규모 복원 모델의 영감을 받았다.
- 그러나 본 연구에서는 3D 복원 품질을 크게 향상시키기 위해 몇 가지 중요한 수정을 도입했다.
- 먼저, 기존 LRM 아키텍처의 몇 가지 단점을 찾아내고 이를 개선하여 더 나은 다중 뷰 이미지 표현과 더 효율적인 학습을 가능하게 했다.
- 둘째, 기하학적 복원을 개선하고 전체 이미지 해상도로 감독을 가능하게 하기 위해 NeRF 필드에서 메쉬를 추출하고 메쉬 렌더링을 통해 NeRF 모델을 미세 조정했다.
- 이러한 수정은 2D와 3D 평가 지표에서 최첨단 성능을 달성하게 해주었다. 예를 들어, GSO 데이터셋에서 PSNR 28.67을 기록했다.
- 뛰어난 결과에도 불구하고, 복잡한 텍스처 복원 (예: 텍스트와 초상화)에 어려움을 겪었다.
- 이를 해결하기 위해, 경량의 인스턴스별 텍스처 개선 절차를 도입했다.
- 이 절차는 입력된 다중 뷰 이미지를 사용하여 4초 만에 트라이플레인 표현과 NeRF 컬러 추정 모델을 메쉬 표면에서 미세 조정한다.
- 이 개선은 PSNR을 29.79로 향상시키고 복잡한 텍스처를 충실하게 복원할 수 있게 했다.
- 우리의 접근 방식은 텍스트나 이미지를 3D 모델로 변환하는 다양한 응용 프로그램을 가능하게 한다.

### [ExtraNeRF: Visibility-Aware View Extrapolation of Neural Radiance Fields with Diffusion Models](https://arxiv.org/abs/2406.06133)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06133.png)

Vote: 2

Authors: Wei-Chiu Ma, Janne Kontkanen, Meng-Li Shih, Brian L. Curless, Forrester Cole, Aleksander Holynski

- ExtraNeRF는 Neural Radiance Field (NeRF)의 시야 범위를 확장하기 위한 새로운 방법을 제안합니다.
- 이 방법은 NeRFs를 사용하여 장면에 특화된 세밀한 정보를 모델링하면서, 확산 모델을 활용하여 관찰된 데이터 너머로 보기를 확장하는 것이 핵심입니다.
- 중요한 요소는 가시성을 추적하여 관찰되지 않은 장면의 부분을 식별하고 해당 영역을 일관되게 확산 모델을 통해 재구성하는 것입니다.
- 주요 기여는 가시성 인식 확산 기반 인페인팅 모듈로, 이는 입력 이미지를 기반으로 미세 조정되어 첫 번째 NeRF의 중간 품질(종종 흐림) 인페인팅된 영역을 생성합니다.
- 이후 입력 이미지를 기반으로 훈련된 두 번째 확산 모델은 첫 번째 패스에서 인페인팅된 이미지를 일관되게 개선하고 특히 선명하게 만듭니다.
- 우리는 소수의 입력 보기(일반적으로 6개 이하)에서 벗어나 네프를 효과적으로 outpainting하고 원래 시야 범위 내부의 새로운 비가시 영역을 inpainting하여 고품질 결과를 입증했습니다.
- 관련 작업과의 비교를 통해 정량적 및 정성적으로 기존 연구보다 유의미한 성능 향상을 보여주었습니다.

