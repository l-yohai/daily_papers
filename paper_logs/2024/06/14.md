## Daily Papers (2024-06-14)

### [Depth Anything V2](https://arxiv.org/abs/2406.09414)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09414.png)

Vote: 87

Authors: Zilong Huang, Jiashi Feng, Bingyi Kang, Xiaogang Xu, Hengshuang Zhao, Lihe Yang, Zhen Zhao

- **What's New**: 이 논문에서는 기존의 단일 카메라 깊이 추정(MDE, Monocular Depth Estimation) 모델의 제한점을 극복하고, 전반적인 성능을 향상시키기 위한 새로운 기초 모델을 제안합니다. Depth Anything V1 모델을 기반으로 과거의 약점을 보완하고, 데이터 주도 접근 방식을 통해 더 정확하고 효율적인 MDE 모델을 개발하는 데 중점을 두었습니다.
- **Technical Details**: 기존 MDE 모델은 크게 두 그룹으로 나뉩니다: 판별 모델(Discriminative models)과 생성 모델(Generative models). Depth Anything 모델은 판별 모델의 대표주자이며, 복잡한 장면에서도 강력한 예측을 제공합니다. 하지만 투명한 객체나 반사 표면에서는 약점을 가지고 있습니다. 본 연구에서는 풍부한 세부 사항을 생성할 수 있는 생성 모델의 장점을 결합하여 더 강력한 모델을 개발하고자 하였습니다. 특히, 새로운 모델은 주로 합성 이미지 기반의 데이터를 활용해 더 정확한 레이블을 제공하며, 대규모의 가상 레이블된 실제 이미지를 사용해 교차모델 학습을 진행합니다.
- **Performance Highlights**: 제안된 모델의 성능을 평가하기 위해 기존 시험 세트를 개선해 더 정밀한 주석과 다양한 장면을 포함한 평가 벤치마크를 구축하였습니다. 시험 결과, 새로 제안된 MDE 모델은 기존 모델보다 훨씬 높은 성능을 보여줬으며, 이는 특히 복잡한 레이아웃과 반사 및 투명 객체 처리에서 두드러졌습니다.

### [An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels](https://arxiv.org/abs/2406.09415)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09415.png)

Vote: 47

Authors: Xinlei Chen, Mahmoud Assran, Martin R. Oswald, Unnat Jain, Duy-Kien Nguyen, Cees G. M. Snoek

- **What's New**: ETRIMIS의 최신 논문은 Vision Transformer (ViT)의 성능을 더욱 향상시키기 위해 'locality'라는 유도 편향(inductive bias)를 제거하는 새로운 접근 방식을 소개합니다. 이를 통해 Pixel Transformer(PiT)라 불리는 새로운 아키텍처를 제시하며, 각 픽셀을 독립적인 토큰으로 취급해 학습합니다. 기존의 이미지 토큰화 방식에서 벗어나 직접 픽셀을 학습 데이터로 사용함으로써, 더 나은 품질의 결과를 얻을 수 있음을 발견했습니다.
- **Technical Details**: PiT는 ViT와 달리 이미지의 2D 그리드 구조에 대한 공간적 계층 구조(spatial hierarchy) 없이, 각 픽셀을 독립된 토큰으로 다룹니다. 포지션 임베딩(position embeddings) 또한 처음부터 학습하며, 이를 통해 'locality' 유도 편향을 완전히 제거할 수 있습니다. 실험을 통해 PiT가 CIFAR-100, ImageNet, 그리고 Diffusion Transformer(DiT) 인프라를 활용한 이미지 생성과 같은 다양한 비전 태스크에서 우수한 성능을 보여주었습니다.
- **Performance Highlights**: PiT는 이미지를 개별 픽셀 세트로 보는 접근법을 통해, CIFAR-100에서의 객체 분류, 자기 지도 학습(self-supervised learning), 그리고 이미지 생성에서 ViT와 비교하여 더 나은 성능을 입증했습니다. 그러나, 각 픽셀을 토큰으로 사용할 때 더 긴 시퀀스 길이를 필요로 하여 효율성이 떨어질 수 있는 단점도 있습니다. 그럼에도 불구하고, 이 연구는 'locality'가 비전 모델 디자인에 필수적인 유도 편향이 아님을 강력하게 보여줍니다.

### [Transformers meet Neural Algorithmic Reasoners](https://arxiv.org/abs/2406.09308)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09308.png)

Vote: 43

Authors: Andrew Dudzik, Wilfried Bounsi, Razvan Pascanu, Jessica B. Hamrick, Alex Vitvitskyi, Petar Veličković, Borja Ibarz, Larisa Markeeva

- **What's New**: 이번 논문에서는 Transformer 기반의 언어 모델과 Graph Neural Network(GNN) 기반의 Neural Algorithmic Reasoner(NAR)을 결합한 새로운 하이브리드 아키텍처를 제안합니다. 이를 통해 비정형적인 입력 데이터에서도 높은 수준의 알고리즘 추론 성능을 유지할 수 있는 TransNAR 모델을 제시하였습니다.
- **Technical Details**: TransNAR 모델은 텍스트와 그래프 입력을 동시에 처리할 수 있는 이중 입력 구조를 갖추고 있습니다. 텍스트 입력 부분은 transformer가 처리하고, 그래프 입력 부분은 GNN 기반의 NAR이 처리합니다. 성능 개선을 위해 Cross-Attention 방식을 사용하여 두 입력 간 정보를 융합합니다. 각 입력은 embedding 레이어를 통해 고차원 벡터 공간에 매핑되며, 이들 벡터를 변형 및 결합하여 최종적인 예측을 수행합니다.
- **Performance Highlights**: CLRS-Text 벤치마크를 기반으로 TransNAR 모델의 성능을 평가한 결과, 기존의 오리지널 transformer 모델과 비교하여 더 나은 일반화 능력과 강견성을 보였습니다. 특히, out-of-distribution 입력에서도 높은 성능을 유지하였으며, 이는 실제 시스템에 적용 가능한 중요한 특성입니다.

### [Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling](https://arxiv.org/abs/2406.07522)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07522.png)

Vote: 34

Authors: Yadong Lu, Liliang Ren, Weizhu Chen, Yang Liu, Chen Liang, Yelong Shen

- **What's New**: 최근 Attention 기반 모델이 대형 언어 모델(LLMs)의 주요 신경 구조로 자리잡은 가운데, State Space Models(SSMs)이 복잡한 장기 종속성을 더 효율적으로 처리할 수 있는 대안으로 주목받고 있습니다. SSM의 변형인 Mamba는 선택적 상태 공간을 통해 좋은 성능과 하드웨어 효율성을 보여줬습니다. 이번 논문에서는 SSM과 Attention 기반 모델의 장점을 결합해 무한한 시퀀스 길이 외삽을 가능하게 하는 새로운 신경 아키텍처인 Samba를 제안합니다.
- **Technical Details**: Samba는 SSM(Mamba), Sliding Window Attention(SWA), SwiGLU를 계층으로 교차하여 결합합니다. Mamba는 시간 종속 의미를 포착하고, SWA는 비 Markovian 종속성을 모델링합니다. 421M, 1.3B, 1.7B, 3.8B 파라미터로 구성된 모델로 확장 가능하며, 최대 3.8B 모델은 3.2T 토큰으로 사전 학습되어 높은 성능을 보입니다. 또한, 4K 시퀀스 길이에서 사전 학습된 모델이 1M 시퀀스 길이까지 외삽 가능함을 증명했습니다.
- **Performance Highlights**: Samba의 3.8B 모델은 MMLU에서 71.2점, HumanEval에서 54.9점, GSM8K에서 69.6점을 기록하며, 최대 8B 파라미터의 오픈 소스 언어 모델을 능가합니다. 또한, 4K 컨텍스트 길이에서 500단계의 지시 튜닝 후 256K 길이까지 완벽한 메모리 호출이 가능함을 보여주었습니다. 이는 심도 있는 분석과 ablation 연구를 통해 모델의 설계를 검증한 결과입니다.

### [Alleviating Distortion in Image Generation via Multi-Resolution Diffusion Models](https://arxiv.org/abs/2406.09416)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09416.png)

Vote: 28

Authors: Zhanpeng Zeng, Qihang Yu, Qihao Liu, Liang-Chieh Chen, Xiaohui Shen, Ju He

- **What's New**: 새로운 연구인 DiMR(Diffusion Multi-Resolution), 고해상도 이미지 생성을 위해 다중 해상도 네트워크를 강화한 디퓨전 모델을 소개합니다. 이는 Transformer 기반 구조와 ConvNeXt 블록을 결합하여 이미지 왜곡을 줄이고 시각적 세부 사항을 보존합니다.
- **Technical Details**: 기존의 U-Net 아키텍처에서 Transformer 기반 설계로 전환하여 성능과 확장성을 개선했으며, 패치 사이즈와 자가-어텐션(self-attention) 연산 사이의 균형을 맞추기 위한 고유한 설계를 적용했습니다. Multi-Resolution Network는 낮은 해상도에서 높은 해상도로 점진적으로 특징을 개선하는 다중 분기 구조를 채택했습니다. 또한, 시간 의존 계층 정규화(Time-Dependent Layer Normalization, TD-LN)를 도입해 시간 정보를 효율적으로 인코딩합니다.
- **Performance Highlights**: DiMR-M (133M 파라미터)와 DiMR-L (284M)는 ImageNet 64x64에서 각각 3.65와 2.21의 FID 점수를 기록하며 Transformer 기반 U-ViT-M/4와 U-ViT-L/4보다 우수한 성능을 보여줍니다. ImageNet 256x256에서는 DiMR-XL (505M)가 4.50 (classifier-free guidance 없이)과 1.70 (classifier-free guidance 포함)의 FID 점수를, ImageNet 512x512에서는 DiMR-XL (525M)가 7.93 (classifier-free guidance 없이)과 2.89 (classifier-free guidance 포함)의 FID 점수를 기록하여 모든 이전 방법론을 능가하는 성능을 보였습니다.

### [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09246.png)

Vote: 28

Authors: Percy Liang, Siddharth Karamcheti, Karl Pertsch, Rafael Rafailov, Pannag Sanketi, Chelsea Finn, Benjamin Burchfiel, Moo Jin Kim, Russ Tedrake, Sergey Levine, Ted Xiao, Dorsa Sadigh, Quan Vuong, Ashwin Balakrishna, Thomas Kollar, Grace Lam, Suraj Nair, Ethan Foster

- **What's New**: OpenVLA는 로봇 조작을 위한 7B-parameter 오픈 소스 비전-언어-액션 모델(Vision-Language-Action, VLA)로, 기존의 state-of-the-art 모델인 RT-2-X를 뛰어넘는 성능을 보입니다. OpenVLA는 SigLIP 및 DinoV2 비전 인코더와 Llama 2 언어 모델 백본을 사용하여 다양한 데이터셋으로부터 파인튜닝(Fine-tuning)됩니다. 이를 통해 새로운 로봇, 환경 및 작업에 적응할 수 있는 강력한 로봇 정책을 제공합니다.
- **Technical Details**: OpenVLA는 인터넷 규모의 데이터를 통해 학습된 시각 및 언어 모델의 전이 학습(Transfer learning)을 활용하여 개발되었습니다. SigLIP, DinoV2, Llama 2를 사용하는 이 모델은 총 970k개의 로봇 조작 트랙터리(trajectories) 데이터를 사용하여 파인튜닝 되었습니다. 특히, 이 모델은 로우 랭크 어댑테이션(Low-Rank Adaptation, LoRA) 및 모델 양자화(Model Quantization)를 통해 소비자 등급 GPU에서도 효과적인 미세조정(Fine-tuning)을 지원합니다.
- **Performance Highlights**: OpenVLA는 WidowX 및 Google Robot와 같은 환경에서 29개의 평가 작업(Task)에서 절대 성공률 16.5% 향상을 보여줍니다. 또한, 다양한 조작 과제를 수행할 때도 기존의 파인튜닝된 모델들보다 뛰어난 성능을 나타냅니다. 예를 들어, Octo와 같은 기존 모델들보다 높은 정확도로 작업을 수행하며, 다중 작업 환경에서 언어와 행동을 연계하는데 있어서도 큰 개선을 보여줍니다.

### [Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning](https://arxiv.org/abs/2406.09170)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09170.png)

Vote: 22

Authors: Karishma Malkan, Anton Tsitsulin, Bahare Fatemi, Jonathan Halcrow, John Palowitch, Sungyong Seo, Jinyeong Yim, Mehran Kazemi, Bryan Perozzi

- **What's New**: 최근 대형 언어 모델(LLM)의 연구와 응용에서 커다란 혁신이 있었습니다. 이 모델들은 새로운 콘텐츠를 생성할 수 있으며, 인공지능 커뮤니티를 매료시키고 다양한 작업과 데이터 유형에 대해 훈련된 수많은 LLM이 출시되고 있습니다. 이번 연구에서는 지능형 시스템에서 필수적인 작업인 시간적 추론에 초점을 맞추었습니다. 시간적 추론을 심도 있게 평가하기 위해 ToT (Test of Time)라는 새 기준을 개발했습니다. ToT는 시간의 의미론과 논리를 이해하는 것과 시간 산술을 수행하는 두 가지 주요 기술을 평가합니다.
- **Technical Details**: 기존의 시간적 추론 벤치마크는 주로 잘 알려진 엔티티에 관한 지식 그래프(KG) 스타일의 시간적 사실에 의존하여 시간적 추론 관계의 범위를 완전하게 측정하지 못합니다. 이러한 제한을 극복하기 위해 ToT는 두 가지 주요 작업으로 구성되어 있으며 각 작업은 시간적 추론의 필수 기술을 독립적으로 평가합니다. ToT-Semantic은 유연한 그래프 구조와 논리 복잡성을 탐색할 수 있는 합성 작업으로, 선행 지식과 독립적인 추론 능력을 평가합니다. ToT-Arithmetic은 군중 소싱 데이터를 사용하여 시간 점과 지속 시간을 포함한 산술 계산 능력을 평가합니다. 또한, 그래프 구조 생성에는 Erdős-Rényi 그래프, Barabási–Albert 모델, 확률적 블록 모델(SBM) 등이 사용됩니다.
- **Performance Highlights**: ToT 벤치마크의 실험 결과는 현재 LLM의 시간적 추론 과제에서의 강점과 약점을 파악하는 데 유용한 통찰력을 제공합니다. ToT-Semantic 작업은 다양한 그래프 구조를 바탕으로 문제를 생성하여 기존의 지식에 의존하지 않고도 명확한 추론 능력을 평가합니다. 이러한 설계는 LLM이 제시된 사실을 통해 진정한 추론을 해야만 하며, 이는 시간적 추론 능력의 더 엄밀한 평가를 장려합니다.

### [DiTFastAttn: Attention Compression for Diffusion Transformer Models](https://arxiv.org/abs/2406.08552)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08552.png)

Vote: 19

Authors: Zhihang Yuan, Linfeng Zhang, Tianchen Zhao, Guohao Dai, Shengen Yan, Pu Lu, Yu Wang, Xuefei Ning, Hanling Zhang

- **What's New**: 이 논문은 DiT(Diffusion Transformers)의 주목받는 성능에도 불구하고 높은 컴퓨팅 요구사항이 큰 도전과제임을 지적합니다. 이를 해결하기 위해, DiT inference 과정에서 발견된 세 가지 유형의 여분 항목을 줄이기 위한 모델 압축 기법, DiTFastAttn을 제안합니다.
- **Technical Details**: DiTFastAttn에는 세 가지 주요 기술이 포함됩니다. 첫째, 공간 차원에서의 여분을 줄이기 위해 'Window Attention with Residual Sharing (WA-RS)' 기법을 도입했습니다. 둘째, 시간 단계별 유사성을 활용하는 'Attention Sharing across Timesteps (AST)' 기법을 통해 모델 효율성을 높였습니다. 셋째, 조건부 및 무조건부 생성 간의 유사성을 이용해 계산 오버헤드를 줄이는 'Attention Sharing across CFG (ASC)' 기법을 제안했습니다.
- **Performance Highlights**: 다양한 DiT 모델에서 실험을 수행한 결과, DiTFastAttn은 일관되게 컴퓨팅 비용을 줄이는 것으로 나타났습니다. 특히 고해상도 이미지(2048x2048) 생성 시 PixArt-Sigma 모델에서는 주목할 만한 속도 향상과 함께 최대 1.6배의 속도 증가와 88% 가량의 계산 비용 절감 효과를 보였습니다.

### [Interpreting the Weight Space of Customized Diffusion Models](https://arxiv.org/abs/2406.09413)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09413.png)

Vote: 18

Authors: Rameen Abdal, Yossi Gandelsman, Kfir Aberman, Alexei A. Efros, Gordon Wetzstein, Amil Dravid, Kuan-Chieh Wang

- **What's New**: 최근 생성 모델 연구에서, 생성적 적대 신경망 (GAN)과 같은 단일 단계 생성 모델은 의미 있는 개념을 선형적으로 인코딩하는 잠재 공간을 가지고 있다고 밝혀졌습니다. 하지만, 다단계 생성 모델, 예를 들어 확산 모델(diffusion models)에서는 이러한 잠재 공간이 아직 발견되지 않았습니다. 이를 해결하기 위해, Dreambooth와 Custom Diffusion 같은 개인화 방법이 제시되었습니다. 이 연구에서는 60,000개 이상의 개인화된 모델을 미세 조정하여, 모델 가중치에서 새로운 잠재 공간 weights2weights (w2w)를 찾습니다.
- **Technical Details**: 우리는 낮은 차원의 데이터 포인트를 만들기 위해 LoRA 방식을 사용하여 사용자별 모델을 미세조정하고, 그런 다음 PCA를 적용하여 최종 잠재 공간을 구성했습니다. 이 공간에서는 각 샘플이 특정한 정체성을 유지하는 모델에 해당합니다. Latent Diffusion Models (LDM), Dreambooth 미세조정, 및 저차원 가중치 업데이트 방식을 사용하여 잠재 공간을 구성했습니다.
- **Performance Highlights**: w2w 공간은 정체성에 대한 의미 있는 편집과 새로운 정체성의 인코딩에서 높은 표현력을 보여줍니다. 한 이미지에서도 새 정체성을 인코딩할 수 있으며, 이는 out-of-distribution 정체성도 캡쳐합니다. 양적 평가를 통해, 개인화된 모델 편집과 새로운 정체성 인코딩에서 w2w 공간이 매우 표현력이 높다는 것을 확인했습니다.

### [MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding](https://arxiv.org/abs/2406.09411)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09411.png)

Vote: 17

Authors: Wenjie Jacky Mo, Tianyi Lorena Yan, Muhao Chen, Mingyu Derek Ma, Kai-Wei Chang, Kai Zhang, Qin Liu, Chaowei Xiao, Pan Lu, Chunyuan Li, Xingyu Fu, Wenxuan Zhou, Sheng Zhang, Nan Xu, Xiaogeng Liu, Dan Roth, Hoifung Poon, Fei Wang, Zekun Li, Hsiang-Hui Liu, James Y. Huang

- **What's New**: MuirBench는 다양한 이미지 유형을 다루는 멀티 이미지 다중 선택 질문 응답 (MCQA) 데이터를 포함하는 새로운 벤치마크입니다. 이는 사진, 의료 이미지, 슬라이드, 드론 및 위성 이미지와 같은 여러 이미지 유형을 포괄합니다.
- **Technical Details**: MuirBench 데이터 수집은 기존 데이터셋, 데이터셋 가공, 신규 데이터 수집의 세 가지 출처로부터 이루어졌습니다. 기존 데이터는 GeneCIS, SeedBench, IconQA 등에서 가져왔습니다. 가공된 데이터는 NLVR2, HallusionBench 등의 데이터셋에서 이진 QA를 MCQA로 변경하거나, ISVQA의 개방형 QA를 MCQA로 재작성하여 얻었습니다. 신규 데이터는 주로 지리적 이해, 복수시점 이미지 관계, 의료 이미지 등의 부족한 영역을 보완하기 위해 HistoricalMap, UnivBuilding, PubMedMQA, SciSlides를 포함하여 수집하였습니다. 특히 새로운 데이터는 37.5%를 차지합니다.
- **Performance Highlights**: 20개의 최근 출시된 멀티모달 LLMs (다중 모드 대형 언어 모델)에서 MuirBench를 평가했습니다. 여기에는 gpt-4, Gemini Pro, Mantis, VILA, Idefics, Emu2 (Chat), OpenFlamingo, LLaVA의 여러 버전, Yi-VL-6B, MiniGPT-4-v2, CogVLM 등이 포함됩니다. 주요 성과들은 각각 모델의 최신버전으로 측정되었으며, 다중 이미지 처리 성능에 대한 종합적인 평가 결과를 제공합니다.

### [Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models](https://arxiv.org/abs/2406.09403)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09403.png)

Vote: 17

Authors: Weijia Shi, Xingyu Fu, Dan Roth, Noah A Smith, Yushi Hu, Luke Zettlemoyer, Ranjay Krishna, Mari Ostendorf

- **What's New**: Visual Sketchpad는 멀티모달 언어 모델(multimodal LMs)이 중간 스케치를 생성해 문제 해결을 돕도록 하는 프레임워크를 도입합니다. 이 프레임워크는 LMs이 시각적 유물을 생성하여 텍스트, 프로그래밍, 시각적 추론의 혼합된 체인으로 작업을 수행하도록 합니다.
- **Technical Details**: Visual Sketchpad는 멀티모달 쿼리에서 생각(Thought), 실행(Action), 관찰(Observation)이라는 세 가지 주요 단계를 거쳐 작업을 해결합니다. 예를 들어, 기하학 문제에서는 모델이 보조선을 추가하여 다이어그램을 수정하고 이를 통해 문제를 해결하는 데 필요한 정보를 얻습니다. 이 프레임워크는 특별한 학습이나 튜닝이 필요 없으며, 기존 멀티모달 LMs에 프롬프트를 통해 스케치를 수행할 수 있도록 합니다.
- **Performance Highlights**: Visual Sketchpad는 다양한 수학 및 컴퓨터 비전 작업에서 GPT-4o 모델 성능을 평균 12.7% 향상시켰습니다. 특히 기하학, 수학적 함수, 그래프 알고리즘, 전략 게임에서 유의미한 개선을 보였고, V*Bench 벤치마크에서는 14.3%, BLINK의 깊이 및 의미적 대응 작업에서는 각각 12.1%, 9.7%의 향상을 기록했습니다.

### [mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus](https://arxiv.org/abs/2406.08707)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08707.png)

Vote: 14

Authors: Pedro Ortiz Suarez, Cordelia Schmid, Benoît Sagot, Armel Zebaze, Julien Abadji, Rachel Bawden, Matthieu Futeral, Rémi Lacroix

- **What's New**: 이번 논문에서는 Common Crawl 데이터를 이용하여 대규모 다중언어 및 다중모달(Multimodal) 문서 데이터셋인 mOSCAR를 수집하고 공개했습니다. 이는 163개 언어로 구성된 3.15억 개의 문서와 1.2억 개의 이미지와 2140억 개의 토큰을 포함합니다. 기존 데이터셋은 대부분 영어에 한정되거나 다국어 지원이 불충분했으나, 이번 데이터셋은 그러한 문제를 극복하려는 시도로 볼 수 있습니다.
- **Technical Details**: mOSCAR는 2023년 Common Crawl 덤프에서 WARC 파일을 수집한 후 FastWARC 라이브러리로 처리하였습니다. DOM(Document Object Model) 트리를 깊이 우선 탐색 알고리즘과 ChatNoir 라이브러리를 사용하여 주요 HTML 태그에서 텍스트와 이미지를 추출했습니다. 500바이트 이하의 작은 문서 또는 텍스트 노드가 3개 미만인 문서와 이미지 노드가 30개 이상인 문서는 노이즈로 간주되어 제거되었습니다. 그리고 Open-LID 언어 탐지기를 통해 문서의 언어를 식별했습니다.
- **Performance Highlights**: mOSCAR를 활용하여 mulitlingual OpenFlamingo 모델을 훈련한 결과 기존의 캡션 데이터에만 의존했던 모델에 비해 소수 샘플(few-shot) 학습 성능에서 큰 향상을 보였습니다. 이는 mOSCAR의 다국어 및 다중모달 데이터셋이 학습에 유용하며, 특히 여러 언어의 텍스트와 이미지가 혼합된 데이터가 중요한 역할을 한다는 것을 확인시켜 주었습니다.

### [CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery](https://arxiv.org/abs/2406.08587)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08587.png)

Vote: 14

Authors: Dayuan Fu, Huangxuan Wu, Qiuna Tan, Muxi Diao, Weihao Zeng, Zhexu Wang, Jianing Yu, Runqi Qiao, Zhengyang Wang, Yejie Wang, Zhuoma GongQue, Guanting Dong, Xiaoshuai Song, Bin Liang, Yujia Fu, Weiran Xu

- **What's New**: 최근의 인공지능(AI) 기술 발전을 대표하는 대형 언어 모델(LLMs), 특히 ChatGPT와 GPT-4가 여러 분야에서 의의를 보이고 있습니다. 이러한 모델들을 컴퓨터 과학(CS)에서 효과적으로 활용하여 인류에게 더 나은 서비스를 제공하는 것은 미래 지능형 시대를 위한 주요 과제입니다. 이를 위해 CS-Bench라는 최초의 벤치마크를 소개합니다.
- **Technical Details**: CS-Bench는 컴퓨터 과학 분야에서 LLMs의 성능을 평가하기 위해 약 5,000개의 테스트 항목을 포함하고 있으며, 이는 26개의 세부 분야에 걸쳐 4개의 주요 CS 도메인을 포함합니다. CS-Bench는 다국어 평가를 지원하며, 다양한 형태의 질문(예: 다지선다형, 부정확인, 빈칸 채우기(FITB), 서술형)으로 구성되어 있습니다. 평가 질문들은 지식형과 추론형으로 구분되어 LLMs의 다양한 측면에서의 역량을 포괄적으로 평가합니다.
- **Performance Highlights**: 본 연구에서는 30개 이상의 주류 LLMs를 CS-Bench에서 평가하였으며, 다음과 같은 주요 발견 사항이 있습니다. 첫째, CS-Bench는 LLMs의 CS 역량을 차별화하면서도 GPT-4와 같은 상위 성능 모델에게도 도전 과제를 제공합니다. 둘째, LLMs는 CS-Bench 점수에서 로그형태의 일관된 성장을 보입니다. 셋째, 주요 성능 저하 원인은 도메인 지식의 부족으로, CS 특정 추론 역량은 일반적 추론 강화만으로는 달성하기 어려워 목표 지향적 강화가 필요합니다. 또한, LLMs의 수학 및 코딩 능력과 CS 능력 간에 강한 상관관계가 있는 것으로 나타났습니다. 일부 전문가 LLMs는 특정 CS 하위 분야에서 개선된 성능을 보였습니다.

### [HelpSteer2: Open-source dataset for training top-performing reward models](https://arxiv.org/abs/2406.08673)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08673.png)

Vote: 14

Authors: Yi Dong, Olivier Delalleau, Makesh Narsimhan Sreedhar, Gerald Shen, Oleksii Kuchaiev, Zhilin Wang, Jimmy J. Zhang, Daniel Egert, Jiaqi Zeng

- **What's New**: 이 논문은 HelpSteer2라는 CC-BY-4.0 라이센스 하에 공개된 새로운 오픈소스 helpfulness 데이터셋을 소개하고 있습니다. 이 데이터셋은 최신 reward 모델을 훈련하기 위해 설계되었으며, 상세한 데이터 수집 정보를 제공하여 유사한 노력에 도움을 주고자 합니다. 또한, SteerLM 2.0이라는 새로운 모델 정렬 패러다임을 제시하여 복잡한 요구 사항을 따르는 모델을 훈련할 수 있게 합니다.
- **Technical Details**: HelpSteer2 데이터셋은 주로 ShareGPT에서 자발적으로 공유된 대화를 기반으로 하며, 여기에서 사용자 입력만 사용하고 Assistant의 응답은 제외되었습니다. 데이터셋의 품질을 높이기 위해 다양한 주제와 복잡성 수준에서 샘플을 골고루 선택하였습니다. 데이터셋의 약 29%는 멀티턴 대화로 구성되어 있습니다. 다양한 크기와 학습 알고리즘을 사용하여 내부 LLM(Nemotron 모델군 등)으로 응답을 생성하였고, 응답의 품질을 높이기 위해 여러 명의 annotators가 각 응답을 평가했습니다.
- **Performance Highlights**: HelpSteer2 데이터셋을 통해 예측된 보상을 학습한 모델들은 인간의 선호도와 잘 일치하는 것으로 나타났습니다. 각 응답에 대해 적어도 세 명 이상의 annotators가 평가를 진행하여 데이터 품질을 높였으며, 다수의 annotators 사이에서도 높은 일관성을 보였습니다. 또한, 내부 모델을 포함한 다양한 소스의 응답을 사용하여 모델의 응답 다양성을 크게 증가시켰습니다.

### [EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts](https://arxiv.org/abs/2406.09162)

![](/avatars/96feb22fb5e8931d6c9e0ea06148266f.svg)

Vote: 13

Authors: Yucheng Han, Chi Zhang, Hanwang Zhang, Pei Cheng, Rui Wang, Bin Fu, Juntao Hu

- **What's New**: 최신 이미지 생성 연구에서는 EMMA라는 새로운 접근 방식을 소개합니다. 이 모델은 다중 모달 조건(Multi-modal prompts)을 텍스트 중심의 이미지 생성 프로세스에 통합하여 혁신성을 높였습니다. 이를 통해 다양한 입력 조건에 반응할 수 있는 고품질 이미지를 생성할 수 있습니다.
- **Technical Details**: EMMA는 최신 텍스트 조건부(diffusion) 모델인 ELLA를 기반으로 하며, 이를 통해 프리트레인된 텍스트 및 확산 모델(Pre-trained text and diffusion models)의 통합을 강화할 수 있습니다. 핵심 기술로는 'Assemblable Gated Perceiver Resampler' (AGPR)를 사용하여 텍스트 외의 다른 모달리티에서 정보를 주입하는 메커니즘을 도입했습니다. EMMA는 Stable Diffusion 프레임워크와 호환되며, 추가적인 학습 없이 다양한 모델에 플러그 앤 플레이(plug-and-play) 모듈로 적용할 수 있습니다.
- **Performance Highlights**: EMMA는 다양한 제어 신호에 대해 강력한 성능을 보이며, 생성된 이미지에서 텍스트 및 시각적 세부 사항을 보존합니다. 실험 결과, EMMA는 텍스트 및 비주얼 디테일을 유지하면서 높은 충실도와 품질을 제공하는 것으로 나타났습니다. 이 모델은 다양한 조건과 애플리케이션을 수용할 수 있도록 확장성과 유연성을 갖추고 있습니다.

### [4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities](https://arxiv.org/abs/2406.09406)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09406.png)

Vote: 11

Authors: Amir Zamir, Ali Garjani, Jiaming Hu, Afshin Dehghan, Roman Bachmann, Oğuzhan Fatih Kar, David Griffiths, Mingfei Gao, David Mizrahi

- **What's New**: 이 논문에서는 다양한 태스크와 모달리티를 처리할 수 있는 단일 뉴럴 네트워크의 가능성을 연구하고 있습니다. 특히, 기존의 멀티태스크 학습에서 발견되는 성능 저하 문제를 극복하고, 많은 수의 다양한 모달리티를 통합한 단일 모델을 제안합니다. 7개의 기존 모달리티에서 21개의 다양한 모달리티까지 확장하여, 크로스모달 검색(cross-modal retrieval) 및 제어 가능한 생성(controllable generation) 등 새로운 기능을 제공합니다.
- **Technical Details**: 제안된 모델은 모달리티별 이산 토크나이저(modality-specific discrete tokenizers)를 사용하여 다양한 모달리티를 이산 토큰으로 변환합니다. 이미지와 같은 모달리티의 경우 ViT 기반 VQ-VAE 토크나이저를 사용하며, 3D 인간 포즈 및 이미지 임베딩과 같은 모달리티에는 MLP 기반 이산 VAE를 사용합니다. 그 외 텍스트 표현으로 맵핑할 수 있는 모달리티는 WordPiece 토크나이저로 인코딩합니다.
- **Performance Highlights**: 논문에서 제안한 모델은 기존의 멀티태스크 학습 어려움을 극복하고, 다양한 모달리티/태스크를 처리할 수 있는 단일 모델을 사용하여 성능 저하 없이 미리 훈련된 다양한 모달리티를 처리할 수 있음을 증명하였습니다. 이를 통해 다양한 모달리티 간의 검색 및 제어 가능한 생성 등의 새로운 가능성을 보여줍니다.

### [Explore the Limits of Omni-modal Pretraining at Scale](https://arxiv.org/abs/2406.09412)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09412.png)

Vote: 10

Authors: Xiangyu Yue, Jing Liu, Yiyuan Zhang, Handong Li

- **What's New**: 이번 논문은 CLIP의 시각적 이해 능력을 강화하고 오디오, 비디오, 3D 콘텐츠 등 더욱 다양한 멀티모달 데이터를 처리할 수 있도록 하기 위해 새로운 옴니모달 학습 아키텍처를 제안합니다. MiCo (Multimodal Context) 프레임워크를 통해 멀티모달 데이터를 통합적인 방식으로 이해하고 모델을 학습시킵니다.
- **Technical Details**: 연구팀은 리처드 메이어의 '멀티미디어 학습의 인지 이론(Cognitive Theory of Multimedia Learning)'을 참고하여 사람들이 멀티미디어 신호를 청각 및 시각 채널을 통해 처리하는 방식을 적용한 옴니모달 학습 아키텍처를 설계했습니다. 지식 모달리티(knowledge modality)와 인터페이스 모달리티(interface modality)로 나눠 각각의 브랜치를 통해 학습을 진행합니다. 추가로 MiCo 프레임워크를 통해 다양한 모달리티를 통합 및 공유하는 백본 네트워크를 이용해 맵핑하고, 같은 위치 임베딩(position embeddings) 및 추가 컨텍스트 임베딩(context embeddings)을 이용해 모달리티 특성 및 교육적 의미를 이해합니다.
- **Performance Highlights**: MiCo는 다양한 벤치마크에서 뛰어난 성능을 보여주었으며, 37개 이상의 새로운 SOTA(State Of The Art) 성능을 달성했습니다. 이와 함께 여러 벤치마크에서 20% 이상의 성능 향상을 이뤘습니다. MiCo는 마스킹된 모델링(masked modeling) 및 대조 학습(contrastive learning)의 장점을 모두 통합하여 확장성과 일반화된 표현(저장방법)을 제공하는 차세대 전처리(paradigm)입니다.

### [Cognitively Inspired Energy-Based World Models](https://arxiv.org/abs/2406.08862)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08862.png)

Vote: 9

Authors: Ganesh Nanduru, Jundong Li, Aman Chadha, Alexi Gladstone, Md Mofijul Islam, Tariq Iqbal

- **What's New**: 이번 연구는 인간의 인지적 능력을 모방한 새로운 아키텍처인 Energy-Based World Model (EBWM)을 제안합니다. 이는 기존의 전통적인 자기회귀 모델(TAMs)과는 다른 방식으로 세계 모델을 훈련시키며, 에너지 기반 모델(EBM)을 이용해 미래 상태 예측과 현재 컨텍스트와의 적합성을 평가합니다.
- **Technical Details**: EBWM은 인간의 고차원적 사고와 계획에 필요한 네 가지 주요 인지적 능력을 포함합니다: 예측이 내부 상태에 미치는 영향, 예측 평가, 자원 할당의 동적 조정, 그리고 연속된 상태 공간에서의 불확실성 모델링. 이를 위해, EBWM은 입력 공간에서 미래 상태를 예측하고, 이러한 예측과 현재 컨텍스트 간의 적합성을 예측하는 에너지 기반 모델을 사용합니다.
- **Performance Highlights**: 실험 결과, EBWM은 데이터와 GPU 시간 측면에서 전통적인 자기회귀 모델(TAMs)보다 더 나은 확장성을 보여줍니다. CV와 NLP 도메인에서 EBWM을 구현한 결과는 전통적인 자기회귀 트랜스포머와 유사한 확장성을 가지면서도, 네 가지 인간 인지적 능력을 포함하여 질적으로 다른 성능을 발휘합니다.

### [Mistral-C2F: Coarse to Fine Actor for Analytical and Reasoning Enhancement in RLHF and Effective-Merged LLMs](https://arxiv.org/abs/2406.08657)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08657.png)

Vote: 9

Authors: Chen Zheng, Xun Zhou, Ke Sun

- **What's New**: 최근 발표된 논문에서는 대규모 언어 모델 (LLMs) 중에서도 GPT-4와 Claude처럼 뛰어난 성능을 보이는 모델들이 주목받고 있으며, 이와 함께 소형 LLMs도 그 성능을 입증하고 있습니다. 이러한 소형 LLMs는 MMLU 및 BBH와 같은 일반 벤치마크에서 좋은 성적을 거두었지만, 대화 능력에서 여전히 한계를 보이고 있습니다. 본 논문에서는 이를 해결하기 위해 'Coarse to Fine Analytical and Reasoning Enhancement LLM'이라는 새로운 접근법을 제안합니다.
- **Technical Details**: 첫 번째 단계는 'Coarse Actor' 분석 및 추론 LLM으로, 여기서는 지속적 최대화(Continuous Maximization) 전략을 도입하여 RLHF 적용 시 출력 길이 제한을 동적으로 확장합니다. 이러한 과정을 통해 더 상세하고 깊이 있는 분석 내용을 생성할 수 있습니다. 두 번째 단계는 'Fine Actor' 지식 정제 LLM으로, Coarse Actor 모델이 생성한 출력을 기존의 SFT 모델과 통합하여 생성물의 품질을 높이고 중복 정보를 줄입니다.
- **Performance Highlights**: Mistral 모델을 기반으로 한 Mistral-C2F LLM은 11개의 일반 언어 과제에서 뛰어난 성능을 발휘하였으며, 유사 규모의 모델들뿐만 아니라 13B 및 30B 파라미터를 가진 대규모 모델들보다도 우수한 성능을 보였습니다. 특히 대화 능력과 분석 추론 능력에서 MT-BENCH 벤치마크를 통해 SOTA(SOTA: State of the Art) 성능을 입증하였습니다. Mistral-C2F 모델은 현재 HuggingFace에서 오픈 소스로 공개되어 있습니다.

### [Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?](https://arxiv.org/abs/2406.07546)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07546.png)

Vote: 8

Authors: Yujie Lu, Xingyu Fu, William Yang Wang, Dan Roth, Muyu He

- **What's New**: 최근의 최신 생성 모델 연구에서 소개된 'Commonsense-T2I' 벤치마크는 텍스트-이미지 변환(text-to-image) 모델의 실생활 상식(reasoning) 적용 능력을 정량적, 정성적으로 평가하기 위한 새로운 평가 방법입니다.
- **Technical Details**: Commonsense-T2I는 150개의 수작업으로 큐레이션된 예제를 포함하고 있으며, 각 예제는 두 개의 대립 텍스트 프롬프트(adversarial text prompts)를 포함하고 있습니다. 이 프롬프트들은 미묘한 차이를 가진 동일한 동작 단어 세트를 포함하며, 결과 이미지가 확연히 다른 상식을 반영해야 합니다. 평가는 멀티모달 대형 언어 모델(multimodal large language models, LLMs)을 통해 자동으로 수행됩니다.
- **Performance Highlights**: Stable Diffusion, Playground v2.5, Openjourney v4, DALL-E 3 등 여러 T2I 모델을 평가한 결과, 최신 DALL-E 3 모델도 48.92%의 정확도만 달성했으며, 다른 모델들은 15-30% 범위에 머물렀습니다. 이는 현재의 T2I 모델들이 상식 추론능력에서 아직 인간 수준에 도달하지 못했음을 시사합니다.

### [TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video Generation](https://arxiv.org/abs/2406.08656)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08656.png)

Vote: 7

Authors: Tsu-jui Fu, William Yang Wang, Jiachen Li, Wenhu Chen, Michael Saxon, Weixi Feng

- **What's New**: 본 연구에서는 영상 생성 모델의 시간적 조합성(temporal compositionality) 문제를 다루기 위해 Temporal Compositionality Benchmark (TC-Bench)를 제안합니다. TC-Bench는 속성 전환(attribute transition), 객체 관계(object relations), 배경 변화(background shifts) 등 세 가지 시나리오에서의 조합적 변화를 평가하는 데 중점을 둡니다. 이를 통해 생성된 영상이 처음과 끝 상태를 명확하게 표현하는지를 측정합니다.
- **Technical Details**: TC-Bench는 텍스트에서 비디오 생성(text-to-video, T2V)과 이미지에서 비디오 생성(image-to-video, I2V) 모델의 평가를 위한 새로운 메트릭인 TCR과 TC-Score를 도입합니다. 이 메트릭은 영상 언어 모델(vision language models, VLMs)을 사용하여 프레임 수준의 조합성(assertions)을 검증하고, 이를 전체 비디오에서 검토합니다. TCR과 TC-Score는 조합적 전환 완료도와 텍스트-비디오 정렬(text-video alignment)을 측정하며, 기존 메트릭보다 인간의 판단에 더 높은 상관관계를 보입니다.
- **Performance Highlights**: 아홉 가지의 기초 모델을 종합적으로 평가한 결과, 대부분의 영상 생성 모델이 테스트 케이스의 약 20%만을 충족시키는 것으로 나타났습니다. 이는 현재의 모델들이 텍스트 또는 이미지 프롬프트를 이해하고, 시간적 일관성을 유지하는 데 있어 아직 많은 발전이 필요함을 시사합니다. 다양한 시각적 엔티티, 장면 및 스타일을 포함한 현실적인 전환을 특징으로 하는 새로운 벤치마크를 통해, 향후 연구의 큰 도약을 기대할 수 있습니다.

### [Real3D: Scaling Up Large Reconstruction Models with Real-World Images](https://arxiv.org/abs/2406.08479)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08479.png)

Vote: 6

Authors: Georgios Pavlakos, Qixing Huang, Hanwen Jiang

- **What's New**: 이번 arxiv 논문은 새로운 인공지능(AI) 모델을 발표했습니다. 이 모델은 자연어 이해(natural language understanding)와 생성(generation)에 초점을 맞추고 있습니다. 이 기술은 사람과 유사한 수준의 텍스트 생성 능력을 갖추고 있습니다.
- **Technical Details**: 이 논문에서는 Transformer 아키텍처를 기반으로 하여, 더 높은 정확도와 효율성을 제공하는 여러 혁신적인 변형을 제안합니다. 특히, Attention Mechanism(어텐션 메커니즘)의 효율성을 높이기 위해 여러 가지 최적화 기법을 도입했습니다. 또한, 데이터 처리와 학습 과정에서의 개선 사항도 포함되어 있어, 대규모 데이터셋 처리에 강점을 가집니다.
- **Performance Highlights**: 테스트 결과, 새로운 모델은 여러 벤치마크에서 기존 최첨단 모델을 뛰어넘는 성능을 보였습니다. 예를 들어, 새로운 기술은 GLUE benchmark에서 이전 최고의 결과를 크게 상회했습니다. 이는 새로운 어텐션 기법과 최적화 덕분에 가능해졌습니다.

### [CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark](https://arxiv.org/abs/2406.05967)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.05967.png)

Vote: 5

Authors: Ganzorig Batnasan, David Ifeoluwa Adelani, Frederico Belcavello, Injy Hamed, Aditya Nanda Kishore, Gisela Vallejo, David Le Meur, Emilio Villa-Cueva, Chenxi Whitehouse, Chenyang Lyu, +, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Artem Abzaliev, Teresa Lynn, Dan John Velasco, Fajri Koto, Haryo Akbarianto Wibowo, Christian Salamea, Fauzan Farooqui, Alina Dragonetti, David Romero, Aishik Mandal

- **What's New**: CVQA는 다국어와 문화적 맥락을 반영한 새로운 VQA 벤치마크를 소개합니다. CVQA는 28개국에서 26개 언어로 이루어진 9천 개 이상의 질문을 포함하며, 각 언어와 국가 쌍(Country-Language pair)별로 세분화되어 총 33개의 서로 다른 쌍을 구성합니다. 모든 샘플은 현지 언어와 영어로 작성되어 다국어 및 영어 전용 MLLM을 평가할 수 있습니다.
- **Technical Details**: CVQA 데이터셋은 다국어 다지선다형(Multiple-choice) 로컬-뉴언스드(Locally-nuanced) 비주얼 질문 응답 데이터셋입니다. 데이터 수집 및 주석 과정에서는 공통 지식을 문화적 대표성의 프록시로 사용했으며, 각 질문은 이미지와 관련이 있도록 설계되었습니다. 10개의 카테고리로 분류된 질문들은 문화적 아이콘과 미디어를 포함한 다양한 문화적 측면을 반영합니다.
- **Performance Highlights**: 여러 MLLM을 벤치마킹한 결과, 많은 모델들이 50% 이상의 정확도를 달성하지 못하며, 현지 언어로 질문을 할 때 모델 성능이 현저히 떨어지는 것을 확인했습니다. 특히 브레통어와 자바어 같은 잘 연구되지 않은 언어에서는 성능 저하가 두드러졌습니다. 이를 통해 다국어 프롬프트 이해의 갭이 크게 존재함을 알 수 있습니다.

### [Language Model Council: Benchmarking Foundation Models on Highly Subjective Tasks by Consensus](https://arxiv.org/abs/2406.08598)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08598.png)

Vote: 5

Authors: Flor Miriam Plaza-del-Arco, Amanda Cercas Curry, Justin Zhao

- **What's New**: 대형 언어 모델(Large Language Models, LLMs)의 평가 과제가 점점 더 어려워지고 있는 상황에서, 주관적인 작업에서 LLM의 품질을 평가하기 위한 언어 모델 위원회(Language Model Council, LMC)를 소개합니다. 이는 감정 지능(emotional intelligence, EI) 작업을 위한 개방형 테스트 세트를 통해 20개의 최신 LLM을 평가하는 유연한 프레임워크를 제안합니다.
- **Technical Details**: LMC는 세 가지 단계로 구성됩니다: 테스트 세트 작성, 반응 수집, 집단 심사. 각 단계는 LLM 기반 에이전트를 통해 자동으로 진행됩니다. 테스트 세트는 EmoBench 데이터셋을 기반으로 하여 다양한 LLM들이 개방형 질문으로 확장하도록 했습니다. ELO 점수 시스템을 사용해 모델의 순위가 결정되며, 결과는 https://llm-council.com 에서 공개됩니다.
- **Performance Highlights**: LMC를 통해 수행된 EI 작업에서 Qwen-1.5-110B 모델이 GPT-4o를 넘어 1위를 차지했습니다. LLM 심사 품질의 주요 측정을 정의하고 평가했으며, claude-3-opus와 mistral-large가 가장 효과적인 심사자로 식별되었습니다. 인간 연구를 통해 LMC의 순위가 다른 벤치마크보다 인간이 설정한 순위와 더 높은 상관관계를 가지는 것을 확립했습니다. 또한 Monte-Carlo 시뮬레이션을 사용해 순위의 안정성과 공격적인 심사자의 견고성을 측정했습니다.

### [Understanding Hallucinations in Diffusion Models through Mode Interpolation](https://arxiv.org/abs/2406.09358)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09358.png)

Vote: 4

Authors: J. Zico Kolter, Zachary C. Lipton, Sumukh K Aithal, Pratyush Maini

- **What's New**: 이 논문에서는 확산 모델(Diffusion Models)에서 발생하는 특정 실패 모드(failure mode)를 공식화하고 연구합니다. 이 실패 모드는 '환각(hallucination)'으로, 모델이 훈련 데이터 분포에 전혀 존재하지 않는 샘플을 생성하는 현상입니다.
- **Technical Details**: 연구에서는 단순한 모양(simple shapes)을 포함한 데이터세트를 사용하여 확산 모델을 훈련하고, 데이터 분포의 불연속 모드에서 발생하는 환각을 관찰했습니다. 이러한 환각의 원인을 '모드 보간(mode interpolation)'이라는 현상으로 설명합니다. 이를 분석하기 위해 1차원 및 2차원 Gaussian 혼합 설정을 사용하여 확산 모델을 훈련하고, 역확산(reverse diffusion) 과정에서의 경로 분산을 이용해 환각을 탐지하는 메트릭을 제안합니다.
- **Performance Highlights**: 제안한 탐지 메커니즘은 샘플링 과정에서 경로 분산을 통해 환각을 민감도 및 특이도 >0.92로 탐지할 수 있으며, 반복적인 훈련 중에 모델 붕괴를 완화하는 데 효과적입니다. 2D 그리드 Gaussian, Simple Shapes, 그리고 MNIST 데이터셋에서 이러한 메커니즘이 모델 붕괴를 완화하는 결과를 보였습니다.

### [CMC-Bench: Towards a New Paradigm of Visual Signal Compression](https://arxiv.org/abs/2406.09356)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09356.png)

Vote: 4

Authors: Xiaohong Liu, Donghui Feng, Chunyi Li, Haoning Wu, Xiongkuo Min, Guangtao Zhai, Weisi Lin, Guo Lu, Zicheng Zhang, Xiele Wu

- **What's New**: 시각 신호 압축(Visual signal compression)은 이미지 데이터를 최소화하여 네트워크 자원과 저장 용량이 제한된 환경에서 고품질의 이미지/비디오 서비스를 제공하는 데 중요한 역할을 합니다. 최근, 전통적인 이미지 코덱은 500배의 압축률을 달성했지만, 차세대 프로토콜에서는 Shannon 한계에 도달하는 1,000 배 압축률에 접근하고 있습니다. 대규모 멀티모달 모델(LMMs, Large Multimodal Models)의 급속한 발전 덕분에 초저비트율(ULB, Ultra-low Bitrate) 압축이 가능해 졌습니다.
- **Technical Details**: LMMs는 여러 모달리티 간의 변환을 지원하여 텍스트는 이미지보다 훨씬 적은 공간을 차지합니다. 이미지-텍스트(I2T, Image-to-Text)와 텍스트-이미지(T2I, Text-to-Image) 모델을 연계하여 이미지를 의미론적 정보에서 압축하고 재구성할 수 있습니다. 이 크로스 모달리티 압축(CMC, Cross-Modality Compression) 패러다임은 전통적인 코덱을 능가하여 초저비트율(ULB)뿐만 아니라 극저비트율(ELB, Extreme-low Bitrate) 압축률까지도 달성할 수 있습니다. 그러나 ULB에서 두 가지 주요 문제인 일관성(consistency)과 인식(perception)이 발생합니다.
- **Performance Highlights**: 가장 진보된 CMC 방법인 M-CMC와 MISC는 전통적인 코덱 이상으로 일관성과 인식 측면에서 우수한 성능을 보입니다. 새로운 벤치마크인 CMC-Bench는 첫 공동 평가 표준으로서 도입되었으며, 58,000개의 이미지로 구성된 대규모 데이터셋과 160,000개의 전문가 주석을 포함하여 정보 손실 모델링을 지원합니다. 4가지 압축 모드 및 두 가지 평가 차원을 포괄하는 종합 평가 기준을 제공하고, 주류 모델 18개를 포함한 최상의 조합을 탐색하기 위한 실험을 진행했습니다.

### [MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding](https://arxiv.org/abs/2406.09297)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09297.png)

Vote: 4

Authors: Muhammad Farid Adilazuarda, Zayd Muhammad Kawakibi Zuhri, Alham Fikri Aji, Ayu Purwarianti

- **What's New**: 이번 연구에서는 Transformer 아키텍처의 기존 제약을 극복하기 위한 Multi-Layer Key-Value (MLKV) sharing 기법을 제안합니다. 이 접근법은 동일 레이어뿐만 아니라 다른 레이어에서도 KV 헤드를 공유하여 메모리 사용량을 획기적으로 절감할 수 있습니다.
- **Technical Details**: Transformer 아키텍처는 auto-regressive 특성으로 인해 추론(inference) 시점마다 키와 값(Key-Value) 데이터를 저장하고 불러오는 과정에서 메모리 대역폭이 병목됩니다. 기존 Multi-Query Attention (MQA) 및 Grouped-Query Attention (GQA)는 동일한 레이어 내에서 KV 헤드를 그룹으로 공유하여 이러한 문제를 해결하려는 시도가 있었습니다. 그러나 MLKV는 이를 넘어 여러 레이어 사이에서도 KV 헤드를 공유하는 방식을 택해 더 큰 메모리 절감을 가능하게 합니다.
- **Performance Highlights**: MLKV 기법은 기존 모델 대비 최대 2/n_layers의 KV 캐시 크기까지 절감할 수 있으며, Pythia-160M 체크포인트를 사용해 업트레이닝(uptraining)된 상태에서도 성능 저하 없이 합리적인 메모리 절감 효과를 보여줍니다.

### [Estimating the Hallucination Rate of Generative AI](https://arxiv.org/abs/2406.07457)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07457.png)

Vote: 4

Authors: Andrew Jesson, Nicolas Beltran-Velez, John P. Cunningham, Sweta Karlekar, Jannik Kossen, Quentin Chu, David Blei, Yarin Gal

- **What's New**: 이번 연구는 In-Context Learning(ICL)의 환각율(hallucination rate)을 추정하는 방법에 관한 것입니다. ICL은 조건부 생성 모델(Conditional Generative Model, CGM)에 데이터를 입력하고 해당 데이터 기반의 예측을 요청하는 방식입니다. 이러한 접근법은 사전 훈련된 모델을 활용하여 최적화되지 않은 문제도 해결하게 해줍니다.
- **Technical Details**: ICL은 Bayesian 관점을 취하여 CGM이 잠재 매개변수와 데이터에 대한 후향 예측 분포(posterior predictive distribution)로부터 샘플링된다고 가정합니다. 이를 통해 관찰된 데이터(여기서는 'context')를 조건으로 후향 환각율(posterior hallucination rate, PHR)을 정의할 수 있습니다. 본 연구에서는 예측 분포를 사용하여 PHR을 추정하는 새로운 방법을 제시합니다.
- **Performance Highlights**: 실험 결과, PHR 추정기는 합성 데이터를 사용한 실험에서 실제 환각율을 정확하게 예측할 수 있음을 확인했습니다. 또한 Llama-2 가족의 사전 훈련된 CGMs를 사용한 자연어 ICL 문제에서도 높은 정확도로 경험적 오류율을 추정할 수 있음을 보였습니다.

### [Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven Text-to-Image Generation](https://arxiv.org/abs/2406.09305)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09305.png)

Vote: 4

Authors: Zichao Wang, Kaizhi Zheng, Nanxuan Zhao, Xin Eric Wang, Tong Sun, Yufan Zhou, Jiuxiang Gu, Ruiyi Zhang

- **What's New**: 최근 사용자가 제공한 단일 이미지나 몇 개의 이미지를 통해 특정 개념을 창의적으로 생성하는 '주제 주도 텍스트-이미지 생성(subject-driven text-to-image generation)'이 많은 관심을 받고 있습니다. 기존의 사전 학습된 텍스트-이미지 생성 모델들은 특정 주제를 가진 이미지를 생성하는 데 실패하는 경우가 많습니다. 다양한 방법들이 이 문제를 해결하기 위해 제안되었습니다. 본 논문에서는 Toffee라는 새로운 방법을 제안하여 효율적이고 고품질의 주제 주도 텍스트-이미지 생성 데이터셋을 구축합니다. Toffee는 기존 방법들보다 훨씬 적은 연산 자원을 필요로 하며, 대규모 데이터셋 구축에 있어서 연산 비용을 수만 GPU 시간 절감할 수 있습니다. 이를 통해 Toffee-5M이라는 대규모 데이터셋을 구축하였고, 본 데이터셋을 통해 이미지 생성 및 편집 작업을 수행할 수 있습니다.
- **Technical Details**: {'Dataset Construction': 'Toffee의 데이터셋 구축 파이프라인은 주제 이미지를 사전 학습된 확산 모델(diffusion model)인 ControlNet에 입력하여 텍스트 정렬된 이미지를 생성하고, Refiner로 세부 내용을 보완하며, View Generator로 다른 뷰의 이미지를 생성합니다. 이때, Refiner와 View Generator는 주제 세부 조정을 필요로 하지 않기 때문에 연산 요구가 크게 줄어듭니다. 이러한 방법을 통해 데이터셋 구성 시 대규모 연산 자원을 절감합니다.', 'Model Architecture': 'ToffeeNet은 새로운 이미지 편집 및 생성을 위한 통합 모델로, 새로운 데이터셋을 기반으로 학습되어 시간 요구가 없는 주제 주도의 이미지를 생성할 수 있습니다. Refiner는 DINO 이미지 인코더로부터 얻은 임베딩을 입력으로 받아 이미지를 재구성하는 확산 모델로 작동하며, UNet의 교차 주의력 계층을 통해 주입됩니다.'}
- **Performance Highlights**: {'Efficiency': '기존 SuTI 및 CAFE 방법들보다 데이터셋 구축 시 필요한 연산 자원이 획기적으로 절감됩니다. 예를 들어 SuTI는 1백만 주제의 데이터셋을 구축하는데 약 83,000 TPU 시간이 필요하지만, Toffee는 전체 데이터셋 구축에 3,000 GPU 시간 이하가 필요합니다.', 'Scalability': 'Toffee 방법을 통해 구축된 Toffee-5M 데이터셋은 관련된 데이터셋 중 가장 큰 규모로, 기존 데이터셋보다 5배 이상 큽니다.', 'Versatility': 'ToffeeNet은 시간 요구 없이 주제 주도의 이미지 생성이 가능하고, 다양한 스타일, 배경을 가진 창의적인 이미지 생성이 가능합니다. 따라서, 주제 주도의 이미지 생성과 편집 작업에서 경쟁력 있는 결과를 나타냅니다.'}

### [LRM-Zero: Training Large Reconstruction Models with Synthesized Data](https://arxiv.org/abs/2406.09371)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.09371.png)

Vote: 3

Authors: Kai Zhang, Yi Zhou, Sören Pirk, Zhixin Shu, Desai Xie, Zexiang Xu, Sai Bi, Arie Kaufman, Hao Tan, Xin Sun

- **What's New**: 최근 놀라운 발전을 이루고 있는 Foundation 모델들은 확장 가능한 모델 아키텍처와 대규모 데이터셋을 통해 강력한 성능을 발휘하고 있습니다. 이번 연구에서는 3D 데이터 부족 및 라이센스 문제를 해결하기 위해 순수히 합성된 데이터로 학습한 LRM-Zero를 제안합니다. 이 모델은 Zeroverse라는 합성 데이터셋을 사용하여 3D 재구성에 뛰어난 성능을 보입니다.
- **Technical Details**: LRM-Zero는 텍스트 & 이미지 & 비디오 도메인에서 성공적인 Transformer 아키텍처를 기반으로 합니다. Zeroverse라는 합성 데이터셋은 절차적으로 생성된 기본 도형과 텍스처 및 다양한 형태의 증강 데이터를 포함합니다. 다섯 가지 원시 형상(큐브, 스피어, 실린더, 콘, 토러스)을 사용해 다양한 표면 및 형태적 특성을 포괄하고, 텍스처와 세 가지 증강 방법을 통해 데이터 다양성을 증가시켰습니다. LRM-Zero는 GS-LRM 아키텍처를 사용하여 sparse-view 3D 재구성을 진행합니다.
- **Performance Highlights**: 초기 실험 결과 LRM-Zero는 Objaverse로 학습한 GS-LRM과 유사한 재구성 품질을 보여주었습니다. 또한, 표준 3D 재구성 벤치마크(ABO, GSO)에서 경쟁력 있는 결과를 기록하였습니다. 흥미로운 점은 sparse-view 재구성에서 'zero'-shot 데이터 일반화가 가능하다는 점이며, 이는 지역적 시각 단서에 의존하기 때문이라고 볼 수 있습니다. 다양한 데이터셋(예: OmniObject3D, OpenIllumination)에서도 일반화 성능을 입증하였으며, 텍스트-3D와 이미지-3D 생성도 지원합니다.

