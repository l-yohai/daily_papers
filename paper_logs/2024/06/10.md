## Daily Papers (2024-06-10)

### [Mixture-of-Agents Enhances Large Language Model Capabilities](https://arxiv.org/abs/2406.04692)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04692.png)

Vote: 16

Authors: Ce Zhang, James Zou, Jue Wang, Ben Athiwaratkun, Junlin Wang

- 최근 대형 언어 모델(LLM)에서 자연어 이해와 생성 작업의 상당한 기능 향상이 나타나고 있습니다.
- 다수의 LLM을 활용하여 이들의 집합적 전문성을 활용하는 방법이 중요한 연구 방향으로 떠오르고 있습니다.
- 이를 위해 Mixture-of-Agents(MoA) 방법론을 제안하며, 각 층에 여러 LLM 에이전트가 포함된 층형 MoA 아키텍처를 구성합니다.
- 각 에이전트는 이전 층의 에이전트들이 생성한 모든 출력을 보조 정보로 활용하여 자신의 응답을 생성합니다.
- MoA 모델은 AlpacaEval 2.0, MT-Bench, FLASK에서 최신 성능을 기록하며 GPT-4 Omni를 능가했습니다.
- 예를 들어, 오픈 소스 LLM만 사용한 MoA는 AlpacaEval 2.0에서 65.1%의 점수를 기록해 GPT-4 Omni의 57.5%를 크게 앞질렀습니다.

### [Large Language Model Confidence Estimation via Black-Box Access](https://arxiv.org/abs/2406.04370)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04370.png)

Vote: 13

Authors: Amit Dhurandhar, Soham Dan, Soumya Ghosh, Prasanna Sattigeri, Tejaswini Pedapati

- 모델의 응답에 대한 불확실성 또는 신뢰도 추정은 모델 자체뿐만 아니라 응답에 대한 신뢰도 평가에서 중요하다.
- 본 논문에서는 단순히 블랙박스 또는 쿼리 접근을 통해 큰 언어 모델(LLM) 응답의 신뢰도를 추정하는 문제를 탐구한다.
- 우리는 새로운 기능을 설계하고 이 기능들을 사용하여 로지스틱 회귀와 같은 (해석 가능한) 모델을 학습시켜 신뢰도를 추정하는 간단하고 확장 가능한 프레임워크를 제안한다.
- 우리는 이 간단한 프레임워크가 flan-ul2, llama-13b 및 mistral-7b 모델의 신뢰도 추정에 효과적임을 실험적으로 증명하였으며, TriviaQA, SQuAD, CoQA 및 Natural Questions와 같은 벤치마크 데이터셋에서 기존 블랙박스 신뢰도 추정 접근 방식을 일관되게 능가하는 경우가 많음을 보여주었다.
- 추가적으로, 우리의 해석 가능한 접근 방식은 신뢰도를 예측하는 기능에 대한 통찰을 제공하며, 하나의 LLM에 대해 구축된 신뢰도 모델이 주어진 데이터셋에서 다른 LLM에 대한 제로샷 일반화가 가능하다는 흥미롭고 유용한 발견을 이끌어낸다.

### [GenAI Arena: An Open Evaluation Platform for Generative Models](https://arxiv.org/abs/2406.04485)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04485.png)

Vote: 12

Authors: Dongfu Jiang, Rongqi Fan, Tianle Li, Wenhu Chen, Shizhuo Sun, Max Ku, Yuansheng Ni

- 생성 AI는 이미지와 비디오 생성 분야를 혁신적으로 변화시켰지만, 신뢰할 수 있는 평가 기준의 부재가 두드러졌다.
- 현재의 자동 평가 방법(FID, CLIP, FVD 등)은 생성 출력물의 미묘한 품질과 사용자 만족도를 충분히 반영하지 못하고 있다.
- 본 논문은 다양한 이미지 및 비디오 생성 모델을 평가하기 위해 사용자 참여를 활성화하는 개방형 플랫폼인 GenAI-Arena를 제안한다.
- GenAI-Arena는 사용자 피드백과 투표를 활용해 모델 성능의 보다 민주적이고 정확한 측정을 목표로 한다.
- 텍스트-이미지 생성, 텍스트-비디오 생성, 이미지 편집 등 세 가지 아레나를 포함하며, 현재 총 27개의 오픈소스 생성 모델을 다루고 있다.
- 플랫폼은 4개월간 운영되어 커뮤니티로부터 6000건 이상의 투표를 받았다.
- 플랫폼 설명과 데이터 분석, 모델 순위를 위한 통계 방법이 설명된다.
- 모델 기반 평가 기준 연구를 촉진하기 위해 GenAI-Bench라는 선호 데이터의 깨끗한 버전을 공개한다.
- Gemini, GPT-4o와 같은 기존 다중 모달 모델을 활용해 인간 투표를 흉내내고, 모델 투표와 인간 투표 간의 상관 관계를 산출하여 평가 능력을 확인한다.
- 기존 다중 모달 모델들이 생성된 시각 콘텐츠 평가에서 여전히 부족함을 보이며, 가장 좋은 모델인 GPT-4o도 품질 하위 평가에서의 Pearson 상관 계수는 0.22에 불과하다.

### [CRAG -- Comprehensive RAG Benchmark](https://arxiv.org/abs/2406.04744)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04744.png)

Vote: 11

Authors: Lei Chen, Yushi Sun, Yue Liu, Lingkun Kong, Sajal Choudhary, Jiaqi Wang, Hao Xin, Nikita Bhalla, Nicolas Scheffer, +, Ziran Will Jiang, Xiangsen Chen, Kai Sun, Eting Yuan, Hanwen Zha, Ziyu Jiang, Xiao Yang, Brian Moran, Rongze Daniel Gui, An Yan, Chenyu Yang, Yifan Ethan Xu, Nan Tang

- 최근 대형 언어 모델(LLM)의 지식 부족 문제를 완화하기 위한 해결책으로 '획득-확장 생성(RAG)'가 주목받고 있음.
- 기존 RAG 데이터셋은 현실 세계의 다양한 동적 질문 응답(QA) 과제를 충분히 반영하지 못함.
- 이를 해결하기 위해, 5개 도메인과 8개 질문 범주에 걸친 4,409개의 질문-응답 쌍 및 웹과 지식 그래프(KG) 검색을 모방한 API를 포함한 포괄적인 사실적 질문 응답 벤치마크인 CRAG를 도입함.
- CRAG는 유명한 엔티티부터 긴 꼬리 엔티티까지의 인기도와 연도에서 초 단위까지의 시간적 변동성을 반영함.
- CRAG에 대한 평가 결과, 현재의 LLM이 34% 이하의 정확도를 달성하며, 기본적인 RAG의 추가로 44%의 정확도를 기록함.
- 최첨단 산업용 RAG 솔루션은 환각 없이 질문의 63%만을 정확히 대답할 수 있음.
- CRAG는 동적 성질이 더 강한 사실, 인기도가 낮은 사실, 복잡성이 높은 사실에 대한 답변 정확도가 훨씬 낮음을 드러냄.
- 이러한 결과는 미래 연구 방향을 제안하며, CRAG 벤치마크는 KDD Cup 2024의 기초로 사용되어 초기 50일 만에 수천 명의 참가자와 제출물을 이끌어냈음.
- 우리는 RAG 솔루션 및 일반 QA 솔루션의 발전을 위해 CRAG를 지속적으로 유지할 것을 약속함.

### [WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild](https://arxiv.org/abs/2406.04770)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04770.png)

Vote: 11

Authors: Ronan Le Bras, Abhilasha Ravichander, Bill Yuchen Lin, Yuntian Deng, Nouha Dziri, Faeze Brahman, Khyathi Chandu, Yejin Choi, Valentina Pyatkin

- WildBench는 실제 사용자 쿼리를 이용한 대형 언어 모델(LLM)의 성능을 평가하기 위한 자동화된 평가 프레임워크입니다.
- 100만 개 이상의 인간-챗봇 대화 로그에서 신중하게 선택된 1,024개의 과제로 구성되었습니다.
- WildBench는 고급 LLM (예: GPT-4-turbo)으로 계산할 수 있는 WB-Reward와 WB-Score 두 가지 주요 평가 지표를 개발했습니다.
- 과제별 체크리스트를 사용하여 모델 출력을 체계적으로 평가하고, 점수와 비교를 정당화하는 구조화된 설명을 제공합니다.
- WB-Reward는 모델 응답 간의 세밀한 쌍별 비교를 통해 다섯 가지 잠재적 결과를 생성합니다: 많이 나음, 약간 나음, 약간 나쁨, 많이 나쁨, 동점.
- 세 개의 다양한 성능 수준의 기준 모델을 선택하여 포괄적인 쌍별 평가를 보장했습니다.
- 길이 편향을 줄이기 위해 간단한 방법을 제안하여, 승자 응답이 패자 응답보다 K 문자 이상 길면 “약간 나음/나쁨”을 “동점”으로 변환합니다.
- WB-Score는 모델 출력의 품질을 개별적으로 평가하여 빠르고 비용 효율적인 평가 지표를 제공합니다.
- WildBench 결과는 어려운 과제에 대한 사람 선호 Elo 등급과 강한 상관관계를 보입니다.
- 특히 WB-Reward는 최상위 모델에서 Pearson 상관계수 0.98을 달성했으며, WB-Score는 0.95로 ArenaHard의 0.91 및 AlpacaEval2.0의 0.89를 초과했습니다.

### [Proofread: Fixes All Errors with One Tap](https://arxiv.org/abs/2406.04523)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04523.png)

Vote: 6

Authors: Shanqing Cai, Michael Xuelin Huang, Haicheng Sun, Renjie Liu, Yun Zhu, Shumin Zhai, Yuanbo Zhang, Yanxiang Zhang, Lei Meng

- 대형 언어 모델(LLM)의 놀라운 능력은 사용자의 타이핑 경험을 재구상하는 강력한 접근법을 제공합니다.
- 이 논문은 서버 측 LLM으로 구동되는 새로운 Gboard 기능인 Proofread를 소개하며, 사용자에게 단일 탭으로 문장 및 문단 수준의 원활한 수정 기능을 제공합니다.
- 데이터 생성, 메트릭 설계, 모델 튜닝 및 배포에 이르는 전체 시스템을 설명합니다.
- 충분한 품질의 모델을 얻기 위해 온라인 사용 사례에 맞춘 데이터 합성 파이프라인을 구현하고, 다각적인 메트릭을 설계하며, 두 단계의 튜닝 접근법을 채택했습니다: 기초 품질을 위한 감독형 미세 튜닝(SFT)과 목표 지향적 개선을 위한 강화 학습(RL) 튜닝.
- SFT 단계에서는 Rewrite와 proofread 작업에 대한 순차적 튜닝이 최고의 품질을 제공하며, RL 튜닝 단계에서는 글로벌 및 직접적인 보상으로 추가 개선을 도모했습니다.
- 인간 라벨링된 기준 집합에 대한 광범위한 실험에서 PaLM2-XS 모델이 85.56%의 좋은 비율을 달성했습니다.
- 이 기능은 TPU v5를 사용하여 모델을 구동하는 방식으로 Pixel 8 장치에 출시되었으며, 일일 활성 사용자 수천 명이 이용 중입니다.
- 정량화, 버킷 추론, 텍스트 분할 및 페널티 디코딩으로 서비스 지연 시간이 크게 줄어들었습니다.
- 데모는 https://youtu.be/4ZdcuiwFU7I{Youtube}에서 확인할 수 있습니다.

### [NATURAL PLAN: Benchmarking LLMs on Natural Language Planning](https://arxiv.org/abs/2406.04520)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04520.png)

Vote: 6

Authors: Azade Nova, Heng-Tze Cheng, Huaixiu Steven Zheng, Hugh Zhang, Quoc V. Le, Le Hou, Denny Zhou, Xinyun Chen, Minmin Chen, Ed H. Chi, Swaroop Mishra

- NATURAL PLAN은 여행 계획, 미팅 계획 및 캘린더 일정 잡기라는 세 가지 주요 작업을 포함하는 현실적인 자연어 계획 벤치마크를 도입합니다.
- 작업에 대한 완전한 정보를 제공하여 LLM의 계획 기능을 평가하며, Google Flights, Google Maps 및 Google Calendar와 같은 도구의 출력을 모델 맥락으로 제공합니다.
- 이를 통해 도구 사용 환경 없이도 LLM의 계획 능력을 평가할 수 있습니다.
- NATURAL PLAN은 최신 모델들에게 도전적인 벤치마크임을 관찰했습니다. 예를 들어, 여행 계획에서 GPT-4와 Gemini 1.5 Pro는 각각 31.1%와 34.8%의 해결율만을 달성했습니다.
- 문제의 복잡성이 증가할수록 모델 성능이 급격히 저하되며, 10개의 도시가 포함된 경우 모든 모델의 성능은 5% 이하로 떨어졌습니다.
- NATURAL PLAN에 대한 광범위한 소거 연구를 통해 자가 수정을 포함한 접근 방식의 (비)효과성에 대해 더 깊이 있는 통찰을 제공합니다.
- 이는 특히 자연어 계획에서 최신 LLM의 큰 격차를 강조합니다.

### [Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?](https://arxiv.org/abs/2406.04391)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04391.png)

Vote: 3

Authors: Sanmi Koyejo, Varun Madan, Herbie Bradley, Hailey Schoelkopf, Rylan Schaeffer, Gabriel Mukobi, Stella Biderman, Brando Miranda, Adam Ibrahim

- AI 시스템 확장은 예측 가능한 성능을 지니는 것이 바람직하지만, 다운스트림 성능 예측은 여전히 어려운 과제이다.
- 사전 학습 성능 확장에 대한 문헌은 잘 정립되어 있지만, 특정 다운스트림 능력 확장에 관한 문헌은 불명확하다.
- 이 연구는 다운스트림 능력 예측이 여전히 어려운 이유를 탐구하고, 여러 요인 중에서도 특정 요인이 이 난제를 야기하고 있음을 식별한다.
- 다섯 개의 모델 패밀리와 열두 개의 다중 선택 질문-응답 벤치마크를 사용하여 다운스트림 성능이 단계적으로 변환되는 과정에서 통계적 관계가 악화됨을 보여준다.
- 다운스트림 메트릭스가 소수의 특정 오답과 비교를 요구하므로, 단순히 정답에 확률 집중을 예측하는 것 외에도 오답에 대한 확률 변동 예측이 필요하다.
- 컴퓨팅 증가에 따른 정답과 오답의 확률 공변성을 실험적으로 연구하며, 오답에 대한 확장 법칙 예측 가능성을 시사한다.
- 이 연구는 사전 학습 확장 법칙이 다운스트림 능력보다 더 예측 가능한 이유를 설명하고, 미래 AI 모델에 대한 확장 예측 평가의 기초를 마련한다.

### [Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach](https://arxiv.org/abs/2406.04594)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04594.png)

Vote: 2

Authors: Bin Luo, Fei Feng, Yikai Zhu, Huang Zhong, Hanyu Zhao, Jiamang Wang, Hairong Jiao, Ang Liu, +, Wencong Xiao, Siran Yang, Jianwei Zhang, Gang Lu, Ennan Zhai, Yu Guan, Rui Men, Zian Chen, Jianbo Dong, Yi Shi, Xiang Li, Man Yuan, Jun Zhang, Pengcheng Zhang

- 대규모 언어 모델의 등장은 대규모 병렬 학습 기술의 채택을 필요로 하며, 이는 수천 개의 GPU를 사용하여 단일 모델을 학습시키는 것을 포함합니다.
- 그러나 현재 병렬 학습의 효율성이 최적화되지 않은 경우가 많으며, 주로 하드웨어 고장과 네트워크 혼잡으로 인한 두 가지 주요 문제로 인해 발생합니다.
- C4는 통신 기반 접근 방식을 통해 이러한 문제를 해결하고자 하며, 주요 통찰력은 두 가지입니다.
- 첫째, 병렬 학습 시 주기적이고 균질적인 특성을 갖는 집합 통신을 활용하여 하드웨어 오작동을 신속히 식별하고 문제를 격리하여 자원 낭비를 방지합니다.
- 둘째, 예측 가능한 통신 모델을 통해 네트워크 혼잡을 줄이고 효율적인 트래픽 계획을 실행합니다.
- C4는 실제 시스템에 광범위하게 구현되어, 오류로 인한 오버헤드를 약 30% 줄이고, 의사소통 비용이 중간 정도인 특정 응용 프로그램의 실행 성능을 약 15% 향상시켰습니다.

