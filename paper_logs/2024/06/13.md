## Daily Papers (2024-06-13)

### [NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing](https://arxiv.org/abs/2406.06523)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06523.png)

Vote: 47

Authors: Hau-Shiang Shiu, Jiewen Chan, Chang-Han Yeh, Yu-Lun Liu, Shih-Han Yen, Ting-Hsuan Chen

- **What's New**: 이번 논문에서는 NaRCan이라는 혁신적인 하이브리드 변형 필드 네트워크 아키텍처를 제안합니다. 이를 통해 다양한 시나리오에서 고품질의 자연스러운 canonical 이미지를 생성할 수 있습니다. 특히, diffusion prior를 통합하여 기존의 비디오 편집 방법들보다 더 나은 성능을 보임을 실험적으로 입증하였습니다.
- **Technical Details**: NaRCan은 homography와 잔여 변형 MLP를 결합한 하이브리드 변형 필드 모델을 도입하였습니다. 또한, LoRA로 미세 조정된 잠재 diffusion 모델(latent diffusion model)을 통합하여 자연스러운 canonical 이미지 생성을 보장했습니다. 기존 방법들이 canonical 이미지 품질 관리에 제한적이었던 반면, NaRCan은 diffusion priors를 사용해 이를 극복합니다. 동적 스케줄링 방법을 통해 전체 훈련 과정을 가속화시킨 것도 주요 기술적 특징 중 하나입니다.
- **Performance Highlights**: 실험 결과 NaRCan은 기존의 비디오 편집 방법들보다 우수한 성능을 보였습니다. 특히, 고품질의 자연스러운 canonical 이미지 생성과 빠른 수렴 속도에서 탁월한 성과를 냈습니다. 이러한 성능 개선은 다양한 영상 편집 작업(예: 횡단 세그멘테이션, 서체 편집 등)에 쉽게 적용될 수 있습니다.

### [Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing](https://arxiv.org/abs/2406.08464)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08464.png)

Vote: 45

Authors: Zhangchen Xu, Radha Poovendran, Bill Yuchen Lin, Yuntian Deng, Yejin Choi, Luyao Niu, Fengqing Jiang

- **What's New**: 최근 대규모 언어 모델(LLM)인 GPT-4와 Llama-3가 다양한 작업에서 뛰어난 성능을 발휘하면서 AI 애플리케이션에서 중요한 역할을 하고 있습니다. 그러나 LLM이 이러한 성능을 발휘하기 위해서는 고품질의 명령어 데이터셋이 필요합니다. Magpie라는 새로운 방법이 제안되어 LLM을 사용하여 고품질의 명령어 데이터셋을 대규모로 생성할 수 있습니다.
- **Technical Details**: Magpie는 LLM을 통해 명령어 데이터를 자동으로 생성하는 방법으로, 인간의 개입 없이도 실행이 가능합니다. 이 방법은 두 가지 주요 단계로 구분됩니다: 1) 명령어 생성, 2) 응답 생성. 특히 Magpie는 기존의 프롬프트 엔지니어링이나 시드 질문(seed question) 없이도 LLM의 사전 정의된 템플릿을 사용하여 명령어를 생성합니다.
- **Performance Highlights**: Magpie는 Llama-3-8B-Instruct와 Llama-3-70B-Instruct 모델에 적용되어 각각 Magpie-Air와 Magpie-Pro 데이터셋을 생성했습니다. 이 데이터셋을 기반으로 모델을 파인 튜닝한 결과, Llama-3-8B-Instruct와 같은 공식 모델보다 AlpacaEval 벤치마크에서 우수한 성능을 보였습니다. 이는 Magpie가 생성한 명령어 데이터의 품질이 매우 높다는 것을 의미합니다. 또한, Magpie-Pro는 Alpaca, Evol Instruct, UltraChat와 같은 다른 데이터셋에 비해 더 광범위한 주제와 더 많은 다양성을 제공함을 보여줬습니다.

### [MotionClone: Training-Free Motion Cloning for Controllable Video Generation](https://arxiv.org/abs/2406.05338)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.05338.png)

Vote: 39

Authors: Pan Zhang, Yi Jin, Xiaoyi Dong, Jiazi Bu, Huaian Chen, Jiaqi Wang, Tong Wu, Yuhang Zang, Pengyang Ling

- **What's New**: 최근 주류 텍스트-비디오(T2V) 확산 모델(text-to-video diffusion model)의 부상과 함께 사람의 의도에 맞추어 고품질 비디오를 생성하는 연구가 주목받고 있습니다. 특히 MotionClone이라는 새로운 프레임워크를 도입하여 참조 비디오(reference video)에서 모션을 클로닝(cloning)하고, 컨트롤 가능한 텍스트-비디오 생성을 실현합니다.
- **Technical Details**: MotionClone은 기존의 밀도 광학 흐름(dense optical flow)이나 궤적(trajectory)을 활용한 방법과 달리, 비디오 생성 모델 내에서 시간 주의 메커니즘(temporal-attention mechanism)을 사용하여 모션을 캡처합니다. 또한 주요 시간 주의 가이드(primary temporal-attention guidance)와 위치 인식 의미론적 가이드(location-aware semantic guidance)를 포함하여 모션 복제 품질을 높입니다. 비디오 확산 모델(video diffusion model)은 사전 학습된 인코더를 사용하여 입력 비디오를 잠재 표현(latent representation)으로 인코딩합니다.
- **Performance Highlights**: MotionClone은 전 세계 카메라 모션 및 로컬 개체 행동에서 모션 충실성(motion fidelity), 텍스트 정렬(text alignment), 시간 일관성(temporal consistency) 측면에서 뛰어난 성능을 보입니다. 이는 고품질 모션과 합리적인 공간적 관계를 유지하면서 자연스러운 비디오 생성을 가능하게 합니다.

### [What If We Recaption Billions of Web Images with LLaMA-3?](https://arxiv.org/abs/2406.08478)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08478.png)

Vote: 38

Authors: Xianhang Li, Mude Hui, Zeyu Wang, Jieru Mei, Cihang Xie, Sucheng Ren, Huangjie Zheng, Yuyin Zhou, Haoqin Tu, Qing Liu, Bingchen Zhao, Junfei Xiao

- **What's New**: 본 논문은 거대 모델인 LLaMA-3를 사용하여 데이터셋을 재캡션 (recaption)함으로써 웹에서 수집된 이미지-텍스트 데이터의 품질을 개선하는 방법을 제안합니다. 이를 통해 Recap-DataComp-1B라는 새로운 고품질 데이터셋을 생성하였습니다. 이 데이터셋은 훈련된 CLIP 모델의 성능을 크게 향상시킬 뿐 아니라 텍스트-이미지 생성 모델의 정확한 이미지 생성 능력을 높이는 데 도움을 줍니다.
- **Technical Details**: LLaMA-3-8B를 언어 디코더로 사용한 LLaVA-1.5라는 프레임워크를 개발하여, 이 모델을 활용해 전체 DataComp-1B 데이터셋을 재캡션하였습니다. 558k 이미지-텍스트 쌍과 665k 명령-따르기 데이터를 활용해 2단계에 걸쳐 모델을 훈련하였습니다. 첫 번째 단계에서는 Vision Encoder 위에 프로젝션 MLP만 훈련하였고, 두 번째 단계에서는 프로젝션 MLP와 언어 디코더를 모두 미세 조정하였습니다. 또한 HQ-Edit 데이터셋을 사용하여 추가적으로 모델의 캡션 품질을 높였습니다.
- **Performance Highlights**: 우리의 LLaVA-1.5-LLaMA3-8B 모델은 MMMU와 MM-Vet 벤치마크에서, LLaVA-1.5-7B 모델과 비교하여 훨씬 높은 성능을 보였습니다. 또한, 상당히 큰 크기의 LLaVA-1.5-13B 모델보다도 뛰어난 시각적 이해와 추론 능력을 입증하였습니다. Recap-DataComp-1B 데이터셋은 제로샷 (zero-shot) 크로스 모달 (cross-modal) 검색 성능을 크게 향상시켰고, 텍스트-이미지 생성 모델의 텍스트 지시와 생성된 이미지 간의 정렬을 개선하였습니다.

### [Are We Done with MMLU?](https://arxiv.org/abs/2406.04127)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04127.png)

Vote: 36

Authors: Xiaotang Du, Yu Zhao, Robert McHardy, Jean Kaddour, Giwon Hong, Rohit Saxena, Xuanli He, Pasquale Minervini, Mohammad Reza Ghasemi Madani, Alberto Carlo Maria Mancino, Joshua Harris, Alessio Devoto, Emile van Krieken, Joshua Ong Jun Leang, Aryo Pradipta Gema, Claire Barale

- **What's New**: 최근 Transformer 기반의 대형 언어 모델(LLM)의 등장으로 컴퓨팅 장치와 자연어를 통해 상호작용하는 것이 가능해졌습니다. 기존 벤치마크와 리더보드는 이로 인해 대부분 시대에 뒤떨어지게 되었으며, 더 도전적이고 포괄적인 테스트가 필요해졌습니다. 이에 따라 MMLU(Massive Multitask Language Understanding)가 주목받고 있지만, 데이터셋의 품질 문제로 벤치마크 결과의 신뢰성이 저해될 수 있습니다. 따라서 본 연구에서는 MMLU 데이터셋의 오류를 분석하고 개선된 MMLU-Redux를 제안합니다.
- **Technical Details**: MMLU-Redux는 14명의 전문가가 30개의 MMLU 하위 집합에서 무작위로 선택된 3,000개의 질문을 다시 주석(annotation)한 결과물입니다. 오류 분류 체계(hierarchical taxonomy)를 개발하여 MMLU의 다양한 오류를 체계적으로 분류하고, 이러한 오류가 LLM 평가에 미치는 영향을 분석합니다. 주요 오류 유형은 질문 명료성 부족, 옵션 명확성 부족, 정답 오류 등으로 식별되었습니다.
- **Performance Highlights**: MMLU-Redux를 사용하여 주요 LLM을 재평가한 결과, 성능 지표가 크게 변경되었음을 확인할 수 있었습니다. 이는 모델의 순위 변동을 초래했습니다. 또한 자동 오류 감지를 위한 새로운 벤치마크로 활용될 수 있으며, 향후 LLM을 통해 데이터셋의 오류를 감지하는 방법을 연구합니다. MMLU-Redux는 향후 NLP 모델 평가를 고도화하는 데 중요한 역할을 할 것입니다.

### [PowerInfer-2: Fast Large Language Model Inference on a Smartphone](https://arxiv.org/abs/2406.06282)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06282.png)

Vote: 34

Authors: Zhenliang Xue, Haibo Chen, Zeyu Mi, Yixin Song, Yubin Xia, Le Chen

- **What's New**: PowerInfer-2는 스마트폰에서 최대 470억 개의 파라미터를 갖는 LLM의 고속 추론을 수행하는 최초의 프레임워크입니다. 이는 기존의 LLM 추론 방법에 비해 평균 3.94배에서 25.4배 향상된 속도를 제공합니다.
- **Technical Details**: PowerInfer-2는 뉴런 클러스터 계산을 통한 코어스 그레인드 매트릭스 계산을 세분화하고, 이질적인 XPU 아키텍처(빅.LITTLE CPU 코어, GPU, NPU)를 활용하여 성능을 최적화합니다. 또한, 네트워크의 가중치를 메모리에 캐시하여 I/O 오버헤드를 최소화하고, 뉴런 클러스터 단위의 파이프라이닝 기술을 도입하여 I/O 지연을 줄입니다.
- **Performance Highlights**: PowerInfer-2는 다양한 모델 크기의 LLM들을 지원하며, Llama-2 (7B, 13B), TurboSparse-Mistral (7B), TurboSparse-Mixtral (47B) 등의 모델에서 최대 29.2배 빠른 속도를 자랑합니다. TurboSparse-Mixtral-47B 모델에서는 Llama.cpp보다 21.2배 빠른 생성 속도(11.68 tokens/s)를 달성했습니다.

### [Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion](https://arxiv.org/abs/2406.04338)

![](/avatars/bbf781594fc8c812316711aa8e2797aa.svg)

Vote: 32

Authors: Fangfu Liu, Hanyang Wang, Shunyu Yao, Jie Zhou, Yueqi Duan, Shengjun Zhang

- **What's New**: 최근 몇 년간 3D 컴퓨터 비전 분야에서는 3D 자산의 복원과 생성에 중점을 둔 연구가 많이 발전했습니다. 이에 따라 비디오 생성 모델에서 학습된 객체 동역학을 이용해 물리적 매개변수를 추정하는 PhysDreamer와 같은 접근 방식이 등장하였습니다. 그러나, 복잡한 혼합 재료를 다루기에는 제한적이었습니다. 이 논문에서는 물리적 특성을 학습하여 다양한 재료의 3D 동역학을 물리적으로 타당하게 시뮬레이션할 수 있는 새로운 시스템 'Physics3D'를 제안합니다.
- **Technical Details**: Physics3D는 3D Gaussian 표현을 기반으로 물리적 매개변수의 차원을 확장하여 탄성(elasticity) 및 점탄성(viscosity)을 포괄합니다. 이를 위해 viscoelastic Material Point Method(MPM)를 설계하여 3D 동역학을 시뮬레이션합니다. 변형 그래디언트를 두 개의 별도 구성 요소로 분해하고 각 요소를 독립적으로 계산합니다. 그리고 Score Distillation Sampling(SDS) 전략을 사용하여 비디오 확산 모델에서 물리적 사전 지식을 추출합니다. MPM 프로세스와 SDS 최적화를 반복하여 Physics3D는 고품질의 사실적인 성능을 달성할 수 있습니다.
- **Performance Highlights**: Physics3D는 다양한 재료 시뮬레이션에서 높은 충실도와 사실적인 3D 동역학을 생성하는 데 효과적입니다. 여러 실험을 통해 기존 방법보다 우수한 성능을 입증하였습니다. 또한 현실 세계의 상호작용에 대한 물리 시뮬레이션을 향상시켜 가상/증강 현실 및 애니메이션과 같은 응용 분야에 적합합니다.

### [VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs](https://arxiv.org/abs/2406.07476)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07476.png)

Vote: 29

Authors: Hang Zhang, Deli Zhao, Zesen Cheng, Sicong Leng, Xin Li, Yongxin Zhu, Guanzheng Chen, Lidong Bing, Ziyang Luo, Yifei Xin, Wenqi Zhang

- **What's New**: 최근 몇 년간 인공지능(AI) 분야는 매우 큰 발전을 이뤄냈으며, 산업과 사회 기능 전반에 걸쳐 많은 변화를 가져왔습니다. 특히, 이미지 인식과 사실적인 이미지 생성 모델은 사람과 유사한 수준에 도달하며 중요한 진보를 이뤄냈습니다. 그러나 비디오 이해와 생성 분야는 상대적으로 초기 단계에 머물러 있으며, 이에 대한 기술적인 도전 과제가 많습니다. 이번 보고서에서는 이러한 문제를 해결하기 위해 VideoLLaMA 2를 도입합니다.
- **Technical Details**: VideoLLaMA 2는 비디오와 오디오 신호의 복잡한 상호작용을 통합하고 해석함으로써 비디오-언어 이해를 강화하는 것을 목표로 합니다. 이 모델은 직전 버전과 마찬가지로 Dual-Branch Framework를 유지하며, 영상-언어 브랜치와 오디오-언어 브랜치로 구성됩니다. 영상 모델로는 CLIP (ViT-L/14)을 사용하고, 공간-시간 컨볼루션 커넥터(STC Connector)를 통해 시공간 표현 학습의 효율성을 높였습니다. 또한, 오디오 입력은 fbank 스펙트로그램으로 변환 후, 첨단 오디오 인코더인 BEATs를 통해 처리됩니다. 마지막으로, Mistral-Instruct와 Mixtral-Instruct를 언어 디코더로 사용하여 모델의 일관성을 유지합니다.
- **Performance Highlights**: VideoLLaMA 2는 다양한 비디오-언어 과제에서 우수한 성능을 보였습니다. 특히, 시공간 동적인 데이터를 효과적으로 처리하는 능력과, 오디오-비주얼 통합을 통해 종합적인 이해력을 증대시킬 수 있는 기능이 돋보입니다. 이러한 기술적 향상은 비디오-언어 분석의 새로운 표준을 설정하며, 비디오 콘텐츠의 풍부한 내러티브를 이해하고 표현하는 데 중대한 기여를 합니다.

### [3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination](https://arxiv.org/abs/2406.05132)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/LsDQPftQrnp_Gvanzim4J.jpeg)

Vote: 27

Authors: Xuweiyi Chen, Jianing Yang, David F. Fouhey, Joyce Chai, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian

- **What's New**: 최첨단 연구는 대규모 3D 장면과 언어 명령을 연결하여 육체화된 인공지능(Embodied AI)의 성능을 개선하려는 목적으로 '3D-GRAND'라는 데이터셋을 소개합니다. 이 데이터셋은 40,087개의 가정 장면과 620만 개가 넘는 장면-언어 명령을 포함하고 있습니다.
- **Technical Details**: 다양한 3D 텍스트 작업에서 사용될 수 있도록 설계된 3D-GRAND에는 조밀하게 연결된 구문-객체 데이터가 포함되어 있습니다. 이는 LLMs가 더욱 사실적인 방식으로 3D 환경을 이해하고 상호작용하게 도와줄 것입니다. 또한, 객체 환각(Object Hallucination)을 평가하기 위한 3D-POPE(Probing Object Hallucination Evaluation)도 함께 도입하여 모델의 신뢰성을 높입니다.
- **Performance Highlights**: 3D-GRAND 데이터셋을 사용한 학습 결과, 환각 현상이 현저히 줄어들고 모델의 객체 접지 능력(Grounding Capabilities)이 향상되었습니다. 특히, 조밀하게 접지된 데이터는 특별히 효과적이며, 데이터 규모가 커질수록 정확성이 높아집니다. 실험 결과는 시뮬레이션 환경에서 실제 환경으로 잘 전이되었음을 보여줍니다.

### [MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos](https://arxiv.org/abs/2406.08407)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08407.png)

Vote: 23

Authors: Zhengyuan Yang, Yujie Lu, Kaizhi Zheng, Wanrong Zhu, Jianfeng Wang, Jiachen Li, Lijuan Wang, Xin Eric Wang, Weixi Feng, William Yang Wang, Linjie Li, Kevin Lin, Yue Fan, Xuehai He

- **What's New**: MMWorld는 미디어 다중 모드 거대 언어 모델(Multimodal Large Language Models, MLLMs)의 '세계 모델링' 능력을 비디오 이해를 통해 평가하기 위한 새로운 기준을 제시합니다. 이 벤치마크는 7개의 주요 학문 분야와 69개의 세부 분야를 아우르는 동적이고 다면적인 질문 형식을 포함하여 다양한 영역에서 모델의 추론 능력을 평가합니다.
- **Technical Details**: MMWorld는 1,910개의 비디오와 1,559개의 질문-답변 페어, 인간에 의해 주석된 비디오 캡션으로 구성됩니다. 또한, 자동 데이터 수집 파이프라인을 만들어 비디오 콘텐츠 선택 및 질문-답 생성 과정을 자동화하였습니다. MMWorld는 시각적 또는 청각적 모달리티 내에서 MLLM의 인식을 분석하기 위한 합성 데이터셋도 포함합니다.
- **Performance Highlights**: 현재 MLLMs는 MMWorld가 제시하는 도전에 대해 상당한 어려움을 겪고 있습니다. 최고의 수행자인 GPT-4V는 전체 정확도에서 오직 52.30%만 달성했으며, 비디오에 특화된 네 개의 MLLMs는 무작위 추정보다 성능이 낮았습니다. 흥미롭게도, 최고 오픈소스 모델인 Video-LLaVA-7B는 Embodied Tasks에서 GPT-4V와 Gemini를 크게 능가했으며, Art & Sports 분야에서도 비슷한 성능을 보였습니다. 이는 시공간적 이해가 중요한 비디오 이해에서 더욱 두드러집니다.

### [Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters](https://arxiv.org/abs/2406.05955)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.05955.png)

Vote: 21

Authors: Li Ma, Bo Wen, Zhengyan Zhang, Haotong Xie, Yixin Song, Haibo Chen, Zeyu Mi

- **What's New**: 최신 연구에서는 대형 언어 모델(LLM)의 비효율성을 해결하기 위해 dReLU라는 새로운 활성화 함수와 다양한 프리트레이닝 데이터를 도입했습니다. 특히, TurboSparse-Mistral-47B와 TurboSparse-Mixtral-47B 모델을 통해 모델의 성능을 유지하면서도 활성화 희소성(sparsity)을 극대화했습니다.
- **Technical Details**: 기존의 LLM 모델들이 모든 매개변수를 사용해 추론을 수행하는 '밀집 모델(dense models)'인 반면, 연구팀은 조건부 계산(conditional computation) 방식을 도입했습니다. 대표적인 방법으로 MoE(Mixture of Experts)와 자연 발생적인 희소 활성화(sparse activation)를 활용하여 효율성을 높였습니다. 특히, ReLU와 같은 활성화 함수를 사용하여 활성화 희소성을 증가시켰습니다.
- **Performance Highlights**: TurboSparse-Mistral-7B 모델의 평균 희소성을 90%로 높이면서도 성능을 유지하였고, MoE 기반 LLM에서도 TurboSparse-Mixtral-47B 모델의 희소성을 75%에서 97%로 증가시켰습니다. 이러한 향상은 추론 시 FLOPs를 크게 줄였습니다. 또한, PowerInfer와 통합하여 평균 2.83배의 생성 속도 향상을 달성했습니다. 이 모델들은 GPU 없이도 최대 10 tokens/s의 속도를 구현할 수 있었습니다.

### [FontStudio: Shape-Adaptive Diffusion Model for Coherent and Consistent Font Effect Generation](https://arxiv.org/abs/2406.08392)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08392.png)

Vote: 17

Authors: Li Chen, Xinzhi Mu, Bohan Chen, Jianmin Bao, Yuhui Yuan, Ji Li, Shuyang Gu, Dong Chen

- **What's New**: 기존의 텍스트-이미지 생성(diffusion techniques) 모델이 직사각형 캔버스에서 사진 같은 이미지를 생성하는 데 성공한 가운데, 비직사각형 캔버스에서 이미지 생성의 가능성은 거의 탐구되지 않았습니다. 이를 해결하기 위해, 연구팀은 임의의 형태를 가진 캔버스에서도 고품질의 시각적 콘텐츠를 생성할 수 있는 혁신적이고 강력한 shape-adaptive diffusion model을 제안했습니다. 이 모델은 다국어 폰트 외곽선 및 프랙탈 구조를 가진 눈꽃 등의 복잡한 패턴에서도 뛰어난 성능을 발휘합니다.
- **Technical Details**: 연구팀은 {irregular-canvas, irregular-image, text prompt}로 구성된 고품질의 shape-adaptive triplet 훈련 데이터를 구축하고, 이를 이용해 conditional diffusion model을 훈련했습니다. 효율적인 훈련을 위해 사각형 캔버스를 자리표시자로 사용하여 비정형 캔버스와 대응되는 비정형 이미지를 모두 수용했습니다. 이 외에도 레퍼런스 글자의 효과를 목표 글자의 형태 마스크와 결합시키는 새로운 트레이닝이 불필요한 효과 전송 방법을 도입했습니다. 이 방법은 레퍼런스 스타일과 질감을 소스에서 타겟 이미지로 전파하여 폰트 효과 일관성을 유지합니다.
- **Performance Highlights**: 연구팀은 폰트 효과 생성의 종합적인 평가를 위해 GenerativeFont benchmark을 수립했습니다. 사용자 연구 결과, Adobe Firefly와 비교하여 FontStudio 시스템은 다양한 항목에서 크게 우수한 성과를 보였습니다. 구체적으로, aesthetics에서 78.68% vs. 10.89%, shape fidelity에서 66.87% vs. 6.04%로 높은 성과를 기록했습니다.

### [Hierarchical Patch Diffusion Models for High-Resolution Video Generation](https://arxiv.org/abs/2406.07792)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07792.png)

Vote: 13

Authors: Aliaksandr Siarohin, Sergey Tulyakov, Ivan Skorokhodov, Willi Menapace

- **What's New**: 최근 연구에서, diffusion models(DMs)는 이미지 및 비디오 생성 분야에서 뛰어난 성과를 보였으나, 고해상도 입력으로의 확장 과정에서 끝에서 끝으로 연결되는 구조가 깨지는 문제를 겪었습니다. 해당 연구에서는 이러한 문제를 해결하기 위해 새로운 기법인 패치 확산 모델(patch diffusion models)을 제안하였습니다.
- **Technical Details**: 기존의 확산 모델들은 고해상도에서 적용하기에 막대한 계산 비용이 소요되었고, 이를 해결하기 위해 패치 단위로 모델을 학습시키는 방법을 제안하였습니다. 패치 단위 학습은 고해상도 비디오 생성에 필요한 계산 비용이 훨씬 더 많이 줄어들며, 단 0.7%의 원본 픽셀만을 사용하여도 충분한 성능을 입증할 수 있습니다. 제안된 기법은 Deep Context Fusion과 Adaptive Computation을 사용하여 하위 해상도의 특성들을 상위 해상도의 생성에 활용하고, 하이 레졸루션 패치의 생성 시 일부 층만 운영되도록 구조를 재편성하였습니다.
- **Performance Highlights**: 제안된 방법은 UCF-101 데이터셋에서 동영상 생성의 state-of-the-art 성능을 달성하였으며, 대규모 텍스트-비디오 생성 실험에서도 강한 확장성을 입증하였습니다.

### [AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation](https://arxiv.org/abs/2406.07686)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07686.png)

Vote: 13

Authors: Shijian Deng, Jing Shi, Dimitrios Hatzinakos, Kai Wang, Yapeng Tian

- **What's New**: 최근 몇 년 동안, 확산 모델(Diffusion Models)은 이미지, 비디오 및 오디오 생성에서 강력한 성능을 발휘하면서 AI 기반 콘텐츠 생성의 주요 기술로 부상했습니다. 그러나 대부분의 기존 연구는 단일 모달리티 콘텐츠 생성에 초점을 맞추어 실제 세계의 다중 모달리티를 무시했습니다. AV-DiT(Audio-Visual Diffusion Transformer)은 이러한 격차를 해소하기 위해 고품질의 영상을 시청하고 들을 수 있는 오디오와 비디오를 동시에 생성합니다.
- **Technical Details**: AV-DiT는 ImageNet에서 사전 학습된 공유 DiT(Diffusion Transformer) 백본과 최소한의 학습이 필요한 경량 레이어(이하 LoRA 및 어댑터)를 활용하여 이미지 생성을 오디오 및 비디오 생성으로 확장합니다. 이는 MM-Diffusion과 달리 추가적인 초해상도 모듈을 도입하거나 전체 파라미터 업데이트를 포함하지 않습니다. 대신, 사전 학습된 DiT 백본은 고정된 상태로 유지되며, 새로 삽입된 레이어만 학습하여 파라미터 및 메모리 소모를 크게 줄입니다. AV-DiT는 이미지에서 오디오-비디오 모달리티로의 확장을 세 가지 과제를 통해 효과적으로 해결합니다: 1. 비디오 생성의 시간적 일관성 도입, 2. 오디오 생성을 위한 이미지와 오디오 간의 도메인 차이 완화, 3. 오디오-비디오 정렬을 위한 멀티모달 상호작용.
- **Performance Highlights**: AIST++ 및 Landscape 데이터셋을 대상으로 한 광범위한 실험에서 AV-DiT는 최신 방법들과 비교하여 경쟁력 있는 성능 또는 더 나은 성능을 보였습니다. 특히, 고품질의 현실적인 비디오 및 오디오를 생성하면서도 학습해야 할 파라미터 수는 적습니다.

### [Discovering Preference Optimization Algorithms with and for Large Language Models](https://arxiv.org/abs/2406.08414)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08414.png)

Vote: 12

Authors: Chris Lu, Samuel Holt, Alex J. Chan, Mihaela van der Schaar, Robert Tjarko Lange, Jakob Foerster, Claudio Fanconi

- **What's New**: 본 연구는 다양한 목적 함수가 LLM(Long Language Models)의 성능 최적화에 미치는 영향을 조사합니다. 이를 통해 발견된 목적 함수가 다른 작업에서도 일반화되는지 또한 평가합니다.
- **Technical Details**: 연구는 'zephyr-7b-gemma-sft' 사전 학습된 모델부터 시작합니다. 이 모델은 'deita-10k-v0-sft' 데이터셋을 슈퍼바이즈드 파인 튜닝(Supervised Fine-Tuning)하여 구축되었습니다. 이후 우리는 'Argilla DPO Mix 7K' 데이터셋을 사용해 모델을 최적화했습니다. 모든 실험은 8개의 NVIDIA A100 GPU에서 수행되었으며, 트레이닝 세션은 약 30분 소요되었습니다. 모델 최적화에 AdamW 최적화 알고리즘을 사용했고, 주된 하이퍼파라미터로는 5e-7의 학습률, bfloat16 포맷, 2 epochs, 디바이스당 2의 배치 사이즈 등을 사용했습니다.
- **Performance Highlights**: 발견된 목적 함수의 일반화 성능을 평가하기 위해 다양한 벤치마크를 사용했습니다. 우선 MT-Bench를 이용해 평가한 결과를 보고하고, Reddit TL;DR 요약 작업으로 확장해 평가했습니다. 특히 IMDb 데이터를 사용한 긍정적 텍스트 생성 작업에서는 GPT-2 모델을 사용했으며, 평가를 위해 Alpaca Eval 2.0과 같은 평가 메트릭스를 활용했습니다. 전반적으로, 발견된 목적 함수는 다양한 작업에서도 우수한 일반화 성능을 보였습니다.

### [VCR: Visual Caption Restoration](https://arxiv.org/abs/2406.06462)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.06462.png)

Vote: 10

Authors: Jie Fu, Tianyu Zhang, Sai Rajeswar, Perouz Taslakian, Bang Liu, Ge Zhang, Yoshua Bengio, Lu Li, Suyuchen Wang

- **What's New**: 최근의 대형 언어 모델(ChatGPT, Llama) 발전은 비전-언어 모델 분야의 중요한 관심과 진전을 이끌었습니다. 새로운 작업으로 'Visual Caption Restoration (VCR)'을 소개합니다. VCR은 이미지 내에서 가려진 텍스트를 복원하는 과제로, 텍스트, 비전, 이미지 내 텍스트의 복잡한 합성을 요구합니다. 이는 기존 모델들이 다루기 어려웠던 영역에서의 이해 및 복원을 목표로 합니다.
- **Technical Details**: VCR 작업은 두 가지 주요 인사이트에 기반합니다: 첫째, 이미지 내 텍스트는 일반적인 시각적 요소와 다른 특성을 가지며, 비전과 텍스트 데이터를 세밀하게 정렬해야 합니다. 둘째, 뇌 과학 연구는 인간이 부분적으로 가려진 객체를 인식하는 데 뛰어난 능력을 가지고 있음을 시사합니다. VQA (Visual Question Answering)와 달리 VCR은 텍스트 복원을 위해 이미지 내 비주얼과 텍스트의 정교한 통합을 필요로 합니다.
- **Performance Highlights**: VCR 과제에서 기존 비전-언어 모델(VLM)은 인간 벤치마크와 비교하여 성능이 현저히 떨어지는 것으로 나타났습니다. 이는 복잡한 텍스트 및 비전의 상호 정렬을 처리하기 위해 새로운 모델 아키텍처와 훈련 패러다임의 필요성을 강조합니다. 실험 결과, 인간이 쉽게 수행할 수 있는 작업에서도 기존 모델들은 많은 경우 실패하였습니다. 이 결과는 VCR이 비전-언어 모델의 발전을 평가하는 데 효과적이라는 것을 입증합니다.

### [Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models](https://arxiv.org/abs/2406.08487)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08487.png)

Vote: 10

Authors: Qingsong Wen, Liang Wang, Rong Jin, Xue Wang, Zhang Zhang, Yi-Fan Zhang, Chaoyou Fu

- **What's New**: 최근 몇 년간 눈부신 성장을 보인 대형 멀티모달 모델(LMMs – Large Multimodal Models)이 여전히 복잡한 시각 인식과 추론 작업에 어려움을 겪고 있다. 이 논문은 이러한 문제를 해결하기 위해 더 고해상도의 이미지를 활용하고, 글로벌 컨텍스트를 우선시하는 새로운 접근법을 제안했다.
- **Technical Details**: 본 연구는 고해상도 이미지를 다수의 패치로 분할하고, 각각의 패치를 독립적으로 인코딩한 후, 글로벌 이미지 토큰과 로컬 이미지 패치를 융합하는 방법을 사용한다. 특히, 글로벌 정보를 최대한 유지하고, 로컬 이미지 패치는 쿼리 변환기(query transformer)를 이용해 컴프레션한다. 학습 과정에서는 글로벌 투영층(projection)을 따로 훈련하여 최적의 성능을 보장한다. 또한 과학 및 수학적 추론을 요구하는 SMR dataset을 도입하여 모델의 역량을 최대한으로 끌어올렸다.
- **Performance Highlights**: 제안한 방법론은 다양한 설정에서 탁월한 성능을 보여주었으며, 8B LLM과 200만 개의 데이터만으로도 기존의 유명 모델들과 견줄 만한 성과를 달성했다. 이는 SliME(복잡한 작업, 로컬 이미지 증강, 그리고 글로벌 전문가의 혼합)를 통해 새로운 벤치마크를 설정할 가능성을 입증한다.

### [Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models](https://arxiv.org/abs/2406.04320)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04320.png)

Vote: 7

Authors: Michele Santacatterina, Ramin Zabih, Ali Behrouz

- **What's New**: 본 연구에서는 다변량 시간 시계열(multi-variate time series)을 효과적으로 모델링하기 위한 Chimera라는 새로운 2차원 State Space Model (SSM)을 소개합니다. Chimera는 시간, 변수, 시간→변수, 변수→시간 방향으로 이루어진 선형 레이어를 기반으로 하여 고차원, 장기적인 패턴을 포착할 수 있도록 설계되었습니다.
- **Technical Details**: Chimera는 커플 매트릭스와 대각 매트릭스를 사용한 세심한 매개변수화로 클래식한 방법, 선형 어텐션, 최근 SSM 기반 모델들을 복구할 수 있는 표현력을 갖추고 있습니다. 또한, 2D SSM의 특별히 설계된 이산화 과정을 사용하여 계절적 패턴을 적응적으로 학습할 수 있습니다. Chimera는 심박, 오디오 음성 데이터 분류, 단기 및 장기 시계열 예측, 이상 감지 작업에서 우수한 성능을 보이며 메모리 소비도 적습니다.
- **Performance Highlights**: 실험 결과, Chimera는 현 상태의 예술적 방법들과 비교하여 우수하거나 동등한 성능을 보였고, 훈련 속도도 더 빠르고 메모리 소비가 적었습니다. 특히 인간 뇌 활동 신호를 활용한 사례 연구를 통해 Chimera의 효과성을 입증하고, 변수 간의 의존성을 모델링하는 것의 중요성을 평가하였습니다.

### [Large Language Model Unlearning via Embedding-Corrupted Prompts](https://arxiv.org/abs/2406.07933)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.07933.png)

Vote: 6

Authors: Yang Liu, Chris Yuhao Liu, Jeffrey Flanigan, Yaxuan Wang

- **What's New**: 새로운 연구는 Embedding-COrrupted (ECO) Prompts라는 가벼운 프레임워크를 소개합니다. 이 프레임워크는 변형된 입력을 통해 원래 모델의 기능을 유지하면서도 특정 데이터를 잊게 만드는 효율적인 unlearning 방법을 제안합니다.
- **Technical Details**: ECO Prompts 프레임워크는 두 단계로 구성됩니다. 첫 번째 단계는 prompt 분류기를 사용해 unlearning 대상이 포함된 입력을 식별합니다. 두 번째 단계에서는 그 입력을 변형시켜 LLM에 전송해, 학습되지 않은 상태처럼 보이게 만듭니다. 변형 과정은 zeroth order optimization을 통해 효율적으로 학습됩니다.
- **Performance Highlights**: ECO Prompts는 multiple tasks와 metrics에서 기존 데이터를 잊으면서, 반드시 기억해야 하는 다른 정보는 유지하는 매우 우수한 성능을 보였습니다. 본 연구는 LLM 규모에 상관없이, 236억개의 파라미터까지 효과적으로 unlearning할 수 있음을 입증한 첫 번째 연구입니다.

### [Hibou: A Family of Foundational Vision Transformers for Pathology](https://arxiv.org/abs/2406.05074)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.05074.png)

Vote: 6

Authors: Ekaterina Ivanova, Dmitry Nechaev, Alexey Pchelnikov

- **What's New**: 최신 연구는 디지털 병리학 (digital pathology) 분야에서 Vision Transform (ViT)을 통한 이미지 분석의 잠재력을 강조합니다. 이 논문에서는 자가 지도 학습 (self-supervised learning)을 이용하여 대규모 데이터셋으로 사전 훈련된 ViT 모형인 Hibou를 소개하며, Hibou-B와 Hibou-L 변형 모델을 제안합니다. 이 모델들은 다양한 조직 종류와 염색 기술을 대표하는 100만 개 이상의 전체 슬라이드 이미지 (Whole Slide Images, WSIs)를 활용하여 사전 훈련되었습니다.
- **Technical Details**: 연구진은 가장 다양한 대규모 데이터셋을 사용하여 Hibou 모델을 훈련했습니다. 데이터셋은 936,441개의 헤마톡실린과 에오신 (H&E) 염색 슬라이드와 202,464개의 비-H&E 염색 슬라이드로 구성되어 있습니다. 데이터 준비 과정은 배경 패치를 제거하기 위해 Otsu 임계값을 사용하여 비중복 패치로 나뉘며, DINOv2 프레임워크로 사전 훈련됩니다. Hibou-B는 8개의 A100-80G GPU에서, Hibou-L는 32개의 A100-40G GPU에서 훈련되었습니다.
- **Performance Highlights**: Hibou-L 모델은 CRC-100K, MHIST, PCam, MSI-CRC, MSI-STAD, TIL-DET 데이터셋에서 높은 평균 정확도를 기록하며 새로운 최고 성능을 달성했습니다. 이는 Hibou-L의 강력한 일반화 능력을 입증하며, 다양한 조직 샘플과 변수를 다룰 수 있는 임상 환경에서의 실용성을 강조합니다. 평가를 위해 TCGA 데이터셋을 사용하여 패치 수준 및 슬라이드 수준의 분류 작업을 수행하였으며, 특성 추출 과정에서 사전 훈련된 모델을 활용합니다.

### [Simplified and Generalized Masked Diffusion for Discrete Data](https://arxiv.org/abs/2406.04329)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04329.png)

Vote: 4

Authors: Jiaxin Shi, Kehang Han, Zhe Wang, Michalis K. Titsias, Arnaud Doucet

- **What's New**: 이 논문에서는 기존의 복잡한 훈련 목표와 공식으로 인해 제한적인 성공을 거둔 'masked' (또는 'absorbing') 디퓨전 모델을 단순화하고 성능을 크게 향상시킬 수 있는 기술적 기여를 다룹니다. 기존 연속 상태 공간과 달리, 이 논문에서는 오스틴(Austin) 등 [14]이 처음 제안한 이산 디퓨전 모델을 바탕으로 하며, 연속 시간 프레임워크를 채택하여 모델의 훈련과 이해를 개선하고자 합니다.
- **Technical Details**: 데이터가 크기가 m(m)인 유한한 이산 상태 공간에 존재한다고 가정합니다. 이 모델은 추가적인 마스크 상태를 도입하여, 데이터 포인트가 무작위 시간에 마스크 상태로 전환되는 '마스킹' 프로세스를 정의합니다. 이 과정은 마르코프 연속적인 이산 랜덤 변수 xt(x_t)로 정의되며, 시퀀스를 역으로 돌려 데이터 생성을 가능하게 합니다. 논문은 또한 이 모델의 정방향 과정과 시간 역전 과정을 개선시키는 여러 특성을 수학적 논리를 통하여 확립합니다.
- **Performance Highlights**: 논문의 단순한 ELBO(Evidence Lower Bound) 목표를 사용하여 훈련된 마스크 디퓨전 모델은 GPT-2 규모의 텍스트 모델링과 픽셀 수준의 이미지 모델링 작업에서 이전 제안들을 능가합니다. 또한 테스트 가능성(likelihood)에서 예측 성능을 더욱 향상시킬 수 있는 상태 의존적 마스킹 스케줄을 허용하는 일반화된 마스크 디퓨전 모델을 제안합니다.

