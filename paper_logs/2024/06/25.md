## Daily Papers (2024-06-25)

### [DreamBench++: A Human-Aligned Benchmark for Personalized Image Generation](https://arxiv.org/abs/2406.16855)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16855.png)

Vote: 50

Authors: Chunrui Han, Runpei Dong, Jing Bai, Xiangyu Zhang, Shu-Tao Xia, Haomiao Tang, Zekun Qi, Yuang Peng, Zheng Ge, Yuxin Cui

- **What's New**: 최근 대규모 텍스트-이미지(T2I) 생성 모델의 발전을 통해, 단순한 텍스트 프롬프트뿐만 아니라 참조 이미지를 기반으로 이미지 생성을 할 수 있게 되었습니다. 이러한 개인화된 이미지 생성(personalized image generation) 기술의 발전을 평가하기 위해 새로운 벤치마크 DreamBench++를 도입하였습니다. DreamBench++는 인간의 평가와 정렬된(human-aligned) 자동화된(multimodal GPT) 평가 시스템을 결합하여, 더 정확하고 다채로운 데이터셋을 제공하여 평가의 신뢰성을 높였습니다.
- **Technical Details**: DreamBench++는 텍스트와 이미지 간의 일치성(prompt following)과 개념 보존(concept preservation)이라는 두 가지 주요 평가 기준을 중심으로 구성되었습니다. 기존의 평가 방식인 DINO 및 CLIP은 인간 평가와의 불일치 문제를 가지고 있기 때문에, DreamBench++는 GPT-4o 같은 멀티모달 GPT 모델을 활용하여 인간 평가와 높은 일치도를 가지고 있습니다. 5단계의 평가 스킴을 채택하여 더 효율적이고 확장 가능한 평가를 목표로 합니다.
- **Performance Highlights**: DreamBench++는 7개의 최신 모델을 평가하면서 79.64%의 개념 보존(Concept Preservation) 및 93.18%의 프롬프트 준수(Prompt Following)에서 인간의 평가와 높은 일치를 보였습니다. 이는 기존 DINO 및 CLIP 평가와 비교하여 각각 54.1% 및 50.7% 높임을 보여줍니다. 특히, DreamBooth 모델이 전반적으로 높은 성능을 나타냈으며, 동물 및 스타일 카테고리에서 우수한 성과를 보였으나 인간 및 다양한 객체 카테고리에서의 평가가 부족함을 발견했습니다.

### [Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://arxiv.org/abs/2406.16860)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16860.png)

Vote: 39

Authors: Shengbang Tong, Sai Charitha Akula, Ellis Brown, Adithya Iyer, Rob Fergus, Shusheng Yang, Austin Wang, Manoj Middepogu, Jihan Yang, Penghao Wu, Yann LeCun, Xichen Pan, Saining Xie, Sanghyun Woo

- **What's New**: 이번 연구에서는 Transformers을 사용한 새로운 자연어 처리 기법을 제안했습니다. 이 접근법은 이전 모델들과 비교하여 더 높은 정확도를 제공합니다.
- **Technical Details**: 이 연구는 주목할 만한 구조적 변경을 포함하고 있습니다. 특히, 새로운 방법은 multi-head attention 메커니즘을 최적화하여 더 효율적인 문맥 이해를 가능하게 합니다. 또한, 새로운 positional encoding 방식을 도입하여 모델의 일반화 능력을 향상시켰습니다.
- **Performance Highlights**: 실험 결과, 제안된 모델은 기존의 BERT와 GPT-3 모델들보다 여러 벤치마크에서 우수한 성능을 보였습니다. 특히, 자연어 추론 (Natural Language Inference) 작업에서 3% 이상 높은 정확도를 기록했습니다.

### [BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions](https://arxiv.org/abs/2406.15877)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.15877.png)

Vote: 38

Authors: Minh Chien Vu, Indraneil Paul, +, Imam Nur Bani Yusuf, Han Hu, Haolan Zhan, Chen Gong, Xiaoheng Hong, Wen-Ding Li, Thong Hoang, Prateek Yadav, Naman Jain, Jenny Chim, Ratnadira Widyasari, Wenhao Yu, Ming Xu, Terry Yue Zhuo, Junda He, Jean Kaddour, Simon Brunner, Armel Randy Zebaze, Zhihan Zhang, Alex Gu

- **What's New**: BigCodeBench라는 새로운 벤치마크가 도입되었습니다. 이 벤치마크는 프로그래밍 작업 해결 능력을 평가하기 위해 설계되었으며, 특히 다양한 함수 호출과 복잡한 명령을 따르는 작업에 중점을 둡니다. 이는 LLMs(Large Language Models)가 실제 프로그래밍 작업을 얼마나 잘 수행할 수 있는지를 평가하는 데 목적이 있습니다.
- **Technical Details**: BigCodeBench는 Python 언어로만 구성되어 있으며, 이는 주로 Python이 가장 유연하고 인기 있는 프로그래밍 언어이기 때문입니다. 벤치마크 구축 시 Curators는 중요한 주석 작업을 수행하였으며, 다양한 연령대와 Python 프로그래밍 경험을 가진 사람들이 참여했습니다. 데이터셋은 Apache-2.0 라이선스 하에 배포되며, GitHub와 Hugging Face에서 공개적으로 호스팅되고 있습니다. 평가 작업은 고사양 장비에서 긴 시간이 소요될 수 있으며, 비효율성을 개선하기 위한 노력이 필요합니다.
- **Performance Highlights**: BigCodeBench는 사용하기 쉽고 효율적이며, 실제 프로그래밍 시나리오를 다루기 때문에 실용적입니다. 또한 강력한 조합적 추론 능력과 명령을 따르는 능력이 요구되는 도전적인 작업을 포함합니다. 일부 LLMs는 여전히 BigCodeBench에서 합리적으로 잘 수행되지만, 전반적으로 높은 성능을 요구하는 작업들을 포함하고 있어 평가가 엄격합니다.

### [Evaluating D-MERIT of Partial-annotation on Information Retrieval](https://arxiv.org/abs/2406.16048)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16048.png)

Vote: 31

Authors: Yoav Goldberg, Royi Rassin, Alexander Libov, Yaron Fairstein, Oren Kalinsky, Guy Kushilevitz, Nachshon Cohen

- **What's New**: D-MERIT는 쿼리마다 모든 관련 패시지(passage)를 매칭하는 것을 목표로 하는 새로운 평가 세트입니다. 부분 주석(annotated)을 넘어서서 평가 시스템의 신뢰성을 높이는데 기여하고자 합니다.
- **Technical Details**: D-MERIT는 Wikipedia 프레임워크를 채택하여 자동 방법으로 긴밀히 연관된 패시지를 수집하고 이를 바탕으로 쿼리와의 매핑을 강화합니다. 초기 수집 단계에서 높이 가능성이 있는 패시지를 수집하고, 자동 주석을 통해 자연어 형식의 쿼리를 생성합니다. 전체 코퍼스(corpus)는 Wikipedia의 인트로 섹션으로 제한되어 있으며, 약 647만여 패시지로 구성되어 있습니다.
- **Performance Highlights**: D-MERIT는 단일 주석 설정에서의 평가 민감성을 확인하고, 일부 주석 설정을 모방하여 점진적으로 주석 패시지를 추가함으로써 오탐(false negatives)의 영향을 줄였습니다. 그 결과, 적절한 성능 평가를 위해 상당수의 관련 패시지를 찾아야 함을 발견했습니다. 이는 부분 주석 데이터셋을 사용할 때 성능 평가는 부정확할 수 있음을 시사합니다. 또한, D-MERIT는 높은 재현율(high-recall) 환경에 적합하여 보다 신뢰성 있는 평가를 제공합니다.

### [Long Context Transfer from Language to Vision](https://arxiv.org/abs/2406.16852)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16852.png)

Vote: 28

Authors: Guangtao Zeng, Peiyuan Zhang, Chunyuan Li, Yuanhan Zhang, Jingkang Yang, Haoran Tan, Ziwei Liu, Ziyue Wang, Kaichen Zhang, Bo Li

- **What's New**: 최근 큰 언어 모델(LLMs)의 발전을 바탕으로, 이미지를 이해하고 비디오를 처리하는 능력을 확장하기 위한 다수의 연구가 진행되고 있습니다. 이 논문에서는 기존의 멀티모달 모델 (LMMs)이 단일 이미지나 짧은 비디오에서 인상적인 성능을 보였지만, 긴 비디오를 효과적으로 처리하는 데 있어서 여전히 큰 도전 과제가 있음을 강조합니다. 이를 해결하기 위해, 저자들은 언어 모델의 컨텍스트 길이를 확장하고, 이를 활용하여 멀티모달 모델의 시각적 컨텍스트 길이를 늘리는 방법론을 제안하였습니다. 또한, V-NIAH라는 새로운 벤치마크를 통해 이러한 모델의 성능을 평가하는 방법을 소개합니다.
- **Technical Details**: 이 연구에서는 시각적 토큰의 수를 줄이기 위해 기존 방법들과는 다른 접근 방식을 채택했습니다. 대신, 언어 모델의 기본 컨텍스트 길이를 확장하여 멀티모달 모델의 시각적 컨텍스트 길이로 직접 전이하는 방법을 사용하였습니다. Qwen2-7B-Instruct라는 언어 모델을 백본으로 사용하여, 긴 컨텍스트 훈련을 통해 텍스트 컨텍스트를 224K로 확장하였습니다. 또한, UniRes라는 통합 인코딩 방식을 사용하여 이미지와 비디오를 확장된 이미지로 표현함으로써, 이미지와 비디오 간의 기능 융합을 강화하였습니다.
- **Performance Highlights**: 새로 제안된 Long Video Assistant(LongVA) 모델은 2000프레임 이상 또는 200K 이상의 시각적 토큰을 인식할 수 있으며, Video-MME 데이터셋에서 최첨단 성능을 달성하였습니다. 실험을 통해 추가 프레임을 통해 긴 비디오 질문 응답 벤치마크에서 성능이 향상됨을 입증하였습니다. 또한, V-NIAH 벤치마크를 통해 LMM의 긴 시각적 입력을 처리하는 능력을 평가할 수 있었습니다.

### [Video-Infinity: Distributed Long Video Generation](https://arxiv.org/abs/2406.16260)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16260.png)

Vote: 23

Authors: Zhenxiong Tan, Xinchao Wang, Songhua Liu, Xingyi Yang

- **What's New**: 긴 비디오 생성을 위한 새로운 프레임워크, Video-Infinity를 소개합니다. 이 프레임워크는 '분할과 정복(divide-and-conquer)' 원칙을 바탕으로 긴 비디오 생성 작업을 더 작은 세그먼트로 분할하여 여러 GPU에서 병렬 처리할 수 있게 합니다. 특수한 기법으로 메모리 오버헤드를 줄여 잠재적으로 무한한 길이의 비디오를 생성할 수 있습니다.
- **Technical Details**: Video-Infinity는 두 가지 시너지 메커니즘—클립 병렬 처리(Clip parallelism)와 이중 범위 주의 메커니즘(Dual-scope attention)을 도입했습니다. 클립 병렬 처리는 상황 정보를 세 부분으로 분할하고 교차 통신 전략을 사용하여 세 단계로 공유를 완료합니다. 이중 범위 주의 메커니즘은 시간적 자기 주의 메커니즘을 기기 간의 전체적 일관성을 유지하도록 조정하여 짧은 클립으로 학습한 모델을 긴 비디오 생성에 활용할 수 있게 합니다.
- **Performance Highlights**: Video-Infinity는 이제 최대 2300 프레임에 이르는 긴 비디오를 5분 안에 생성할 수 있습니다. 8개의 Nvidia 6000 Ada (48G) 설정에서 높은 성능을 발휘하며, 기존의 초장기 텍스트-비디오 방법인 Streaming T2V에 비해 최대 100배 빠른 속도를 자랑합니다.

### [VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models](https://arxiv.org/abs/2406.16338)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16338.png)

Vote: 22

Authors: Cihang Xie, Yuxuan Wang, Dongyan Zhao, Yueqian Wang, Zilong Zheng

- **What's New**: 최초의 종합적인 평가 벤치마크인 VideoHallucer를 도입하여, 비디오-언어 대규모 모델들(LVLMs)에서 발생하는 환각 문제를 평가합니다. 이 벤치마크는 모델이 비디오 소스 정보를 직접 반박하는 내재적 환각과 소스에서 확인할 수 없는 외재적 환각을 구분하여 평가합니다.
- **Technical Details**: VideoHallucer는 물체-관계, 시간적, 의미적 세부사항 환각으로 분류되는 내재적 환각, 그리고 사실적/비사실적 외재적 환각으로 분류됩니다. 본 연구는 이진 VQA 기반 방법을 사용하여 비디오-언어 접지에서 환각을 식별합니다. 모델들을 엄격하게 평가하기 위해 고의적인 환각 질문을 포함한 짝을 이룬 질문들로 적대적 평가를 도입하였습니다.
- **Performance Highlights**: 12개의 LVLM를 Comprehensive하게 평가한 결과, 첫째로 대부분의 모델들이 많은 환각 이슈를 가지고 있으며 인간과 큰 성능 차이가 나타났습니다. 둘째로 데이터셋 크기와 모델 파라미터를 확장함에 따라 기본적인 시각적 단서와 반사실적 환각 탐지가 개선되었지만, 외재적 사실적 환각 탐지에는 제한적 영향을 미쳤습니다. 마지막으로 고품질의 설명 메커니즘을 도입할 경우, 외재적 사실적 환각 인식을 부분적으로 개선할 수 있음을 확인했습니다.
- **Innovations**: 자체 개선 역량을 강화하는 Self-PEP라는 플러그 앤 플레이 프레임워크를 개발하였습니다. 이는 모델의 설명 과정을 통합하여 VideoHallucer 벤치마크에서 대부분의 모델들이 평균 5.38%의 성능 향상을 보였습니다.

### [Scaling Laws for Linear Complexity Language Models](https://arxiv.org/abs/2406.16690)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16690.png)

Vote: 19

Authors: Dong Li, Zhen Qin, Xuyang Shen, Yiran Zhong, Ruitao Leng, Weigao Sun

- **What's New**: 최근 언어 모델 (LLMs)의 번영은 모델 크기 증가와 데이터 확장을 최적화하기 위한 스케일링 법칙(scaling laws) 개발을 필요로 했습니다. 이에 따라 전통적인 소프트맥스 어텐션(transformers)이 아닌 선형 복잡도 언어 모델(linear complexity language models)의 확장성을 조사하고자 이번 연구를 진행했습니다.
- **Technical Details**: 이 논문에서는 새로운 선형 복잡도 언어 모델의 사전 학습 스케일링 법칙을 개발했습니다. 기존의 Hoffmann 등의 방법론을 따라서, FLOPs와의 관계를 통해 최적의 모델 크기와 데이터 세트를 유도했습니다. 우리는 TNL, HGRN2, cosFormer2라는 세 가지 효율적인 아키텍처를 조사했으며, LLaMA를 소프트맥스 어텐션 트랜스포머의 기준으로 사용했습니다.
- **Performance Highlights**: 연구 결과, 선형 복잡도 모델이 전통적인 트랜스포머 모델과 유사한 스케일링 추세를 보였고, 교차 도메인(perplexity)과 상식적 추론의 평균 정확도에서 LLaMA를 넘어섰습니다. 그러나 정보 검색(task에서는)에서는 약점을 드러냈습니다. 구체적으로, 모델 크기와 데이터 세트 크기는 계산 예산(computation budget)과 지수 법칙 관계를 가지며(휘발성) 데이터 의존성 감소가 정보 검색(task에서는)에서 유리했습니다.

### [Efficient Continual Pre-training by Mitigating the Stability Gap](https://arxiv.org/abs/2406.14833)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14833.png)

Vote: 18

Authors: Huishuai Zhang, Dongyan Zhao, Jie Fu, Yiduo Guo, Yikang Shen

- **What's New**: 본 논문에서는 대규모 언어 모델(LLM)의 지속적 사전 학습(continual pre-training) 과정에서 나타나는 성능 변화 현상을 연구합니다. 특히, 초기 의료 작업 성능 저하와 그 이후의 성능 회복 현상을 발견하였습니다.
- **Technical Details**: 연구팀은 OpenLlama-3B 모델을 사용하여 지속적 사전 학습을 수행하였으며, 의료 도메인을 주요 대상 도메인으로 설정했습니다. 학습 과정에서 성능이 초기에는 하락하다가 결국 회복된다는 안정성 격차(stability gap) 개념을 도입하여 이를 설명했습니다. 반복된 실험을 통해 모델의 전반적인 성능과 특정 도메인에서의 성능 저하 및 회복 현상을 관찰하였습니다.
- **Performance Highlights**: 제안된 효율적인 지속적 사전 학습 전략을 통해 OpenLlama-3B 모델의 성능이 가속적으로 향상되었고, 최고 성능도 개선되었습니다. 또한, Llama-3-8B 모델을 대상으로 한 실험에서 제안된 전략이 기존 오픈 소스 모델들을 능가하는 성능을 보였습니다.

### [WARP: On the Benefits of Weight Averaged Rewarded Policies](https://arxiv.org/abs/2406.16768)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16768.png)

Vote: 17

Authors: Nino Vieillard, Olivier Bachem, Pierre-Louis Cedoz, Léonard Hussenot, Johan Ferret, Robert Dadashi, Arthur Douillard, Alexandre Ramé, Pier Giuseppe Sessa, Sertan Girgin

- **What's New**: 최근 논문에서는 제조된 LLM(Large Language Models)의 인간 가치와의 정렬(alignment)을 개선하기 위한 새로운 접근법인 WARP(Weight Averaged Rewarded Policies)을 제안합니다. 이는 모델의 무게(weight)를 평균화하는 방법을 통해 더 나은 KL-보상(Pareto front)을 최적화함으로써 기존의 RLHF(Reinforcement Learning from Human Feedback) 접근법에서 발생하는 여러 문제를 완화하려고 합니다.
- **Technical Details**: WARP는 정렬 절차의 세 가지 단계에서 모델들 간의 무게를 평균화하는 세 가지 변형을 사용합니다. 초기화된 SFT(Supervised Fine-Tuning) 모델을 앵커로 사용하는 KL 정규화(Kullback-Leibler regularization)를 통합하여 보상 하킹(Reward Hacking) 및 잊어버림(forgetting) 문제를 방지합니다. 모델 병합(weight averaging)은 선형 모드 연결성(linear mode connectivity)을 활용하여 다양한 모델의 성능을 결합하고 일반화를 촉진하며 기억 기능을 줄입니다. 이 방식은 특히 분명한 분배 변화(distribution shifts)에서의 강건성 향상, 협업 학습, 그리고 대규모 분산 학습을 가능하게 합니다.
- **Performance Highlights**: 실험 결과, WARP는 'Gemma "7B"' 모델의 미세 조정 시 뛰어난 효능을 보였습니다. 추가적으로, KL-보상 최적화의 파레토 전선을 최적화하는 데 있어 긍정적인 성과를 나타냈습니다. 이는 분포 변화 및 모델 학습의 지속적인 정렬에 필요한 필수적인 요소로 작용할 수 있음을 보여줍니다.

### [Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters](https://arxiv.org/abs/2406.16758)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16758.png)

Vote: 16

Authors: Taehyeon Kim, Se-Young Yun, Hongseok Jeung, Euiin Yi, Du-Seong Chang

- **What's New**: 대형 언어 모델(LLM)의 멀티링구얼 문맥 내 추론 시간을 단축하기 위해 'speculative decoding' 기법을 활용한 연구가 소개되었습니다. 이 연구는 멀티링구얼 설정에서의 'speculative decoding' 접근 방식을 최적화하고, 그 결과를 다양한 언어 번역 작업에서 검증합니.
- **Technical Details**: 이 논문은 'speculative decoding'을 적용하여 LLM 추론 시간을 단축하는 방법을 설명합니다. 'speculative decoding'은 초안 생성, 검증, 수락 세 단계를 통해 수행되며, 여기서 보조 모델(Mp)은 미래 토큰을 예측하고 목표 LLM(Mq)는 이를 검증합니다. 검증 기준을 통과한 토큰만 최종 출력에 포함되며, 본 연구에서는 거절 샘플링(rejection sampling)을 통해 이를 실행합니다.
- **Performance Highlights**: 멀티링구얼 번역 작업에서 'speculative decoding'을 적용한 결과, 작업 특화된 토큰 수가 많을수록 추론 속도가 로그 비례로 증가함을 확인하였습니다. GPT-4o 판단 점수와 정성적 분석을 통해 결과를 검증하였으며, 같은 데이터셋에서 훈련된 입력 언어는 두드러진 성능 향상을 보였습니다. 하지만, 도메인과 일치하는 출력이 반드시 성능 개선으로 이어지지는 않았습니다.

### [Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers](https://arxiv.org/abs/2406.16747)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16747.png)

Vote: 14

Authors: Chao Lou, Kewei Tu, Zixia Jia, Zilong Zheng

- **What's New**: 연구자는 Transformer 디코더에서 SparseK Attention이라는 혁신적인 기법을 제안하여 학습 및 추론 시의 주의(attention) 계산을 계산 및 메모리 효율적으로 만들었습니다. SparseK Attention은 키-값 쌍(key-value pairs)의 중요성을 평가하는 스코어링 네트워크(scoring network)와 선형 시간 내에 소프트 마스크(soft mask)로 점수를 정규화하는 새로운 차별화 가능한 top-k 마스크 연산자(operator)를 포함합니다.
- **Technical Details**: SparseK Attention은 기존의 학습 가능한 희소 주의(attention) 메서드가 메모리 적재와 학습 시첩복잡도에서 겪는 문제를 해결하도록 설계되었습니다. 제안된 기법은 유효한 중요도를 평가하는 스코어링 네트워크와 SparseK를 사용하여 학습 시 O(n) 복잡도로 키-값 쌍을 선택하며, 이는 기존의 희소 주의 기법보다 메모리 효율을 크게 개선합니다.
- **Performance Highlights**: SparseK Attention은 GPT2나 Pythia와 같은 다양한 모델에서 기존 풀 어텐션을 대체하는 실험을 통해 검증되었습니다. 실험 결과, 동일한 컨텍스트 크기를 사용할 때 SparseK는 다른 효율적인 주의 메서드보다 일관되게 더 나은 성능을 보였으며, 풀 어텐션과 비교했을 때 학습 속도에서 유망한 향상을 제공했습니다. 또한 새로운 Triton 커널을 개발해, 4096개의 입력 토큰을 처리할 때 SparseK가 FlashAttention을 능가하는 성능을 보였습니다.

### [Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models](https://arxiv.org/abs/2406.15718)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.15718.png)

Vote: 13

Authors: Shengding Hu, Maosong Sun, Weilin Zhao, Xinrong Zhang, Yingfa Chen, Zhiyuan Liu, Xu Han, Zihang Xu, Yuanwei Xu

- **What's New**: 최근 논문에서는 이전의 성과를 바탕으로 하여 '듀플렉스 모델(duplex models)'이라는 새로운 개념을 제안하였습니다. 기존의 큰 언어 모델(LLMs)이 사용자의 모든 입력을 처리한 후 출력을 생성하는 데 반해, 듀플렉스 모델은 입력을 처리하면서 동시에 출력을 생성할 수 있습니다. 이러한 모델은 사용자와의 상호작용을 더욱 자연스럽고 인간다운 대화로 개선하려는 목적으로 개발되었습니다.
- **Technical Details**: 듀플렉스 모델은 '시간분할 다중화(TDM, Time-Division Multiplexing)' 인코딩-디코딩 메커니즘을 사용하여 대화를 시간 단위로 나누어 처리합니다. 시간 슬라이스(time slices)로 대화 메시지를 분할하고, 부분 입력 슬라이스를 기반으로 출력을 생성합니다. 새로운 입력이 도착하면 현재의 출력 생성을 멈추고 새로운 시퀀스를 시작하여 즉각적인 응답을 가능하게 합니다. 이를 위해 듀플렉스 전용 학습 데이터셋(Duplex-UltraChat)이 만들어져 기존의 LLM을 듀플렉스 모델로 변환하는데 사용됩니다.
- **Performance Highlights**: 제안된 듀플렉스 모델 미니CPM-듀플렉스(MiniCPM-duplex)는 일반 벤치마크에서 성능 저하 없이 동적 응답을 가능하게 만들었습니다. 사용성 연구에서 미니CPM-듀플렉스는 원래의 미니CPM보다 반응성, 인간다움, 사용자 만족도 면에서 상당한 개선을 보였습니다.

### [Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs](https://arxiv.org/abs/2406.15927)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.15927.png)

Vote: 13

Authors: Muhammed Razzak, Jiatong Han, Shreshth Malik, Lisa Schut, Jannik Kossen, Yarin Gal

- **What's New**: 이번 연구에서는 Large Language Models(LLMs)의 Hallucination(환각)을 검출하기 위해 새로운 접근법인 Semantic Entropy Probes(SEPs)를 제안합니다. SEPs는 모델의 히든 스테이트(hidden states)에서 유의미한 불확실성을 캡처하여, 높은 비용을 요하는 기존의 샘플링 기반 접근법을 대체하며, 비용 효율적이고 신뢰성 높은 환각 검출 방법을 제공합니다.
- **Technical Details**: SEPs는 Linear Probes(선형 프로브)로, LLMs의 히든 스테이트를 학습하여 Semantic Entropy(의미적 엔트로피)를 예측합니다. SEPs는 기존 방식처럼 여러 모델 샘플을 생성할 필요 없이, 단일 모델의 히든 스테이트에서 직접 작동하며, ground truth 정확성 레이블 없이도 의미적 불확실성을 계산할 수 있습니다. 이를 통해 모델의 응답이 진실에 얼마나 가까운지 예측할 수 있습니다.
- **Performance Highlights**: SEP은 다양한 모델, 태스크, 레이어 및 토큰 위치에서 일관되게 뛰어난 성능을 발휘합니다. SEP 예측은 진실성을 나타내는 효과적인 대리자이며, 이전의 정확성 예측에 기반한 프로브보다 새로운 태스크에 더 잘 일반화됩니다. 이는 비용 효율적인 환각 검출의 새로운 표준을 설정하며, 모델 히든 스테이트가 의미의 불확실성을 직접적으로 포착함을 강력히 시사합니다.

### [Preference Tuning For Toxicity Mitigation Generalizes Across Languages](https://arxiv.org/abs/2406.16235)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16235.png)

Vote: 12

Authors: Zheng-Xin Yong, Xiaochen Li, Stephen H. Bach

- **What's New**: 최근 연구는 다국어 대형 언어 모델(multilingual LLMs)의 독성 문제가 심각하며, 다국어 독성 완화의 필요성을 강조하고 있습니다. 이번 연구는 번역 없이 영어 기반 선호 조정(English preference tuning)을 사용하여 LLM의 독성을 줄이는 방법을 탐구했습니다. 이 연구는 17개 다른 언어에서 영어 데이터를 사용한 Direct Preference Optimization (DPO)를 적용하여 독성 수준을 현저히 낮추는 것을 입증했습니다.
- **Technical Details**: 연구에서는 다양한 크기와 사전 훈련 구성을 가진 다국어 LLM(mGPT, Llama3, Aya-23)을 대상으로 영어 데이터로 DPO를 수행하여 다국어 독성 완화의 효과를 분석했습니다. 24,576개의 영문 프롬프트와 독성 및 비독성 연속체 데이터 세트를 사용하여 5개의 기본 LLM(mGPT, BLOOM, Aya-23, Llama2, Llama3)을 세밀 조정했습니다. MLP의 key 벡터와 value 벡터가 다국어 속성을 가지고 있으며, DPO 이후에 이러한 벡터의 독성 자극 활성화를 효과적으로 억제하는 것을 발견했습니다.
- **Performance Highlights**: 영어 기반 선호 조정이 다국어에서 zero-shot으로 일반화되는 것이 확인되었습니다. 특히, 모델의 크기나 사전 훈련 데이터에 관계없이 독성 수준을 크게 낮출 수 있음을 입증하였으며, MLP의 key와 value 벡터가 다국어 속성을 가지고 있어 독성 프롬프트를 억제하는 데 크게 기여하는 것을 보였습니다.

### [AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models](https://arxiv.org/abs/2406.16714)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16714.png)

Vote: 10

Authors: Jiale Cheng, Pei Ke, Xiao Liu, Hongning Wang, Minlie Huang, Jie Tang, Yida Lu, Xiaotao Gu, Yuxiao Dong

- **What's New**: 최신 언어 모델(LLMs) 발전의 중대한 한계를 극복하기 위해 새로운 프레임워크 AutoDetect를 소개합니다. 이 프레임워크는 다양한 작업에서 LLM의 잠재적 약점을 체계적이고 자동으로 노출함으로써 모델 성능과 신뢰성을 향상시키는 것을 목표로 합니다.
- **Technical Details**: AutoDetect는 Examiner, Questioner, Assessor라는 세 가지 LLM 기반 에이전트 역할을 통해 구현됩니다. Examiner는 포괄적인 분류 체계를 구축하여 대상 모델의 성능에 따라 프레임워크를 최적화합니다. Questioner는 각 테스트 포인트에 따라 도전적인 질문을 생성하며, Assessor는 모델의 응답을 분석하여 새로운 약점을 식별합니다. 이 역할들 간의 협업을 통해 체계적이고 모델 특정적인 약점 식별이 가능해집니다.
- **Performance Highlights**: AutoDetect는 수학적 추론, 코딩 및 지침 따르기와 같은 작업에서 LLM의 약점을 50% 이상 성공률로 식별했습니다. 또한, AutoDetect를 통해 얻은 데이터를 사용하여 LLM 모델을 미세 조정한 결과, 학습 성능이 10% 이상 향상되었습니다. 이러한 데이터는 Llama와 Mistral과 같은 오픈 소스 모델에서 긍정적인 성과를 보였습니다.

### [Confidence Regulation Neurons in Language Models](https://arxiv.org/abs/2406.16254)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16254.png)

Vote: 9

Authors: Ben Wu, Wes Gurnee, Yonatan Belinkov, Xingyi Song, Neel Nanda, Alessandro Stolfo, Mrinmaya Sachan

- **What's New**: 최근 대형 언어 모델(LLMs)이 고위험 응용 분야에 점점 더 많이 사용되면서, 이들의 의사결정 과정에서의 투명성 부족으로 인해 상당한 취약성과 위험이 발생하고 있습니다. 이 연구는 LLM이 자신들의 예측에 대한 자신감을 어떻게 조절하는지를 이해하는 것이 모델 개발을 진전시키고, 안전한 배포를 보장하는 데 얼마나 중요한지를 강조하고 있습니다. 특히, 이 논문은 Transformer 기반 언어 모델에서 발견된 엔트로피 뉴런(entropy neurons)과 새롭게 정의된 토큰 빈도 뉴런(token frequency neurons)이 칼리브레이션 기능을 수행한다고 주장합니다.
- **Technical Details**: 연구는 두 가지 유형의 구성 요소에 주목합니다: 최근에 식별된 엔트로피 뉴런과 새롭게 정의된 토큰 빈도 뉴런입니다. 엔트로피 뉴런은 높은 가중치 노름(weight norm)과 낮은 unembedding 행렬 조성을 특징으로 하며, 이는 다음 토큰 예측에서 미미한 역할을 한다고 알려져 있습니다. 새로운 토큰 빈도 뉴런은 각 토큰의 로그값이 그 빈도에 비례하여 증가하거나 감소하게 합니다. 이 뉴런은 모델의 출력 분포와 유니그램 분포 간의 거리를 조절하며, 높은 불확실성 상황에서 유니그램 분포로 전환하는 역할을 합니다.
- **Performance Highlights**: 연구는 다양한 모델(GPT-2, Pythia, Phi-2, Gemma 2B, LLaMA2 7B)에서 엔트로피 뉴런의 역할을 처음으로 입증하였습니다. 엔트로피 뉴런은 모델의 출력 분포의 엔트로피를 증가시키며, 자신감이 높은 오답 예측을 줄여줍니다. 이는 LLM의 칼리브레이션 메커니즘으로 작용합니다. 언급된 연구는 이들 뉴런이 어떻게 각 모델에서 자신감을 조절하는지를 상세히 분석하였습니다.

### [How Many Parameters Does it Take to Change a Light Bulb? Evaluating Performance in Self-Play of Conversational Games as a Function of Model Characteristics](https://arxiv.org/abs/2406.14051)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14051.png)

Vote: 8

Authors: Jonathan Jordan, Sherzod Hakimov, David Schlangen, Nidhir Bhavsar

- **What's New**: LLM(대형 언어 모델)들이 스스로 대화 게임을 플레이할 수 있는 능력과 그 성능이 모델을 구별하는 중요한 요소로 사용할 수 있다는 이전 연구들이 있습니다. 이번 논문은 이러한 성능을 결정짓는 모델의 속성들이 무엇인지 탐구하며, 특정 모델의 추론 시 성능에 영향을 미치는 요소들을 조사합니다.
- **Technical Details**: 본 논문은 대화 게임을 진행하는 에이전트를 LLM으로부터 유도하는 방법을 설명합니다. 여기에는 게임 목표와 가능한 움직임(𝒢ᵍ) 및 응답 공식을 포함(𝒢ᶠ)하는 프롬프트의 요소들이 포함됩니다. 또한, 각 포인트에서 어떤 움직임을 취할 것인지 결정하는 Markov Decision Process (MDP)를 활용하여 모델의 정책(𝜋)을 최적화합니다.
- **Performance Highlights**: 이전 연구에 의해 개발된 Clembench 벤치마크를 사용하여 성능을 측정합니다. Clembench는 세밀한 측정과 형식적 지침(𝒢ᶠ)과 모델의 이해도(𝒢ᵍ)를 구분하는데 유용합니다. 성능은 품질 점수와 퍼센티지 재생 점수를 통해 평가되며, 이러한 도구를 사용해 모델 특성과 성능을 연관시킵니다.

### [ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians](https://arxiv.org/abs/2406.16815)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16815.png)

Vote: 7

Authors: Shijie Zhang, Yufei Liu, Junshu Tang, Junwei Zhu, Dongjin Huang, Jinkun Hao, Chu Zheng

- **What's New**: 3D 의류 생성은 패션 디자인, 몰입형 상호작용, 가상 시착과 같은 다양한 응용 분야에서 필요성이 증가하고 있습니다. ClotheDreamer라는 새로운 접근법을 제안하여 텍스트 프롬프트만으로도 다양한 입을 수 있는 3D 의류를 생성할 수 있게 되었습니다. 이 방법은 옷과 사람의 몸을 분리하여 모델링하는 데 중점을 두어, 스타일 수정, 의류 교체 및 의류와 신체 간 상호작용을 보다 쉽게 수행할 수 있습니다.
- **Technical Details**: ClotheDreamer는 Disentangled Clothe Gaussian Splatting (DCGS)이라는 새로운 표현 방식을 통해 옷과 사람의 몸을 분리하여 최적화할 수 있습니다. Bidirectional Score Distillation Sampling (SDS) 가이던스를 도입하여 의상의 기하학적 구조를 효율적으로 규제하고, 새로운 가지치기 전략을 통해 헐렁한 의류의 완전성을 유지합니다. 또한, 템플릿 메시 지침을 활용하여 맞춤형 의류 생성을 지원하며, 시뮬레이션된 메시 시퀀스를 사용하여 정확한 의상 애니메이션을 가능하게 합니다.
- **Performance Highlights**: ClotheDreamer는 텍스트 일관성과 전체적인 품질 면에서 기존 방법보다 일관되게 우수한 성과를 보입니다. 다양한 신체 유형을 지원하며, Linear Blend Skinning (LBS)을 사용하여 강성 애니메이션 뿐만 아니라 시뮬레이션된 메시 시퀀스를 통해 더욱 정교한 의상 애니메이션을 구현합니다.

### [IRASim: Learning Interactive Real-Robot Action Simulators](https://arxiv.org/abs/2406.14540)

![](/avatars/0cf0050b272d1c8e46e1299de2aea22a.svg)

Vote: 6

Authors: Chilam Cheang, Hongtao Wu, Song Guo, Yuxiao Liu, Tao Kong, Fangqi Zhu

- **What's New**: IRASim이라는 새로운 방법을 제안했습니다. 이는 주어진 초기 프레임에서 출발하여 로봇이 실행할 행동 궤적을 매우 현실감 있게 비디오로 생성합니다. 이를 'trajectory-to-video' 작업이라 부릅니다. IRASim은 고해상도 (최대 288×512)와 긴 시간의 비디오를 생성할 수 있으며, 정확한 프레임 단위 정렬과 물리 법칙을 엄격히 준수합니다.
- **Technical Details**: IRASim은 'Diffusion Transformer'를 백본으로 사용하여 로봇-객체 상호작용을 모델링하는 신기술을 활용합니다. 또한, 프레임 수준의 조건 방법(frame-level condition method)을 도입해 행동과 비디오 프레임 사이의 정확한 프레임 단위 정렬을 달성합니다. 이 방식은 기존 텍스트-비디오 생성 모델과 차별화됩니다.
- **Performance Highlights**: 새로운 벤치마크 IRASim Benchmark를 사용하여 세 개의 실제 로봇 조작 데이터셋(RT-1, Bridge, Language-Table)에서 IRASim의 성능을 평가했습니다. 실험 결과, IRASim은 비교 대상인 베이스라인 방법들을 능가했으며, 모든 데이터셋에서 인간 평가에서도 더 선호되었습니다. 또한, IRASim은 RT-1과 Bridge에서 3D 공간, Language-Table에서 2D 공간 내에서 로봇을 텔레오퍼레이트(teleoperate)하는 데 사용될 수 있음을 보여주었습니다.

### [Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization](https://arxiv.org/abs/2406.16008)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16008.png)

Vote: 5

Authors: Cheng-Yu Hsieh, James Glass, Chun-Liang Li, Alexander Ratner, Ranjay Krishna, Tomas Pfister, Yung-Sung Chuang, Long T. Le, Abhishek Kumar, Zifeng Wang, Chen-Yu Lee

- **What's New**: 대형 언어 모델(LLMs)의 입력 프롬프트 중간에 중요한 문서가 있을 때 이를 제대로 찾지 못하는 'lost-in-the-middle' 현상을 해결하기 위한 새로운 연구가 발표되었습니다. 이 논문은 주의(attention) 편향에 대한 분석을 통해 이 문제를 해결할 수 있는 방법을 제시합니다.
- **Technical Details**: 연구진은 'lost-in-the-middle' 현상이 발생하는 원인을 모델의 내재된 주의 편향으로 분석했습니다. LLM들은 입력 프롬프트의 시작과 끝 부분에 높은 주의(attention) 값을 할당하는 U자형 주의 분포를 보입니다. 이러한 편향을 제거하기 위해 주의 점수를 교정하는 메커니즘을 개발하였습니다. 이는 LLM의 주의를 교정하여 모델이 중간에 위치한 문서를 더 잘 활용할 수 있게 합니다.
- **Performance Highlights**: 교정된 주의 메커니즘은 열린 도메인 질문 응답 과제에서 기존 방법 대비 높은 성능을 보였습니다. 특히, NaturalQuestions 데이터셋에서는 최대 15 퍼센트 포인트의 성능 향상을 기록했습니다. 이는 LLM이 중간에 삽입된 문맥도 효과적으로 파악할 수 있음을 시사합니다.

### [Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations](https://arxiv.org/abs/2406.13632)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.13632.png)

Vote: 5

Authors: Hannah Rashkin, Dror Marcus, Yossi Matias, Avinatan Hassidim, Alon Jacovi, Idan Szpektor, Roee Aharoni, Alex Fabrikant, Avi Caciularu, Jonathan Herzig, Arie Cattan

- **What's New**: 이 연구에서는 대형 언어 모델(LLMs)이 긴 문맥을 처리하는 능력을 높이기 위한 새로운 방법인 DoubleDipper를 제안합니다. 이 방법은 주어진 입력 문맥을 재활용하여 몇 가지 샷 예제(few-shot examples)를 자동으로 생성하고, 이를 통해 모델이 문맥 내에서 관련 구절을 명확하게 식별하도록 학습시킵니다.
- **Technical Details**: DoubleDipper는 두 가지 주요 원칙에 기반합니다. 첫째, 주어진 입력 문맥에서 몇 가지 문단을 무작위로 선택하고, 이들 문단에 대해 질문-답변 쌍(QA pairs)을 생성합니다. 생성된 QA 쌍은 이후 입력 문맥과 목표 질문 사이에 배치됩니다. 둘째, 각 ICL 예제를 풍부하게 만들어, 모델이 먼저 관련 정보를 포함하는 문단을 식별한 후 답변을 생성하도록 유도합니다. 이러한 구조화된 Chain of Thought 접근법은 긴 문맥을 처리할 때 매우 유용합니다.
- **Performance Highlights**: DoubleDipper를 상용 및 오픈소스 LLM들을 대상으로 다양한 QA 데이터셋을 사용하여 평가한 결과, 3개의 자동 생성된 few-shot 예제만으로도 기준치를 평균 23% 초과하는 성능을 보였습니다. 특히 모델이 관련 정보의 위치에 더 강인한 성능을 보였으며, 다중 구절에서 정보를 요구하는 multi-hop QA에서도 잘 일반화되었습니다.

### [video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models](https://arxiv.org/abs/2406.15704)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.15704.png)

Vote: 5

Authors: Wenyi Yu, Yuxuan Wang, Changli Tang, Wei Li, Chao Zhang, Xianzhao Chen, Lu Lu, Guangzhi Sun, Tian Tan, Zejun Ma

- **What's New**: 최신 연구는 대형 언어 모델(LLM)이 텍스트 기반에서 뛰어난 성과를 보이는 것에 더해, 영상과 오디오 인식 능력을 부여하는 새로운 연구를 제시하고 있습니다. 특히 이번 연구에서는 동영상의 전반적인 이해를 위해 말하기(speech), 오디오 및 영상 데이터를 통합한 video-SALMONN 모델을 제안합니다.
- **Technical Details**: video-SALMONN은 speech audio language music open neural network의 약자로, 자연 이미지, 시각적 프레임 시퀀스, 말하기, 오디오 이벤트 및 음악 요소를 포함하는 전반적인 동영상 데이터를 처리할 수 있습니다. MRC Q-Former 구조를 사용하여 여러 시간적 스케일에서 오디오-비주얼 입력 특징과 텍스트 표현 공간을 정렬합니다. 이를 통해 차후의 비디오 프레임 간의 인과 관계를 강화합니다. 또한 특정 프레임이나 단일 모달리티의 지배를 방지하기 위해 diversity loss와 새로운 음성-비주얼 혼합 전략을 도입했습니다.
- **Performance Highlights**: video-SALMONN은 새로운 평가 기준인 SAVE 벤치마크에서 다양한 성과를 확인했습니다. 영상 질문 응답(Video QA)과 오디오-비디오 질문 응답(AV QA) 데이터셋에서 각각 25%와 30% 이상의 정확도 향상을 보였습니다. 이 모델은 말하기 이해 및 인과 추론을 요구하는 오디오-비주얼 과제에서 탁월한 성과를 보여줍니다.

### [Repulsive Score Distillation for Diverse Sampling of Diffusion Models](https://arxiv.org/abs/2406.16683)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16683.png)

Vote: 4

Authors: Nicolas Zilberstein, Santiago Segarra, Morteza Mardani

- **What's New**: 이 논문에서는 새로운 AI 기반 접근 방식을 제안하여 데이터 처리 및 해석의 효율성을 극대화하려고 시도합니다. 특히, 기존 방법들의 한계를 극복하고 더 높은 정확도를 달성하는 데 중점을 두고 있습니다. AI 모델은 새로운 알고리즘을 도입해 더욱 복잡한 데이터 구조를 이해할 수 있게 되었습니다.
- **Technical Details**: 제안된 시스템은 Transformer architecture(트랜스포머 아키텍처)을 기반으로 하며, Attention mechanism(어텐션 메커니즘)과 Deep Learning(딥 러닝) 기술을 결합하였습니다. 특히, Multi-head attention(멀티-헤드 어텐션)과 Encoder-Decoder 구조를 활용하여 성능을 최적화합니다. 이를 통해 데이터의 다양한 패턴을 더욱 정교하게 학습할 수 있는 능력을 갖추고 있습니다.
- **Performance Highlights**: 제안된 모델은 벤치마크 테스트에서 기존의 SOTA(State-of-the-art) 모델들을 뛰어넘는 성능을 보였습니다. 특히, 이미지 분류 및 텍스트 처리 분야에서 높은 F1-score(에프원-스코어)를 기록하며 그 우수성을 입증하였습니다. 실험 결과, 모델의 정확도는 기존 방식에 비해 평균 10% 향상되었습니다.

### [OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?](https://arxiv.org/abs/2406.16772)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.16772.png)

Vote: 3

Authors: Shijie Xia, Pengfei Liu, Zengzhi Wang, Zhen Huang

- **What's New**: Huang et al. (2024)는 최근 OlympicArena를 도입하였으며, 여기에는 62개의 국제 올림픽 대회를 포함하여 7개의 일반 교과와 텍스트 전용 및 텍스트-이미지 혼합 모달리티에서 11,163개의 이중언어 문제들이 포함되어 있습니다. 이 데이터셋은 철저한 데이터 누출 검사를 거쳤으며, AI 모델의 인지적 추론 능력을 테스트하는 데 사용됩니다. OlympicArena는 AI 모델의 성능을 공정하고 경쟁력 있게 비교할 수 있는 OlympicArena Medal Table과 같은 새로운 랭킹 메커니즘을 제공합니다.
- **Technical Details**: OlympicArena의 테스트 셋은 공개되지 않아 데이터 유출을 방지하며, 규칙 기반 매칭을 통해 평가됩니다. 이는 대형 멀티모달 모델(LMM)과 대형 언어 모델(LLM) 모두에 적용됩니다. 텍스트 전용 설정을 사용하여 이미지 관련 정보를 모델 입력값으로 제공하지 않습니다. 또한, 최신 모델인 Claude-3.5-Sonnet와 Gemini-1.5-Pro을 OpenAI의 GPT 시리즈와 비교했습니다. 이 비교는 최신 모델의 발전된 성능을 평가하는 데 도움을 줍니다.
- **Performance Highlights**: Claude-3.5-Sonnet은 GPT-4o와 거의 비슷한 성능을 보이며, Gemini-1.5-Pro는 대부분의 학문에서 GPT-4V를 능가했습니다. OlympicArena Medal Table에 따르면 GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro는 상위 3개 모델로 언급되었습니다. 또한, 개방형 소스 모델들은 메달을 획득하지 못한 반면, GPT-4o는 수학 및 전산학에서 뛰어난 성능을 보였습니다. Claude-3.5-Sonnet은 물리학, 화학, 생물학 등에서 더 높은 성능을 보였습니다. 이 결과는 각기 다른 모델 시리즈가 특정 학문분야에 대해 갖는 독특한 강점을 강조합니다. 예를 들어, GPT-4o는 전통적인 수학적 추론 및 코딩 능력에 우수한 반면, Claude-3.5-Sonnet과 Gemini-1.5-Pro는 물리학, 화학, 생물학에서 경쟁력이 있습니다.

