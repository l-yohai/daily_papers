## Daily Papers (2024-06-05)

### [Seed-TTS: A Family of High-Quality Versatile Speech Generation Models](https://arxiv.org/abs/2406.02430)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02430.png)

Vote: 16

Authors: Zhuo Chen, +, Peisong Huang, Philip Anastassiou, Jian Cong, Mingqing Gong, Lu Gao, Lelai Deng, Chumin Li, Xingxing Li, Jiaxin Li, Chuang Ding, Yuanyuan Huo, Yuanzhe Chen, Ziyi Chen, Hui Li, Feiya Li, Jitong Chen, Jiawei Chen, Qingqing Huang, Xiaoyang Li, Dongya Jia, Zhiying Huang

- 'Seed-TSS'라는 대규모 자동 회귀 텍스트 음성 변환(TTS) 모델군을 소개하며, 이 모델은 인간의 음성과 구분할 수 없는 수준의 음성을 생성할 수 있습니다.
- Seed-TTS는 음성 생성의 기반 모델로서, 맥락 학습에서 우수하며, 화자 유사성과 자연스러움에서 기존 인간 음성의 수준에 도달하는 성능을 객관적 및 주관적 평가에서 보여줍니다.
- 감정과 같은 다양한 음성 특성을 제어할 수 있는 우수한 기능을 제공하며, 야생의 화자들을 위해 매우 표현력 있고 다양한 음성을 생성할 수 있습니다.
- 음성 분할을 위한 자체 증류 방법과 모델의 강건성, 화자 유사성 및 제어력을 향상시키기 위한 강화 학습 접근 방식을 제안합니다.
- 비자동 회귀(NAR) 변형 모델인 Seed-TTS_DiT를 소개하며, 이는 전적으로 확산 기반 구조를 사용합니다. 이 모델은 사전에 예측된 음소 지속 시간에 의존하지 않고 종단 처리를 통해 음성을 생성합니다.
- Seed-TTS_DiT는 언어 모델 기반 변형과 비교할 수 있는 성능을 달성하며, 음성 편집에서의 효과를 보여줍니다.
- 데모는 https://bytedancespeech.github.io/seedtts_tech_report에서 들을 수 있습니다.

### [To Believe or Not to Believe Your LLM](https://arxiv.org/abs/2406.02543)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02543.png)

Vote: 12

Authors: Ilja Kuzborskij, Csaba Szepesvári, András György, Yasin Abbasi Yadkori

- 본 연구는 대규모 언어 모델(LLM)에서 응답의 불확실성을 정량화하고, 주어진 질의에 대한 응답의 불확실성이 클 때 이를 식별하는 것을 목향합니다.
- 주된 두 가지 불확실성, 즉 지식 부족으로 인한 에피스테믹 불확실성과 여러 가능한 답변과 같은 불가피한 무작위성에서 비롯된 알레아토릭 불확실성을 동시에 고려합니다.
- 특히, 에피스테믹 불확실성이 클 때 모델의 출력이 신뢰할 수 없다는 것을 신뢰성 있게 탐지할 수 있는 정보 이론적 지표를 도출합니다.
- 이 조건은 모델의 이전 반응에 기반한 특정 반복적 프롬프팅을 통해서만 계산될 수 있으며, 예를 들어 단일 및 다중 답변 응답에서 에피스테믹 불확실성이 높은 환각(헛것을 봄) 상태를 탐지하는 데 사용될 수 있습니다.
- 우리의 접근 방식은 다중 답변 경우에 환각을 탐지할 수 없는 많은 표준 불확실성 정량화 전략들과 대비됩니다.
- 연구를 통해 수행된 일련의 실험들은 우리의 공식화가 가지는 이점을 입증합니다.
- 추가적으로, LLM에 의해 주어진 출력에 할당된 확률이 반복적 프롬프팅에 의해 어떻게 증폭될 수 있는지에 대한 조명도 제공됩니다. 이는 독립적인 관심사가 될 수 있습니다.

### [I4VGen: Image as Stepping Stone for Text-to-Video Generation](https://arxiv.org/abs/2406.02230)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02230.png)

Vote: 11

Authors: Di Huang, Jinlin Liu, Miaomiao Cui, Xiefan Guo

- 텍스트에서 비디오 생성은 공간-시간 모델링의 복잡성과 한정된 비디오-텍스트 데이터셋 때문에 텍스트에서 이미지 합성보다 품질과 다양성에서 뒤처져 있다.
- 본 논문은 I4VGen이라는 훈련이 필요 없고 플러그 앤 플레이 가능한 비디오 확산 추론 프레임워크를 제시하며, 이는 견고한 이미지 기술을 활용하여 텍스트에서 비디오 생성을 향상시킨다.
- I4VGen은 텍스트에서 비디오 생성을 두 단계로 나눈다: 앵커 이미지 합성과 앵커 이미지 가이드 비디오 합성.
- 시각적으로 사실적이고 의미론적으로 충실한 앵커 이미지를 달성하기 위해 잘 설계된 생성-선택 파이프라인이 사용되고, 동적 비디오로 이미지를 애니메이트하기 위해 혁신적인 Noise-Invariant Video Score Distillation Sampling이 통합된다.
- 이 추론 전략은 비-제로 종단 신호 대 잡음비 문제를 효과적으로 완화한다.
- 광범위한 평가를 통해, I4VGen은 시각적 리얼리즘과 텍스트 충실성이 더 높은 비디오를 생성할 뿐만 아니라 기존 이미지-비디오 확산 모델에 원활하게 통합하여 전반적인 비디오 품질을 개선한다.

### [Self-Improving Robust Preference Optimization](https://arxiv.org/abs/2406.01660)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.01660.png)

Vote: 9

Authors: Arash Ahmadian, Mohammad Gheshlaghi Azar, Oilvier Pietquin, Eugene Choi, Matthieu Geist

- 온라인 및 오프라인 RLHF 방법론인 PPO와 DPO는 인공지능을 인간의 선호도에 맞추는 데 큰 성공을 거두었으나, 이들 방법론은 최적 해결책이 과제에 매우 의존적이라는 근본적인 문제를 가지고 있습니다.
- 이러한 도전을 극복하기 위해, SRPO(Self-Improving Robust Preference Optimization)를 제안합니다. 이는 과제 변화에 완전히 강건한 오프라인 RLHF 프레임워크로, 자기 개선 과정을 수학적으로 표현하여 적대적 방식으로 자기 개선 정책과 생성 정책의 공동 최적화를 추구합니다.
- SRPO의 해결책은 훈련 과제와 독립적이므로 변화에 강건하며, 비적대적 오프라인 손실 형태로 재표현되어 표준 감독 최적화 기법을 사용하여 대규모로 최적화할 수 있습니다.
- SRPO는 인간(GOLD) 완성과의 대결에서 인공지능이 승률(WR) 90%로, 특히 과제-외 분포(OOD)인 XSUM 데이터 세트에서 평가될 때 유명한 DPO를 15%의 명확한 차이로 능가하며 그 효과를 입증합니다.

### [Guiding a Diffusion Model with a Bad Version of Itself](https://arxiv.org/abs/2406.02507)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02507.png)

Vote: 8

Authors: Timo Aila, Tero Karras, Tuomas Kynkäänniemi, Samuli Laine, Jaakko Lehtinen, Miika Aittala

- 이미지 생성 확산 모델의 주요 관심사는 이미지 품질, 결과의 변이량 및 주어진 조건(예: 클래스 라벨 또는 텍스트 프롬프트)과 결과의 일치성입니다.
- 인기 있는 무조건적 모델을 활용한 분류자 없는 지도 방법은 무조건적 모델을 사용하여 조건부 모델을 안내함으로써, 프롬프트 정렬 및 이미지 품질을 동시에 향상시키지만, 결과의 변이를 감소시킵니다.
- 이 효과는 본질적으로 얽혀 있어 제어가 어렵습니다.
- 놀랍게도, 모델 자체의 덜 훈련된 작은 버전을 사용하여 생성을 안내함으로써 이미지 품질을 희생하지 않고 이미지 품질과 변이량을 분리하여 제어할 수 있음을 발견했습니다.
- 이 방법은 ImageNet 생성에서 상당한 개선을 이루어내어, 공개적으로 이용 가능한 네트워크를 사용하여 64x64 크기의 이미지에 대해 1.01, 512x512 크기의 이미지에 대해 1.25의 기록적인 FID를 달성탁.
- 또한, 이 방법은 무조건적인 확산 모델에도 적용 가능하며, 그 품질을 대폭 향상시킬 수 있습니다.

### [V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation](https://arxiv.org/abs/2406.02511)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02511.png)

Vote: 6

Authors: Cong Wang, Wei Yang, Feng Luo, Qing Gu, Kuan Tian, Zhiwei Jiang, Yonghang Guan, Xiao Han, Fei Shen, Jun Zhang

- 인물 비디오 생성 분야에서 단일 이미지를 사용하여 인물 비디오를 생성하는 방법이 점점 더 널리 사용되고 있습니다.
- 제어 가능한 생성을 위해 생성 모델을 활용하는 것이 일반적인 접근 방식이지만, 텍스트, 오디오, 참조 이미지, 포즈, 깊이 맵 등 다양한 제어 신호의 강도 차이로 인해 약한 조건들이 강한 조건들에 의해 효과적인 제어가 어려워지는 문제가 있습니다.
- 특히 오디오 신호가 약하게 작용하여 종종 얼굴 포즈나 참조 이미지와 같은 강한 신호에 가려지는 경우가 많아 연구에서 이를 해결하기 위해 V-Express 방법을 제안했습니다.
- V-Express는 서서히 각기 다른 제어 신호를 균형 있게 처리하고 조건부 드롭아웃 작업을 통해 점진적으로 훈련하는 방법으로, 약한 조건에서도 효과적으로 제어할 수 있게 합니다.
- 이 방법은 얼굴 포즈, 참조 이미지, 오디오를 동시에 고려하는 인물 비디오 생성 능력을 달성하며, 다양한 강도의 조건을 동시에 효과적으로 사용할 수 있는 해결책을 제공합니다.

### [RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots](https://arxiv.org/abs/2406.02523)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02523.png)

Vote: 5

Authors: Aaron Lo, Ajay Mandlekar, Soroush Nasiriany, Abhiram Maddukuri, Adeet Parikh, Lance Zhang, Yuke Zhu, Abhishek Joshi

- 최근 인공지능(AI)의 발전은 데이터셋의 확장을 통해 주도되었지만, 로봇공학에서는 대규모 로봇 데이터에 대한 접근성 부졠으로 확장이 제한적입니다.
- RoboCasa는 일상 환경에서 범용 로봇을 훈련시키기 위한 대규모 시뮬레이션 프레임워크로, 주방 환경에 초점을 맞춘 현실적이고 다양한 장면을 제공합니다.
- 수천 개의 3D 자산과 150개 이상의 객체 범주, 다수의 상호 작용 가능한 가구 및 가전 제품을 포함하며, 텍스트에서 3D 모델로, 환경 질감을 텍스트에서 이미지 모델로 생성하는 생성 AI 도구로 시뮬레이션의 현실성과 다양성을 향상시킵니다.
- 100개의 시스템적 평가를 위한 작업을 설계하고, 대규모 언어 모델의 가이드에 따라 생성된 복합 작업을 포함시킵니다.
- 고품질의 인간 시연을 제공하고 자동 궤적 생성 방법을 통합하여 최소한의 인간 부담으로 데이터셋을 대폭 확장합니다.
- 실험 결과는 합성 생성된 로봇 데이터를 사용하여 대규모 모방 학습에서 명확한 확장 추세를 보여주며 시뮬레이션 데이터를 실제 작업에 활용할 수 있는 큰 가능성을 보여줍니다.
- 관련 동영상과 오픈소스 코드는 https://robocasa.ai/에서 제공됩니다.

### [CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation](https://arxiv.org/abs/2406.02509)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02509.png)

Vote: 4

Authors: Weili Nie, Sifei Liu, Arash Vahdat, Chao Liu, Zhangyang Wang, Dejia Xu, Jan Kautz

- 최근에 등장한 비디오 확산 모델은 일반 사용자에게 고품질의 비디오 콘텐츠 생성을 가능하게 하는 강력한 생성 도구로 자리잡고 있지만, 카메라 포즈에 대한 정밀한 제어를 제공하지 않아 시네마틱 언어의 표현 및 사용자 제어에 한계가 있습니다.
- 이러한 문제를 해결하기 위해, 'CamCo'를 도입하여 이미지에서 비디오로의 생성 과정에서 카메라 포즈를 세밀하게 조정할 수 있도록 하였습니다.
- CamCo는 플뤼커 좌표를 사용하여 정확하게 매개변수화된 카메라 포즈 입력을 통해 사전 훈련된 이미지-비디오 생성기를 갖추고 있습니다.
- 비디오에서 3D 일관성을 향상시키기 위해 각 주의 블록에 에피폴라 제약을 특징 지도에 적용하는 에피폴라 주의 모듈을 통합하였습니다.
- 또한, 실세계 비디오와 구조로부터의 운동 알고리즘을 통해 추정된 카메라 포즈로 CamCo를 미세 조정하여 객체 동작을 보다 잘 합성할 수 있습니다.
- 실험 결과, CamCo는 이전 모델들에 비해 3D 일관성과 카메라 제어 능력이 크게 향상되었으며 설득력 있는 객체 동작 생성이 가능함을 보여줍니다.

