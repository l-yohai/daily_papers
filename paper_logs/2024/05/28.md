## Daily Papers (2024-05-28)

### [An Introduction to Vision-Language Modeling](https://arxiv.org/abs/2405.17247)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.17247.png)

Vote: 41

Authors: Richard Yuanzhe Pang, Anurag Ajay, Melissa Hall, Karthik Padthe, Suzanne Petryk, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, Yunyang Xiong, Alexander C. Li, Oscar Mañas, Diane Bouchacourt, Chuan Guo, Hu Xu, Mark Ibrahim, Florian Bordes, Srihari Jayakumar, Candace Ross, +, Jonathan Lebensold, Adrien Bardes, Haider Al-Tahan, Vasu Sharma

- 최근 대형 언어 모델(LLMs)의 인기에 힘입어 시각 분야로 확장하려는 여러 시도가 이루어졌습니다.
- 시각-언어 모델(VLM) 응용 프로그램은 이미지를 생성하거나 낯선 환경을 안내하는 시각적 도우미 등을 통해 기술과의 관계에 중대한 영향을 미칠 전망입니다.
- 언어는 이산적이지만, 시각은 개념을 쉽게 이산화할 수 없는 훨씬 높은 차원의 공간에서 진화합니다.
- VLM의 기본 원리와 언어 매핑을 더 잘 이해하기 위해, 이 논문은 VLM에 대한 소개와 이 분야에 진입하고자 하는 이들에게 도움이 되기를 바라며 작성되었습니다.
- VLM이 무엇인지, 어떻게 작동하는지, 그리고 어떻게 훈련시키는지 소개합니다.
- VLM을 평가하는 방법을 제시하고 논의합니다.
- 주로 이미지를 언어로 매핑하는 작업에 초점을 맞추고 있지만, 비디오로 VLM을 확장하는 것도 논의됩니다.

### [Transformers Can Do Arithmetic with the Right Embeddings](https://arxiv.org/abs/2405.17399)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.17399.png)

Vote: 26

Authors: Avi Schwarzschild, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Arpit Bansal, Sean McLeish, Tom Goldstein, John Kirchenbauer, Jonas Geiping, Neel Jain, Alex Stein

- 변형자(transformers)는 대규모 숫자 내에서 각 숫자의 정확한 위치를 추적하지 못하는 것이 산술 작업에서의 부진한 성능의 주된 원인인 것으로 보인다.
- 이 문제를 해결하기 위해, 숫자의 시작 위치에 상대적인 각 숫자의 위치를 인코딩하는 임베딩을 추가함으로써 성능이 향상된다.
- 이러한 임베딩은 단독으로 성능을 향상시킬 뿐만 아니라, 입력 주입과 순환 레이어와 같은 구조적 수정을 가능하게 하여 성능을 더욱 향상시킨다.
- 위치가 해결되면, 변형자가 훈련 데이터보다 크고 복잡한 산술 문제를 해결할 수 있는지의 논리적 추론 능력을 연구할 수 있다.
- 단 하루 동안 단일 GPU로 20자리 숫자에 대해서만 훈련하여, 100자리 덧셈 문제에서 최대 99%의 정확도를 달성하며 최고의 성능을 보여준다.
- 이러한 산수 능력의 향상은 정렬과 곱셈을 포함한 다른 다단계 추론 작업에서의 개선을 가능하게 한다.

### [Matryoshka Multimodal Models](https://arxiv.org/abs/2405.17430)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.17430.png)

Vote: 19

Authors: Jianwei Yang, Mu Cai, Yong Jae Lee, Jianfeng Gao

- 대규모 다양체 모델(LMMs)은 시각-언어 추론에서 강력한 성능을 보이지만 고해상도 이미지와 비디오와 같은 밀집 시각 시나리오에서 많은 토큰 수를 필요로 하는 비효율성을 발생시킵니다.
- 기존의 토큰 축소/병합 방법들은 각 이미지에 대해 단일 길이의 출력을 생성하며, 정보 밀도와 효율성 사이의 타협을 유연하게 조절할 수 없습니다.
- 저자들은 중첩된 다양한 굵기의 시각 토큰을 사용하여 시각 콘텐츠를 표현하는 M3: Matryoshka Multimodal Models를 제안합니다.
- 이 접근법은 LMMs에 여러 독특한 이점을 제공합니다: 테스트 인스턴스의 시각적 세부 사항을 명시적으로 조절할 수 있으며, COCO 스타일 벤치마크에서는 모든 576개의 토큰을 사용하는 것과 유사한 정확도를 얻기 위해 약 9개의 시각적 토큰만 필요로 합니다.
- 또한, 이 연구는 데이터 샘플 수준에서 성능과 시각 토큰 길이 사이의 최적의 균형을 탐구하는 기초를 제공합니다.

### [Zamba: A Compact 7B SSM Hybrid Model](https://arxiv.org/abs/2405.16712)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.16712.png)

Vote: 11

Authors: Jonathan Pilault, Paolo Glorioso, Beren Millidge, James Whittington, Adam Ibrahim, Yury Tokpanov, Quentin Anthony

- 본 기술 보고서에서는 Zamba라는 새로운 7B 규모의 SSM-트랜스포머 하이브리드 모델을 소개하며, 이는 비승 방식의 주요 모델과 비슷한 규몗에서 경쟁력 있는 성능을 달성합니다.
- Zamba는 공개적으로 이용 가능한 데이터셋에서 1T 토큰에 대해 학습되었으며, 이 규모에서 최고의 비트랜스포머 모델입니다.
- Zamba는 Mamba 기반 구조와 단일 공유 주의 모듈을 결합한 독창적인 아키텍처를 도입하여 최소한의 파라미터 비용으로 주의 메커니즘의 이점을 얻습니다.
- 이 아키텍처 덕분에 Zamba는 비슷한 규모의 트랜스포머 모델보다 추론 속도가 훨씬 빠르며, 긴 시퀀스 생성에 훨씬 적은 메모리를 요구합니다.
- Zamba는 두 단계에 걸쳐 사전훈련되었는데, 첫 번째 단계는 기존 웹 데이터셋을 기반으로 하고, 두 번째 단계는 고품질 지시 및 합성 데이터셋을 사용하여 모델을 어닐링하는 데 초점을 맞추고 빠르게 학습률이 감소하는 것이 특징입니다.
- Zamba의 가중치와 모든 체크포인트는 사전 훈련 1단계와 어닐링 단계를 통틀어 오픈 소스로 제공됩니다.

### [Looking Backward: Streaming Video-to-Video Translation with Feature Banks](https://arxiv.org/abs/2405.15757)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.15757.png)

Vote: 9

Authors: Chenfeng Xu, Feng Liang, Diana Marculescu, Kurt Keutzer, Masayoshi Tomizuka, Akio Kodaira

- 이 논문은 사용자 프롬퍼트로 실시간 스트리밍 비디오 대 비디오 변환을 가능하게하는 StreamV2V라는 확산 모델을 소개합니다.
- 이전 V2V 방법과 달리 제한된 프레임을 처리하는 대신 스트리밍 방식을 통해 무제한 프레임을 처리합니다.
- StreamV2V의 핵심은 과거의 프레임으로부터 수집된 정보를 보관하는 'feature bank'에 의존하여 현재와 과거를 연결하는 'backward-looking' 원칙에 있습니다.
- 새로운 프레임이 들어올 때, StreamV2V는 자기 주의(self-attention)를 확장하여 저장된 키와 값에 대한 은행이 포함되어 유사한 과거 특징들을 직접 혼합합니다.
- feature bank는 저장된 특징과 새로운 특징을 결합하여 지속적으로 업데이트되므로 컴팩트하면서도 유익합니다.
- StreamV2V는 이미지 확산 모델과의 매끄러운 통합이 가능하며, 추가 튜닝 없이도 적응성과 효율성을 자랑합니다.
- A100 GPU 하나에서 20 FPS를 실행할 수 있으며 FlowVid, CoDeF, Rerender 및 TokenFlow보다 각각 15배, 46배, 108배, 158배 빠릅니다.
- 정량적 지표와 사용자 연구는 StreamV2V가 시간적 일관성을 유지하는 뛰어난 능력을 입증합니다.

### [I2VEdit: First-Frame-Guided Video Editing via Image-to-Video Diffusion Models](https://arxiv.org/abs/2405.16537)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.16537.png)

Vote: 9

Authors: Wenqi Ouyang, Lei Yang, Yi Dong, Jianlou Si, Xingang Pan

- 본 논문은 이미지 편집 도구의 범용성을 동영상으로 확장하여 단일 프레임에서 전체 비디오로 편집을 전파하는 새로운 방법을 제시합니다.
- 제안된 I2VEdit 방법은 소스 비디오의 시각적 및 동작 무결성을 수정의 정도에 따라 적응적으로 유지하며, 기존 방법들이 완전히 달성하지 못한 폭넓은 수정, 지역적 수정 및 적당한 형태 변경을 효과적으로 처리합니다.
- 핵심 프로세스로는 원본 비디오와 기본 동작 패턴을 정레하는 Coarse Motion Extraction과 세밀한 주의집중 매칭을 사용하여 정밀 조정을 수행하는 Appearance Refinement가 있습니다.
- 다중 비디오 클립에 걸쳐 자동 회귀 생성으로부터 품질 저하를 완화하기 위해 스킵 간격 전략을 통합합니다.
- 실험 결과는 우리 프레임워크가 미세 조정된 비디오 편집에서 우수한 성능을 발휘하며, 고품질이면서 시간적으로 일관된 결과물을 생성할 수 있는 능력을 입증합니다.

### [NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models](https://arxiv.org/abs/2405.17428)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.17428.png)

Vote: 9

Authors: Jonathan Raiman, Bryan Catanzaro, Wei Ping, Rajarshi Roy, Mohammad Shoeybi, Chankyu Lee, Mengyao Xu

- NV-Embed 모델은 단순성과 재현성을 유지하면서 LLM(Large Language Model)을 범용 임베딩 모델로서의 성능을 크게 향상시키기 위한 다양한 구조 설계와 훈련 절차를 도입하였습니다.
- 모델 구조에서는 더 나은 검색 및 하류 작업 정확도를 향상시키기 위해 잠재 주의력 계층을 제안하며, 이는 평균 풀링이나 LLM의 마지막 <EOS> 토큰 임베딩 사용보다 일관되게 더 나은 결과를 제공합니다.
- 대조적인 학습에서 LLM의 인과주의 마스크를 제거하여 표현 학습을 강화합니다.
- 모델 훈련 절차로는 검색 데이터셋에서의 지시사항에 대해 대조적인 학습을 먼저 적용한 후, 여러 비검색 데이터셋을 지시 튜닝에 혼합하는 두 단계 대조적인 지시 튜닝 방법을 소개합니다.
- 이러한 기술들을 결합하여 NV-Embed 모델은 69.32의 기록적인 점수를 달성했으며, Massive Text Embedding Benchmark(MTEB)에서 검색, 재정렬, 분류, 클러스터링 및 의미적 텍스트 유사성 작업 등 56개의 과제를 포함하여 1위를 차지했습니다.
- 특히, MTEB 벤치마크에서 검색 과제 15개에서도 최고 점수인 59.36을 달성했습니다.
- 개발된 모델은 https://huggingface.co/nvidia/NV-Embed-v1에서 오픈 소스로 제공될 예정입니다.

### [$\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning](https://arxiv.org/abs/2405.17258)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.17258.png)

Vote: 9

Authors: Rogerio Feris, Soumya Ghosh, Diego Antognini, David Cox, Leonid Karlinsky, Runqian Wang, Aude Oliva

- 낮은 순위 어댑터(LoRA) 및 그 변형은 조정 대상 모델의 전체 모델 성능에 근접하면서 소수의 추가 파라미터만을 필요로 하는 인기 있는 효율적인 파라메터 미세조정(PEFT) 기술입니다.
- LoRA 모듈은 기본 모델에 특화되어 있으멀로, 기본 모델이 폐기되고 새로운 모델로 대체될 때 모든 관련 LoRA 모듈을 재훈련해야 합니다.
- 재훈련 시 기존 기본 모델을 위해 LoRA를 훈련시킨 데이터에 접근해야 하는데, 이는 상업용 클라우드 애플리케이션에서 문제가 될 수 있습니다.
- 이 문제에 대처하기 위해 우리는 Trans-LoRA라는 새롭고 혁신적인 방법을 제안합니다. 이는 기본 모델 간에 LoRA를 거의 데이터 없이 손실 없이 전송할 수 있습니다.
- 우리의 접근 방식은 합성 데이터를 사용하여 LoRA 모듈을 전송하며, 큰 언어 모델을 활용하여 관찰된 태스크 데이터 생성 과정을 근사하는 합성 데이터 생성기를 설계합니다.
- 합성 데이터셋에서의 훈련을 통해 LoRA 모듈을 새로운 모델로 전송하며, LLama 및 Gemma 모델 패밀리를 사용한 실험을 통해 우리의 접근 방식의 효과를 입증합니다.
- 이 접근법은 다양한 태스크에서 모델 패밀리 내외에 따른 LoRA 전송성을 개선시키며, 심지어는 다른 PEFT 방법 간에도 효과적임을 보여줍니다.

### [Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels](https://arxiv.org/abs/2405.16822)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.16822.png)

Vote: 8

Authors: Xinzhou Wang, Zilong Chen, Yikai Wang, Jun Zhu, Fuchun Sun, Zhengyi Wang

- 이 연구에서는 동영상 생성 모델을 사용하여 비정형성 및 레임 왜곡 문제를 해결하면서 4D(연속적 3D) 표현을 정확하게 재구성하는 새로운 모델 Vidu4D를 제시합니다.
- Vidu4D의 핵심 기술인 동적 가우스 서펠(Dynamic Gaussian Surfels, DGS)은 시간이 변화함에 따라 가우스 서펠(표면 요소)을 정적 상태에서 동적으로 변형된 상태로 변환시키는 시간 변화 와핑 함수를 최적화합니다.
- 이 변환은 시간에 따른 동작 및 변형을 정밀하게 묘사할 수 있게 하며, 연속적인 와핑 필드에 기반한 기하 정규화를 통해 구조적 무결성을 유지합니다.
- 또한, 가우스 서펠의 회전 및 스케일링 매개변수에 대한 세부 조정을 통해 질감 깜빡임을 크게 완화하고 세밀한 외형 디테일 캡처를 향상시킵니다.
- Vidu4D는 DGS의 와핑 필드를 위한 적절한 시작 상태를 제공하는 새로운 초기화 상태도 포함하고 있으며, 기존의 비디오 생성 모델과 결합하여 외형 및 기하학적으로 고품질의 텍스트-투-4D 생성을 보여줍니다.

### [EM Distillation for One-step Diffusion Models](https://arxiv.org/abs/2405.16852)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.16852.png)

Vote: 8

Authors: Sirui Xie, Zhisheng Xiao, Tim Salimans, Ruiqi Gao, Diederik P Kingma, Kevin Patrick Murphy, Tingbo Hou, Ying Nian Wu, Ben Poole

- 확산 모델이 복잡한 분포를 학습할 수 있지만, 샘플링에는 계산 비용이 많이 드는 반복 과정이 필요합니다.
- 기존의 증류 방법은 효율적인 샘플링을 가능하게 하지만, 아주 적은 샘플링 단계에서 성능 저하, 훈련 데이터 접근에 대한 의존성, 전체 분포를 포착하지 못하는 모드 탐색 최적화 등의 한계가 있습니다.
- 우리는 최대 우도 기반 접근법인 EM Distillation (EMD)을 제안하여 지각 품질의 최소 손실로 확산 모델을 한 단계 생성 모델로 증류합니다.
- 이 접근법은 기대값 최대화(EM)를 통해 파생되며, 생성기 매개변수는 확산 교사 사전과 추론된 생성기 라텐트의 공동 분푬에서 샘플을 사용하여 업데이트됩니다.
- 우리는 재매개변수화된 샘플링 방식과 노이즈 취소 기술을 개발하여 증류 과정을 안정화시킵니다.
- 또한, 모드 탐색 KL을 최소화하는 기존 방법들과의 흥미로운 연관성을 밝혀냈습니다.
- EMD는 ImageNet-64와 ImageNet-128에서 FID 점수 면에서 기존의 한 단계 생성 방법들을 능가하며, 텍스트-이미지 확산 모델을 증류하는 이전 연구와 유리하게 비교됩니다.

### [Part123: Part-aware 3D Reconstruction from a Single-view Image](https://arxiv.org/abs/2405.16888)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.16888.png)

Vote: 7

Authors: Ping Luo, Xiaoxiao Long, Anran Liu, Cheng Lin, Yuan Liu, Wenping Wang, Zhiyang Dou, Hao-Xiang Guo

- 최근 확산 모델의 등장은 단일 시점 이미지에서의 3D 재구성에 새로운 기회를 열었지만, 기존의 방법들은 구조적 정보가 결여된 닫힌 메시로 대상 객체를 나타내므로, 재구성된 형상의 부분 구조를 간과한다.
- 이 연구에서는 단일 시점 이미지로부터 부분 인식이 가능한 3D 재구성을 위해 Part123, 즉 새로운 프레임워크를 제시한다.
- Part123는 주어진 이미지에서 다시점 일관성 이미지를 생성하기 위해 확산 모델을 사용하고, 임의 객체에 강력한 일반화 능력을 보여주는 Segment Anything Model (SAM)을 사용하여 다시점 분할 마스크를 생성한다.
- 2D 부분 기반 정보를 3D 재구성에 효과적으로 통합하고 불일치를 처리하기 위해, 다시점 분할 마스크를 기반으로 한 부분 인식 기능 공간을 학습하기 위해 신경 렌더링 프레임워크에 대조 학습을 도입한다.
- 재구성된 모델로부터 자동으로 3D 부분 분할 결과를 도출하기 위해 클러스터링 기반 알고리즘을 개발했다.
- 실험 결과, 우리의 방법은 다양한 객체에 대해 고품질의 분할된 부분을 가진 3D 모델을 생성할 수 있으멀로 특성 보존 재구성, 기본 형태 피팅 및 3D 형상 편집과 같은 중요한 응용 프로그램에 도움이 될 수 있다.

### [Human4DiT: Free-view Human Video Generation with 4D Diffusion Transformer](https://arxiv.org/abs/2405.17405)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.17405.png)

Vote: 7

Authors: Jingxiang Sun, Zerong Zheng, Youxin Pang, Ruizhi Shao, Yebin Liu

- 단일 이미지에서 임의의 관점 하에 시공간적으로 일관된 고품질 인간 비디오를 생성하기 위한 새로운 접근 방식을 제시합니다.
- 이 프레임워크는 정확한 조건 주입을 위한 U-Nets와 시간 및 관점 전반에 걸친 글로벌 상관 관계를 포착하는 확산 트랜스포머의 강점을 결합합니다.
- 핵심은 관점, 시간 및 공간 차원을 효율적으로 모델링할 수 있도록 주의를 분화하는 캐스케이드 4D 트랜스포머 아키텍처입니다.
- 정밀 조건 설정은 인간의 정체성, 카메라 매개변수, 그리고 시간적 신호를 해당 트랜스포머에 주입함으로써 달성됩니다.
- 복잡한 움직임과 시점 변경에 어려움을 겪는 기존의 GAN이나 UNet 기반 확산 모델의 한계를 극복합니다.
- 광범위한 실험을 통해, 우리의 방법이 실제적이고 일관되며 자유시점 인간 비디오를 합성할 수 있는 능력을 입증하였습니다.
- 가상 현실과 애니메이션과 같은 고급 멀티미디어 응용 분야에 사용될 수 있는 길을 열어줍니다.

### [LoGAH: Predicting 774-Million-Parameter Transformers using Graph HyperNetworks with 1/100 Parameters](https://arxiv.org/abs/2405.16287)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.16287.png)

Vote: 6

Authors: Jie Fu, Alexia Jolicoeur-Martineau, Xinyu Zhou, Boris Knyazev

- 심층 학습 모델의 초기화는 더 나은 수렴과 더 빠른 수렴을 돕기 때문에 중요하며, 대형 모델의 사전 학습은 많은 연구자들에게 비실용적이기 때문에 초기 매개변수에 대한 예측이 요구됩니다.
- Graph HyperNetworks(GHNs)는 대규모 비전 모델 초기화에서 강력한 성능을 보였지만, 매우 넓은 네트워크의 매개변수를 예측하는 것은 매개변수의 작은 부분을 여러 번 복사하고 매우 많은 수의 매개변수를 필요로 하여 실제 채택을 방해합니다. 
- 이러한 한계를 극복하기 위해, 우리는 LoGAH(Low-rank GrAph Hypernetworks)를 제안하는데, 이는 낮은 순위 매개변수 디코더를 가진 GHN으로, 이전 시도보다 많은 매개변수 증가 없이 훨씬 더 넓은 네트워크로 확장할 수 있습니다. 
- LoGAH을 사용하면 774백만 개의 대형 신경망의 매개변서를 메모리 효율적으로 예측할 수 있습니다.
- 비전 및 언어 모델(ViT와 GPT-2)은 LoGAH으로 초기화될 때 임의로 또는 기존 하이퍼네트워크를 사용하여 초기화된 모델보다 더 나은 성능을 보여줍니다. 
- 또한 소규모 데이터셋에서 LoGAH을 학습하고 예측된 매개변수를 사용하여 더 큰 작업을 위한 초기화로 활용하는 전이 학습 결과도 유망합니다.
- 코드는 https://github.com/Blackzxy/LoGAH 에서 제공됩니다.

### [Greedy Growing Enables High-Resolution Pixel-Based Diffusion Models](https://arxiv.org/abs/2405.16759)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.16759.png)

Vote: 5

Authors: Hongliang Fei, Keyang Xu, Abdullah Rashwan Austin Waters, Sarah Rosston, Rui Qian, Wenlei Zhou, Jimmy Yan, Mandy Guo, Zarana Parekh, Roopal Garg, Jordi Pont-Tuset, Kevin Swersky, Cristina N. Vasconcelos, Ivana Kajic, David J. Fleet, Trevor Walker, Shixin Luo, +, Yeqing Li, Andrew Bunner, Yasumasa Onoe, Su Wang, Henna Nandwani

- 이 연구는 대규모 고해상도 이미지 확산 모델을 안정적으로 학습할 수 있는 간단한 탐욕적 성장 방법을 제안하여 효과적인 픽셀 기반 이미지 확산 모델의 오래된 문제를 다룹니다.
- 핵심적인 통찰은 텍스트에서 이미지로의 정렬과 고해상도 렌더링을 담당하는 핵심 구성 요소의 신중한 사전 학습에 기반을 두고 있습니다.
- 'Shallow UNet'의 스케일링을 통해 정렬, 객체 구조, 그리고 구성의 향상을 보여줍니다.
- 이 핵심 모델을 바탕으로, 아키텍처를 고해상도 엔드투엔드 모델로 성장시키는 탐욕적 알고리즘을 제안하며, 이는 사전 훈련된 표현의 무결성을 보존하고, 훈련을 안정화시키며, 대규모 고해상도 데이터셋의 필요성을 줄입니다.
- 이를 통해 슈퍼 해상도 캐스케이드 없이 고해상도 이미지를 생성할 수 있는 단일 단계 모델을 가능하게 합니다.
- 공개 데이터셋을 활용한 주요 결과는 추가 규제 체계 없이 최대 8B 매개변수의 비캐스케이드 모델을 훈련할 수 있음을 보여줍니다.
- 내부 데이터셋으로 훈련된 전체 파이프라인 모델인 Vermeer는 1024x1024 이미지를 캐스케이드 없이 생성하며, 44.0% 대 21.4%의 비율로 인간 평가자에게 선호됩니다.

### [Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control](https://arxiv.org/abs/2405.17414)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.17414.png)

Vote: 5

Authors: Zhengfei Kuang, Hao He, Shengqu Cai, Yinghao Xu, Leonidas Guibas, Gordon Wetzstein, Hongsheng Li

- 최근 텍스트 프롬프트나 이미지에서 고품질 비디오를 생성하는 비디오 생성 연구가 크게 발전하였습니다.
- 이러한 비디오 제어 과정에 대한 조정은 비디오 생성 모델을 카메라 궤적에 조건을 부과함으로써 진전을 이루었습니다.
- 그러나 여러 카메라 궤적에서 동일한 장면의 비디오를 생성하는 것은 여전히 도전적인 과제입니다.
- 본 연구에서는 대규모 3D 장면 생성과 편집 가능한 카메라 궤적 등의 응용을 가능하게 하는 다중 비디오 생성 문제에 대한 해결책을 제시합니다.
- 우리는 이 비전을 향한 중요한 단계로 협업 비디오 확산(Collaborative Video Diffusion, CVD)을 도입합니다.
- CVD 프레임워크는 에피폴라 주의 메커니즘을 사용하여 다른 카메라 포즈에서 렌더링된 동일 비디오의 해당 프레임 간 일관성을 증진하는 새로운 크로스-비디오 동기화 모듈을 포함합니다.
- 최첨단 카메라 제어 모듈 위에서 훈련된 CVD는 기존의 기준보다 훨씬 더 일관된 여러 비디오를 생성하며, 이는 광범위한 실험에서 입증되었습니다.
- 프로젝트 페이지: [https://collaborativevideodiffusion.github.io/](https://collaborativevideodiffusion.github.io/).

