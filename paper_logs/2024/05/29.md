## Daily Papers (2024-05-29)

### [Phased Consistency Model](https://arxiv.org/abs/2405.18407)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.18407.png)

Vote: 24

Authors: Keqiang Sun, Weikang Bian, Xiaogang Wang, Michael Lingelbach, Zhaoyang Huang, Dazhong Shen, Guanglu Song, Yu Liu, Alexander William Bergman, Hongsheng Li, Peng Gao, Fu-Yun Wang

- 최근 일관성 모델(CM)이 확산 모델 생성을 가속화하는 데 큰 진전을 이루었지만, 고해상도 텍스트 조건부 이미지 생성을 위한 잠재 공간에서의 적용(LCM)은 아직 불만족스럽습니다.
- 본 논문에서는 LCM의 현재 설계에서 발견된 세 가지 주요 결점을 확인하고 이러한 한계의 원인을 조사합니다.
- 이러한 한계를 해결하기 위해 Phased Consistency Model (PCM)을 제안하여 설계 공간을 일반화하고 모든 확인된 제한을 해결합니다.
- 평가 결과, PCM은 1단계에서 16단계 생성 설정에서 LCM보다 현저하게 우수한 성능을 보여줍니다.
- 다단계 정제에 특별히 설계된 PCM은 이전의 최고의 1단계 방법과 비교해도 유사하거나 우수한 1단계 생성 결과를 달성합니다.
- 또한, PCM의 방법론은 비디오 생성에도 적용 가능하며, 몇 단계의 텍스트-투-비디오 생성기를 훈련시켜 최신 기술을 가능하게 합니다.
- 자세한 내용은 https://g-u-n.github.io/projects/pcm/에서 확인할 수 있습니다.

### [2BP: 2-Stage Backpropagation](https://arxiv.org/abs/2405.18047)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.18047.png)

Vote: 12

Authors: Joseph K. L. Lee, James Richings, Christopher Rae

- 딥 뉴럴 네트워크(DNN)가 커짐에 따라 단일 가속기의 메모리 용량을 초과하여 모델 파라미터를 여러 가속기에 분산시켜야 하는 경우가 많다.
- 파이프라인 병렬 처리는 대규모 DNN을 훈련시키기 위한 일반적인 샤딩 전략으로 사용되지만, 기존의 자동 미분 도구로 인해 비효율적인 병목 현상이 발생한다.
- 이 논문에서는 역전파 단계를 두 개의 별도 단계로 나누어 계산 시간의 유휴 상태를 감소시키는 2-stage backpropagation(2BP)을 도입한다.
- 다양한 모델 구조와 파이프라이닝 스케줄에서 2BP를 테스트하였으며, 모든 경우에서 처리량이 증가하였다.
- 특히, 4개의 GPU에서 70억 개의 파라미터를 가진 LLaMa와 유사한 트랜스포머를 훈련할 때 전통적인 방법에 비해 1.70배의 처리량 증가를 달성하였다.

### [GFlow: Recovering 4D World from Monocular Video](https://arxiv.org/abs/2405.18426)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.18426.png)

Vote: 9

Authors: Zhenxiang Jiang, Xinchao Wang, Qiuhong Shen, Xingyi Yang, Shizun Wang

- 단일 카메라 비디오만 사용하여 동적인 4D 세계와 카메라 위치를 복원하는 'AnyV4D'라는 새로운 과제를 해결하기 위해 본 논문에서는 GFlow라는 새로운 프레임워크를 제안합니다.
- GFlow는 2D 사전 정보(깊이와 광학 흐름)만을 사용하여 비디오(3D)를 시간과 공간을 관통하는 가우스 스플래팅 흐름의 명시적인 4D 표현으로 확장합니다.
- 이 프레임워크는 먼저 장면을 고정 부분과 움직이는 부분으로 클러스터링 한 다음, 2D 사전 정보와 장면 클러스터링에 기반한 카메라 위치와 3D 가우스 점의 동역학을 최적화하는 순차적 최적화 과정을 적용합니다.
- 동적인 장면에서 새로운 시각적 내용을 통합하기 위해 새로운 픽셀 단위 밀도화 전략도 제안됩니다.
- GFlow는 단순한 4D 재구성을 넘어, 이전 교육 없이 프레임 간 점을 추적하고 장면에서 움직이는 객체를 비지도 방식으로 분리할 수 있습니다.
- 또한, 각 프레임의 카메라 위치를 GFlow로부터 파생시켜 비디오 장면의 새로운 시점을 렌더링할 수 있습니다.
- 명시적 표현을 사용함으로써, 원하는 대로 장면 레벨 또는 객체 레벨 편집을 자유롭게 수행할 수 있어, 이 기술의 다양성과 강력함을 강조합니다.
- 자세한 내용은 프로젝트 웹사이트(https://littlepure2333.github.io/GFlow)를 방문하시기 바랍니다.

### [Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning](https://arxiv.org/abs/2405.18386)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.18386.png)

Vote: 7

Authors: Yukara Ikemiya, Yuki Mitsufuji, Woosung Choi, Yixiao Zhang, Wei-Hsiang Liao, Marco A. Martínez-Ramírez, Naoki Murata, Liwei Lin, Simon Dixon, Gus Xia

- 최근 텍스트 쿼리를 사용하여 음악의 스타일을 변경하거나 악기 구성을 조정하는 등의 작업을 가능하게 하는 텍스트-투-뮤직 편집 기술이 AI 지원 음악 창작에서 독특한 과제와 기회를 제공하고 있습니다.
- 그동안의 접근 방식은 특정 편집 모델을 처음부터 훈련시켜야 하는 제약이 있었고, 이는 자원 집약적이고 비효율적이었습니다; 또 다른 연구는 큰 언어 모델을 사용하여 편집된 음악을 예측하지만, 이 방법은 정밀한 오디오 재구성에서 부정확하였습니다.
- 이러한 한계를 극복하고 강점을 결합하기 위해, 우리는 기존의 MusicGen 모델을 세밀하게 조정하여 추가, 제거 또는 분리와 같은 편집 지시를 효율적으로 따를 수 있는 새로운 방법인 Instruct-MusicGen을 소개합니다.
- 저희의 접근 방식에는 원래 MusicGen 아키텍처를 수정하여 텍스트 융합 모듈과 오디오 융합 모듈을 추가하는 것이 포함되어, 모델이 지시 텍스트와 오디오 입력을 동시에 처리하여 원하는 편집된 음악을 생성할 수 있도록 합니다.
- 놀랍게도, Instruct-MusicGen은 원래 MusicGen 모델에 새로운 파라미터를 8%만 추가하고 5K 스텝만 훈련하면서도, 기존 기준보다 모든 작업에서 우수한 성능을 달성하고 특정 작업을 위해 훈련된 모델과 비교할 수 있는 성능을 보여줍니다.
- 이러한 발전은 텍스트-투-뮤직 편집의 효율성을 크게 향상시킬 뿐만 아니라 동적인 음악 제작 환경에서 음악 언어 모델의 적용 가능성을 확장합니다.

### [LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models](https://arxiv.org/abs/2405.18377)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.18377.png)

Vote: 6

Authors: Sharath Nittur Sridhar, Sairam Sundaresan, Maciej Szankin, Anthony Sarah

- 현대 대규모 언어 모델(LLM)은 자연어 처리, 복잡한 추론, 감정 분석 등의 분야에서 뛰어난 성능을 보이며 널리 사용되고 있으나, 높은 메모리 및 계산 비용이 요구됩니다.
- 이를 해결하기 위해, LLaMA2-7B를 기반으로 한 번의 파인튜닝 후 유전 알고리즘 기반 탐색을 통해 더 작고 계산 복잡성이 낮은 네트워크 구조를 찾는 효과적인 방법을 제안합니다.
- 특정 벤치마크 작업에서 기존 LLaMA2-7B 네트워크가 과도하게 크고 복잡하다는 것을 보여주고, 모델 크기를 1.5배 줄이고 처리 속도를 1.3배 향상시키면서 정확도 저하는 미미하게 유지합니다.
- 또한, 이 방범은 일부 프루닝이나 희소화 기술보다 효과적이고 효율적으로 더 성능이 뛰어난 네트워크 구조를 찾을 수 있음을 입증합니다.
- 양자화를 통해 네트워크 크기와 복잡성을 더 줄일 수 있는데, 이는 우리의 방법과 보완적으로 작용합니다.
- 결론적으로, 이 연구는 비용이 적게 들고 보다 쉽게 구할 수 있는 하드웨어 플랫폼에서 사용할 수 있는 LLM을 자동으로 생성하는 방법을 제공합니다.

### [VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections](https://arxiv.org/abs/2405.17991)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.17991.png)

Vote: 6

Authors: Roy Miles, Ismail Elezi, Jiankang Deng, Pradyumna Reddy

- 큰 언어 모델(Large language models, LLMs)은 다양한 언어 처리 작업에 효과적인 도구로 부상했지만, 이러한 모델들을 훈련하고 미세 조정하는 것은 여전히 많은 계산 및 메모리 자원을 요구합니다.
- 본 논문에서는 기울기 하강법을 사용하여 모델 수렴을 효과적으로 달성하기 위한 중요한 구성 요소를 식별하고 설명합니다.
- 연구 결과에 따르면, 역전파를 구현하는 중간 활성화를 과도하게 압축하여도 성능 저하가 발생하지 않는 것으로 나타났습니다.
- 이러한 결과를 바탕으로, 토큰을 더 작은 하위 토큰으로 나누어 전방 패스 중에 고정된 1차원 부공간으로 투영하는 비용 효율적이고 메모리 효율적인 알고리즘을 제안합니다.
- 제안된 알고리즘은 업데이트 규칙을 구현하기 위해 역방향 패스 중에 이러한 특징들을 대략적으로 재구성합니다.
- VTAB-1k 미세 조정 벤치마크에서 다양한 최신 PEFT 방법들과 상호 보완적인 효과를 지닌 알고리즘의 유효성을 확인했습니다.
- 또한, LLaMA를 미세 조정하는 데 있어서 QLoRA를 능가하고, 대규모 C4 데이터셋에서 다른 메모리 효율적인 사전 훈련 방법들과 경쟁력 있는 성능을 보여주었습니다.

### [Yuan 2.0-M32: Mixture of Experts with Attention Router](https://arxiv.org/abs/2405.17976)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.17976.png)

Vote: 6

Authors: Lingjun Li, Fei Wang, Houbo He, Zeyu Sun, Zeru Zhang, Jiangang Luo, Xi Chen, Xudong Zhao, Chao Wang, Yue Wang, Weixu Qiao, Shaohua Wu, Junxiong Mao, Tong Yu, Chong Shen

- Yuan 2.0-M32은 Yuan-2.0 2B와 유사한 기본 아키텍처를 사용하며, 32개의 전문가 중 2개의 활성 전문가가 있는 전문가 혼합 아키텍처를 사용합니다.
- 새로운 라우터 네트워크인 Attention Router를 제안하고 도입하여, 전문가의 선택을 더 효율적으로 하여 정확도를 기존 라우터 네트워크보다 3.8% 향상시켰습니다.
- Yuan 2.0-M32는 2000B 토큰을 사용해 처음부터 훈련되었으며, 동일한 파라미터 규모의 밀집 모델 대비 훈련 계산 소비는 단지 9.25%에 불과합니다.
- 총 40B 중 활성화 파라미터 3.7B를 사용하며, 토큰 당 전방 계산은 7.4 GFlops에 불과하고, 이는 Llama3-70B의 1/19에 해당합니다.
- Yuan 2.0-M32은 코딘그, 수학, 다양한 전문 분야에서 경쟁력을 보여주며, MATH 및 ARC-Challenge 벤치마크에서 Llama3-70B를 초과하는 55.89 및 95.8의 정확도를 달성했습니다.
- Yuan 2.0-M24의 모델과 소스 코드는 Github에서 공개되었습니다.

### [3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting](https://arxiv.org/abs/2405.18424)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.18424.png)

Vote: 1

Authors: Gordon Wetzstein, Qihang Zhang, Yinghao Xu, Ceyuan Yang, Hsin-Ying Lee, Bolei Zhou, Chaoyang Wang

- 엔터테인먼트, 사진촬영, 광고 디자인 분야에서 중요한 요소인 장면 이미지 편집에 관한 논문입니다.
- 기존 방식은 2D 개별 객체 편집이나 3D 전체 장면 편집에만 초점을 맞추어 3D 수준의 다양한 세부 조정을 통합적으로 제어하는데 한계가 있었습니다.
- 이 연구에서는 언어 가이드를 통한 분리된 가우스 튀김을 활용한 새로운 통합 장면 편집 프레임워크인 3DitScene을 제안합니다.
- 3D 가우시안을 사용하여 생성 머신러닝 사전 지식과 최적화 기법을 통해 세밀하게 조정되며, CLIP의 언어 기능을 통해 3D 기하학에 의미론적 정보를 추가합니다.
- 이 분리된 가우스 기법은 개별 객체와 전체 장면을 모두 조작할 수 있도록 해, 창조적 표현을 혁신하고 장면 및 객체에 대한 통제력을 강화합니다.
- 3DitScene의 효과성과 다양성은 실험 결과를 통해 입증되었으며, 관련 코드와 온라인 데모는 프로젝트 홈페이지에서 확인할 수 있습니다.

