## Daily Papers (2024-05-17)

### [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/abs/2405.09818)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.09818.png)

Vote: 43

Authors: Chameleon Team

- 'Chameleon'이라는 새로운 초창기 융합 토큰 기반 혼합 모달 모델을 소개하며, 이미지와 텍스트를 임의의 순서로 이해하고 생성할 수 있는 능력을 제공합니다.
- 안정적인 훈련 접근법, 정렬 요리법, 그리고 초기 융합, 토큰 기반 혼합 모달 환경을 위한 아키텍처 매개변수화를 제시합니다.
- 시각적 질문 응답, 이미지 자막, 텍스트 생성, 이미지 생성, 그리고 장형 혼합 모달 생성을 포함한 다양한 작업에서 모델을 평가합니다.
- Chameleon은 이미지 자막 작업에서 최첨단 성능을 보이며, 텍스트 전용 작업에서는 Llama-2를 능가하고 Mixtral 8x7B 및 Gemini-Pro와 같은 모델과 경쟁합니다.
- 단일 모델에서 비상식적인 이미지 생성을 수행하며, Gemini Pro 및 GPT-4V와 같은 더 큰 모델의 성능에 맞추거나 능가합니다.
- 새로운 장형 혼합 모달 생성 평가에서, 텍스트와 이미지가 혼합된 프롬프트나 출력을 포함할 경우, 인간 판단에 따라 성능이 매우 뛰어납니다.
- Chameleon은 전체 다중 모달 문서를 통합 모델링하는 중대한 진전을 표시합니다.

### [LoRA Learns Less and Forgets Less](https://arxiv.org/abs/2405.09673)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.09673.png)

Vote: 25

Authors: Jonathan Frankle, Mansheej Paul, Connor Jennings, Cody Blakeney, Philip Greengard, Sam Havens, Dan Biderman, John P. Cunningham, Jose Gonzalez Ortiz, Jacob Portes, Vitaliy Chiley, Daniel King

- Low-Rank Adaptation(LoRA)은 대규모 언어 모델의 파라미터 효율적인 파인튜닝 방법으로, 선택된 가중치 행렬에 대한 낮은 순위의 변형만을 학습하여 메모리를 절약합니다.
- 프로그래밍 및 수학이라는 두 대상 도메인에서 LoRA와 전체 파인튜닝의 성능을 비교했으며, 지시 파인튜닝(약 100K 프롬프트-응답 쌍) 및 지속적인 사전 학습(약 10B 비구조화 토큰) 데이터 체제에서 평가하였습니다.
- 대부분의 설정에서 LoRA는 전체 파인튜닝에 비해 상당히 낮은 성능을 보였으나, 기존 도메인 외 작업에 대한 기본 모델의 성능을 더 잘 유지하는 바람직한 정규화 형태를 보여줍니다.
- LoRA는 가중치 감소나 드롭아웃과 같은 일반적인 기법들보다 더 강한 정규화를 제공하며, 더 다양한 생성을 유지하는 데 도움을 줍니다.
- 전체 파인튜닝이 LoRA 설정보다 10-100배 더 큰 순위의 변형을 학습한다는 것을 밝혀냈으며, 이는 보고된 격차의 일부를 설명할 수 있습니다.
- LoRA를 사용한 파인튜닝에 대한 최선의 사례를 제안함으로써 결론을 내립니다.

### [Many-Shot In-Context Learning in Multimodal Foundation Models](https://arxiv.org/abs/2405.09798)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.09798.png)

Vote: 21

Authors: Ji Hun Wang, Muhammad Ahmed Chaudhry, Yixing Jiang, Jeremy Irvin, Andrew Y. Ng, Jonathan H. Chen

- 이 연구는 여러 예시를 보여주는 인-컨텍스트 학습(ICL)의 향상된 성능을 평가하기 위해 멀티모달 기반 모델이 확장되는 것을 탐구합니다.
- GPT-4o 및 Gemini 1.5 Pro 모델을 자연 이미지, 의료 이미지, 원격 감지 및 분자 이미지를 포함하는 여러 도메인과 작업에서 벤치마킹합니다.
- 수백 개의 예시를 사용한 많은-샷 ICL이 100개 미만의 예시를 사용한 적은-샷 ICL보다 모든 데이터세트에서 현저하게 개선된다는 것을 발견했습니다.
- 특히 Gemini 1.5 Pro는 테스트된 예시의 최대 수에 도달할 때까지 많은 데이터세트에서 로그 선형적으로 성능이 향상됩니다.
- 많은-샷 ICL을 위해 요구되는 긴 프롬프트에 따른 높은 추론 비용을 고려하여, 이 연구는 하나의 API 호출에서 여러 쿼리를 배치하는 것의 영향을 탐구합니다.
- 배치는 여러 데이터세트에서 제로-샷 및 많은-샷 ICL에 대한 성능 향상을 가능하게 하며, 쿼리당 비용과 지연을 크게 줄입니다.
- 또한, 모델이 보여주는 예시에서 얼마나 효과적으로 학습하는지를 측정하여 GPT-4o와 Gemini 1.5 Pro가 대부분의 데이터세트에서 제로-샷 성능이 유사하지만, Gemini 1.5 Pro가 대부분의 데이터세트에서 더 높은 ICL 데이터 효율성을 보여줍니다.
- 이 결과들은 사용자가 멀티모달 기반 모델을 새로운 응용 프로그램과 도메인에 효율적으로 적용할 수 있게 하는 많은-샷 ICL의 가능성을 제시합니다.

### [CAT3D: Create Anything in 3D with Multi-View Diffusion Models](https://arxiv.org/abs/2405.10314)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.10314.png)

Vote: 18

Authors: Jonathan T. Barron, Pratul Srinivasan, Philipp Henzler, Ruiqi Gao, Ben Poole, Ricardo Martin-Brualla, Aleksander Holynski, Arthur Brussee

- CAT3D는 3D 재구성의 발전을 바탕으로 수백에서 수천 개의 이미지 수집을 필요로 하는 기존 3D 캡처 과정을 모방하는 다시점 확산 모델을 사용하여 모든 사물을 3D로 생성하는 방법을 제시합니다.
- 사용자는 어떠한 수의 입력 이미지와 목표 새로운 시점의 집합을 제공함으로써, 모델은 일관된 새로운 관점의 장면을 생성할 수 있습니다.
- 생성된 뷰는 강력한 3D 재구성 기술에 입력으로 사용되어, 실시간으로 어떤 관점에서도 렌더링할 수 있는 3D 표현을 생성할 수 있습니다.
- CAT3D는 단 1분 내에 전체 3D 장면을 생성할 수 있으며, 단일 이미지 및 소수 뷰를 사용한 3D 장면 생성에 있어 기존 방법들을 능가합니다.
- 프로젝트 결과와 인터랙티브 데모는 https://cat3d.github.io 에서 확인할 수 있습니다.

### [Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection](https://arxiv.org/abs/2405.10300)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.10300.png)

Vote: 12

Authors: Qing Jiang, Zhaoyang Zeng, Kent Yu, Yihao Chen, Han Gao, Lei Zhang, Hongjie Huang, Wenlong Liu, Hao Zhang, Yuda Xiong, Feng Li, Peijun Tang, Zhengyu Ma, Shilong Liu, Tianhe Ren, Xiaoke Jiang

- 이 논문은 IDEA Research에서 개발한 고급 오픈셋 객체 검출 모델인 Grounding DINO 1.5를 소개하며, 오픈셋 객체 검출의 "Edge"를 발전시키는 것을 목표로 합니다.
- Grounding DINO 1.5는 두 가지 모델을 포함하는데, 다양한 시나리오에서 더 강한 일반화 능력을 설계한 고성능 모델인 Grounding DINO 1.5 Pro와, 엣지 배포가 요구되는 많은 애플리케이션에서 빠른 속도를 최적화한 효율적인 모델인 Grounding DINO 1.5 Edge로 구성됩니다.
- Grounding DINO 1.5 Pro는 모델 아키텍처를 확장하고, 향상된 비전 백본을 통합하며, 20백만 개 이상의 영상을 포함하는 광범위한 훈련 데이터셋을 확장하여 더 풍부한 의미론적 이해를 달성합니다.
- Grounding DINO 1.5 Edge는 기능 스케일을 축소하여 설계되었음에도 불구하고 같은 포괄적인 데이터셋에서 훈련되어 강력한 검출 능력을 유지합니다.
- 실증적 결과에 따르면, Grounding DINO 1.5 Pro는 COCO 검출 벤치마크에서 54.3 AP, LVIS-minival 제로샷 전환 벤치마크에서 55.7 AP를 달성하여 오픈셋 객체 검출에 대한 새로운 기록을 세웠습니다.
- 또한, TensorRT로 최적화된 Grounding DINO 1.5 Edge 모델은 LVIS-minival 벤치마크에서 제로샷 성능 36.2 AP, 75.2 FPS의 속도를 달성하여 엣지 컴퓨팅 시나리오에 더 적합합니다.
- 모델 예시와 데모는 https://github.com/IDEA-Research/Grounding-DINO-1.5-API에서 제공될 예정입니다.

### [Dual3D: Efficient and Consistent Text-to-3D Generation with Dual-mode Multi-view Latent Diffusion](https://arxiv.org/abs/2405.09874)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.09874.png)

Vote: 10

Authors: Linning Xu, Shengchuan Zhang, Bo Dai, Liujuan Cao, Xinyang Li, Jianfei Guo, Rongrong Ji, Zhangyu Lai

- Dual3D는 텍스트로부터 고품질 3D 자산을 단 1분 내에 생성하는 새로운 프레임워크입니다.
- 주요 구성 요소는 이중 모드 멀티뷰 잠재 확산 모델로, 2D 모드는 단일 잠재 덴노이징 네트워크로 잡음이 많은 멀티뷰 잠재 변수를 효율적으로 정제합니다.
- 3D 모드는 일관된 렌더링 기반 덴노이징을 위해 트라이-플레인 신경 표면을 생성합니다.
- 이미 훈련된 텍스트-투-이미지 잠재 확산 모델에서 모듈을 조정하여 처음부터 훈련하는 데 드는 비용을 피합니다.
- 추론 중 높은 렌더링 비용을 극복하기 위해, 3D 모드에서 덴노이징 단계를 1/10만 사용하는 이중 모드 전환 추론 전략을 제안합니다.
- 이 전략을 사용함으로써 품질을 저하시키지 않으면서 단 10초 만에 3D 자산을 성공적으로 생성할 수 있습니다.
- 3D 자산의 텍스처는 짧은 시간 동안 효율적인 텍스처 정제 과정을 통해 추가로 향상될 수 있습니다.
- 광범위한 실험을 통해, 우리의 방법은 상태-의-예술 성능을 제공하며 생성 시간을 크게 줄이는 것을 입증합니다.
- 프로젝트 페이지는 https://dual3d.github.io 에서 확인할 수 있습니다.

### [Toon3D: Seeing Cartoons from a New Perspective](https://arxiv.org/abs/2405.10320)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.10320.png)

Vote: 8

Authors: Angjoo Kanazawa, Rohan Mathur, Frederik Warburg, Alexei A. Efros, Riley Peterlinz, Ethan Weber

- 이 연구에서는 기하학적으로 일관성이 없는 장면의 기본적인 3D 구조를 복구합니다.
- 주로 만화와 애니메이션에서 손으로 그린 이미지를 분석합니다. 이러한 만화는 대부분 아티스트가 3D 렌더링 엔진 없이 창작하므로, 새로운 장면 이미지도 손으로 그려집니다.
- 손으로 그린 이미지들은 대체로 세계의 신뢰할 수 있는 표현이지만, 사람이 객체나 장면을 3D로 일관성 있게 다양한 관점에서 그리기는 어렵습니다.
- 그럼에도 불구하고, 사람들은 일관성 없는 입력으로부터 3D 장면을 쉽게 인식할 수 있습니다!
- 이 연구에서는 2D 그림의 불일치를 수정하여 그림들이 서로 일관되게 보이도록 신뢰할 수 있는 3D 구조를 복구합니다.
- 저희의 방법론은 사용자 친화적인 주석 도구, 카메라 포즈 추정, 이미지 변형을 포함하여 밀도 높은 구조를 복구합니다.
- 저희 방법은 이미지를 관점 카메라 모델을 따르도록 변형시켜, 이전에 그려지지 않은 관점에서 만화를 체험할 수 있도록 신규 시점 합성 재구성 방법에 연결할 수 있는 정렬된 결과를 가능하게 합니다.
- 프로젝트 페이지는 https://toon3d.studio/에서 확인하실 수 있습니다.

### [TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction](https://arxiv.org/abs/2405.10315)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.10315.png)

Vote: 6

Authors: Li Fei-Fei, Chen Wang, Ruohan Zhang, Yunfan Jiang, Jiajun Wu

- 시뮬레이션에서 학습한 정책을 실제 세계로 전환하는 것은 범용 로봇을 가능하게 할 수 있는 잠재력을 가지고 있습니다.
- 이 접근법의 주요 도전 과제는 시뮬레이션과 실제간의 간극(sim-to-real gaps)을 해결하는 것입니다.
- 기존 방법들은 종종 사전에 도메인 특정 지식을 필요로 하지만, TRANSIC은 실제 세계에서 로봇 정책 실행을 관찰하고 돕는 인간의 개입을 통해 이러한 지식을 획득할 수 있는 직관적인 방법을 제안합니다.
- TRANSIC은 인간이 개입하고 실시간 수정을 통해 다양한 시뮬레이션 간극을 극복할 수 있도록 시뮬레이션 정책을 보완하는 데이터 기반 접근 방식을 사용합니다.
- 인간의 수정으로부터 학습한 잔여 정책은 시뮬레이션 정책과 통합되어 자율적 실행을 위해 사용될 수 있습니다.
- 이 방법은 가구 조립과 같이 복잡하고 접촉이 많은 조작 작업에서 성공적인 실제로의 전환을 달성할 수 있음을 보여줍니다.
- TRANSIC은 시뮬레이션에서 학습한 정책과 인간으로부터 학습한 정책의 융합을 통해 다양한 심투리얼 간극을 효과적으로 해결하는 포괄적인 접근 방식입니다.
- 또한 인간 노력과 함께 확장될 수 있는 매력적인 특성을 보여줍니다. 관련 비디오와 코드는 https://transic-robot.github.io/에서 확인할 수 있습니다.

