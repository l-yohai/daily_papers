## Daily Papers (2024-05-20)

### [INDUS: Effective and Efficient Language Models for Scientific Applications](https://arxiv.org/abs/2405.10725)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.10725.png)

Vote: 6

Authors: Aashka Trivedi, Megan Ansdel, Iksha Gurung, Mike Little, Sylvain Costes, Kelly Lockhart, Muthukumaran Ramasubramanian, Rong Zhang, Masayasu Muraoka, Alberto Accomazzi, Takuma Udagawa, Rahul Ramachandran, Felix Grazes, Lauren Sanders, Bishwaranjan Bhattacharjee, Kayleen Bugbee, Elizabeth Fancher, Sergi Blanco-Cuaresma, Yousef El-Kurdi, +, Manil Maskey, Thomas Allen, Bharath Dandala

- 일반 영역 코퍼스로 훈련된 대규모 언어 모델(LLMs)이 자연 언어 처리(NLP) 작업에서 놀라운 결과를 보였습니다.
- 특정 영역에 초점을 맞춘 코퍼스를 사용하여 훈련된 LLM들이 전문화된 작업에서 더 우수한 성능을 보인다는 선행 연구에 영감을 받아 INDUS가 개발되었습니다.
- INDUS는 지구 과학, 생물학, 물리학, 태양물리학, 행성 과학 및 천체 물리학 영역을 위해 맞춤화된 LLM들의 포괄적인 제품군으로 다양한 데이터 소스에서 추출한 정제된 과학 코퍼스를 사용하여 훈련되었습니다.
- 이 모델 제품군에는 자연 언어 이해 작업을 위해 도메인 특정 어휘와 코퍼스를 사용하여 훈련된 인코더 모델, 정보 검색 작업을 위해 다양한 데이터 세트를 사용하여 훈련된 대조 학습 기반 일반 텍스트 임베딩 모델, 지연이나 자원 제한이 있는 어플리케이션을 위한 더 작은 모델 버전들이 포함됩니다.
- 또한, 다학제 분야의 연구를 촉진하기 위해 CLIMATE-CHANGE-NER(개체 인식), NASA-QA(추출형 QA), NASA-IR(정보 검색)과 같은 새로운 과학 벤치마크 데이터셋 세 개를 생성했습니다.
- 마지막으로, 우리의 모델들이 기존 벤치마크 작업뿐만 아니라 새로운 작업에서 일반적인 목적의 인코더(RoBERTa)와 기존 도메인 특화 인코더(SciBERT)를 모두 능가하는 성능을 보였다는 것을 보여줍니다.

### [Grounded 3D-LLM with Referent Tokens](https://arxiv.org/abs/2405.10370)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.10370.png)

Vote: 4

Authors: Haifeng Huang, Runsen Xu, Dahua Lin, Jiangmiao Pang, Tai Wang, Yilun Chen, Shuai Yang, Ruiyuan Lyu

- 이 연구에서는 3D 대형 멀티모달 모델(3D LMMs)의 가능성을 탐구하여 다양한 3D 비전 작업을 통합할 수 있는 통합 생성 프레임워크를 제안합니다.
- 모델은 3D 장면을 참조하는 특수 명사구인 장면 참조 토큰을 사용하여 3D 및 텍스트 데이터가 교차하는 시퀀스를 처리합니다.
- 3D 비전 작업을 언어 형식으로 변환하기 위해 작업별 지시 템플릿을 사용하는 자연스러운 접근 방식을 제공합니다.
- 후속 언어 모델링에서 참조 토큰을 사용할 수 있도록 기존 객체 레이블을 활용하여 구문 수준에서 더 세밀한 장면-텍스트 대응을 제공하는 대규모 지상 언어 데이터 세트를 구축했습니다.
- 이 데이터를 효과적으로 활용하기 위해 Contrastive LAnguage-Scene Pre-training (CLASP)을 도입하여 3D 비전과 언어 모델을 통합했습니다.
- 포괄적인 평가는 밀집 캡셔닝 및 3D QA와 같은 개방형 작업과 객체 감지 및 언어 지상화와 같은 폐쇄형 작업을 모두 포함합니다.
- 다양한 3D 벤치마크에서 수행된 실험들은 Grounded 3D-LLM의 선도적인 성능과 광범위한 적용 가능성을 나타냅니다.
- 코드와 데이터 셋은 프로젝트 페이지에서 공개될 예정입니다: https://groundedscenellm.github.io/grounded_3d-llm.github.io/.

### [Observational Scaling Laws and the Predictability of Language Model Performance](https://arxiv.org/abs/2405.10938)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.10938.png)

Vote: 4

Authors: Tatsunori Hashimoto, Chris J. Maddison, Yangjun Ruan

- 언어 모델의 성능이 스케일에 따라 어떻게 변화하는지 이해하는 것은 벤치마크와 알고리즘 개발에 중요하다.
- 연구진은 다양한 규모의 모델을 훈련할 필요 없이 약 80개의 공개 모델을 사용하여 관찰적 접근 방식을 제안한다.
- 다수 모델 계열을 하나의 스케일링 법칙으로 통합하는 것은 훈련 계산 효율성과 능력의 큰 변화로 인해 도전적이다.
- 그러나 연구진은 언어 모델 성능이 낮은 차원의 능력 공간의 함수이며, 모델 계열은 훈련 계산을 능력으로 전환하는 효율성에서만 차이가 난다는 간단한 일반화된 스케일링 법칙이 이러한 변화와 일치함을 보여준다.
- 이 접근법을 사용하여 복잡한 스케일링 현상의 예측 가능성을 보여준다; 예를 들어 여러 급진적 현상이 부드러운 시그모이드 행동을 따르고 작은 모델에서 예측 가능하다는 것을 보여준다.
- GPT-4와 같은 모델의 에이전트 성능이 더 간단한 비에이전트 벤치마크에서 정확히 예측될 수 있음을 보여준다.
- 언어 모델 능력이 계속 향상됨에 따라 사고의 연쇄와 자기 일관성과 같은 사후 훈련 개입의 영향을 예측하는 방법을 보여준다.

### [Layer-Condensed KV Cache for Efficient Inference of Large Language Models](https://arxiv.org/abs/2405.10637)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.10637.png)

Vote: 3

Authors: Haoyi Wu, Kewei Tu

- 대규모 언어 모델을 실제 애플리케이션에 배포할 때 주된 걸림돌은 막대한 메모리 소비입니다.
- 트랜스포머 구조에서 주의 메커니즘을 위한 키-값(KV) 캐시는 특히 계층의 수가 많은 깊은 언어 모델에서 상당한 양의 메모리를 소비합니다.
- 본 논문에서는 소수의 계층에 대해서만 KV를 계산하고 캐시하는 새로운 방법을 제안하여 메모리 소비를 크게 절약하고 추론 처리량을 향상시킵니다.
- 실험 결과, 제안하는 방법은 표준 트랜스포머보다 최대 26배 높은 처리량을 달성하며 언어 모델링 및 하위 작업에서 경쟁력 있는 성능을 보였습니다.
- 이 방법은 기존의 트랜스포머 메모리 절약 기술과 정렬되어 있으며, 모델과 함께 쉽게 통합할 수 있어 추론 효율성을 더욱 향상시킬 수 있습니다.
- 관련 코드는 https://github.com/whyNLP/LCKV 에서 제공됩니다.

### [Dynamic data sampler for cross-language transfer learning in large language models](https://arxiv.org/abs/2405.10626)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.10626.png)

Vote: 1

Authors: Yuhao Feng, Xianxu Hou, Wen Zhou, Linlin Shen, Yudong Li, Zhe Zhao, Cheng Hou

- 대규모 언어 모델(LLM)은 다양한 국어 처리(NLP) 응용 프로그램으로 인해 주목 받고 있으나, 영어 이외의 언어로 LLM을 훈련하는 것은 대규모 코퍼스 확보와 필요한 컴퓨팅 자원으로 인해 큰 도전이 됩니다.
- 본 논문에서는 이러한 도전을 해결하고 비용 효율적인 방법으로 대규모 중국어 모델을 훈련하고자 크로스 언어 전달 기반 LLM인 ChatFlow를 제안합니다.
- ChatFlow는 중국어, 영어 및 병렬 코퍼스 혼합을 활용하여 LLaMA2 모델을 지속적으로 훈련시켜 크로스 언어 표현을 맞추고 중국어 모델로의 지식 전달을 촉진합니다.
- 또한, 동적 데이터 샘플러를 사용하여 모델을 비지도 선행 학습에서 지도 미세 조정으로 점진적으로 전환합니다.
- 실험 결과, 우리의 접근 방식은 모델 수렴을 가속화하고 우수한 성능을 달성함을 보여줍니다.
- ChatFlow는 인기 있는 중국어 및 영어 벤치마크에서 평가되었으며, LLaMA-2-7B에서 후처리된 다른 중국어 모델들보다 뛰어난 성능을 보였습니다.

