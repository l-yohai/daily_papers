## Daily Papers (2024-05-21)

### [FIFO-Diffusion: Generating Infinite Videos from Text without Training](https://arxiv.org/abs/2405.11473)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.11473.png)

Vote: 37

Authors: Junoh Kang, Bohyung Han, Jihwan Kim, Jinyoung Choi

- 우리는 사전 학습된 확산 모델을 기반으로 한 새로운 추론 기술을 제안하며, 이를 통해 훈련 없이 무한한 길이의 비디오를 생성할 수 있습니다.
- 이 방법은 'FIFO-Diffusion'으로 명명되었으며, 대각선 잡음 제거를 반복적으로 수행하여 점진적으로 노이즈가 증가하는 연속 프레임을 큐에서 동시에 처리합니다.
- 완전히 잡음이 제거된 프레임은 큐의 앞쪽에서 디큐되고 새로운 무작위 노이즈 프레임이 뒤쪽으로 인큐됩니다.
- 그러나 대각선 잡음 제거는 이중적인 효과를 가지며, 꼬리 부분에 있는 프레임들은 앞서가는 깨끗한 프레임들을 참조할 수 있는 이점이 있지만, 이러한 전략은 훈련과 추론 사이에 차이를 야기할 수 있습니다.
- 따라서 우리는 훈련-추론 간격을 줄이기 위해 잠재적인 분할을 도입하고 전방 참조의 이점을 활용하기 위해 선견 잡음 제거 기법을 도입했습니다.
- 제안된 방법의 유망한 결과와 효과성은 기존 텍스트-비디오 생성 베이스라인에서 입증되었습니다.

### [MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2405.12130)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.12130.png)

Vote: 30

Authors: Weiwei Deng, Shengyue Luo, Shaohan Huang, Qi Zhang, Feng Sun, Deqing Wang, Furu Wei, Haizhen Huang, Zihan Zhang, Fuzhen Zhuang, Ting Jiang

- 저자는 대형 언어 모델을 위한 저랭크 적응 방법의 한계를 분석하고, 저랭크 업데이트가 새로운 지식 학습과 기억에 제한을 가할 수 있다고 밝혔습니다.
- 이를 개선하기 위해, 동일한 수의 훈련 가능한 파라미터를 유지하면서 고랭크 업데이트를 달성할 수 있는 새로운 방법인 MoRA를 제안합니다.
- MoRA는 입력 차원을 줄이고 출력 차원을 늘리기 위해 비파라미터 연산자를 도입하여 정사각 행렬을 사용합니다.
- 이 연산자들은 LoRA처럼 대형 언어 모델에 통합될 수 있도록 보장하며, 우리의 방법은 기억 중심 작업에서 LoRA를 능가하고 다른 작업에서는 비슷한 성능을 달성하였다는 평가 결과를 제공합니다.

### [OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/abs/2405.11143)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.11143.png)

Vote: 24

Authors: Jian Hu, Xianyu, Xibin Wu, Yu Cao, Dehao Zhang, Weixun Wang

- 대규모 언어 모델(Large Language Models, LLMs)이 규모 확장 법칙으로 계속 성장함에 따라 인간 피드백에서의 강화 학습(Reinforcement Learning from Human Feedback, RLHF)이 뛰어난 성능으로 주목받고 있습니다.
- RLHF를 대규모 언어 모델 훈련에 적용하는 것은 네 모델 간의 조정 문제를 야기해, 기존의 RLHF 프레임워크에서는 이 네 모델을 같은 GPU에 배치하는 방식이 일반적이었습니다.
- OpenRLHF는 이러한 기존 방식과 달리 70B 파라미터를 넘는 모델들을 위한 스케줄링을 재설계하여 Ray, vLLM, DeepSpeed를 활용하고, 자원 활용을 향상시키며 다양한 훈련 접근 방식을 적용합니다.
- 허깅페이스(Hugging Face)와 원활하게 통합되어 최적화된 알고리즘과 구동 스크립트를 제공함으로써 사용자 친화적인 솔루션을 제공합니다.
- OpenRLHF는 RLHF, DPO, 거부 샘플링, 기타 정렬 기술을 구현하여 최첨단 LLM 개발을 지원하며, 코드는 https://github.com/OpenLLMAI/OpenRLHF에서 확인할 수 있습니다.

### [Imp: Highly Capable Large Multimodal Models for Mobile Devices](https://arxiv.org/abs/2405.12107)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.12107.png)

Vote: 18

Authors: Zhenbiao Gai, Lihao Zheng, Zhou Yu, Jiajun Ding, Xuecheng Ouyang, Mingyang Wang, Jun Yu, Zhenwei Shao

- 대규모 언어 모델(LLMs)을 이용한 최근의 대규모 멀티모달 모델(LMMs)은 오픈월드 멀티모달 이해에서 뛰어난 다재다능함을 보여주고 있으나, 이들은 대체로 매개 변수가 많고 계산이 많이 필요하여 자원 제한 상황에서의 적용 가능성이 제한됩니다.
- 이에 따라, 제한된 규모(예: 3B)에서의 능력을 최대화하기 위해 여러 경량 LMM이 연이어 제안되었으며, 이들은 격려할 만한 결과를 달성했지만, 대부분은 설계 공간의 한두 가지 측면에만 초점을 맞추고 모델 능력에 영향을 미치는 주요 설계 선택사항은 아직 철저히 조사되지 않았습니다.
- 본 논문에서는 모델 아키텍처, 훈련 전략 및 훈련 데이터 측면에서 경량 LMM에 대한 체계적인 연구를 수행합니다.
- 연구 결과를 바탕으로, 저희는 2B-4B 규모에서 매우 뛰어난 능력을 지닌 LMM인 Imp를 개발했으며, 특히 Imp-3B 모델은 비슷한 크기의 기존 경량 LMM을 모두 뛰어넘을 뿐만 아니라 13B 규모의 최신 LMM까지도 초과합니다.
- 낮은 비트 양자화 및 해상도 감소 기술을 적용하여, Imp 모델은 Qualcomm Snapdragon 8Gen3 모바일 칩에서 약 13 토큰/초의 높은 추론 속도로 배포될 수 있습니다.

### [Octo: An Open-Source Generalist Robot Policy](https://arxiv.org/abs/2405.12213)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.12213.png)

Vote: 15

Authors: Homer Walke, Oier Mees, Joey Hejna, Charles Xu, Dibya Ghosh, Quan Vuong, Sergey Levine, Chelsea Finn, Sudeep Dasari, Tobias Kreiman, Karl Pertsch, Jianlan Luo, Ted Xiao, Pannag Sanketi, You Liang Tan, Dorsa Sadigh, Octo Model Team, Kevin Black

- 로봇 학습 분야에서 다양한 로봇 데이터셋에서 사전 훈련된 대형 정책은 새로운 정책을 처음부터 훈련하는 대신 도메인 내 소량의 데이터로 미세 조정을 통해 광범위하게 일반화할 수 있는 잠재력을 지니고 있습니다.
- 본 연구에서는 다양한 센서 및 액션 공간을 처리하고 다양한 로봇 플랫폼에서 사용할 수 있으며 새로운 도메인에 신속하고 효율적으로 미세 조정할 수 있는 개방형 일반 로봇 정책 개발을 위한 기초 작업을 수행하였습니다.
- 이를 위해 저자들은 로봇 조작 데이터세트 중 가장 큰 Open X-Embodiment 데이터셋에서 800,000개의 궤적을 학습한 대형 변형기 기반 정책인 Octo를 소개합니다.
- Octo는 언어 명령이나 목표 이미지를 통해 조작할 수 있으며, 새로운 감각 입력 및 행동 공간을 가진 로봇 설정에 몇 시간 내에 효과적으로 미세 조정할 수 있습니다.
- 연구팀은 9개의 로봇 플랫폼에서의 실험을 통해 Octo가 새로운 관찰 및 행동 공간에 효과적으로 미세 조정될 수 있는 다용도 정책 초기화로서의 역할을 수행함을 입증했습니다.
- Octo 모델의 설계 결정에 대한 상세한 분석도 수행하여 일반 로봇 모델 구축에 대한 향후 연구를 안내합니다.

### [Towards Modular LLMs by Building and Reusing a Library of LoRAs](https://arxiv.org/abs/2405.11157)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.11157.png)

Vote: 11

Authors: Matheus Pereira, Oleksiy Ostapenko, Lucas Caccia, Nicolas Le Roux, Edoardo Maria Ponti, Zhan Su, Laurent Charlin, Alessandro Sordoni

- 본 연구는 다양한 태스크에 대해 어댑터 라이브러리를 구축하는 최적의 방법을 연구하고, 라이브러리에서 라우팅을 통한 제로샷 및 감독된 태스크 일반화 기술을 고안합니다.
- 어댑터 파라미터의 유사성을 기반으로 태스크를 그룹화하는 모델 기반 클러스터링(MBC) 방법을 도입하여 다태스크 데이터셋 전반에 걸쳐 전이를 간접적으로 최적화합니다.
- 새로운 입력에 대해 가장 관련성 높은 어댑터를 동적으로 선택할 수 있는 새로운 제로샷 라우팅 메커니즘인 Arrow를 제시합니다. 
- Phi-2 및 Mistral과 같은 여러 대형 언어 모델(LLMs)을 사용하여 실험을 수행하고, 기존의 전체 훈련 접근방식과 비교하여 MBC 기반 어댑터와 Arrow 라우팅이 새로운 태스크에 대한 일반화 능력이 우수함을 확인합니다.
- 이 연구는 전통적인 공동 훈련을 능가하거나 맞출 수 있는 모듈형, 적응 가능한 대형 언어 모델을 구성하기 위한 단계를 밟습니다.

### [Dreamer XL: Towards High-Resolution Text-to-3D Generation via Trajectory Score Matching](https://arxiv.org/abs/2405.11252)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.11252.png)

Vote: 9

Authors: Tejal Shah, Jun Song, Xingyu Miao, Rajiv Ranjan, Haoran Duan, Varun Ojha, Yang Long

- 이 논문에서는 디노이징 확산 암시적 모델(DDIM) 역전 과정을 사용할 때 발생하는 누적 오류로 인해 발생하는 가짜 기준치 인식성 문제를 해결하기 위해 새로운 궤적 점수 매칭(TSM) 방법을 제안합니다.
- TSM 방법은 하나의 시작점에서 두 개의 경로를 생성하여 계산함으로써 ISM의 단일 경로 계산 방식과 달리, 누적 오류를 줄이고 가짜 기준치 일관성 문제를 완화합니다.
- 모델의 생성 경로의 안정성과 일관성을 향상시키며, 실험을 통해 ISM이 TSM의 특수한 경우임을 보여줍니다.
- TSM은 고해상도 텍스트에서 3D 생성으로의 다단계 최적화 과정을 개선하기 위해 안정된 확산 XL을 사용합니다.
- 안정된 확산 XL을 사용하는 3D 가우시안 스플래팅 과정 중 불안정한 그래디언트로 인한 비정상 복제 및 분리 문제를 해결하기 위해 픽셀 단위 그래디언트 클리핑 방법을 제안합니다.
- 광범위한 실험을 통해, 우리의 모델은 시각적 품질과 성능 면에서 최신 모델을 크게 능가함을 보여줍니다.
- 코드 링크: https://github.com/xingy038/Dreamer-XL.

### [SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization](https://arxiv.org/abs/2405.11582)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.11582.png)

Vote: 6

Authors: Xinghao Chen, Jialong Guo, Yunhe Wang, Yehui Tang

- 자연어 처리 및 컴퓨터 비전 작업을 위해 필수적인 구조인 트랜스포머는 고도의 컴퓨팅 비용 때문에 자원 제한 장치에서의 배포가 어렵습니다.
- 트랜스포머의 계산 병목 구성 요소는 정규화 계층과 주의 모듈입니다; 트랜스포머에서 일반적으로 사용되는 LayerNorm은 추론 중 통계 계산으로 인해 계산 친화적이지 않습니다.
- 트레이닝 중 LayerNorm을 재매개변수화된 배치 정규화로 점진적으로 교체하는 새로운 방법인 PRepBN을 제안하여 LayerNorm을 배치 정규화로 효율적으로 대체할 수 있는 방법을 제시합니다.
- 또한, 강력한 성능을 달성하기 위해 간단하면서도 효과적인 간소화된 선형 주의(SLA) 모듈을 제안합니다.
- 이미지 분류 및 객체 탐지 분야에서 광범위한 실험을 통해 제안된 방법의 효과를 입증하였으며, 예를 들어, SLAB-Swin은 ImageNet-1K에서 16.2밀리초의 지연시간으로 83.6%의 최고 정확도를 달성하였습니다.
- 언어 모델링 작업에 대해서도 비교 가능한 성능 및 낮은 지연시간을 얻었습니다.
- 관련 코드는 온라인에서 공개적으로 이용 가능합니다. (https://github.com/xinghaochen/SLAB 및 https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB)

