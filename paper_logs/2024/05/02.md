## Daily Papers (2024-05-02)

### [A Careful Examination of Large Language Model Performance on Grade School Arithmetic](https://arxiv.org/abs/2405.00332)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.00332.png)

Vote: 16

Authors: Russell Kaplan, Dylan Slack, Pranav Raja, Qin Lyu, Sean Hendryx, Will Song, Hugh Zhang, Tiffany Zhao, Vaughn Robinson, Catherine Wu, Dean Lee, Summer Yue, Lunati, Michele, Jeff Da

- 큰 언어 모델(LLMs)이 수학적 추론을 위한 다양한 벤치마크에서 인상적인 성과를 보여줬으나, 이러한 성능이 진정한 추론 능력이 아닌 훈련 데이터에 벤치마크 문제와 유사한 데이터가 유출되어 나타난 결과일 수 있다는 우려가 제기되었습니다.
- 이러한 주장을 철저히 조사하기 위해, 저자들은 초등학교 수학 벤치마크인 GSM8k의 스타일과 복잡성을 모방한 새로운 벤치마크 GSM1k를 개발하고, 두 벤치마크가 사람의 해결률, 해결 단계 수, 답의 크기 등 중요한 척도에서 비교 가능하도록 조정했습니다.
- GSM1k에서 공개 및 비공개 소스의 주요 LLM들을 평가한 결과, 최대 13%까지 정확도가 떨어지며, Phi와 Mistral 같은 일부 모델군에서는 거의 모든 모델 크기에서 체계적인 과적합이 나타났습니다.
- 그러나 Gemini/GPT/Claude 같은 최선단 모델들은 과적합 징후가 거의 없는 것으로 나타났으며, 추가 분석에서는 GSM8k의 예제를 생성할 확률과 GSM8k와 GSM1k 간의 성능 차이 간에 양의 상관관계(Spearman의 r^2=0.32)가 있음을 제시합니다.
- 이러한 결과는 많은 모델들이 GSM8k를 부분적으로 암기했을 가능성을 시사하며, 진정한 수학적 추론 능력과 데이터 세트 오염 간의 구분이 중요함을 강조합니다.

### [Paint by Inpaint: Learning to Add Image Objects by Removing Them First](https://arxiv.org/abs/2404.18212)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.18212.png)

Vote: 15

Authors: Navve Wasserman, Noam Rotstein, Ron Kimmel, Roy Ganz

- 이 연구는 텍스트 기반 확산 모델의 도입으로 크게 발전한 이미지 편집 분야에서, 사용자가 제공하는 입력 마스크 없이 텍스트 지시에 따라 이미지에 객체를 추가하는 과제에 대응합니다.
- 연구진은 객체 제거(Inpaint)가 객체 추가(Paint)보다 상대적으로 더 간단하다는 통찰을 활용하여, 객체가 제거된 이미지와 그 원본 이미지 쌍을 포함하는 대규모 이미지 데이터셋을 자동으로 구축했습니다.
- 구축된 데이터셋을 사용하여 확산 모델을 훈련시켜, 이미지에 효과적으로 객체를 추가하는 역-Inpainting 과정을 수행하도록 합니다.
- 연구진이 개발한 데이터셋은 합성된 것이 아닌 자연스러운 대상 이미지를 특징으로 하며, 원본과 대상 이미지 간의 일관성을 유지합니다.
- 대규모 시각-언어 모델을 사용하여 제거된 객체에 대한 자세한 설명을 제공하고, 큰 언어 모델을 이용해 이러한 설명을 다양한 자연어 지시로 변환합니다.
- 훈련된 모델은 기존 모델들을 질적, 양적으로 능가함을 보여주며, 연구진은 대규모 데이터셋과 훈련된 모델을 공동체에 공개합니다.

### [Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model Editing with Llama-3](https://arxiv.org/abs/2405.00664)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.00664.png)

Vote: 11

Authors: Gopala Anumanchipalli, Junsang Yoon, Akshat Gupta

- 이 연구는 최신 대형 언어 모델인 Llama-3을 대상으로 한 타깃 모델 편집 분석을 제시합니다.
- 인기 있는 모델 편집 기술인 ROME, MEMIT, EMMET이 특정 레이어 개입을 위해 설계되었으며, 이를 통해 가장 효과적인 레이어를 식별합니다.
- 순차 편집, 배치 편집, 순차-배치 편집이라는 하이브리드 접근 방식을 포함하여 최대 4096개의 편집을 평가합니다.
- 큰 편집 배치 크기를 증가시키는 것은 동등한 수의 편집을 위해 작은 편집 배치를 순차적으로 사용하는 것보다 모델 성능을 더 크게 저하시킬 수 있음을 발견했습니다.
- 이러한 발견은 모델 편집 방법을 확장하는 데 있어 순차 모델 편집이 중요한 구성 요소임을 주장하며, 미래의 연구는 배치 및 순차 편집을 결합하는 방법에 초점을 맞춰야 한다고 합니다.
- 이 관찰은 더 큰 편집 배치 크기를 향한 현재의 모델 편집 방법에 잠재적인 한계를 제시하며, 배치 크기 및 모델 편집 성능을 최적화하는 데 대한 미래의 조사를 위한 길을 열어줄 것으로 기대됩니다.

### [SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound](https://arxiv.org/abs/2405.00233)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.00233.png)

Vote: 9

Authors: Yi Yuan, Haohe Liu, Mark D. Plumbley, Mengyue Wu, Xuenan Xu, Wenwu Wang

- 다양한 오디오 유형, 특히 음성, 일반 오디오, 음악 등의 문맥에서 오디오를 초당 100개 이하의 토큰으로 압축하는 새로운 코덱, SemantiCodec을 소개합니다.
- SemantiCodec은 음성인식에 필요한 의미 정보를 포함하여 언어 모델링 기술을 보다 효과적으로 적용할 수 있도록 설계되었습니다.
- 이 코덱은 자기지도 학습을 사용하는 의미적 인코더와 k-means 클러스터링을 통해 이산화된 음향 인코더로 이루어진 이중 인코더 구조를 특징으로 합니다.
- SemantiCodec은 초당 25, 50, 100 토큰의 세 가지 변형을 제공하며, 이는 0.31 kbps에서 1.43 kbps 사이의 매우 낮은 비트율을 지원합니다.
- 실험 결과는 SemantiCodec이 기존의 최고 성능을 자랑하는 Descript 코덱을 복원 품질 면에서 크게 능가함을 보여줍니다.
- 또한, SemantiCodec은 훨씬 낮은 비트율에서도 평가된 모든 오디오 코덱보다 풍부한 의미 정보를 포함하고 있음을 시사합니다.
- 관련 코드와 데모는 https://haoheliu.github.io/SemantiCodec/ 에서 확인할 수 있습니다.

### [Self-Play Preference Optimization for Language Model Alignment](https://arxiv.org/abs/2405.00675)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.00675.png)

Vote: 8

Authors: Yiming Yang, Huizhuo Yuan, Quanquan Gu, Kaixuan Ji, Yue Wu, Zhiqing Sun

- 기존의 인간 피드백 기반 강화 학습(RLHF) 방법은 휴먼의 선호도의 비순차성과 비합리성을 효과적으로 반영하지 못하는 문제를 가졌으나, 이 논문에서는 선호도 확률을 직접 다루는 새로운 방식을 제안하여 언어 모델 정렬을 더 정확하고 유연하게 수행할 수 있다고 제시한다.
- 제안된 자기대전 선호 최적화(Self-Play Preference Optimization, SPPO) 방법은 두 플레이어가 상수 합 게임을 통해 나쉬 균형을 찾는 문제로 접근하며, 반복적 정책 갱신을 통해 이론적으로 수렴 보장을 가진 나쉬 균형을 추정한다.
- SPPO는 기존의 대칭 쌍비교 손실 방법들과 달리, 선택된 반응의 로그 가능도를 증가시키고 거부된 반응의 로그 가능도를 감소시키는 효과를 낼 수 있다.
- 실험에서 60k개의 프롬프트만을 사용하고, 어떠한 프롬프트 증강도 없이, 0.4B 파라미터를 가진 선호도 모델 PairRM을 사용하여 UltraFeedback 데이터셋에서 미스트랄-7B-Instruct-v0.2 모델을 미세 조정함으로써 최신 AlpacaEval 2.0에서 GPT-4-Turbo에 대한 28.53%의 최고 승률을 달성했다고 보고한다.
- 또한, SPPO는 MT-Bench 및 Open LLM 리더보드에서 DPO와 IPO를 초과하는 성능을 보였으며, 이는 GPT-4나 다른 강력한 언어 모델로부터 추가 외부 감독 없이도 달성되었다.

### [Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge](https://arxiv.org/abs/2405.00263)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.00263.png)

Vote: 8

Authors: Bin Xiao, Lei Su, Bin Cui, Xiaonan Nie, Chunan Shi, Weipeng Chen, Xiangwei Deng, Fan Yang

- 대규모 언어 모델들은 자동 회귀 디코딩의 요구와 현대 GPU 설계 간의 불일치로 인해 효율성이 낮습니다.
- 최근 병렬 디코딩이라는 추측적 디코딩 알고리즘이 인기를 얻고 있으며, 생성에서의 효율성 향상을 입증하였습니다.
- 이러한 방식은 대형 모델에 추가 디코딩 헤드를 도입하여 여러 후속 토큰을 동시에 예측하고 단일 디코딩 단계에서 이 후보들을 검증합니다.
- 그러나, 이 접근법은 사전 훈련 중 사용된 다음 토큰 예측의 훈련 목표와 상이하여 후보 토큰의 적중률이 낮습니다.
- 본 논문에서는 순차 지식을 병렬 디코딩 과정에 통합하는 새로운 추측적 디코딩 알고리즘인 Clover를 제안합니다.
- Clover는 사전 추측된 토큰에서 순차 지식을 Regressive Connection을 통해 전달하고 Attention Decoder를 사용하여 이러한 추측된 토큰을 통합합니다.
- 또한, Clover는 추측적 생성의 목적에 더 잘 맞도록 숨겨진 상태를 수정하는 Augmenting Block을 포함합니다.
- 실험 결과, Clover는 기존의 최고 성과 모델인 Medusa를 크게 앞서 Baichuan-Small에서 최대 91%, Baichuan-Large에서 최대 146%의 성능 향상을 보였습니다.

### [Automatic Creative Selection with Cross-Modal Matching](https://arxiv.org/abs/2405.00029)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.00029.png)

Vote: 6

Authors: Rob Monarch, Parmeshwar Khurd, Jia Huang, Goodman Gu, Kailash Thiyagarajan, Alex Kim, Jerry Kwac, Anikesh Kamath

- 앱 개발자들은 앱 이미지와 검색어를 활용하여 앱을 광고하며, 앱 이미지가 검색어와 높은 관련성을 지니는 것이 중요합니다.
- 이 논문에서는 사전 훈련된 LXMERT 모델을 미세 조정하여 앱 이미지와 검색어의 일치를 평가하는 새로운 접근 방식을 제시합니다.
- 트랜스포머 모델과 ResNet 모델을 사용한 기준 모델 및 미세 조정된 CLIP 모델과 비교할 때, 제안된 방식은 매칭 정확성에서 크게 향상됨을 보여줍니다.
- 광고주 지정 이미지-검색어 쌍을 이용한 평가에서 0.96의 AUC 점수를 달성하였고, 이는 기준 모델과 미세 조정된 CLIP 모델을 각각 8%, 14% 상회합니다.
- 인간 평가 결과를 이용한 평가에서도 0.95의 AUC 점수를 달성하여 기준 모델과 미세 조정된 CLIP 모델보다 각각 16%, 17% 우수한 성능을 보였습니다.

### [Spectrally Pruned Gaussian Fields with Neural Compensation](https://arxiv.org/abs/2405.00676)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.00676.png)

Vote: 6

Authors: Zhenxin Zhu, Jian Zhao, Zhou Jiang, Runyi Yang, Hao Zhao, Yifei Zhang, Xiaoxue Chen, Baijun Ye, Yuantao Chen

- 최근에는 빠른 렌더링 속도와 높은 렌더링 품질로 인해 3D 가우시안 스플래팅이 새로운 3D 표현으로 주목받고 있지만, 이는 고용량 메모리 소비라는 문제를 가지고 있습니다.
- 이 논문은 가우시안 원시 데이터 간의 관계를 고려하지 않는 것이 높은 메모리 사용량의 원인으로 보고, 스펙트럼 가지 치기와 신경 보상을 통한 메모리 효율적인 가우시안 필드인 'SUNDAE'를 제안합니다.
- 가우시안 원시 데이터의 집합에 그래프를 구성하여 관계를 모델링하고, 원하는 신호를 보존하면서 원시 데이터를 가지치기하는 스펙트럼 다운 샘플링 모듈을 설계합니다.
- 가우시안의 가지치기로 인한 품질 손실을 보상하기 위해, 가볍지만 효과적인 신경망 헤드를 활용하여 스플랏된 특징을 혼합함으로써 품질 손실을 효과적으로 보상하고 원시 데이터 간의 관계를 그 가중치에 포착합니다.
- 예를 들어, SUNDAE는 Mip-NeRF360 데이터셋에서 523MB의 메모리를 사용하는 기존 가우시안 스플래팅 알고리즘보다 더 적은 104MB 메모리를 사용하면서도 26.80 PSNR을 145 FPS로 달성했습니다.

### [STT: Stateful Tracking with Transformers for Autonomous Driving](https://arxiv.org/abs/2405.00236)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.00236.png)

Vote: 4

Authors: Congcong Li, Weilong Yang, Colin Graber, Zijian Guo, Han Deng, Xu Chen, Wei-Chih Hung, Shiwei Sheng, Longlong Jing, Yin Zhou, Shangxuan Wu, Mingxing Tan, Farshid Moussavi, Qiurui He, Tong He, Zhengli Zhao, Xingyi Zhou, Chris Sweeney, Sangjin Lee, Qi Chen, Qinru Li, Ruichi Yu

- 자율주행을 위해 STT(Stateful Tracking with Transformers)라는 새로운 추적 모델을 제안하며 이 모델은 3차원 공간에서 객체를 일관되게 추적하고 그 상태를 정확하게 예측할 수 있습니다.
- 기존 연구들이 데이터 연결 작업에 치중하는 동안 상태 추정에 대한 모델 성능을 간과하거나 복잡한 휴리스틱을 사용하였지만, STT는 외형, 기하학적, 운동 신호를 장기적인 검출 이력을 통해 통합하여 데이터 연결 및 상태 추정 작업 모두에 대해 최적화되었습니다.
- 표준 추적 메트릭인 MOTA와 MOTP는 두 작업의 결합 성능을 폭넓은 객체 상태 스펙트럼에서 포착하지 못하는 한계를 가지므로, 이를 해결하기 위해 S-MOTA와 MOTPS라는 새로운 메트릭을 도입합니다.
- STT는 Waymo Open Dataset에서 경쟁력 있는 실시간 성능을 달성하였습니다.

