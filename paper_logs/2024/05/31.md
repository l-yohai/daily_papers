## Daily Papers (2024-05-31)

### [Jina CLIP: Your CLIP Model Is Also Your Text Retriever](https://arxiv.org/abs/2405.20204)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.20204.png)

Vote: 9

Authors: Han Xiao, Isabelle Mohr, Joan Fontanals Martínez, Mohammad Kalim Akram, Susana Guzman, Maximilian Werk, Scott Martens, Saahil Ognawala, Bo Wang, Georgios Mastrapas, Saba Sturua, Michael Günther, Andreas Koukounas, Nan Wang

- 대조 언어-이미지 사전학습(CLIP)은 이미지와 텍스트를 고정 크기의 벡터로 매핑하여 공통 임베딩 공간에서 정렬하는 모델을 훈련하는 데 널리 사용됩니다.
- 그러나 CLIP 모델은 전문 텍스트 모델에 비해 텍스트 전용 작업에서 성능이 저하되는 경향이 있습니다.
- 이는 텍스트 전용 및 다중 모달 작업에 대해 별도의 임베딩과 모델을 유지하는 정보 검색 시스템의 비효율성을 초래합니다.
- 본 연구에서는 이 문제를 해결하기 위해 새로운 다중 작업 대조 학습 방법을 제안하고, 이를 사용하여 jina-clip-v1 모델을 훈련하여 텍스트-이미지 및 텍스트-텍스트 검색 작업에서 최신 성능을 달성합니다.

### [Xwin-LM: Strong and Scalable Alignment Practice for LLMs](https://arxiv.org/abs/2405.20335)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.20335.png)

Vote: 8

Authors: JingCheng Hu, Bolin Ni, Yixuan Wei, Houwen Peng, Han Hu, Zheng Zhang, Gaofeng Meng

- 본 연구에서는 대규모 언어 모델(LLMs)을 위한 종합적인 정렬 방법론 집합인 Xwin-LM을 제시합니다.
- 이 집합에는 지도 학습(SFT), 보상 모델링(RM), 거부 샘플링 학습(RS), 직접 선호 최적화(DPO) 등 여러 주요 기술이 포함됩니다.
- Xwin-LM-SFT는 고품질 지시 데이터로 초기에 미세 조정된 모델, Xwin-Pair는 GPT-4를 사용하여 세심하게 주석이 달린 대규모 다단계 선호도 데이터셋입니다.
- Xwin-RM은 7B, 13B, 70B 파라미터로 개발된 Xwin-Pair에서 훈련된 보상 모델이며, Xwin-Set은 각 프롬프트가 Xwin-LM-SFT에 의해 생성된 64개의 고유 응답과 Xwin-RM에 의해 점수가 매겨진 다중 선호도 데이터셋입니다.
- Xwin-LM-RS는 Xwin-Set의 가장 높은 점수를 받은 응답들로 미세 조정된 모델, Xwin-LM-DPO는 DPO 알고리듬을 사용하여 Xwin-Set에서 추가로 최적화된 모델입니다.
- AlpacaEval 및 MT-bench 평가에서 모델은 일관되고 상당한 성능 향상을 보여주어 Xwin-LM의 강점과 확장성을 입증합니다.
- 연구 커뮤니티가 연구를 지속할 수 있도록 https://github.com/Xwin-LM/Xwin-LM 저장소가 지속적으로 업데이트 될 예정입니다.

### [Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts](https://arxiv.org/abs/2405.19893)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.19893.png)

Vote: 6

Authors: Lin Ju, Dan Yang, Chunjing Gan, Lei Liang, Zhiqiang Zhang, Jun Zhou, Binbin Hu, Ziqi Liu, Jinjie Gu, Siyuan Li, Hanxiao Zhang, Yue Shen

- 최근 몇 년 동안 대규모 언어모델(Large Language Models, LLMs)은 다양한 분야에서 놀라운 성과를 이루었지만, 지식 업데이트의 비시기성과 비용, 그리고 환각 문제 등으로 인해 지식 집약적 작업에서의 활용이 제한되었다.
- 본 연구에서는 유사성만을 기반으로 하는 기존의 검색 강화 모델(Retrieval Augmented Generation, RAG)의 한계를 지적하고, 유사성에만 전적으로 의존하는 것이 때때로 성능 저하를 일으킬 수 있다고 주장한다.
- 이에 따라, 다층적인 사고를 통합한 새로운 모델인 MetRag(Multi layEred Thoughts enhanced Retrieval Augmented Generation framework)를 제안한다.
- MetRag는 단순한 유사성 중심 사고를 넘어서, 유용성 지향적 사고를 위한 소규모 유틸리티 모델을 LLM의 감독 하에 개발하고, 유사성 및 유용성 지향적 사고를 종합적으로 결합한 더 현명한 모델을 제안한다.
- 또한, 검색된 문서 집합이 방대하고 각각을 독립적으로 사용하는 것이 공통점과 특징을 파악하기 어려운 문제를 해결하기 위해, LLM을 과제 적응형 요약자로 활용하여 검색 강화 생성에 컴팩트 지향적 사고를 부여한다.
- 이러한 다층적 사고를 통해, 마지막 단계에서는 LLM이 지식 증강 생성을 수행한다.
- 지식 집약적 작업에 대한 광범위한 실험을 통해 MetRag의 우수성이 입증되었다.

### [MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model](https://arxiv.org/abs/2405.20222)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.20222.png)

Vote: 5

Authors: Xintao Wang, Yong Zhang, Xiaodong Cun, Muyao Niu, Yinqiang Zheng, Ying Shan

- MOFA-Video는 주어진 이미지에서 다양한 제어 신호(예: 인간 상징 레퍼런스, 수동 궤적, 제공된 다른 비디오)를 사용하여 비디오를 생성하는 고급 제어 가능 이미지 애니메이션 방법을 제시합니다.
- 이 방법은 특정 동작 도메인에만 작동하거나 확산 전에 제어 능력이 약한 이전 방법들과는 다릅니다.
- 비디오 생성 파이프라인에서 생성된 동작을 제어하기 위해 여러 도메인 인식 모션 필드 어댑터(MOFA-Adapters)를 설계했습니다.
- MOFA-Adapters는 주어진 희소 제어 조건에서부터 처음에는 밀도 높은 동작 흐름을 생성하고, 그 다음으로 주어진 이미지의 다중 크기 특성을 안정된 비디오 확산 생성을 위한 유도된 특성으로 감쌉니다.
- 수동 궤적과 인간 랜드마크 모두 제어에 대한 희소 정보를 포함하고 있기 때문에 이들에 대해 별도로 두 개의 동작 어댑터를 순진하게 훈련합니다.
- 훈련 후, 서로 다른 도메인의 MOFA-Adapters는 더욱 제어 가능한 비디오 생성을 위해 함께 작동할 수 있습니다.

### [DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation](https://arxiv.org/abs/2405.20289)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.20289.png)

Vote: 4

Authors: Taylor Berg-Kirkpatrick, Nicholas Bryan, Zachary Novack, Julian McAuley

- 인간 중심의 AI 기반 음악 생성을 위한 제어 가능한 음악 생성 방법이 중요하지만, 현재 속도, 품질, 제어 설계의 상충 관계에 의해 제한되고 있다.
- 기존의 DITTO 방법은 최첨단 결과를 제공하지만 실시간보다 10배 느려 실용적인 사용에 제약이 있다.
- 본 논문에서는 DITTO-2, 즉 증류된 확산 추론 시간 T-최적화 방법을 제안하여 추론 시간 최적화 기반 제어를 가속화하고 실시간보다 빠른 생성을 가능하게 한다.
- 이 방법은 사전 훈련된 확산 모델을 효율적인 수정된 일관성 또는 일관성 궤적 증류 과정을 통해 빠른 샘플링을 위해 증류하고, 증류된 모델을 사용하여 효율적인 대리 최적화 작업으로서 단일 단계 샘플링을 수행하는 추론 시간 최적화를 실행한다.
- 최종적으로 추정된 노이즈 잠재 변수를 사용한 다단계 샘플링 생성(디코딩)을 실행하여 최고 품질의 빠르고 제어 가능한 생성을 달성한다.
- 철저한 평가를 통해 본 방법이 생성 속도를 10-20배 이상 향상시킬 뿐만 아니라 제어 준수 및 생성 품질을 동시에 개선함을 발견했다.
- 또한, 조건 없는 확산 모델을 텍스트 입력 없이 최첨단 텍스트 제어가 가능한 모델로 변환하는 새로운 응용 프로그램에 접근법을 적용하였다.
- 해당 연구와 관련된 사운드 예제는 https://ditto-music.github.io/ditto2/ 에서 확인할 수 있다.

### [GECO: Generative Image-to-3D within a SECOnd](https://arxiv.org/abs/2405.20327)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.20327.png)

Vote: 4

Authors: Chen Wang, Jiatao Gu, Xiaoxiao Long, Yuan Liu, Lingjie Liu

- 최근 몇 년 동안 3D 생성은 눈에 띄는 발전을 이루었습니다.
- 기존의 점수 증류 방법은 주목할 만한 결과를 만들어내지만, 장면마다 최적화가 필요하여 시간 효율성에 영향을 미칩니다.
- 반면에, 재구성 기반 접근법은 효율성을 우선시하지만 불확실성을 제한적으로 다루기 때문에 품질이 저하됩니다.
- GECO는 단 1초 내에 작동하는 고품질 3D 생성 모델링을 위한 새로운 방법을 도입합니다.
- 우리의 접근 방식은 초기 단계에서 단일 단계 다시점 생성 모델을 점수 증류로 훈련시키고, 다음에는 다시점 예측에서 나타나는 뷰 불일치 문제를 해결하기 위해 두 번째 단계 증류가 적용됩니다.
- 이 두 단계 과정은 3D 생성을 위해 품질과 효율성을 최적화하는 균형 잡힌 접근법을 보장합니다.
- 종합적인 실험을 통해 GECO가 전례 없는 수준의 효율성으로 고품질의 이미지-부터-3D 생성을 달성한다는 것을 입증합니다.

### [PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting](https://arxiv.org/abs/2405.19957)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.19957.png)

Vote: 3

Authors: Qiaowei Miao, Yi Yang, Yawei Luo

- 이미지, 비디오, 3D 생성에서 성과를 거둔 텍스트 조건부 확산 모델(DMs)에 이어, 연구는 더 도전적인 텍스트-투-4D 합성, 즉 동적 3D 개체를 생성하는 시간 차원을 도입하는 작업에 초점을 맞추고 있습니다.
- 텍스트-투-3D 합성에 널리 사용되는 점수 증류 샘플링(SDS) 기술이 텍스트-투-4D 성능 저하의 주요 장애물로 지목되었습니다. 이는 진페이스 문제와 비현실적인 텍스쳐, 높은 계산 비용 때문입니다.
- 본 논문에서는 텍스트-투-비디오 프레임을 명시적 픽셀 정렬 대상으로 활용하여 정적 3D 오브젝트를 생성하고 움직임을 주입하는 새로운 방법인 픽셀 수준 정렬을 위한 텍스트-투-4D 가우시안 스플래팅(PLA4D)을 제안합니다.
- 특히, PLA4D는 카메라 포즈를 조정하기 위한 포컬 정렬, 픽셀 수준에서 렌더링된 이미지 대비에서 기하학적 선행 지식을 추출하는 GS-메시 대조 학습을 도입합니다.
- 또한, 가우스 변화를 유도하는 변형 네트워크를 이용해 움직임 정렬을 개발하고, 매끄러운 4D 오브젝트 표면을 위한 참조 정제를 구현했습니다.
- 이 기술들은 4D 가우시안 스플래팅이 생성된 비디오와 기하학, 텍스쳐, 움직임을 픽셀 수준에서 정렬할 수 있게 해줍니다.
- PLA4D는 이전 방법들보다 텍스쳐 세부사항이 뛰어나고 시간이 적게 걸리며 진페이스 문제를 효과적으로 완화합니다.
- PLA4D는 오픈 소스 모델을 사용하여 전체적으로 구현되었으멑, 4D 디지털 콘텐츠 생성을 위한 접근하기 쉽고 사용자 친화적이며 유망한 방향을 제공합니다.

### [MotionLLM: Understanding Human Behaviors from Human Motions and Videos](https://arxiv.org/abs/2405.20340)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.20340.png)

Vote: 2

Authors: Lei Zhang, Ailing Zeng, Benyou Wang, Hao Zhang, Ruimao Zhang, Ling-Hao Chen, Shunlin Lu

- 이 연구는 대규모 언어 모델(Large Language Models, LLMs)의 강력한 능력을 활용하여 다중 모달리티(비디오와 동작 모달리티) 인간 행동 이해에 초점을 맞춥니다.
- 기존의 비디오만 혹은 동작만을 이해하는 LLM과 다르게, 인간 행동을 이해하기 위해서는 비디오와 동작 시퀀스(예: SMPL 시퀀스)의 조합 모델링이 필요하다고 주장합니다.
- 이를 위해, MotionLLM은 통합된 비디오-동작 훈련 전략을 채택하여 풍부한 공간-시간적 통찰력을 얻기 위해 기존의 거친 비디오-텍스트 데이터와 세밀한 동작-텍스트 데이터의 보완적 장점을 이용합니다.
- 또한, 다양한 비디오, 동작, 캡션 및 지시사항을 포함하는 방대한 데이터셋 MoVid를 수집하고, 비디오 및 동작에 대한 인간 행동 이해를 더 잘 평가하기 위해 MoVid-Bench를 제안합니다.
- 광범위한 실험을 통해 MotionLLM이 캡션 생성, 공간-시간 이해 및 추론 능력에서 우수성을 보여준다는 것을 입증했습니다.

### [Parrot: Efficient Serving of LLM-based Applications with Semantic Variable](https://arxiv.org/abs/2405.19888)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.19888.png)

Vote: 1

Authors: Zhenhua Han, Fan Yang, Chen Chen, Lili Qiu, Chaofan Lin, Yuqing Yang, Chengruidong Zhang

- 대규모 언어 모델(LLM)의 발전으로 새로운 소프트웨어 패러다임인 LLM 기반 애플리케이션(예: AI 에이전트, 코파일럿)이 등장하였으며, 이는 LLM의 강점과 전통적 소프트웨어가 결합된 형태입니다.
- 다양한 테넌트의 LLM 애플리케이션은 여러 LLM 요청을 활용한 복잡한 워크플로를 설계하여 하나의 작업을 수행하지만, 현재의 공개 LLM 서비스는 단순화된 요청 수준 API만을 제공하여 애플리케이션 수준 정보를 잃어버립니다.
- 이에 Parrot이라는 LLM 서비스 시스템이 제안되었고, 이는 애플리케이션 수준 지식을 공개 LLM 서비스에 노출시키는 통합 추상체인 시맨틱 변수를 도입합니다.
- 시맨틱 변수는 요청의 프롬프트에서 입력/출력 변수에 주석을 달고 여러 LLM 요청을 연결할 때 데이터 파이프라인을 생성하여 LLM 애플리케이션 프로그래밍을 자연스럽게 합니다.
- 공개 LLM 서비스가 시맨틱 변수를 활용함으로써, 전통적 데이터 흐름 분석을 수행하여 여러 LLM 요청 간의 상관관계를 밝힐 수 있게 되어 LLM 기반 애플리케이션의 종단 간 성능 최적화의 새로운 가능성을 열었습니다.
- 광범위한 평가를 통해 Parrot이 인기 있고 실용적인 LLM 애플리케이션 사용 사례에 대해 최대 한 자릿수의 성능 향상을 달성할 수 있음을 입증하였습니다.

### [DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories](https://arxiv.org/abs/2405.19856)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.19856.png)

Vote: 1

Authors: Lecheng Wang, Fei Huang, Xuanming Zhang, Kaibo Liu, Binhua Li, Yuqi Zhu, Huanyu Liu, Hao Zhu, Yongbin Li, Jiazheng Ding, Ge Li, Zhi Jin, Lanshen Wang, Yunfei Zhao, Yihong Dong, Yongmin Li, Zheng Fang, Jia Li

- DevEval 벤치마크는 실제 코드 저장소에 부합하는 다양한 차원(예: 코드 분포 및 의존성 분포)으로 구성되어 있습니다.
- 이 벤치마크는 13명의 개발자에 의해 주석이 달려 있으며, 요구 사항, 원본 저장소, 참조 코드, 참조 의존성 등을 포함한 포괄적인 주석이 포함되어 있습니다.
- DevEval은 117개 저장소에서 추출한 1,874개의 테스트 샘플을 포함하며, 인터넷, 데이터베이스 등 10개의 인기 있는 도메인을 포괄합니다.
- 이 벤치마크를 통해, 연구자들은 저장소 수준의 코드 생성을 제안하고 GPT-4, GPT-3.5, StarCoder 2 등의 대형 언어 모델(Large Language Models, LLMs)을 평가했습니다.
- 실험 결과, 예를 들어 GPT-4-turbo의 최고 통과율(Pass@1)은 53.04%에 불과했습니다.
- 연구자들은 실패한 케이스를 분석하고 LLM의 단점을 요약하며, 이 벤치마크가 실제 코드 저장소에서 LLM의 발전을 촉진할 수 있기를 기대합니다.
- DevEval, 프롬프트, LLM의 예측 내역이 공개되어 있습니다.

### [DeMamba: AI-Generated Video Detection on Million-Scale GenVideo Benchmark](https://arxiv.org/abs/2405.19707)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.19707.png)

Vote: -

Authors: Jun Lan, Yan Hong, Zhangxuan Gu, Weiqiang Wang, Yaohui Li, Huaxiong Li, Zhuoer Xu, Zizheng Huang, Jianfu Zhang, Haoxing Chen, Huijia Zhu

- 최근 비디오 생성 기술이 빠르게 발전하면서 소셜 미디어 플랫폼에서 가짜 정보의 확산에 대한 우려가 커지고 있습니다.
- 이에 따라 가짜 AI 생성 비디오와 실제 비디오를 구별할 수 있는 탐지기에 대한 수요가 증가하고 있습니다.
- 하지만 가장 발전된 비디오 생성기로부터의 대규모 데이터셋 부족으로 이러한 탐지기들의 개발에 장애가 되고 있습니다.
- 이 문제를 해결하기 위해, 100만 개가 넘는 AI 생성 및 실제 비디오를 포함하는 대규모 'GenVideo' 데이터셋을 소개합니다.
- 'GenVideo'는 다양한 비디오 분류와 생성 기술을 아우르는 풍부한 내용을 특징으로 합니다.
- 데이터셋을 통해 세밀화된 실세계 설정에서의 두 가지 평가 방법을 제시하여 탐지기의 성능을 평가합니다: 교차 생성기 영상 분류 작업과 품질 저하된 영상 분류 작업.
- 또한, 시공간적 불일치를 분석하여 AI 생성 비디오를 식별하는 플러그 앤 플레이 모듈인 'Detail Mamba (DeMamba)'를 도입합니다.
- DeMamba는 기존 탐지기들에 비해 GenVideo 데이터셋에서 우수한 일반화 능력과 강건성을 입증합니다.
- GenVideo 데이터셋과 DeMamba 모듈이 AI 생성 비디오 탐지 분야를 크게 발전시킬 것이라고 믿습니다.
- 코드와 데이터셋은 https://github.com/chenhaoxing/DeMamba 에서 접근할 수 있습니다.

