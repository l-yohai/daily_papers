## Daily Papers (2024-04-02)

### [Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order](https://arxiv.org/abs/2404.00399)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.00399.png)

Vote: 24

Authors: Yekun Chai, Jason T Stillerman, Taishi Nakamura, Tommaso Furlanello, Prateek Yadav, Suhas Pai, Ben Bogin, Rio Yokota, Marzena Karpinska, Tanmay Laud, Xuan-Son Vu, Mayank Mishra, +, Simone Tedeschi, Tosin Adewumi, Terry Yue Zhuo, Diganta Misra, Veronika Laippala, Vu Minh Chien, Wojciech Kusa, Arnav Varma Dantuluri, Felix Friedrich, Niklas Muennighoff

- 본 논문은 이전 모델들이 직면한 한계, 즉 제한된 다국어 지원, 지속적 사전학습으로 인한 잊어버림 현상, 그리고 높은 컴퓨팅 비용 등의 문제를 해결하기 위한 150억 개 파라미터를 갖는 다국어 오픈 소스 모델인 Aurora-M을 소개한다.
- Aurora-M은 영어, 핀란드어, 힌디어, 일본어, 베트남어 및 프로그래밍 코드를 포함한 StarCoderPlus에서 추가로 4350억 토큰에 대해 지속적으로 사전학습되었으며, 총 학습 토큰 수가 2조를 초과한다.
- 이 모델은 인간이 검토한 안전 지침에 따라 미세 조정되어, 전통적인 red-teaming 접근법뿐만 아니라 바이든-해리스 행정명령이 명시한 안전하고, 보안이 강화된, 신뢰할 수 있는 인공지능의 개발 및 사용과 관련된 특정 우려사항과도 일치하도록 개발되었다.
- 다양한 작업과 언어에 걸쳐 엄격하게 평가된 Aurora-M은 재앙적인 망각에 대한 강건함을 보여주고, 특히 안전 평가에서 다국어 환경에서 대안 모델들보다 우수한 성능을 발휘한다.
- 책임감 있는 오픈 소스 대규모 언어 모델(LLM) 개발을 촉진하기 위해 Aurora-M 및 그 변형 모델들이 https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407 에서 공개되었다.

### [Getting it Right: Improving Spatial Consistency in Text-to-Image Models](https://arxiv.org/abs/2404.01197)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.01197.png)

Vote: 22

Authors: Gabriela Ben Melech Stan, Hannaneh Hajishirzi, Ludwig Schmidt, Chitta Baral, Estelle Aflalo, Tejas Gokhale, Yezhou Yang, Dhruba Ghosh, Sayak Paul, Vasudev Lal, Agneet Chatterjee

- 현재의 텍스트-투-이미지(T2I) 모델에서의 주요 단점 중 하나는 텍스트 프롬프트에서 지정된 공간적 관계를 충실하게 따르는 이미지를 일관성 있게 생성하지 못한다는 것입니다.
- 이 논문에서는 이러한 한계에 대한 종합적인 조사를 실시하고, 최고 수준의 성능을 달성하기 위한 데이터셋 및 방법론을 개발하였습니다.
- 연구팀은 현재의 시각-언어 데이터셋이 공간 관계를 충분히 잘 나타내지 못한다는 것을 발견하였고, 이를 해결하기 위해 네 가지 널리 사용되는 시각 데이터셋에서 600만 이미지를 재캡셔닝하여 최초의 대규모 공간 중심 데이터셋인 SPRIGHT를 만들었습니다.
- 3단계 평가 및 분석 파이프라인을 통해 SPRIGHT 데이터셋이 기존 데이터셋보다 공간 관계를 포착하는 데 있어 크게 개선되었음을 확인하였습니다.
- 또한 SPRIGHT의 단지 약 0.25%만을 활용하여 공간적으로 정확한 이미지 생성에서 22%의 향상을 이루었으며, FID 및 CMMD 점수도 향상시켰습니다.
- 많은 객체를 포함하는 이미지에 대한 훈련이 공간 일관성 향상에 크게 도움이 됨을 발견하였고, 500개 미만의 이미지에 대한 파인튜닝을 통해 T2I-CompBench에서 0.2133의 공간 점수로 최고 성능을 달성했습니다.
- 일련의 통제된 실험 및 제거 연구를 통해 텍스트-투-이미지 모델의 공간 일관성에 영향을 미치는 요소에 대한 이해를 높일 믿음을 문서화하였습니다.
- 이 연구 분야에서의 추가 연구를 촉진하기 위해 데이터셋과 모델을 공개적으로 공개하였습니다.

### [FlexiDreamer: Single Image-to-3D Generation with FlexiCubes](https://arxiv.org/abs/2404.00987)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.00987.png)

Vote: 10

Authors: Jun Zhu, Zhengyi Wang, Zihan Zhou, Yikai Wang, Ruowen Zhao

- 텍스트 프롬프트나 단일 이미지에서 3D 콘텐츠를 생성하는 기술이 최근 품질과 속도 면에서 눈에 띄는 발전을 이루었습니다.
- 현재 주요한 방법론들은 일관된 다중 시점 이미지를 생성하고 희소 시점 복원을 통해 3차원 구조를 재구성하는 것을 포함합니다.
- 많은 방법론들이 메시 표현을 직접 변형하는 도전에 직면하여, 희소 시점 복원 과정에서 암시적 표현(예: NeRF)을 학습하고, 타겟 메시를 후처리 추출을 통해 얻습니다.
- 그러나, 암시적 표현은 풍부한 3D 정보를 효과적으로 모델링 할 수 있지만, 그 훈련은 일반적으로 긴 수렴 시간을 요구합니다.
- 또한, 암시적 필드로부터의 후처리 추출 작업은 바람직하지 않은 시각적 아티팩트를 가져올 수 있습니다.
- 이 논문에서는 끝까지 타겟 메시를 재구성하는 새로운 단일 이미지-3D 생성 프레임워크인 FlexiDreamer를 제안합니다.
- FlexiCubes라고 알려진 유연한 그래디언트 기반 추출을 활용하여, 이 방법은 후처리에 의한 결함을 우회하고 타겟 메시를 직접 획득합니다.
- 더욱이, 우리는 기하학적 디테일을 포착하기 위해 FlexiCubes의 암시적 필드에 점진적으로 인코딩 레벨을 활성화하는 다중 해상도 해시 그리드 인코딩 스킴을 통합합니다.
- 주목할 만한 것은, FlexiDreamer는 단 한 장의 이미지에서 세밀한 3D 구조를 단일 NVIDIA A100 GPU에서 대략 1분 만에 복구할 수 있으며, 이는 이전 방법론들을 크게 앞지르는 성능을 보여줍니다.

### [Condition-Aware Neural Network for Controlled Image Generation](https://arxiv.org/abs/2404.01143)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.01143.png)

Vote: 9

Authors: Qinsheng Zhang, Zhuoyang Zhang, Ming-Yu Liu, Muyang Li, Song Han, Han Cai

- 이 논문에서는 조건을 기반으로 이미지 생성 모델에 제어를 추가하기 위해 Condition-Aware Neural Network (CAN)이라는 새로운 방법을 제시합니다.
- CAN은 입력 조건에 기반하여 합성곱/선형 계층에 대한 조건부 가중치를 생성하는 조건 인식 가중치 생성 모듈을 도입함으로써 신경망의 가중치를 동적으로 조작하여 이미지 생성 과정을 제어합니다.
- ImageNet에서의 클래스 조건부 이미지 생성 및 COCO 데이터셋에 대한 텍스트-이미지 생성에서 CAN을 테스트하였으며, 확산 트랜스포머 모델에 대해 일관된 개선을 보여줍니다.
- 특히, EfficientViT (CaT)과 결합된 CAN은 ImageNet 512x512에서 2.78 FID를 달성하여, 샘플링 단계당 MACs 수가 52배 적은 경우에도 DiT-XL/2를 능가합니다.

### [CosmicMan: A Text-to-Image Foundation Model for Humans](https://arxiv.org/abs/2404.01294)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.01294.png)

Vote: 7

Authors: Wayne Wu, Wentao Wang, Kwan-Yee Lin, Shikai Li, Jianglin Fu, Kaiyuan Liu

- 이 연구에서는 사람 이미지를 고해상도로 생성하는 데 특화된 텍스트-이미지 기반 모델인 CosmicMan을 제시합니다.
- 현재의 일반 목적 기반 모델이 사람의 텍스트-이미지 정합성과 품질 면에서 한계를 보이는 반면, CosmicMan은 상세한 묘사를 바탕으로 사실적인 인간 이미지를 정확하게 생성할 수 있습니다.
- CosmicMan의 성공 비결은 두 가지 새로운 관점에 있습니다: (1) 고품질 데이터와 확장 가능한 데이터 생산 흐름의 중요성을 인식하고, 지속적으로 고품질 데이터를 정확하고 비용 효과적인 주석을 통해 생산할 수 있는 'Annotate Anyone'이라는 새로운 데이터 생산 패러다임을 제안합니다.
- 이를 기반으로, 평균 해상도가 1488x1255인 600만 장의 고품질 실세계 인간 이미지와 1억 1500만 개의 다양한 세부 사항을 포함한 정확한 텍스트 주석으로 구성된 대규모 데이터셋 CosmicMan-HQ 1.0을 구축하였습니다.
- (2) 인간을 위한 텍스트-이미지 기반 모델은 현실적이어야 하며, 하류 과제에 손쉽게 통합할 수 있으면서도 고해상도 인간 이미지를 효과적으로 생산할 수 있어야 합니다.
- 이에 우리는 텍스트 설명과 이미지 픽셀 간의 관계를 분해적으로 모델링하고, Decomposed-Attention-Refocusing(Daring) 트레이닝 프레임워크를 제안합니다.
- Daring은 기존 텍스트-테이지 확산 모델에서 교차 주목 기능을 자연스럽게 분해하며, 추가 모듈 없이 주목을 다시 집중시킬 수 있습니다.
- Daring을 통해, 사람의 신체 구조와 일치하는 기본 그룹으로 연속적인 텍스트 공간을 명시적으로 구분하는 것이 텍스트-이미지 정합성 문제를 쉽게 해결하는 핵심임을 보여줍니다.

### [Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward](https://arxiv.org/abs/2404.01258)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.01258.png)

Vote: 7

Authors: Chunyuan Li, Zhiqing Sun, Ruohong Zhang, Liangke Gui, Yihao Feng, Di Fu, Yiming Yang, Yonatan Bisk, Keyang Xu, Yuanhan Zhang, Alexander Hauptmann

- 직접 선호 최적화(DPO) 기술은 대규모 언어 모델(LLM)의 일반화 능력을 향상시키는데 효과적이지만, 영상 지시사항을 따르는 작업에서 생성된 응답의 환각을 감지하는 정보적 피드백을 제공하는 것이 여전히 큰 도전입니다.
- 이전 연구들은 대규모 다중모달 모델(LMM)을 선호 모델링을 안내하는 보상 모델로 사용하는 것을 탐구했지만, 이러한 모델들이 생성된 응답의 사실성을 비디오와 대조하여 정확하게 평가하는 능력은 명확히 입증되지 않았습니다.
- 본 논문은 언어 모델이 비디오 내용의 대리 데이터로 상세한 비디오 캡션을 사용함으로써 비디오 질의응답(QA) 예측을 점수화하는데 필요한 증거를 통합할 수 있게 하는 새로운 프레임워크를 제시합니다.
- 우리의 접근법은 비디오 프레임을 직접 입력으로 사용하는 OpenAI GPT-4V 모델의 보상 메커니즘과 견고한 일치성을 보여줍니다.
- 또한, DPO를 통해 맞춤형 보상을 적용함으로써 비디오 LMM의 비디오 QA 작업에 대한 성능이 크게 향상됨을 보여줍니다.

### [Measuring Style Similarity in Diffusion Models](https://arxiv.org/abs/2404.01292)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.01292.png)

Vote: 6

Authors: Tom Goldstein, Jonas Geiping, Abhinav Shrivastava, Gowthami Somepalli, Micah Goldblum, Anubhav Gupta, Shramay Palta, Kamal Gupta

- 디자이너와 아티스트들이 널리 사용하는 생성 모델은 종종 훈련 데이터의 콘텐츠를 기억하여 재현합니다.
- 생성된 이미지가 전문 목적으로 사용되기 전에 이미지의 속성이 특정 훈련 데이터에 기인하는지 알아보기 위한 데이터베이스 검색의 중요성이 커지고 있습니다.
- 기존 도구들은 유사한 의미적 콘텐츠를 가진 이미지 검색에 중점을 두지만, 많은 예술가들은 텍스트-이미지 모델의 스타일 복제에 주목합니다.
- 본 연구는 이미지에서 스타일 서술자를 이해하고 추출하는 프레임워크를 제시하며, 스타일이 컬러, 텍스처, 형태 등의 복잡하지만 의미 있는 상호작용을 포착하는 주관적 속성임을 감안하여 새로운 데이터셋을 큐레이션했습니다.
- 텍스트-이미지 모델의 훈련 데이터셋에 사용된 이미지의 스타일을 생성된 이미지의 스타일에 귀속시키는 데 사용할 수 있는 스타일 서술자 추출 방법을 제안합니다.
- 다양한 스타일 검색 작업에서 유망한 결과를 보여주며, Stable Diffusion 모델에서 스타일 귀속 및 매칭을 정량적 및 정성적으로 분석합니다.
- 코드 및 아티팩트는 https://github.com/learn2phoenix/CSD 에서 확인할 수 있습니다.

### [MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview and Text](https://arxiv.org/abs/2404.00345)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.00345.png)

Vote: 6

Authors: Takayuki Hara, Tatsuya Harada

- 사용자 지정 조건에서 3D 장면을 생성하는 방법은 3D 애플리케이션의 제작 부담을 덜 수 있는 유망한 방법입니다.
- 기존 연구는 제한된 제어 조건으로 인해 원하는 장면을 실현하는데 상당한 노력이 필요했습니다.
- 저희는 부분 이미지, 탑뷰에서의 레이아웃 정보, 텍스트 프롬프트를 사용하는 다중 모드 조건하에서 3D 장면을 제어하고 생성하는 방법을 제안합니다.
- 이러한 조건들을 결합하여 3D 장면을 생성하는 것은 큰 데이터셋 생성, 다중 모드 조건의 상호작용 반영, 레이아웃 조건의 도메인 의존성 등의 중요한 어려움을 포함합니다.
- 저희는 3D 장면 생성 과정을 주어진 조건에서 2D 이미지 생성과 2D 이미지에서 3D 장면 생성으로 분해합니다.
- 2D 이미지 생성은 부분 이미지와 레이아웃의 소규모 인공 데이터셋으로 사전 훈련된 텍스트-이미지 모델을 미세조정함으로써 달성되며, 3D 장면 생성은 레이아웃 조건의 심층 추정과 신경 반사 필드(NeRF)를 통해 이루어집니다.
- 360도 이미지를 사용하는 공간 정보의 공통 표현을 사용함으로써 다중 모드 조건 상호작용을 고려할 수 있고 레이아웃 제어의 도메인 의존성을 줄일 수 있습니다.
- 실험 결과는 내부부터 외부까지 다양한 도메인에 걸쳐 다중 모드 조건에 따라 3D 장면을 생성할 수 있음을 질적 및 양적으로 입증했습니다.

### [WavLLM: Towards Robust and Adaptive Speech Large Language Model](https://arxiv.org/abs/2404.00656)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.00656.png)

Vote: 5

Authors: Jinyu Li, Hongkun Hao, Shujie Liu, Jing Pan, Furu Wei, Sunit Sivasankaran, Linquan Liu, Shujie Hu, Sanyuan Chen, Long Zhou, Xunying Liu

- 이 연구에서는 견고하고 적응성 있는 대규모 음성언어모델인 WavLLM을 소개되며, 듀얼 인코더와 프롬프트 인식 LoRA 가중치 어댑터를 통해 최적화되었습니다.
- WavLLM은 음성의 의미 내용을 처리하는 Whisper 인코더와 화자의 고유한 특성을 포착하는 WavLM 인코더를 활용함으로써 다양한 종류의 음성 정보를 분리합니다.
- 두 단계의 커리큘럼 학습 방식을 통해, WavLLM은 기본적인 능력 구축과 복잡한 음성 작업을 포함한 고급 다중 작업 훈련에 최적화됩니다.
- 다양한 작업과 지시사항에 맞게 유연하고 적응하기 위해 프롬프트 인식 LoRA 가중치 어댑터가 고급 다중 작업 훈련 단계에서 도입됩니다.
- 제안된 모델은 ASR, ST, SV, ER 등의 범용 음성 벤치마크를 포함하여 고등학교 영어 듣기 이해 세트인 Gaokao SQA와 음성 사고 연쇄(CoT) 평가 세트에 적용되어 우수한 성능을 증명합니다.
- 실험 결과, 제안된 모델은 동일한 모델 크기에서 복잡한 작업을 CoT 접근법을 사용하여 실행하는 데 있어 견고한 일반화 능력을 보여주며, 다양한 음성 작업에서 최신 성능을 달성합니다.
- 또한, 우리의 모델은 전문적인 훈련 없이도 Gaokao 작업을 성공적으로 완료합니다.
- 코드, 모델, 오디오 및 Gaokao 평가 세트는 aka.ms/wavllm에서 접근 가능합니다.

### [Streaming Dense Video Captioning](https://arxiv.org/abs/2404.01297)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.01297.png)

Vote: 5

Authors: Anurag Arnab, Shyamal Buch, Xingyi Zhou, Austin Myers, Shen Yan, Arsha Nagrani, Cordelia Schmid, Xuehan Xiong

- 본 논문은 긴 입력 비디오를 처리할 수 있고, 풍부하고 상세한 텍스트 설명을 예측하며, 전체 비디오 처리가 끝나기 전에 출력을 생성할 수 있는 이상적인 조밀한 비디오 캡셔닝(dense video captioning) 모델을 제안한다.
- 현재 최고 성능 모델은 고정된 수의 다운샘플링된 프레임을 처리하고, 전체 비디오를 본 후에 한 번에 전체 예측을 수행하지만, 제안된 모델은 두 가지 새로운 구성 요소를 포함한다.
- 첫째, 입력된 토큰을 클러스터링하여 고정된 크기의 메모리로 임의 길이의 비디오를 처리할 수 있는 새로운 메모리 모듈을 제안한다.
- 둘째, 전체 비디오 처리가 완료되기 전에 모델이 예측을 수행할 수 있도록 하는 스트리밍 디코딩 알고리즘을 개발했다.
- 이 스트리밍 기능을 통해 모델은 ActivityNet, YouCook2 및 ViTT 세 가지 조밀한 비디오 캡셔닝 벤치마크에서 현재의 최고 성능을 상당히 향상시킨다.
- 연구에 사용된 코드는 https://github.com/google-research/scenic 에서 확인할 수 있다.

### [Noise-Aware Training of Layout-Aware Language Models](https://arxiv.org/abs/2404.00488)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.00488.png)

Vote: 4

Authors: Ritesh Sarkhel, Xiaoqi Ren, Arnab Nandi, Vincent Perot, Lauro Beltrao Costa, Guolong Su, Yanan Xie, Emmanouil Koukoumidis

- 시각적으로 풍부한 문서에서 정보를 전달하는데 시각적 특징과 언어적 단서를 모두 활용할 수 있는 맞춤형 추출기의 훈련에 다량의 텍스트 및 시각적 모달리티가 주석된 대상 문서 유형의 인스턴스가 필요하나, 이는 기업 환경에서 비용이 많이 드는 병목 현상을 초래합니다.
- 연구진은 작업량이 많은 다양한 문서 유형에 대한 맞춤형 추출기를 확장 가능한 방식으로 훈련하기 위해 빠른 훈련 시간을 초과하지 않으면서, 비싼 인간의 주석이 없는 문서를 사용하는 'Noise-Aware Training(소음 인식 훈련, NAT)' 방법을 제안합니다.
- NAT는 모델 훈련에 약한 레이블이 붙은 문서를 활용하고, 잡음이 많은 레이블로 인한 모델 품질 저하를 방지하기 위해 각 학습 샘플의 신뢰도를 추정하고 훈련 중 불확실성 측정으로 활용합니다.
- 여러 최신 추출기 모델을 NAT를 사용해 훈련시킨 결과, NAT로 훈련된 모델은 안정적인 성능을 보여주며, 전이 학습 기준을 최대 6%까지 매크로-F1 점수로 능가하고, 비교할 수 있는 성능을 얻기 위해 필요한 인간 노력을 최대 73%까지 줄이는 것으로 나타났습니다.

### [ST-LLM: Large Language Models Are Effective Temporal Learners](https://arxiv.org/abs/2404.00308)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.00308.png)

Vote: 2

Authors: Ying Shan, Haoran Tang, Chen Li, Ruyang Liu, Ge Li, Yixiao Ge

- ST-LLM은 공간-시간 토큰을 대규모 언어 모델(LLM)에 직접 입력하여 동영상 이해 능력을 향상시키는 새로운 접근 방식을 제안합니다.
- 이 방법은 간단하면서도 놀라운 향상을 가져오고, 효과적인 비디오 시퀀스 모델링을 위한 ST-LLM을 소개합니다.
- 비압축 비디오 토큰으로 인한 부하와 안정성 문제를 해결하기 위해 동적 마스킹 전략과 맞춤형 훈련 목표를 개발합니다.
- 특히 긴 비디오의 경우, 효율성과 효과성의 균형을 맞추기 위한 글로벌-로컬 입력 모듈을 설계하였습니다.
- ST-LLM은 효율성과 안정성을 유지하면서 LLM을 사용하여 숙련된 공간-시간 모델링을 수행합니다.
- 광범위한 실험 결과는 제안된 방식의 효과를 증명하며, ST-LLM은 VideoChatGPT-Bench와 MVBench에서 새로운 최고 성과를 달성합니다.
- 소스 코드는 https://github.com/TencentARC/ST-LLM 에서 제공됩니다.

