## Daily Papers (2024-04-09)

### [Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs](https://arxiv.org/abs/2404.05719)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.05719.png)

Vote: 35

Authors: Jeffrey Nichols, Yinfei Yang, Zhe Gan, Haotian Zhang, Eldon Schoop, Amanda Swearngin, Keen You, Floris Weers

- 최근의 다양체 큰 언어 모델(MLLMs)이 주목할 만한 발전을 이루었지만, 일반 영역의 MLLMs는 사용자 인터페이스(UI) 화면을 이해하고 상호작용하는 데 있어서 종종 한계가 있습니다.
- 본 논문에서는 모바일 UI 화면의 이해를 향상시키기 위해 참조, 구상, 추리 능력이 갖춰진 새로운 MLLM인 Ferret-UI를 제안합니다.
- UI 화면은 자연 이미지보다 종횡비가 길고 관심 대상(예: 아이콘, 텍스트)이 더 작으므로, 우리는 'any resolution'를 Ferret에 도입하여 세부 사항을 확대하고 강화된 시각적 특징을 활용합니다.
- 각 화면은 원래의 종횡비에 따라 2개의 하위 이미지(세로 화면은 수평 분할, 가로 화면은 수직 분할)로 나누어지며, 이들 각각이 별도로 인코딩되어 LLM으로 전송됩니다.
- 아이콘 인식, 텍스트 찾기, 위젯 리스트 등의 기본 UI 작업에서 꼼꼼하게 수집한 훈련 샘플을 지시사항을 따르는 형식에 지역 주석을 넣어 정확한 참조와 구상을 용이하게 합니다.
- 모델의 추리 능력을 증진시키기 위해, 상세한 설명, 인식/상호작용 대화, 기능 추론을 포함하는 고급 작업을 위한 데이터 세트를 추가로 컴파일합니다.
- 정교하게 구성된 데이터 세트에서 훈련 후 Ferret-UI는 UI 화면에 대한 뛰어난 이해력과 개방형 지시사항을 수행하는 능력을 보여줍니다.
- 모델 평가를 위해 앞서 언급한 모든 작업을 포함하는 포괄적인 벤치마크를 설정하였으며, Ferret-UI는 대부분의 오픈소스 UI MLLMs를 능가할 뿐만 아니라 모든 기본 UI 작업에서 GPT-4V를 초월합니다.

### [ByteEdit: Boost, Comply and Accelerate Generative Image Editing](https://arxiv.org/abs/2404.04860)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04860.png)

Vote: 20

Authors: Qianqian Wang, Huafeng Kuang, Xuefeng Xiao, Pan Xie, Yixing Zhu, Yuxi Ren, Yanzuo Lu, Min Zheng, Lean Fu, Shiyin Wang, Xionghui Wang, Yitong Wang, Jie Wu, Xin Xia

- 디퓨전 기반의 생성적 이미지 편집 기술의 최근 발전에도 불구하고, 품질 저하, 일관성 부족, 지시 사항 미준수, 생성 효율성 미흡 등의 도전 과제가 있습니다.
- 우리는 이러한 장애물을 해결하기 위해 ByteEdit이라는 혁신적인 피드백 학습 프레임워크를 제안하여 이미지 편집 작업을 촉진시키고, 준수하며, 가속화합니다.
- ByteEdit는 미적 아름다움과 이미지-텍스트 정렬을 개선하기 위한 이미지 보상 모델을 통합하고, 결과의 일관성을 증진시키기 위한 밀도 높은 픽셀 수준의 보상 모델을 새롭게 도입합니다.
- 또한, 모델의 추론 속도를 가속화하기 위해 선도적인 적대적이고 점진적인 피드백 학습 전략을 제안합니다.
- 대규모 사용자 평가를 통해 ByteEdit이 Adobe, Canva, MeiTu 등 최고의 생성적 이미지 편집 제품을 품질과 일관성 면에서 뛰어넘는 것으로 나타났습니다.
- ByteEdit-Outpainting은 기준 모델 대비 품질 388%, 일관성 135% 향상을 보여주는 등의 주목할 만한 강화를 시현합니다.
- 실험은 또한 우리의 가속화 모델이 품질과 일관성 면에서 뛰어난 성능 결과를 유지하는 것을 확인시켜 주었습니다.

### [BeyondScene: Higher-Resolution Human-Centric Scene Generation With Pretrained Diffusion](https://arxiv.org/abs/2404.04544)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04544.png)

Vote: 20

Authors: Hayeon Kim, Hoigi Seo, Gwanghyun Kim, Se Young Chun, Dong Un Kang

- 기존 텍스트-이미지 확산 모델의 낮은 훈련 이미지 크기, 텍스트 인코더의 용량 제한, 복잡한 인간 중심의 장면 생성의 어려움으로 인해 고해상도 인간 중심 장면을 세부적으로 제어하여 생성하는 것이 여전히 도전적입니다.
- BeyondScene은 기존의 사전 훈련된 확산 모델을 사용하여 8K 이상의 고해상도 인간 중심 장면을 탁월한 텍스트-이미지 일치와 자연스러움으로 생성하도록 설계된 새로운 프레임워크입니다.
- 초기에 다중 인간 인스턴스 생성에 중요한 요소에 집중하여 텍스트 토큰 한계를 넘어선 상세한 기본 이미지를 생성한 후, 다단계 및 계층적 접근을 통해 이를 더 높은 해상도의 출력으로 자연스럽게 변환합니다.
- 고주파 주입 전진 확산과 적응형 결합 확산을 포함하는 고안된 인스턴스 인식 계층적 확대 과정을 사용하여 훈련 이미지 크기를 초과하고 텍스트 및 인스턴스를 인식하는 세부 사항을 포함하게 합니다.
- BeyondScene은 상세한 텍스트 설명과 자연스러움과의 일치성 측면에서 기존 방법들을 능가하며, 비용이 많이 드는 재훈련 없이도 사전 훈련된 확산 모델의 능력을 넘어서는 고해상도 인간 중심 장면 생성을 위한 고급 응용 프로그램의 길을 열게 합니다.
- 프로젝트 페이지는 https://janeyeon.github.io/beyond-scene 에서 확인할 수 있습니다.

### [UniFL: Improve Stable Diffusion via Unified Feedback Learning](https://arxiv.org/abs/2404.05595)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.05595.png)

Vote: 19

Authors: Weilin Huang, Huafeng Kuang, Xuefeng Xiao, Jiacheng Zhang, Pan Xie, Yuxi Ren, Min Zheng, Lean Fu, Jiashi Li, Guanbin Li, Jie Wu, Xin Xia

- 확산 모델을 기반으로 한 이미지 생성 분야에 혁신을 일으킨 가운데, 다양한 하류 응용 프로그램에서 고품질 모델이 대거 등장했음에도 불구하고, 기존의 경쟁 솔루션들은 여전히 시각적 품질, 미적 매력, 효율성에 있어 명확한 해결책 없이 여러 제한 사항을 안고 있습니다.
- 이러한 도전을 극복하기 위해 우리는 획기적인 유명 방법인 SD1.5와 SDXL을 포함한 다양한 확산 모델에 적용할 수 있는 범용적이고 효과적이며 일반화 가능한 솔루션인 UniFL, 통합 피드백 학습 프레임워크를 제시합니다.
- UniFL은 세 가지 중요한 구성 요소를 통합했습니다: 시각적 품질을 향상시키는 감각적 피드백 학습, 미적 매력을 향상시키는 분리된 피드백 학습, 그리고 추론 속도를 최적화하는 적대적 피드백 학습.
- 심층 실험과 폭넓은 사용자 연구를 통해 생성된 모델의 품질 향상과 가속화에 있어 우리의 제안 방법이 뛰어난 성능을 검증하였습니다. 예를 들면, UniFL은 생성 품질 면에서 사용자 선호도가 17% 높았으며, 4단계 추론에서 LCM과 SDXL Turbo보다 각각 57% 및 20% 빠른 속도를 보였습니다.
- 또한, Lora, ControlNet, AnimateDiff 등의 하류 작업에서 우리의 접근법이 효과적임을 확인했습니다.

### [MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators](https://arxiv.org/abs/2404.05014)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.05014.png)

Vote: 18

Authors: Ruijie Zhu, Jinfa Huang, Xinhua Cheng, Yujun Shi, Yongqi Xu, Jiebo Luo, Bin Lin, Shenghai Yuan, Li Yuan

- 텍스트에서 동영상으로 변환하는 기술에 있어 기존 모델은 현실 세계의 물리적 지식을 충분히 표현하지 못해, 제한된 움직임과 다양성을 나타내는 문제가 있었습니다.
- 본 논문에서는 움직이는 물체의 변화 과정을 학습하여 물리 지식을 반영한 MagicTime, 즉 시간 경과 동영상을 생성하는 메타모픽 모델을 제안합니다.
- MagicAdapter라는 방식을 도입해 공간적, 시간적 학습을 분리하고, 기존 텍스트-비디오 변환 모델을 개조하여 변화 과정을 나타내는 동영상을 생성할 수 있게 합니다.
- 실세계의 물리학적 지식이 더 많이 들어 있는 변형 시간 경과 동영상에 적합하도록 Dynamic Frames Extraction 전략을 소개합니다.
- 변형 비디오 프롬프트에 대한 이해를 향상시키기 위해 Magic Text-Encoder를 도입하였습니다.
- 특히 메타모픽 비디오 생성 능력을 개발하기 위해 ChronoMagic이라는 시간 경과 비디오-텍스트 데이터 세트를 생성하였습니다.
- 광범위한 실험을 통해 MagicTime이 고품질이며 역동적인 메타모픽 비디오 생성에 탁월함을 입증했으며, 시간 경과 비디오 생성이 물리 세계의 메타모픽 시뮬레이터를 구축하는 유망한 방향임을 보여줍니다.

### [SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing](https://arxiv.org/abs/2404.05717)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.05717.png)

Vote: 15

Authors: Zhifei Zhang, Wei Xiong, HyunJoon Jung, Xin Eric Wang, He Zhang, Jing Gu, Qing Liu, Jianming Zhang, Yilin Wang, Nanxuan Zhao

- 개인 창의성을 표현하고 시각적 이야기의 질과 영향력을 향상시키는데 중요한 역할을 하는 퍼스널 콘텐츠 편집을 강화하기 위하여, 본 연구에서는 이미지 속 임의의 객체를 참조된 개념으로 교체하면서 맥락은 유지하는 새로운 프레임워크인 SwapAnything을 소개한다.
- 기존의 개인화된 주체 교체 방법들과 비교하여, SwapAnything은 주체뿐만 아니라 임의의 객체 및 부분에 대한 정밀한 컨트롤, 맥락 픽셀의 더 충실한 보존, 이미지에 개인화된 개념을 더 잘 적용할 수 있는 세 가지 독특한 장점을 가진다.
- 먼저, 신뢰성 있는 맥락 보존과 초기 의미론적 개념 교체를 위해 마스킹된 변수를 교체하는 'targeted variable swapping'을 제안하며, 여기서 잠재 특징 맵 위에 영역 컨트롤을 적용한다.
- 이어서, 생성 과정 중에 대상 위치, 모양, 스타일 및 내용 측면에서 원본 이미지에 의미론적 개념을 자연스럽게 적용하기 위한 'appearance adaptation'을 도입한다.
- 인간 및 자동 평가 결과를 통해 다양한 실험에서 기존 방법론들에 비해 개인화된 교체에 있어서 SwapAnything의 뛰어난 성능 개선을 입증하였다.
- SwapAnything은 단일 객체, 다중 객체, 부분 객체 및 크로스 도메인 교체 작업을 포함하여, 정확하고 신뢰할 수 있는 교체 능력을 보여준다.
- 나아가, SwapAnything은 텍스트 기반 교체 및 객체 삽입과 같이 교체를 넘어선 작업에서도 훌륭한 성능을 달성한다.

### [SpatialTracker: Tracking Any 2D Pixels in 3D Space](https://arxiv.org/abs/2404.04319)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04319.png)

Vote: 14

Authors: Qianqian Wang, Shangzhan Zhang, Sida Peng, Xiaowei Zhou, Yuxi Xiao, Nan Xue, Yujun Shen

- 동영상에서의 밀도 높고 장거리 픽셀 운동을 복구하는 것은 어려운 문제입니다.
- 이 문제의 일부는 3D에서 2D로의 투영 과정에서 발생하는 가림 현상과 불연속성 때문입니다.
- 우리는 기본적인 3D 운동이 종종 간단하고 저차원일 수 있다고 가정하며, 이미지 투영에 의한 문제를 완화하기 위해 점 궤적을 3D 공간에서 추정하려고 합니다.
- 우리의 방법론, 'SpatialTracker'는 단안 깊이 추정기를 사용하여 2D 픽셀을 3D로 변환하고, 각 프레임의 3D 콘텐츠를 효율적으로 트라이플랜 표현을 사용하여 나타내며, 변환기를 사용하여 반복 업데이트를 수행하여 3D 궤적을 추정합니다.
- 3D 추적을 통해 유연성 있는 구속 조건(Arap constraints)을 활용하고, 동시에 픽셀을 다른 강성 부분으로 클러스터링하는 강성 임베딩을 학습합니다.
- 광범위한 평가를 통해 우리의 접근법이 특히 평면 회전과 같이 도전적인 시나리오에서 질적 및 양적으로 최신 추적 성능을 달성한다는 것을 보여줍니다.

### [PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations](https://arxiv.org/abs/2404.04421)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04421.png)

Vote: 12

Authors: Thabo Beeler, Gordon Wetzstein, Qingqing Zhao, Yang Zheng, Federico Tombari, Florian Dubost, Leonidas Guibas, Guandao Yang, Dmitry Lagun, Donglai Xiang, Wang Yifan

- 이 연구는 시각 관측을 통해 옷 입은 인간의 3D 아바타를 재구축하는 새로운 프레임워크인 PhysAvatar를 도입합니다.
- 주요 기술로는 공간-시간 메시 추적을 위한 메시 정렬 4D 가우시안 기술과 물질의 본질적 특성을 추정하기 위한 물리 기반 역 렌더링을 채택했습니다.
- PhysAvatar는 경사 기반 최적화를 사용하여 원리적인 방식으로 옷감의 물리적 매개변수를 추정하는 물리 시뮬레이터를 통합합니다.
- 이 프레임워크는 훈련 데이터에서 보지 못한 동작과 조명 조건에서 느슨한 옷을 입은 아바타의 고품질 새로운 시점 렌더링을 생성할 수 있는 새로운 기능을 제공합니다.
- 물리 기반 역 렌더링과 물리 루프를 사용하여 사실적인 디지털 인간을 모델링하는 방향으로 중요한 진전을 나타냅니다.
- 프로젝트 웹사이트 주소는 다음과 같습니다: https://qingqing-zhao.github.io/PhysAvatar

### [MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding](https://arxiv.org/abs/2404.05726)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.05726.png)

Vote: 11

Authors: Ashish Shah, Abhinav Shrivastava, Menglin Jia, Bo He, Xuefei Cao, Young Kyun Jang, Hengduo Li, Ser-Nam Lim

- 대규모 언어 모델(LLMs)의 성공을 바탕으로, 최근 비전-언어 기초 모델을 구축하기 위해 비전 모델을 LLMs에 통합하는 데 더 많은 관심이 집중되고 있습니다.
- 현재의 LLM 기반 대규모 멀티모달 모델들은 제한된 프레임 수 만을 입력받아 단기 비디오 이해에 초점을 맞추고 있습니다.
- 본 연구에서는 장기 비디오 이해를 위한 효율적이고 효과적인 모델 설계에 주로 집중하였습니다.
- 동시에 많은 프레임을 처리하려는 기존 작업과 달리, 온라인 방식으로 비디오를 처리하고 과거 비디오 정보를 메모리 뱅크에 저장하는 방법을 제안합니다.
- 이 메모리 뱅크를 통해 모델은 LLM의 컨텍스트 길이 제약이나 GPU 메모리 한계를 넘지 않으면서 역사적 비디오 콘텐츠를 참조할 수 있습니다.
- 제안된 메모리 뱅크는 현재의 멀티모달 LLMs에 즉시 통합할 수 있습니다.
- 장기 비디오 이해, 비디오 질문 응답, 비디오 캡션 생성 등 다양한 비디오 이해 작업에 대한 광범위한 실험을 수행하였고, 다수의 데이터셋에서 최신 성능을 달성하였습니다.
- 관련 코드는 https://boheumd.github.io/MA-LMM/ 에서 제공됩니다.

### [Diffusion-RWKV: Scaling RWKV-Like Architectures for Diffusion Models](https://arxiv.org/abs/2404.04478)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04478.png)

Vote: 8

Authors: Junshi Huang, Debang Li, Mingyuan Fan, Zhengcong Fei, Changqian Yu

- 트랜스포머는 컴퓨터 비전과 자연어 처리(NLP) 분야에서 진보를 촉진했으나 그들의 복잡한 계산량 때문에 고해상도 이미지 생성 같은 장문의 상황에서의 활용에 제한이 있다.
- 본 논문에서는 NLP에서 사용되는 RWKV 모델에서 유래한 일련의 구조들을 소개하고, 이미지 생성을 위한 확산 모델에 적용하기 위해 필요한 수정을 했으며 이를 'Diffusion-RWKV'라고 명명한다.
- 변형된 RWKV 구조는 패치로 나눈 입력을 순차적으로 효율적으로 다룰 수 있으며, 대규모 인자와 방대한 데이터셋을 지원하면서 효과적으로 확장 가능하다.
- 특히 Diffusion-RWKV는 공간 집계의 복잡성을 줄여, 고해상도 이미지 처리에 매우 적합하며 창이나 그룹 캐시 작업이 필요 없어진다는 이점이 있다.
- 조건부 및 비조건부 이미지 생성 작업에 대한 실험 결과, Diffusion-RWKV는 기존의 CNN이나 Transformer 기반 확산 모델과 비교해 FID 및 IS 지표에서 동등하거나 더 나은 성능을 보이는 동시에, 총 연산 FLOP 사용량을 현저히 줄였음을 입증한다.

### [MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation](https://arxiv.org/abs/2404.05674)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.05674.png)

Vote: 7

Authors: Kunpeng Song, Ahmed Elgammal, Qing Yan, Yizhe Zhu, Xiao Yang, Bingchen Liu

- 본 논문에서는 영상생성에 강건한 영상대영상(이미지 투 이미지) 번역 수요에 부응하여, 텍스트 기반 및 영상 기반 정보를 통합하는 변형 없는 오픈 사전 개인화 영상 모델 MoMA를 제시한다.
- MoMA는 오픈소스 다중모드 대규모 언어 모델(Multimodal Large Language Model, MLLM)을 활용하여 특징 추출기와 생성기로서의 이중 역할을 수행하도록 훈련되었다.
- 이 모델은 참조 영상과 텍스트 프롬프트 정보를 효과적으로 결합하여 가치 있는 영상 특징을 생산함으로써 확산모델에 적합하게 만든다.
- 또한 생성된 특징을 더 잘 활용하기 위해 새로운 자기주의(self-attention) 단축 방법을 도입하여 영상 특징을 효율적으로 확산모델로 전송되게 하여 목표 객체의 유사성을 향상시킨다.
- 기존의 방법들보다 구체적인 디테일 충실도, 신원 유지, 프롬프트 충실도 측면에서 더 뛰어난 이미지를 생성할 수 있음에도 불구하고, 주목할만한 점은 단 하나의 참조 이미지만을 필요로 하는 튜닝-프리 plug-and-play 모듈이란 점이다.
- 이 연구는 오픈소스로 제공되어, 이러한 진보를 전 세계적으로 접근 가능하게 한다.

### [YaART: Yet Another ART Rendering Technology](https://arxiv.org/abs/2404.05666)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.05666.png)

Vote: 7

Authors: Alexander Shishenya, Alexander Ustyuzhanin, Denis Kuznedelev, Artem Khurshudov, Anastasiia Tabisheva, Marina Kaminskaia, Valerii Startsev, Eugene Lyapustin, Grigoriy Livshits, Mikhail Romanov, Daniil Shlenskii, Sergey Kastryulin, Nikita Vinokurov, Liubov Chubarova, Dmitrii Kornilov, Sergei Ovcharenko, Alexander Markovich, Artemii Shvetsov, Alexander Tselousov, Artem Babenko, Artem Konev, +, Alexey Kirillov

- 본 연구에서는 생성 모델의 빠르게 진행되는 분야에서 효율적이고 고해상도의 텍스트-이미지 확산 시스템을 개발하는 것이 중요한 전선임을 소개하고 있습니다.
- YaART는 인간 선호도에 맞춰 강화 학습을 통해 조정된 생산급 캐스캐이드 확산 모델을 제안합니다.
- 연구팀은 특히 모델 및 훈련 데이터셋 크기 선택에 중점을 두고 이들이 캐스캐이드 확산 모델의 텍스트-이미지 변환에 미치는 영향을 체계적으로 조사했습니다.
- 고품질 이미지의 작은 데이터셋으로 훈련된 모델이 더 큰 데이터셋으로 훈련된 모델과 경쟁할 수 있음을 보여주어, 확산 모델 훈련의 더 효율적인 시나리오를 설정합니다.
- YaART는 사용자 평가에서 기존의 최첨단 모델들보다 일관되게 선호되어 질적 관점에서의 우수성을 입증합니다.

### [Aligning Diffusion Models by Optimizing Human Utility](https://arxiv.org/abs/2404.04465)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04465.png)

Vote: 7

Authors: Yusuke Kato, Akash Gokul, Konstantinos Kallidromitis, Shufan Li, Kazuki Kozuka

- 본 논문에서는 인간의 유틸리티 기대치 극대화를 정렬 목표로 하는 새로운 접근법인 Diffusion-KTO를 제시합니다.
- 기존의 방법과 달리, Diffusion-KTO는 비싼 쌍(pairwise) 선호 데이터 수집이나 복잡한 보상 모델 학습을 필요로 하지 않습니다.
- 본 연구의 목표는 단순 이미지 별 이진 피드백 신호(예: 좋아요 또는 싫어요)를 필요로 하며, 이러한 데이터는 풍부하게 사용 가능합니다.
- Diffusion-KTO로 미세 조정된 텍스트-이미지 확산 모델은 최신 기법들, 특히 감독된 미세 조정 및 Diffusion-DPO 기법에 비해 우수한 성능을 보여줍니다.
- 이러한 성과는 인간의 판단뿐만 아니라 PickScore와 ImageReward 같은 자동 평가 메트릭에 근거하여 입증되었습니다.
- 따라서, Diffusion-KTO는 이미지 별 바이너리 신호의 잠재력을 활용하여 텍스트-이미지 확산 모델을 인간의 선호도와 일치시킬 수 있는 적용 가능성을 확대합니다.

### [DATENeRF: Depth-Aware Text-based Editing of NeRFs](https://arxiv.org/abs/2404.04526)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04526.png)

Vote: 7

Authors: Julien Philip, Sai Bi, Fujun Luan, Sara Rojas, Kai Zhang, Kalyan Sunkavall, Bernard Ghanem

- 최근 확산 모델의 발전은 텍스트 프롬프트를 기반으로 2D 이미지를 편집하는데 놀라운 능력을 보여주었지만, 이러한 기술을 Neural Radiance Fields(NeRF)의 장면 편집으로 확장하는 것은 여러 관점에서 일관성을 유지하는 복잡함이 있습니다.
- 본 논문에서는 NeRF 장면의 기하학적 구조를 활용 함으로써 이러한 2D 편집 기술을 통합할 수 있는 핵심적인 통찰을 제시합니다.
- 깊이 정보 조건을 갖는 ControlNet을 사용하여 각 2D 이미지 수정의 일관성을 강화하는 새로운 접근법을 도입하였습니다.
- 또한, NeRF 장면의 깊이 정보를 이용한 인페인팅 방법을 소개하여 다양한 이미지에 걸친 2D 편집을 분배함으로써 오류와 재샘플링 문제에 대한 강인함을 제공합니다.
- 우리의 방법론은 기존의 NeRF 장면 편집 방법들보다 더 일관되고 생생하며 상세한 편집 결과를 달성한다는 것을 결과를 통해 보여줍니다.

### [Koala: Key frame-conditioned long video-LLM](https://arxiv.org/abs/2404.04346)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04346.png)

Vote: 5

Authors: Kate Saenko, Jui-hsien Wang, Bryan A. Plummer, Ximeng Sun, Hanieh Deilamsalehy, Reuben Tan, Bryan Russell, Ping Hu

- 장시간 비디오 질문 응답은 단기간 활동 인식과 세밀한 관계 추론을 포함하는 어려운 과제입니다.
- 최신 비디오 대형 언어 모델(vLLM)은 새로운 과제에 대한 능력을 보여줬지만, 몇 분 길이의 영상을 이해하고 이에 대한 질문에 정확히 답변하는데 한계가 있습니다.
- 이 문제를 해결하기 위해 Koala(Key frame-conditioned long video-LLM)이라는 경량 자기 감독 방식을 제안합니다.
- Koala는 기존 vLLM에 학습 가능한 시공간 쿼리를 도입하여 장기간 비디오로 일반화하는 방법을 적용합니다.
- 이 방법은 비디오의 키 프레임에서 계산된 시각 토큰에 조건을 부여하는 두 가지 새로운 토크나이저를 사용합니다.
- Koala는 HowTo100M 데이터셋으로 학습되어, 제로샷 장시간 비디오 이해 벤치마크에서 최신 대형 모델보다 3-6%의 절대 정확도로 성능이 우수함을 보여줍니다.
- 놀랍게도, 이 접근은 비디오 LL을 사용하여 장기간 비디오를 이해하는 데만 도움이 되는 것이 아니라 단기간 행동 인식의 정확도도 향상시킨다는 것을 실증적으로 보여줍니다.

