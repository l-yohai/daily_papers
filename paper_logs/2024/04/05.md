## Daily Papers (2024-04-05)

### [ReFT: Representation Finetuning for Language Models](https://arxiv.org/abs/2404.03592)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03592.png)

Vote: 23

Authors: Christopher D. Manning, Zhengxuan Wu, Christopher Potts, Aryaman Arora, Zheng Wang, Dan Jurafsky, Atticus Geiger

- 큰 모델을 소수의 가중치 업데이트를 통해 적응시키는 파라미터 효율적인 파인튜닝(PEFT) 방법을 추구하고 있다.
- 기존 연구에서 대표성(representations)이 풍부한 의미 정보를 인코딩 한다는 점을 드러냄에 따라, 대표성을 수정하는 것이 더 효과적인 대안일 수 있다는 가설을 추구하고 있다.
- 고정된 기본 모델에 작업 특정적인 개입을 학습하는 다양한 대표성 파인튜닝(ReFT) 방법을 개발하였다.
- ReFT의 강력한 사례로서, 저랭크 선형 부공간 ReFT (LoReFT)를 정의했다.
- LoReFT는 기존 PEFT들을 대체할 수 있는 방법으로, 이전 최첨단의 PEFT보다 10배에서 50배까지 더 매개변수 효율이 우수하다.
- 상식적 추론 작업 8가지, 산수 추론 작업 4가지, Alpaca-Eval v1.0, 그리고 GLUE 등 다양한 평가에서 LoReFT는 효율성과 성능의 최적 균형을 제공하며 대부분의 평가에서 최신 PEFT 성능을 넘어섰다.
- 일반적인 ReFT 훈련 라이브러리를 https://github.com/stanfordnlp/pyreft 에서 공개하였다.

### [CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching](https://arxiv.org/abs/2404.03653)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03653.png)

Vote: 17

Authors: Dongzhi Jiang, Dazhong Shen, Renrui Zhang, Yu Liu, Zhuofan Zong, Guanglu Song, Hongsheng Li, Xiaoshi Wu

- 텍스트-이미지 생성 분야에서 확산 모델의 성공에도 불구하고, 텍스트 프롬프트와 이미지 간의 부정확한 정렬 문제가 지속적으로 제기되고 있다.
- 이러한 정렬 오류의 근본적 원인은 충분히 조사되지 않았으며, 불충분한 토큰 주의 활성화에 의해 발생한다는 점을 관찰하였다.
- 확산 모델의 훈련 패러다임에 의한 조건 활용 부족이 이 현상을 유발한다고 판단하였다.
- 본 연구에서는 이미지-텍스트 개념 정렬 메커니즘을 이용하여 끝까지 학습되는 확산 모델을 미세 조정하는 새로운 전략인 CoMat를 제안한다.
- 이미지 캡션 모델을 사용해 이미지-텍스트 정렬을 측정하고, 무시된 토큰에 대한 확산 모델의 주의를 다시 이끌어내는 방식을 채택하였다.
- 또한, 속성 결합 문제를 해결하기 위해 새로운 속성 집중 모듈을 제안한다.
- 어떠한 이미지나 인간 선호 데이터 없이, 오직 20K 텍스트 프롬프트만을 사용하여 SDXL을 미세 조정해 CoMat-SDXL을 얻었다.
- CoMat-SDXL은 텍스트-이미지 정렬 벤치마크 두 개에서 기존 모델 SDXL을 크게 앞서며 최첨단 성능을 달성하였다는 결과가 포괄적인 실험을 통해 입증되었다.

### [MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens](https://arxiv.org/abs/2404.03413)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03413.png)

Vote: 14

Authors: Eslam Abdelrahman, Deyao Zhu, Xiaoqian Shen, Jian Ding, Essam Sleiman, Mohamed Elhoseiny, Kirolos Ataallah

- 본 논문은 비디오 이해를 위해 특별히 설계된 다모드 대규모 언어 모델(Large Language Model, LLM), MiniGPT4-Video를 소개합니다.
- 시간에 따른 시각적 및 텍스트 데이터 모두를 처리할 수 있는 이 모델은 비디오의 복잡성을 이해하는 데 능숙합니다.
- 단일 이미지에 대한 시각적 특징을 LLM 공간으로 효과적으로 변환하여 이미지-텍스트 벤치마크에서 인상적인 성과를 거둔 MiniGPT-v2의 성공을 바탕으로, 프레임 시퀀스를 처리하여 비디오를 이해할 수 있는 능력으로 확장되었습니다.
- MiniGPT4-Video 모델은 시각적 컨텐츠 뿐만 아니라 텍스트 대화도 포함하여 시각적 및 문자 요소가 모두 포함된 질의에 효과적으로 답변할 수 있습니다.
- 제안된 모델은 MSVD, MSRVTT, TGIF, TVQA 벤치마크에서 각각 4.22%, 1.13%, 20.82%, 13.1%의 개선으로 기존 최고 수준의 방법을 능가합니다.
- 모델과 코드는 https://vision-cair.github.io/MiniGPT4-video/ 에서 공개적으로 사용할 수 있습니다.

### [Training LLMs over Neurally Compressed Text](https://arxiv.org/abs/2404.03626)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03626.png)

Vote: 13

Authors: Brian Lester, Jascha Sohl-Dickstein, Jeffrey Pennington, Noah Constant, Jaehoon Lee, Alex Alemi, Adam Roberts

- 본 논문은 고도로 압축된 텍스트에서 대규모 언어 모델(LLMs)을 훈련하는 아이디어를 탐구합니다.
- 표준 서브워드 토크나이저들은 텍스트를 소규모로 압축하지만, 신경 텍스트 압축기는 훨씬 높은 압축률을 달성할 수 있습니다.
- 신경 압축 텍스트를 직접 훈련하는 것이 가능하다면, 훈련과 서빙 효율성 개선 및 장문의 텍스트 처리 용이성을 제공할 것입니다.
- 강력한 압축은 배울 수 있는 내용을 만들지 못하는 불투명한 출력을 만들어 이 목표에 장애물이 됩니다.
- LLMs가 아리스메틱 코딩으로 단순히 압축된 텍스트를 쉽게 학습할 수 없다는 것을 발견했습니다.
- 이를 극복하기 위해, 동일 정보 윈도우(Equal-Info Windows)라는 새로운 압축 기술을 제안합니다; 여기서 텍스트는 각각 같은 비트 길이로 압축되는 블록으로 세분화됩니다.
- 이 방법을 사용하여, 신경 압축된 텍스트에서 효과적으로 학습하는 것을 보여주며, 규모와 함께 개선되어, 놀라움과 추론 속도 지표에서 바이트 수준의 기준보다 훨씬 뛰어나다고 입증합니다.
- 비록 동일한 파라미터 수로 훈련된 모델에 비해 더 나쁜 놀라움을 제공하지만, 더 짧은 시퀀스 길이의 혜택을 가지고 있습니다.
- 더 짧은 시퀀스 길이는 더 적은 자동회귀 생성 단계가 필요하며, 지연 시간을 감소시킵니다.
- 마지막으로, 학습 가능성에 기여하는 속성에 대한 광범위한 분석을 제공하며, 고압축 토크나이저의 성능을 더욱 향상시키기 위한 구체적인 제안을 제시합니다.

### [LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models](https://arxiv.org/abs/2404.03118)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03118.png)

Vote: 12

Authors: Anahita Bhiwandiwalla, Raanan Yehezkel Rohekar, Matthew Lyle Olson, Vasudev Lal, Estelle Aflalo, Gabriela Ben Melech Stan, Nan Duan, Chenfei Wu, Shao-Yen Tseng, Yaniv Gurwicz

- 복합 모달 대규모 언어 모델 분야에 대한 관심이 증가함에 따라, 이 모델들의 내부 작동 원리를 이해하는 일은 여전히 복잡한 과제입니다.
- 본 논문에서는 대규모 시각-언어 모델의 내부 메커니즘을 이해하기 위한 새로운 인터랙티브 애플리케이션을 제시합니다.
- 본 애플리케이션은 이미지 패치의 해석 가능성을 증진시키고, 언어 모델의 출력이 이미지에 근거를 두고 있는지를 평가하는 데 목적을 두고 있습니다.
- 사용자는 이 애플리케이션을 통해 모델을 체계적으로 조사하고 시스템의 한계를 발견함으로써 시스템 능력의 향상을 위한 길을 열 수 있습니다.
- 마지막으로, 대규모 복합 모달 모델인 LLaVA의 실패 메커니즘을 이해하는 데 본 애플리케이션을 어떻게 활용할 수 있는지에 대한 사례 연구를 제시합니다.

### [PointInfinity: Resolution-Invariant Point Diffusion Models](https://arxiv.org/abs/2404.03566)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03566.png)

Vote: 12

Authors: Chao-Yuan Wu, Zixuan Huang, Shoubhik Debnath, James M. Rehg, Justin Johnson

- 본 논문에서는 PointInfinity라는 효율적인 점구름 확산 모델 패밀리를 제시합니다.
- 이 모델은 고정된 크기의 해상도 불변 잠재 표현을 활용하는 트랜스포머 기반 아키텍처를 사용합니다.
- 이를 통해 저해상도 점구름으로 효율적인 학습을 실시하면서, 추론 시에는 고해상도 점구름 생성이 가능합니다.
- 더욱이, 훈련 해상도를 넘어서는 테스트 시 해상도를 확장하면 생성된 점구름과 표면의 진실성이 향상된다는 것을 보여줍니다.
- 이러한 현상을 분석하고, 확산 모델에서 흔히 사용되는 분류자 없는 유도 방식과의 연결성을 드러내며, 이 두 방식이 추론 동안에 진실성과 다양성 사이의 균형을 조절할 수 있음을 입증합니다.
- CO3D 실험 결과에 따르면, PointInfinity는 Point-E보다 31배 많은 최대 131k 점으로 이루어진 고해상도 점구름을 최고의 품질로 효율적으로 생성할 수 있습니다.

### [AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent](https://arxiv.org/abs/2404.03648)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03648.png)

Vote: 11

Authors: Iat Long Iong, Yuxiao Dong, Jie Tang, Yuxuan Chen, Xiaohan Zhang, Xiao Liu, Hanchen Zhang, Hanyu Lai, Shuntian Yao, Hao Yu, Pengbo Shen

- 대규모 언어 모델(Large Language Models, LLMs)을 활용하여, ChatGLM3-6B를 기반으로 한 자동화된 웹 탐색 에이전트 AutoWebGLM이 개발되었다.
- 웹 페이지에 대한 다양한 액션, HTML 텍스트의 대량 처리, 열린 웹의 복잡성 등 실제 웹페이지에서의 에이전트 성능 문제를 해결하기 위한 접근법이 제안되었다.
- 인간의 웹 브라우징 패턴에 기초하여, 핵심 정보를 간결하게 보존하는 HTML 단순화 알고리즘이 설계되었다.
- 커리큘럼 트레이닝을 위해 하이브리드 인간-AI 방법을 사용하여 웹 브라우징 데이터를 구축했다.
- 강화 학습과 거부 샘플링을 통해 모델을 부트스트랩하여 웹 페이지 이해, 브라우저 조작, 그리고 효율적인 작업 분해능력을 더욱 향상시켰다.
- 실제 웹 탐색 과제에 대한 이중 언어 벤치마크인 AutoWebBench를 설정하고 AutoWebGLM을 여러 웹 탐색 벤치마크에서 평가하여 개선 사항과 현실 환경에서의 도전을 밝혔다.
- 관련 코드, 모델, 데이터는 https://github.com/THUDM/AutoWebGLM 에서 공개될 예정이다.

### [CodeEditorBench: Evaluating Code Editing Capability of Large Language Models](https://arxiv.org/abs/2404.03543)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03543.png)

Vote: 10

Authors: Ruibo Liu, Kaijing Ma, Yue Wang, Yizhi LI, Jiawei Guo, Ziming Li, Tianyu Zheng, Ding Pan, Wenhu Chen, Shuyue Guo, Jie Fu, Xingwei Qu, Ge Zhang, Xueling Liu, Zhouliang Yu, Xiang Yue

- 코드 편집능력을 중요한 기능으로 평가하고자 CodeEditorBench, 새로운 평가 프레임워크를 소개합니다.
- 이 평가는 디버깅, 번역, 코드 개선, 요구사항 변경 등 다양한 프로그래밍 언어, 복잡도, 편집 작업을 포함한 여러 시나리오에서 대규모 언어 모델(Large Language Models, LLMs)의 성능을 검증합니다.
- 기존의 코드 생성 위주 벤치마크와 달리, CodeEditorBench는 실제 소프트웨어 개발의 실용적 측면과 실제 시나리오를 강조합니다.
- 5개의 다양한 출처에서 가져온 코딩 도전과 시나리오를 선별함으로써 LLM의 코드 편집 능력을 체계적으로 평가합니다.
- 19개의 LLM을 평가한 결과, Gemini-Ultra와 GPT-4 같은 폐소스 모델이 CodeEditorBench에서 오픈소스 모델보다 우수한 성능을 보임을 밝혔습니다.
- 또한, 문제 유형과 프롬프트에 대한 민감성에 따라 모델의 성능 차이를 강조합니다.
- CodeEditorBench는 LLM의 코드 편집 능력을 평가하는 강력한 플랫폼을 제공하여 LLM의 발전을 촉진합니다.
- 모든 프롬프트와 데이터셋은 공개될 예정이며, 커뮤니티가 데이터셋을 확장하고 새로운 LLM을 벤치마크할 수 있게 도울 것입니다.
- CodeEditorBench를 도입함으로써, 코드 편집 분야에서 LLM의 발전에 기여하고 연구원 및 실무자에게 소중한 자원을 제공합니다.

### [RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis](https://arxiv.org/abs/2404.03204)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03204.png)

Vote: 6

Authors: Hiroshi Saruwatari, Shujie Liu, Sheng Zhao, Xu Tan, Dongchao Yang, Yuancheng Wang, Kai Shen, Zeqian Ju, Detai Xin, Jinyu Li, Shinnosuke Takamichi

- 이 연구에서는 음성 합성(text-to-speech, TTS)을 위해 RALL-E라는 견고한 언어 모델링 방법을 제시합니다.
- 기존의 큰 언어 모델들(Large Language Models, LLMs)을 이용한 연구가 제로샷 TTS에서 인상적인 성능을 보여주었지만, 자동회귀 예측 방식으로 인한 불안정한 프로조디(이상한 피치 및 리듬/기간)와 높은 단어 오류율(word error rate, WER)에 취약하다는 문제가 있었습니다.
- RALL-E는 작업을 단순화하는 과정으로 분해하여 LLM 기반 TTS의 견고함을 향상시키는 체인 오브 쏘트(chain-of-thought, CoT) 프롬프팅 기술을 활용합니다.
- 먼저, RALL-E는 입력 텍스트의 프로조디 특성(피치와 기간)을 예측하고, 이를 중간 조건으로 활용하여 CoT 스타일로 음성 토큰을 예측합니다.
- 또한, 변압기(Transformer)의 자기 주의 가중치 계산을 이끌기 위해 예측된 지속 시간 프롬프트를 활용합니다. 이는 모델이 음성 토큰을 예측할 때 해당 음소와 프로조디 특성에 집중하도록 합니다.
- 광범위한 객관적 및 주관적 평가 결과는, VALL-E라는 강력한 기준 방법에 비해, RALL-E가 제로샷 TTS의 WER을 6.3% (재정렬 없이) 및 2.1% (재정렬 있음)에서 각각 2.8%와 1.0%로 크게 줄였음을 보여줍니다. 
- 더욱이, RALL-E는 VALL-E가 처리하기 어려운 문장을 정확하게 합성하며, 오류율을 68%에서 4%로 줄입니다.

### [Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?](https://arxiv.org/abs/2404.03411)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03411.png)

Vote: 6

Authors: Shuo Chen, Wenqian Yu, Philip Torr, Bailan He, Volker Tresp, Jindong Gu, Zifeng Ding, Zhen Han

- 다양한 '잘브레이크 공격(jailbreak attacks)'이 큰 언어 모델(Large Language Models, LLMs)의 취약점을 드러내었으며, 일부 방법들은 텍스트 모델뿐만 아니라 비주얼 입력을 변형시켜 다중 모달 대형 언어 모델(Multimodal Large Language Models, MLLMs)에 이를 확장합니다.
- 현재 유니버셜하게 평가할 수 있는 벤치마크의 부재는 성능 재현과 공정한 비교를 복잡하게 만들며, 특히 비공개 최신 모델들, 예를 들어 GPT-4V 같은 MLLMs의 포괄적인 평가가 부족합니다.
- 이 연구는 11가지 다른 안전 정책을 커버하는 1445개의 유해 질문을 포함하는 포괄적인 잘브레이크 평가 데이터셋을 구축하고, 이를 기반으로 다양한 LLMs 및 MLLMs 에 대한 광범위한 레드팀 실험을 수행합니다.
- 다양한 모델들 중에서 GPT-4와 GPT-4V는 오픈 소스 LLMs 및 MLLMs에 비해 잘브레이크 공격에 대한 내성이 더 뛰어난 것으로 나타났습니다.
- Llama2와 Qwen-VL-Chat과 같은 다른 오픈 소스 모델들에 비해 더 강인함을 보였으며, 시각적 잘브레이크 방법의 이전 가능성은 텍스트 잘브레이크 방법에 비해 상대적으로 제한적이었습니다.
- 관련 데이터셋 및 코드는 제공된 링크를 통해 접근할 수 있습니다.

