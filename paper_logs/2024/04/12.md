## Daily Papers (2024-04-12)

### [Rho-1: Not All Tokens Are What You Need](https://arxiv.org/abs/2404.07965)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07965.png)

Vote: 36

Authors: Yeyun Gong, Yelong Shen, Ruochen Xu, Chen Lin, Jian Jiao, Zhibin Gou, Nan Duan, Zhenghao Lin, Weizhu Chen, Xiao Liu, Yujiu Yang

- 이전의 언어 모델 사전 학습 방법은 모든 훈련 토큰에 대해 동일하게 다음 토큰 예측 손실을 적용했지만, Rho-1 연구에서는 "모든 토큰이 언어 모델 학습에 동등하게 중요한 것은 아니다"라는 새로운 관점을 제시한다.
- Rho-1 모델은 전통적인 언어 모델이 사용하는 방식과 달리, 유용한 토큰에 집중하여 Selective Language Modeling (SLM)를 사용하여 사전 학습한다.
- SLM 접근 방식은 참조 모델을 사용하여 사전 훈련 토큰을 점수 매기고, 더 높은 초과 손실이 있는 토큰에 집중적으로 학습하는 과정을 포함한다.
- 15B 개의 OpenWebMath 코퍼스에 지속적인 사전 학습을 수행할 때, Rho-1은 9개의 수학 작업에서 최대 30%의 절대 정확도 향상을 이끌어냈다.
- Rho-1-1B와 7B는 미세 조정 후 MATH 데이터셋에서 각각 40.6%, 51.8%로 최고의 성능을 달성했으며, 이는 3%의 사전 훈련 토큰만을 사용하면서도 DeepSeekMath와 동등한 결과를 보였다.
- 80B 개의 일반 토큰에 대한 사전 학습 시, Rho-1은 15개의 다양한 작업에서 평균 6.8%의 성능 향상을 달성하며, 언어 모델 사전 학습의 효율성 및 성능을 모두 개선시켰다.

### [ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback](https://arxiv.org/abs/2404.07987)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07987.png)

Vote: 30

Authors: Jie Wu, Huafeng Kuang, Zhaoning Wang, Xuefeng Xiao, Ming Li, Chen Chen, Taojiannan Yang

- 텍스트-이미지 확산 모델의 제어 가능성을 향상시키기 위해 ControlNet과 같은 기존의 이미지 기반 조건부 제어 방법이 포함되었음에도 불구하고, 이미지 조건부 제어와 일치하는 이미지를 생성하는 데 있어 상당한 도전이 여전히 존재함을 밝힙니다.
- ControlNet++를 제안하여 생성된 이미지와 조건부 제어 사이의 픽셀 수준 사이클 일관성을 명시적으로 최적화함으로써 제어 가능한 생성을 개선합니다.
- 입력된 조건부 제어에 대해, 사전 훈련된 판별 보상 모델을 사용하여 생성된 이미지의 해당 조건을 추출하고, 입력된 조건부 제어와 추출된 조건 사이의 일관성 손실을 최적화합니다.
- 전통적인 구현은 임의의 노이즈에서 이미지를 생성한 다음 일관성 손실을 계산하지만, 이 같은 방법은 여러 샘플링 시간 단계에 대한 그라디언트를 저장해야 하므로 상당한 시간과 메모리 비용이 발생합니다.
- 이를 해결하기 위해 입력 이미지에 의도적으로 노이즈를 추가하고 단일 단계에서 노이즈가 제거된 이미지를 사용하여 보상을 미세 조정하는 효율적인 보상 전략을 도입합니다.
- 이는 이미지 샘플링과 관련된 광범위한 비용을 피하면서 보다 효율적인 보상 미세 조정을 가능하게 합니다.
- 광범위한 실험을 통해 ControlNet++는 다양한 조건부 제어 하에서 제어 가능성을 크게 향상시키는 것으로 나타났으며, 예를 들어 세분화 마스크, 선 아트 에지, 및 깊이 조건에 대해 각각 ControlNet 대비 7.9% mIoU, 13.4% SSIM, 7.6% RMSE의 개선을 달성했습니다.

### [OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments](https://arxiv.org/abs/2404.07972)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07972.png)

Vote: 27

Authors: Tianbao Xie, Danyang Zhang, Siheng Zhao, Fangyu Lei, Dongchan Shin, Silvio Savarese, Tao Yu, Victor Zhong, Yiheng Xu, Zhoujun Cheng, Jixuan Chen, Shuyan Zhou, Xiaochuan Li, Toh Jing Hua, Ruisheng Cao, Yitao Liu, Caiming Xiong

- 인간의 최소한의 개입으로 복잡한 컴퓨터 작업을 수행할 수 있는 자율 에이전트는 인간-컴퓨터 상호작용을 변화시키고 접근성 및 생산성을 크게 향상시킬 잠재력을 가지고 있습니다.
- 기존 벤치마크는 상호작용 가능한 환경이 부족하거나 특정 애플리케이션 또는 도메인에 한정된 환경으로 제한되어 있어 실제 컴퓨터 사용의 다양하고 복잡한 성격을 반영하지 못하고 작업 범위와 에이전트 확장성이 제한됩니다.
- 이러한 문제를 해결하기 위해, 본 연구는 Ubuntu, Windows, macOS와 같은 다양한 운영 체제에서 작업 설정, 실행 기반 평가, 상호 작용적 학습을 지원하는 최초의 확장 가능한 실제 컴퓨터 환경인 OSWorld를 소개합니다.
- OSWorld는 임의의 응용 프로그램을 포함하는 개방형 컴퓨터 작업을 평가할 수 있는 통합된 컴퓨터 환경으로서 사용될 수 있습니다.
- 우리는 실제 웹 및 데스크탑 애플리케이션, OS 파일 I/O, 다중 애플리케이션에 걸친 워크플로우를 포함하는 369개의 컴퓨터 작업을 포함하는 벤치마크를 OSWorld를 통해 만들었습니다.
- 각 작업 예제는 실제 컴퓨터 사용 사례에서 파생되었으며, 신뢰할 수 있고 재현 가능한 평가를 위한 자세한 초기 상태 설정 구성 및 맞춤 실행 기반 평가 스크립트를 포함합니다.
- OSWorld에서 LLM/VLM 기반 최첨단 에이전트의 평가를 통해 컴퓨터 보조로서의 중대한 결함을 드러냅니다. 인간은 72.36% 이상의 작업을 수행할 수 있는 반면, 최고의 모델은 GUI 그라운딩과 운영 지식에서 주로 어려움을 겪으며 단지 12.24%의 성공률을 달성합니다.
- OSWorld를 사용한 포괄적인 분석은 이전 벤치마크에서는 불가능했던 다중 모달 일반 에이전트 개발에 대한 귀중한 통찰력을 제공합니다.
- 우리의 코드, 환경, 기본 모델 및 데이터는 https://os-world.github.io에서 공개적으로 이용할 수 있습니다.

### [RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](https://arxiv.org/abs/2404.07839)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07839.png)

Vote: 20

Authors: Surya Bhupatiraju, Cassidy Hardin, Anushan Fernando, Razvan Pascanu, Leonard Berrada, Laurent Sifre, Morgane Rivière, Shreya Pathak, Olivier Bachem, George-Cristian Muraru, Sertan Girgin, Soham De, Alek Andreev, +, Ruba Haroun, Thomas Mesnard, Johan Ferret, Aleksandar Botev, Kathleen Kenealy, Léonard Hussenot, Pier Giuseppe Sessa, Samuel L Smith, Robert Dadashi

- RecurrentGemma는 Google의 새로운 Griffin 구조를 사용하는 오픈 언어 모델을 소개합니다.
- Griffin은 선형 재발과 지역적 주의를 결합하여 언어 처리에 뛰어난 성능을 달성합니다.
- 고정 크기의 상태를 가지고 있어 메모리 사용량을 줄이며 긴 시퀀스에서의 효율적인 추론을 가능하게 합니다.
- 20억 개의 non-embedding 파라미터를 갖는 사전 훈련된 모델과 지시어 튜닝된 변형 모델을 제공합니다.
- 두 모델 모두 적은 토큰으로 훈련되었음에도 불구하고 Gemma-2B와 비교할 수 있는 성능을 달성합니다.

### [WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents](https://arxiv.org/abs/2404.05902)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.05902.png)

Vote: 17

Authors: Giovanni Campagna, Michael Lutz, Artem Harutyunyan, Manvel Saroyan, Arth Bohra

- 웹 에이전트 연구에서 일반화와 정확도 모두를 달성하는 것은 도전적인 문제인데, 웹사이트 구조의 높은 변동성으로 기존 접근법은 종종 실패한다.
- 기존의 미세조정(fine-tuning) 및 상황 내 학습(in-context learning) 기법들이 여러 웹사이트에 걸쳐 일반화하는데 실패함에 따라, 연구팀은 Wilbur라는 새로운 접근법을 도입했다.
- Wilbur는 이전 실행에서의 작업 시연을 최적으로 포함시키기 위해 차별화된 랭킹 모델과 새로운 지시 합성 기술을 사용하여 대형 언어 모델의 프롬프트를 채워 넣는 방식을 사용한다.
- 연구팀은 또한 단일 인스턴스에서 실수를 학습하고 회복하는 지능형 백트래킹 메커니즘을 제안하여 종단 간 성공률을 극대화한다.
- 창안된 랭킹 모델은 수동 주석 없이 에이전트를 실행하고 자동으로 평가하는 생성적 자동교육(auto-curriculum)에서 데이터를 통해 훈련될 수 있다.
- Wilbur은 WebVoyager 벤치마크에서 최신 기술 수준의 결과를 달성하여 텍스트만을 사용하는 모델을 가로질러 8% 이상, 특정 웹사이트에서는 최대 36%까지 능가했다.
- 같은 벤치마크에서 Wilbur은 텍스트 입력만을 받음에도 불구하고 강력한 멀티모달 모델의 성능과 단 5% 차이를 보였고, 추가 분석은 웹 운영의 엔지니어링 도전 때문에 발생하는 실패가 상당수임을 밝혔다.

### [Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models](https://arxiv.org/abs/2404.07973)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07973.png)

Vote: 17

Authors: Zhe Gan, Hong-You Chen, Shih-Fu Chang, Philipp Dufter, Yinfei Yang, William Yang Wang, Haoxuan You, Bowen Zhang, Haotian Zhang, Tsu-Jui Fu, Chen Chen

- Ferret는 대규모 언어 모델에 지역적 이해를 통합하여 지칭 및 정합 기능을 용이하게 하였지만, 고정된 시각적 인코더에 의해 제약을 받고 넓은 범위의 작업에서 성능이 떨어지는 문제가 있었다.
- 본 연구에서는 Ferret의 상당한 업그레이드 버전인 Ferret-v2를 소개하며, 이는 세 가지 주요 설계를 통해 개선되었다.
- 첫째, 해상도에 구애받지 않는 지칭 및 정합: 모델이 이미지를 보다 상세하게 처리하고 이해할 수 있도록 불러온 높은 해상도 이미지를 자유롭게 다루는 유연한 접근법을 제시한다.
- 둘째, 다중 입도 시각 인코딩: 추가된 DINOv2 인코더를 통합함으로써 모델은 전역적 및 세밀한 시각 정보에 대해 더 잘하고 다양한 배경 컨텍스트를 학습한다.
- 셋째, 세 단계 훈련 패러다임: 이미지-캡션 정렬을 넘어 높은 해상도에서의 조밀한 정렬을 위한 추가 단계를 포함한 최종 지시사항 튜닝 전에 제안된다.
- 실험 결과, Ferret-v2는 높은 해상도 스케일링 및 세밀한 시각적 처리 능력 덕분에 Ferret 및 기타 최신 기술보다 뚜렷한 개선을 보여준다.

### [JetMoE: Reaching Llama2 Performance with 0.1M Dollars](https://arxiv.org/abs/2404.07413)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07413.png)

Vote: 16

Authors: Yikang Shen, Tianle Cai, Zhen Guo, Zengyi Qin

- 'JetMoE-8B'는 오픈소스 말뭉치로부터 조심스럽게 섞은 1.25T 토큰과 30,000 H100 GPU 시간을 사용, 10만 달러 미만의 비용으로 훈련된 새로운 대규모 언어 모델(LLM)입니다.
- 이 모델은 저렴한 비용에도 불구하고 Llama2-7B 모델보다 뛰어난 성능을 보이며, 특히 JetMoE-8B-Chat은 Llama2-13B-Chat 모델을 능가합니다.
- JetMoE-8B는 효율적인 Sparsely-gated Mixture-of-Experts (SMoE) 구조를 바탕으로 하며, 이를 통해 8B개의 매개변수를 가지면서도 각 입력 토큰에 대해 2B의 매개변수만을 활성화시켜 Llama2-7B와 비교하여 추론 계산량을 약 70% 줄입니다.
- 또한, JetMoE-8B는 모든 공개 데이터셋과 훈련 코드를 사용하는 등 매우 개방적이고 학계 친화적인 모델로, 향후 개방형 기반 모델의 발전을 촉진할 수 있도록 모든 훈련 파라미터와 데이터 혼합이 상세히 보고되어 있습니다.
- 이러한 투명성은 접근성과 효율성 높은 LLM 분야의 발전을 위한 협력과 추가 개선을 장려하기 위함입니다.
- 모델의 가중치는 https://github.com/myshell-ai/JetMoE에서 공개적으로 이용 가능합니다.

### [Best Practices and Lessons Learned on Synthetic Data for Language Models](https://arxiv.org/abs/2404.07503)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07503.png)

Vote: 12

Authors: Yanzhe Zhang, Daiyi Peng, Diyi Yang, Denny Zhou, Fangyu Liu, Jerry Wei, Andrew M. Dai, Jinmeng Rao, Steven Zheng, Chenglei Si, Ruibo Liu

- 인공 지능 모델의 성공은 크고 다양하며 고품질의 데이터셋에 달려 있으나 데이터 부족, 프라이버시 문제, 높은 비용으로 인해 얻기 어려운 경우가 많습니다.
- 인공적으로 실제 세계 패턴을 모방하는 합성 데이터가 해결책으로 부상하고 있으며, 이 논문은 합성 데이터 연구에 대한 개요와 응용, 도전과제 및 향후 방향을 논의합니다.
- 선행 연구에서 얻은 실증적 증거를 통해 합성 데이터의 효과를 보여주고, 사실성, 충실도 및 비편향성을 보장하는 것의 중요성을 강조합니다.
- 합성 데이터를 책임감 있게 사용하여 더 강력하고 포괄적이며 신뢰할 수 있는 언어 모델을 구축할 필요성을 강조하고 있습니다.

### [HGRN2: Gated Linear RNNs with State Expansion](https://arxiv.org/abs/2404.07904)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07904.png)

Vote: 11

Authors: Xuyang Shen, Dong Li, Songlin Yang, Weixuan Sun, Weigao Sun, Yiran Zhong, Zhen Qin

- HGRN(Hierarchically Gated Linear RNN, Qin et al. 2023)은 언어 모델링에서 경쟁력 있는 학습 속도와 성능을 보였으며, 효율적인 추론이 가능해졌지만, 상대적으로 작은 순환 상태 크기가 표현력을 제한한다는 문제가 있었습니다.
- 이를 해결하기 위해, 선형 어텐션에 영감을 받아 추가적인 매개변수를 도입하지 않고 순환 상태 크기를 크게 확장할 수 있는 간단한 외적 기반 상태 확장 메커니즘을 도입하였습니다.
- 선형 어텐션 형태는 하드웨어 효율적인 학습을 가능하게 합니다.
- 우리의 광범위한 실험은 HGRN2가 언어 모델링, 이미지 분류, Long Range Arena에서 HGRN1을 능가하는 이점을 검증하였습니다.
- 가장 큰 3B HGRN2 모델은 제어된 실험 환경에서 Mamba와 LLaMa 아키텍처 트랜스포머보다 언어 모델링에서 약간 앞서는 성능을 보였으며, 학습에 사용된 총 토큰 수가 훨씬 적음에도 불구하고 다운스트림 평가에서 많은 오픈 소스 3B 모델들과 경쟁력 있는 성능을 보여주었습니다.

### [Audio Dialogues: Dialogues dataset for audio and music understanding](https://arxiv.org/abs/2404.07616)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07616.png)

Vote: 9

Authors: Arushi Goel, Bryan Catanzaro, Zhifeng Kong, Rafael Valle

- 기존 오디오 이해를 위한 데이터셋이 단일 턴 상호작용(예: 오디오 캡셔닝, 오디오 질문 대답)에 중점을 두었던 것과 달리, 본 연구에서는 일반 오디오 사운드 및 음악에 대한 163.8k 샘플을 포함하는 다중 턴 대화 데이터셋인 Audio Dialogues을 소개합니다.
- Audio Dialogues는 대화뿐만 아니라 여러 입력 오디오를 함께 이해하고 비교하기 위한 질문-응답 쌍도 포함하고 있습니다.
- 이 데이터셋은 기존 데이터셋들의 자막 주석을 활용하고 Large Language Model (LLM)을 사용하여 다중 턴 대화를 생성하는 프롬프트 기반 접근방식을 채택합니다.
- 오디오 기능을 강화한 대규모 언어 모델이 Audio Dialogues 데이터셋에서 평가되었으며, 이는 데이터셋의 복잡성 및 적용 가능성을 입증합니다.
- 데이터셋 생성을 위한 코드는 공개될 예정이며, 자세한 프롬프트와 생성된 대화 예시는 데모 웹사이트 https://audiodialogues.github.io/에서 확인할 수 있습니다.

### [Transferable and Principled Efficiency for Open-Vocabulary Segmentation](https://arxiv.org/abs/2404.07448)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07448.png)

Vote: 8

Authors: Yunchao Wei, Jingxuan Xu, Yao Zhao, Wuyang Chen

- 최근 사전 학습된 기초 비전-언어 모델의 성공으로 오픈 어휘 세그멘테이션(OVS)이 가능해졌으나, 큰 모델 크기와 높은 파인 튜닝 비용으로 인해 실제 응용에는 제한이 있다.
- 전통적인 모델 압축과 효율적인 파인 튜닝 방법은 경험적이며, 모델에 따라 재학습이 필요하고 비용이 많이 든다는 단점이 있다.
- 본 연구에서는 큰 비전-언어 기초 모델을 기반으로 한 이전 OVS 연구와 비교하여 상응하거나 더 나은 성능을 보이면서도, 더 작은 모델과 낮은 훈련 비용을 유발하는 효율적 OVS를 지향한다.
- 효율성을 원칙적이고 전이 가능하게 만들어서, 다른 OVS 프레임워크로의 추가 맞춤 없이도 쉽게 전달될 수 있도록 한 전략의 핵심이다.
- 다양한 OVS 벤치마크에서의 포괄적인 실험을 통해, 세그멘테이션 정확도와 계산 비용 사이의 우수한 균형을 이전 연구들보다 더 나은 결과로 보여준다.
- 본 연구의 코드는 https://github.com/Xujxyang/OpenTrans에서 확인할 수 있다.

### [From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples](https://arxiv.org/abs/2404.07544)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/7CByxImTpp5SFfRMLS2S2.png)

Vote: 8

Authors: Vlad-Andrei Negru, Vasile Suciu, Robert Vacareanu, Mihai Surdeanu

- 연구에서는 추가 학습이나 기울기 업데이트 없이 맥락 예제를 제공받은 사전 훈련된 대형 언어 모델들(Llama2, GPT-4, Claude 3 등)의 선형 및 비선형 회귀 성능을 분석하였습니다.
- 대형 언어 모델들 중 일부(예: GPT-4, Claude 3)는 전통적인 감독 방식의 회귀 방법(랜덤 포레스트, 배깅, 그라디언트 부스팅 등)과 유사하거나 더 우수한 성능을 보였습니다.
- 예를 들어, Claude 3는 프리드먼 #2 회귀 데이터셋에서 AdaBoost, SVM, 랜덤 포레스트, KNN, 그라디언트 부스팅과 같은 많은 감독 방식의 방법들보다 더 나은 성능을 보였습니다.
- 또한, 맥락 예제의 수가 증가함에 따라 언어 모델 성능이 어떻게 확장되는지 조사하였습니다.
- 온라인 학습에서의 후회 개념을 차용하여 대규모 언어 모델(LLMs)이 선형 이하의 후회를 얻을 수 있는 능력을 경험적으로 보여주었습니다.

### [LLoCO: Learning Long Contexts Offline](https://arxiv.org/abs/2404.07979)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07979.png)

Vote: 7

Authors: Tianjun Zhang, Joseph E. Gonzalez, Shishir Patil, Kurt Keutzer, Sijun Tan, Xiuyu Li, Raluca Ada Popa, Ziyang Wu

- 큰 언어 모델에서 긴 문맥 처리에 대한 어려움을 극복하기 위해 오프라인 문맥 압축 및 도메인별 효율적인 미세조정을 통해 문맥을 학습하는 새로운 접근법을 제안합니다.
- 해당 방법은 언어 모델이 원문맥의 간결한 표현을 생성하고 정확한 답변을 위해 관련 정보를 효율적으로 검색할 수 있게 합니다.
- LLoCO라는 기술은 문맥 압축, 검색, 그리고 LoRA를 사용한 효율적인 미세조정을 결합하여 4k 토큰짜리 LLaMA2-7B 모델의 효과적인 문맥 창을 최대 128k 토큰까지 처리할 수 있게 확장합니다.
- 긴 문맥 질의응답 데이터셋들에 대한 평가를 통해 LLoCO가 인-컨텍스트 학습을 훨씬 뛰어넘는 성능을 보이며, 추론 시 사용하는 토큰 수를 30배 줄임을 입증합니다.
- LLoCO는 긴 문서 질의응답 작업의 속도를 최대 7.62배 향상시키고, 관련 비용을 크게 절감시켜 효율적인 긴 문맥 처리를 위한 유망한 해결책임을 보여줍니다.
- 이 기법에 대한 코드는 https://github.com/jeffreysijuntan/lloco 에서 공개적으로 이용할 수 있습니다.

### [Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models](https://arxiv.org/abs/2404.07724)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07724.png)

Vote: 6

Authors: Timo Aila, Tuomas Kynkäänniemi, Tero Karras, Jaakko Lehtinen, Samuli Laine, Miika Aittala

- 이미지 생성 확산 모델에서 가장 좋은 성능을 내기 위해 가이던스는 매우 중요한 기술이며, 이미지의 샘플링 체인 전반에 걸쳐 일정한 가이던스 가중치가 적용되었습니다.
- 연구팀은 가이던스가 체인의 시작 부분(높은 잡음 수준)에서는 명백히 해롭고, 끝 부분(낮은 잡음 수준)에서는 대부분 불필요하며, 중간 부분에서만 유익함을 발견했습니다.
- 결과적으로, 오직 특정 잡음 수준 범위에 가이던스를 제한함으로써 추론 속도와 결과 품질을 향상시키는 것이 가능해졌습니다.
- 이 제한된 가이던스 구간을 통해 ImageNet-512에서의 최고 FID 기록이 1.81에서 1.40으로 상당히 향상되었습니다.
- 다양한 샘플러 매개변수, 네트워크 아키텍처, 데이터셋을 포함하여 대규모 설정인 Stable Diffusion XL에서도 이 방법이 정량적 및 정성적으로 유익함을 증명했습니다.
- 따라서 연구팀은 가이던스를 사용하는 모든 확산 모델에서 가이던스 구간을 하이퍼파라미터로 제공할 것을 제안합니다.

### [Sparse Laneformer](https://arxiv.org/abs/2404.07821)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07821.png)

Vote: 5

Authors: Mingjie Lu, Jinzhang Peng, Yile Xie, Ji Liu, Emad Barsoum, Dong Li, Hongyang Wei, Zifeng Zhang, Lu Tian, Ashish Sirasao

- 자율 주행에서 기본적인 과제인 차선 탐지를 개선하기 위해, 기존의 밀집 앵커 기반 방식이 훈련 데이터에 의존적이고 추론 중에 고정된다는 문제점을 분석하였습니다.
- 차선 탐지에 밀집 앵커가 반드시 필요하지 않다는 점을 근거로, 위치 인식 차선 쿼리와 각도 쿼리를 이용하여 희소 앵커를 생성하는 트랜스포머 기반의 차선 탐지 프레임워크를 제안했습니다.
- 수평 방향의 차선 특성을 집약하기 위한 수평 인식 주의 기능(Horizontal Perceptual Attention, HPA)과 차선 쿼리 및 각도 쿼리 간의 상호작용을 수행하는 차선-각도 교차 주의 기능(Lane-Angle Cross Attention, LACA)를 채택하였습니다.
- 변형 가능한 교차 주의 기반의 차선 인식 주의 기능(Lane Perceptual Attention, LPA)을 도입하여 차선 예측을 더욱 세밀하게 다듬습니다.
- Sparse Laneformer로 명명된 우리의 방법은 구현이 간편하고 엔드투엔드 학습이 가능합니다.
- 광범위한 실험을 통해, Sparse Laneformer는 CULane에서 동일한 ResNet-34 백본을 사용하면서도 최신 기술들과 비교하여 주목할 만한 성능을 보여주었으며, 예를 들어 Laneformer보다 F1 스코어에서 3.0% 더 높고 O2SFormer보다는 0.7% 더 높은 성능을 더 적은 MACs로 달성하였습니다.

