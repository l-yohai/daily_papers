## Daily Papers (2024-04-08)

### [Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences](https://arxiv.org/abs/2404.03715)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03715.png)

Vote: 24

Authors: Arindam Mitra, Ahmed Awadallah, Ching-An Cheng, Corby Rosset, Tengyang Xie, Michael Santacroce

- 이 논문은 선호 반응(feedback)을 통해 대규모 언어 모델(Large Language Models, LLMs)이 스스로를 반복적으로 개선할 수 있도록 하는 과정에 대해서 연구한다.
- 전통적인 인간 피드백으로부터의 강화 학습(Reinforcement Learning from Human Feedback, RLHF)은 보상 학습과 정책 최적화를 분리하지만, 복잡한 비순환적이거나 순환적인 선호 관계를 표현하는 데 한계가 있다.
- 최근의 연구는 직접적인 상대적 선호나 보다 일반적인 선호를 최적화하는 것이 보상 최대화 가정을 벗어나는 새로운 방법을 모색하고 있다.
- 본 논문에서는, 대비 학습의 간단함과 이론적 범용성을 합친 증명 가능하고 확장 가능한 알고리즘인 Direct Nash Optimization(DNO)을 소개한다.
- DNO는 회귀 기반 목표를 사용하는 배치 처리 정책 알고리즘으로, 실행이 간단하고 효율적이다.
- DNO는 반복을 거듭하며 단조롭게 성능이 개선되고, 이를 통해 강력한 교사 모델(예: GPT-4)까지도 개선할 수 있다.
- 실험에서 DNO에 의해 조정된 7B 파라미터 Orca-2.5 모델은 GPT-4-Turbo에 대한 AlpacaEval 2.0에서 33%의 최고 승률을 달성했으며, 초기 모델 대비 26% 절대 이득(7%에서 33%로)을 보였다.
- 또한, DNO는 Mistral Large, Self-Rewarding LM(70B 파라미터), 그리고 이전 버전의 GPT-4와 같이 훨씬 많은 파라미터를 가진 모델들을 능가한다.

### [No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance](https://arxiv.org/abs/2404.04125)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04125.png)

Vote: 18

Authors: Vishaal Udandarao, Adhiraj Ghosh, Adel Bibi, Matthias Bethge, Samuel Albanie, Philip H. S. Torr, Ameya Prabhu, Yash Sharma

- 이 연구는 웹에서 크롤링 한 사전 훈련 데이터셋이 다중모달 모델들, 예를 들어 분류/검색을 위한 CLIP과 이미지 생성을 위한 Stable-Diffusion, 의 "제로-샷"(zero-shot) 평가 성능에 얼마나 영향을 미치는지 탐구합니다.
- "제로-샷" 일반화의 개념이 얼마나 의미 있는지, 사전 훈련 데이터셋이 아래방향(downstream) 개념을 어느 정도 포괄하는지에 대한 이해가 부족합니다.
- 연구는 사전 훈련 데이터셋 내의 개념 빈도가 다중모달 모델의 하류 개념 성능에 어떻게 영향을 미치는지에 관하여 34개 모델과 다섯 개의 표준 사전 훈련 데이터셋(CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics)을 사용해 철저하게 조사합니다.
- 모델들이 "제로-샷" 성능 향상을 위해 기하급수적으로 많은 데이터가 필요하며, 로그-선형 규모로 샘플 비효율적인 경향을 보임을 일관성 있게 발견했습니다.
- 사전 훈련 및 하류 데이터셋 간 샘플 수준 유사성을 통제한 상태에서도 이러한 경향은 지속되며, 순수합성(synthetic) 데이터 분포에서도 확인할 수 있습니다.
- 또한, 분석 기반으로 샘플링된 장기적(long-tailed) 데이터에서 모델들을 벤치마킹한 결과, 대부분의 다중모달 모델들이 불량한 성능을 보이는 것으로 나타났습니다.
- 해당 장기적 데이터셋은 "Let it Wag!" 벤치마크로 제공되며, 이 연구 방향에 대한 추가 조사를 지원합니다.
- 전반적으로, 이 연구는 "제로-샷" 일반화 능력이 대규모 훈련 패러다임 아래에서 아직 발견되지 않은 중요한 요소임을 드러내고 있으며, 훈련 데이터에 대한 기하급수적인 요구를 밝혀냈습니다.

### [Stream of Search (SoS): Learning to Search in Language](https://arxiv.org/abs/2404.03683)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03683.png)

Vote: 13

Authors: Archit Sharma, Winson Cheng, Muxin Liu, Kanishk Gandhi, Gabriel Grand, Denise Lee, Noah D. Goodman

- 이 논문에서는 언어 모델이 검색 과정을 언어로 나타내며 어떻게 검색을 배울 수 있는지 보여줍니다.
- 연구팀은 다양한 상징적 검색 전략을 포착할 수 있는 통합된 검색 언어를 제안합니다.
- 'Countdown'이라는 단순하지만 어려운 게임을 사용하여 접근법을 시연하며, 이 게임은 주어진 숫자들을 산술 연산으로 조합하여 목표 숫자를 찾는 것을 목표로 합니다.
- 연구팀은 수집된 검색 스트림 데이터셋에서 휴리스틱 솔버들에 의해 생성된 검색 스트림으로 트랜스포머 기반 언어 모델을 처음부터 사전 훈련합니다.
- SoS 사전 훈련은 최적의 검색 궤적을 예측하기 위해 훈련된 모델들에 비해 검색 정확도를 25% 향상시킵니다.
- 또한, Advantage-Induced Policy Alignment (APA) 및 Self-Taught Reasoner (STaR)라는 두 가지 정책 개선 방법으로 모델을 미세 조정합니다.
- 미세 조정된 SoS 모델들은 기존 휴리스틱 솔버들로 해결할 수 없었던 문제들 중 36%를 해결합니다.
- 연구 결과는 언어 모델이 검색을 통해 문제를 해결하고, 다양한 검색 전략을 유연하게 사용하여 스스로 개선하고, 새로운 전략을 발견할 가능성이 있음을 시사합니다.

### [RL for Consistency Models: Faster Reward Guided Text-to-Image Generation](https://arxiv.org/abs/2404.03673)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03673.png)

Vote: 11

Authors: Jonathan D. Chang, Kianté Brantley, Wen Sun, Owen Oertell, Yiyi Zhang

- 강화 학습(RL)은 이미지의 품질, 미적 감각, 지시 사항에 따른 능력과 같은 보상을 직접 최적화하는 확산 모델에 도입되어 가이드된 이미지 생성을 개선했습니다.
- 그러나 생성된 정책은 확산 모델의 반복적 샘플링 과정을 물려받아 이미지 생성 속도가 느린 문제가 있었습니다.
- 본 연구에서는 특정 보상에 최적화된 텍스트-이미지 생성 모델을 빠르게 훈련하고 추론할 수 있도록, RL을 통해 일관성 모델을 미세 조정하는 프레임워크를 제안합니다.
- 제안된 프레임워크인 Reinforcement Learning for Consistency Model(RLCM)은 일관성 모델의 반복적 추론 과정을 RL 절차로 프레임화합니다.
- RLCM은 텍스트-이미지 생성 능력에 있어서 RL로 미세 조정된 확산 모델을 개선하며, 추론 시간 동안의 계산을 예시 품질로 교환합니다.
- 실험적으로 RLCM은 프롬프트를 사용하기 어려운 목표, 예를 들어 이미지 압축성 또는 인간 피드백에서 파생된 미적 품질 같은 목표에 텍스트-이미지 일관성 모델을 적응시킬 수 있음을 보여줍니다.
- RL로 미세 조정된 확산 모델과 비교할 때, RLCM은 훨씬 빠른 훈련 속도를 보이며, 보상 목표 아래에서 생성 품질을 향상시키고, 불과 두 번의 추론 단계로 고품질 이미지를 생성함으로써 추론 절차를 가속화합니다.
- 관련 코드는 https://rlcm.owenoertell.com 에서 확인할 수 있습니다.

### [Social Skill Training with Large Language Models](https://arxiv.org/abs/2404.04204)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04204.png)

Vote: 7

Authors: Michael S. Bernstein, Diyi Yang, Omar Shaikh, Caleb Ziems, William Held, John Mitchell

- 사람들은 효과적인 의사소통과 직장 및 개인생활에서의 성공을 위해 갈등 해결과 같은 사회 기술에 의존한다.
- 그러나 대부분의 사람들에게 사회 기술을 연습할 수 있는 환경은 대체로 접근하기 어렵다.
- 본 연구는 의사소통 및 심리학의 학제간 연구를 활용하여 전문 분야로의 진입에 있어서 사회 기술의 장벽을 파악한다.
- 이어서, 대규모 언어 모델을 사용하여 실제 경험 학습, 현실적인 연습 및 맞춤형 피드백을 결합한 일반적인 프레임워크인 AI 파트너, AI 멘토 프레임워크를 제시한다.
- 이 작업은 결국 교차 분야의 혁신을 요구하며, 노동력 개발과 사회 평등에 대한 더 넓은 함의를 다루는 문제에 대한 대응을 촉구한다.

### [CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues](https://arxiv.org/abs/2404.03820)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03820.png)

Vote: 7

Authors: Traian Rebedea, Makesh Narsimhan Sreedhar, Shaona Ghosh, Christopher Parisien

- 대화 중 주제 유지에 초점을 맞춘 데이터 세트 개발이 부족한 가운데, 저자들은 대화에서 주제 관련성 유지를 위한 언어 모델 조정을 위한 CantTalkAboutThis 데이터 세트를 소개했습니다.
- 이 데이터 세트는 다양한 분야에서 선정된 여러 주제에 대한 합성 대화들로 구성되어 있으며, 대화의 주제에서 벗어나도록 의도적으로 삽입된 교란 사례들이 포함되어 있습니다.
- 언어 모델을 이 데이터 세트로 미세조정하면 GPT-4-turbo와 Mixtral-Instruct 같은 일반 목적의 지시-튜닝된 LLM과 비교하여 주제 일관성 유지 능력이 향상됩니다.
- 초기 관찰에 따르면, 이 데이터 세트에서 모델을 훈련시키면 세밀한 지시 사항을 따르는 과제에서도 성능이 향상된다고 합니다.

### [Robust Gaussian Splatting](https://arxiv.org/abs/2404.04211)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04211.png)

Vote: 6

Authors: François Darmon, Samuel Rota-Bulò, Lorenzo Porzi, Peter Kontschieder

- 본 논문에서는 핸드폰 캡처에서의 재구성과 같은 실용적인 응용 프로그램에 대한 3D 가우시안 스플래팅(3DGS)의 강인성을 향상시키기 위해 흐림, 불완전한 카메라 포즈, 색상 불일치와 같은 일반적인 오류 원인들을 해결합니다.
- 핵심 기여로, 모션 블러를 카메라 포즈에 대한 가우시안 분포로 모델링하여, 카메라 포즈 정제와 모션 블러 수정을 통합적인 방식으로 처리합니다.
- 또한, 색상 불일치를 해결하기 위해 환경광, 그림자 또는 카메라 관련 요인들(예: 다양한 화이트 밸런스 설정)로 인한 것과 같이 디포커스 블러 보상 메커니즘을 제안합니다.
- 제안된 솔루션들은 3DGS 공식과 원활하게 통합되면서도 훈련 효율성과 렌더링 속도 측면에서의 이점을 유지합니다.
- Scannet++ 및 Deblur-NeRF와 같은 관련 벤치마크 데이터셋들에서 실험적으로 본 기여도들을 검증하고 있으며, 관련 기준들에 대한 일관된 개선을 통해 최신 기술 결과를 얻습니다.

### [Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model](https://arxiv.org/abs/2404.04167)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04167.png)

Vote: 6

Authors: Yuyang Cheng, Ding Pan, Zhouliang Yu, Xinchen Luo, Wenhu Chen, Tianyu Zheng, Jiaheng Liu, Xinrun Du, Xingwei Qu, Ge Zhang, Ruibin Yuan, Binhang Yuan, Guorui Zhou, Jie Fu, Songyang Gao, Ziyang Ma

- 본 연구에서는 중국어를 중심으로 하는 대규모 언어 모델인 CT-LLM을 소개하며, 중국어에 우선을 둔 대규모 언어 모델(LLM) 개발을 향한 핵심적인 변화를 보여준다.
- CT-LLM은 처음부터 독자적으로 시작되었으며, 8000억 개의 중국어 토큰, 3000억 개의 영어 토큰 및 1000억 개의 코드 토큰을 포함하는 12000억 토큰의 방대한 말뭉치를 사용함으로써 기존 방식과 차별화된다.
- 이 전략적 구성은 중국어 이해 및 처리 능력에서 모델의 탁월한 숙련도를 촉진하며, 정렬 기술을 통해 더욱 향상시킨다.
- CT-LLM은 CHC-Bench에서 뛰어난 성능을 보여주며, 중국어 언어 작업에 탁월함을 나타내고 SFT를 통해 영어에서도 능숙함을 보여준다.
- 이 연구는 대규모 언어 모델을 주로 영어 말뭉치 위주로 훈련한 뒤 다른 언어로 적용하는 기존 패러다임에 도전하고, LLM 훈련 방법론의 범위를 넓힌다.
- 우리는 훈련 과정을 공개함으로써 포괄적이고 다재다능한 언어 모델을 위한 길을 닦는 동시에 학계와 산업계에서의 추가적인 탐구와 혁신을 촉진하고자 한다.
- 이 모델은 MAP-CC와 CHC-Bench를 포함한 상세한 데이터 처리 절차와 2B 크기의 Chinese Tiny LLM (CT-LLM)을 포함한 완전한 중국어 LLM 훈련 과정을 공개하고 있다.

### [Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation](https://arxiv.org/abs/2404.04256)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.04256.png)

Vote: 3

Authors: Zifu Wan, Katia Sycara, Yuhao Wang, Yaqi Xie, Silong Yong, Simon Stepputtis, Pingping Zhang

- 본 논문에서는 기존 RGB 이미지 외에 적외선 및 깊이 데이터 같은 추가 모달리티(X-modality)를 활용하여 인공지능 에이전트의 인식 및 장면 이해를 개선하는 다중 모달 의미론적 세분화를 위한 새로운 네트워크인 Sigma를 제안한다.
- 이 연구에서는 Selective Structured State Space 모델인 Mamba를 바탕으로 한 샴(Siamese) Mamba 네트워크를 개발하여 한정된 지역적 수용 영역을 가진 CNN이나 복잡도가 높은 ViTs 대신 선형 복잡성으로 글로벌 수용 영역을 다룰 수 있도록 했다.
- 샴 인코더 구조를 사용하고 Mamba 융합 메커니즘을 혁신하여 다양한 모달리티에서 중요한 정보를 효과적으로 선택함으로써, 모델의 채널별 모델링 능력을 강화시키는 디코더를 개발하였다.
- 제안된 Sigma 방법은 RGB-Thermal 및 RGB-Depth 세그먼테이션 작업에 대해 엄격한 평가를 거쳤으며, 다중 모달 인지 작업에서 State Space Models (SSMs)의 첫 성공적 적용을 나타내며 그 우수성을 입증하였다.
- 관련 코드는 https://github.com/zifuwan/Sigma 에서 제공된다.

