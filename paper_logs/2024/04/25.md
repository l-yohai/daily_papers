## Daily Papers (2024-04-25)

### [CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data](https://arxiv.org/abs/2404.15653)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.15653.png)

Vote: 9

Authors: Mohammad Rastegari, Mohammad Hossein Sekhavat, Sachin Mehta, Maxwell Horton, Fartash Faghri, Mahyar Najibi, Oncel Tuzel, Mehrdad Farajtabar

- 대조 학습은 이미지와 텍스트 임베딩의 정렬을 통해 효과적인 시각 표현을 학습하는 혁신적인 방법으로 부상했습니다.
- 그러나 이미지와 텍스트 쌍 간의 대조 손실에서 쌍별 유사성 계산은 계산상의 도전을 제기합니다.
- 이 논문은 웹 규모의 이미지-텍스트 데이터에서 비전 모델의 약한 감독 사전 훈련을 제안하는 새로운 방법을 제시합니다.
- 제안된 방법은 이미지-텍스트 데이터에서의 사전 훈련을 분류 작업으로 재구성함으로써 대조 손실에서 쌍별 유사성 계산의 필요성을 제거합니다.
- 결과적으로, 이 방법은 웹 규모 데이터에서의 대조 학습에 비해 훈련 속도를 2.7배 가속화합니다.
- 다양한 비전 작업을 포함한 광범위한 실험을 통해, 제안된 방법이 높은 표현 품질을 유지함을 입증합니다.
- 소스 코드, 사전 훈련된 모델 가중치 및 훈련 레시피는 https://github.com/apple/corenet 에서 이용 가능합니다.

### [PuLID: Pure and Lightning ID Customization via Contrastive Alignment](https://arxiv.org/abs/2404.16022)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16022.png)

Vote: 8

Authors: Lang Chen, Qian He, Yanze Wu, Zinan Guo, Zhuowei Chen

- PuLID는 텍스트에서 이미지 생성을 위한 새로운 조정이 필요 없는 ID 커스텀 방법을 제안합니다.
- 이 방법은 Lightning T2I 브랜치와 표준 확산 방식을 통합하여, 대조 정렬 손실과 정확한 ID 손실을 도입함으로써, 원래 모델의 교란을 최소화하고 높은 ID 충실도를 보장합니다.
- PuLID는 ID 충실도와 편집 가능성 모두에서 우수한 성능을 달성한다고 실험을 통해 입증되었습니다.
- 또한, PuLID는 ID 삽입 전후로 이미지 요소(예: 배경, 조명, 구성, 스타일)가 가능한 한 일관되게 유지되는 것이 매력적인 특성입니다.
- 관련 코드와 모델은 https://github.com/ToTheBeginning/PuLID 에서 제공될 예정입니다.

### [ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with Reward Feedback Learning](https://arxiv.org/abs/2404.15449)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.15449.png)

Vote: 5

Authors: Jie Wu, Hefeng Wu, Jiacheng Zhang, Liang Lin, Weifeng Chen, Xuefeng Xiao

- 확산 모델의 급속한 발전으로 인하여, AI 초상화 및 광고와 같은 다양한 응용 분야에서 주목받는 ID-T2I(Identity-Preserving Text-to-Image Generation)이 큰 관심을 받고 있습니다.
- 기존의 ID-T2I 방법들은 인상적인 결과를 보여주었지만, 참조 초상화의 정체성 특성을 정확하게 유지하는 것이 어렵고, 정체성 유지를 강조할 때 생성된 이미지가 미적 매력을 결여하며, LoRA 기반과 Adapter 기반 방법 모두와 호환되지 않는 한계가 있습니다.
- 이러한 문제들을 해결하기 위해, 우리는 ID-T2I 성능을 향상시키기 위한 일반 피드백 학습 프레임워크인 ID-Aligner를 제안합니다.
- 정체성 특징의 손실을 해결하기 위해 얼굴 감지 및 인식 모델로부터의 피드백을 활용하는 정체성 일관성 보상 미세조정을 도입하였습니다.
- 또한, 인간이 주석한 선호 데이터와 자동 구성된 피드백에 따른 보상을 활용하여 정체성 미적 보상 미세조정을 제안하여 미적 조정 신호를 제공합니다.
- 범용 피드백 미세조정 프레임워크 덕분에 우리의 방법은 LoRA 및 어댑터 모델 모두에 쉽게 적용될 수 있으며, 일관된 성능 향상을 달성할 수 있습니다.
- SD1.5 및 SDXL 확산 모델에 대한 광범위한 실험을 통해 우리 접근 방식의 유효성이 검증되었습니다. 프로젝트 페이지: \url{https://idaligner.github.io/}.

### [Editable Image Elements for Controllable Synthesis](https://arxiv.org/abs/2404.16029)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16029.png)

Vote: 3

Authors: Michaël Gharbi, Richard Zhang, Nuno Vasconcelos, Taesung Park, Xiaolong Wang, Jiteng Mu, Eli Shechtman

- 확산 모델이 글기반 합성에서 주목할 만한 발전을 이루었지만, 사용자가 제공한 이미지를 편집하는 것은 여전히 어려움을 겪고 있습니다.
- 이 연구에서는 이미지의 공간적 편집을 촉진하는 이미지 표현 방식을 제안합니다.
- "이미지 요소"로 입력 이미지를 인코딩하여 해당 이미지를 정확하게 재구성하고, 사용자가 직관적으로 편집할 수 있도록 합니다.
- 이러한 요소는 확산 모델에 의해 현실적인 이미지로 디코딩됩니다.
- 객체 크기 조정, 재배열, 드래깅, 가려짐 해소, 요소 제거, 변형 및 이미지 구성 등 다양한 이미지 편집 작업에서 이 표현 방식의 효과를 입증하였습니다.
- 프로젝트 페이지: [링크](https://jitengmu.github.io/Editable_Image_Elements/)

### [MotionMaster: Training-free Camera Motion Transfer For Video Generation](https://arxiv.org/abs/2404.15789)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.15789.png)

Vote: 3

Authors: Ran Yi, Lizhuang Ma, Teng Hu, Yating Wang, Jiangning Zhang, Hongrui Huang, Yabiao Wang, Jieyu Weng

- 확산 모델의 등장은 이미지 및 비디오 생성 분야에서 큰 발전을 이루었습니다.
- 비디오 동작 제어, 특히 카메라 동작 제어는 컨트롤 가능한 비디오 생성에서 중요한 주제입니다.
- 기존의 카메라 동작 제어 방법들은 시간적 카메라 모듈의 학습에 의존하고, 비디오 생성 모델의 많은 파라미터로 인해 상당한 계산 자원이 필요합니다.
- 기존 방법들은 학습 중에 카메라 동작 유형을 사전에 정의하여 카메라 제어의 유연성을 제한합니다.
- 따라서, 학습 비용을 줄이고 유연한 카메라 제어를 달성하기 위해 우리는 COMD라는 새로운 무학습 비디오 동작 이전 모델을 제안합니다.
- 이 모델은 원본 비디오에서 카메라 동작과 객체 동작을 분리하고 추출된 카메라 동작을 새로운 비디오로 전달합니다.
- 특히, 단발 카메라 동작 분리 방법을 제안하여 배경에서의 움직임에 기반한 움직이는 객체 지역의 카메라 동작을 추정합니다.
- 또한, 유사한 카메라 동작을 가진 여러 비디오에서 공통 카메라 동작을 추출하는 다발 카메라 동작 분리 방법을 제안합니다.
- 마지막으로, 다양한 유형의 카메라 동작을 결합하는 동작 조합 방법을 통해 카메라 제어의 유연성과 다양성을 개선합니다.
- 광범위한 실험을 통해 이 무학습 방식이 카메라-객체 동작을 효과적으로 분리하고 다양한 컨트롤 가능한 비디오 생성 작업에 적용될 수 있음을 보여줍니다.

### [MaGGIe: Masked Guided Gradual Human Instance Matting](https://arxiv.org/abs/2404.16035)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16035.png)

Vote: 2

Authors: Abhinav Shrivastava, Seoung Wug Oh, Chuong Huynh, Joon-Young Lee

- 인간 전경 픽셀을 추출하는 이미지 및 비디오 처리의 기초 작업인 인간 매트링(human matting)에 관한 연구입니다.
- 새로운 프레임워크인 MaGGIe는 각 인간 인스턴스에 대해 점진적으로 알파 매트(alpha mattes)를 예측하면서 계산 비용, 정확성 및 일관성을 유지합니다.
- 이 방법은 트랜스포머 주의(transformer attention)와 희소 컨볼루션(sparse convolution)을 포함한 현대적인 아키텍처를 활용하여 메모리와 지연 시간이 증가하지 않으면서 모든 인스턴스의 매트를 동시에 출력합니다.
- 여러 인스턴스 시나리오에서 일정한 추론 비용을 유지하면서도, 제안된 합성 벤치마크에 대해 견고하고 다양한 성능을 달성합니다.
- 높은 품질의 이미지 및 비디오 매트링 벤치마크와 함께, 공개적으로 이용 가능한 소스에서 다중 인스턴스 합성 접근 방식이 소개되어 모델의 실제 시나리오에서의 일반화를 증진시킵니다.

### [XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference](https://arxiv.org/abs/2404.15420)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.15420.png)

Vote: 2

Authors: Perouz Taslakian, Christopher Pal, Étienne Marcotte, Nicolas Chapados, Valentina Zantedeschi, João Monteiro, David Vázquez, Pierre-André Noël

- 이 연구는 기존의 인컨텍스트 학습(ICL)에 필요한 대규모 프롬프트 의존성을 줄이고 참조 텍스트를 기반으로 하는 효율적인 언어 모델 생성을 위해 캐시된 컨텍스트에서 교차 주의를 이용하는 새로운 모델을 제안합니다.
- 참조 정보에 따라 모델 생성을 조건화하기 위해 인코더-디코더 구조에서 영감을 받은 교차 주의 메커니즘이 도입되어, 기존 디코더만 있는 모델에 추가된 소수의 층만을 훈련시킵니다.
- QA(Question-Answering)를 실험 환경으로 사용하여, 제안된 모델이 조건부 생성 수행능력에서 기존 ICL을 뛰어넘고, 세밀하게 튜닝된 프롬프트 LLM과 비슷한 성능을 보이며, 표준 KV 캐싱 대비 공간을 상당히 줄인 것을 확인하였습니다.

### [BASS: Batched Attention-optimized Speculative Sampling](https://arxiv.org/abs/2404.15778)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.15778.png)

Vote: 2

Authors: Sudipta Sengupta, Mingyue Shang, Haifeng Qian, Sungsoo Ha, Sanjay Krishna Gouda, Xiaofei Ma, Anoop Deoras, Ramesh Nallapati, Sujan Kumar Gonugondla

- 추론 디코딩이 높은 성능을 구현하여 대규모 언어 모델의 응답 속도와 처리량을 개선하는 데 유용하게 사용되고 있지만, 대부분의 기존 구현은 단일 시퀀스 생성에 중점을 두고 있다.
- 실제 세계의 생성 AI 애플리케이션은 여러 응답이 필요하며, 추론 디코딩을 배치 처리 설정에서 실행하면서도 지연 시간 이점을 유지하는 것은 중대한 도전 과제다.
- 이 논문은 다중 시퀀스 생성 지연 시간을 혁신적으로 개선하고 GPU 활용도 및 제한된 시간 내에 생성 품질을 높이는 배치 추론 디코딩 시스템을 설명한다.
- 예를 들어, 7.8B 크기의 모델에서 단일 A100 GPU와 배치 크기 8을 사용할 때, 각 시퀀스는 토큰 당 평균 5.8ms의 속도로 생성되며, 전체 처리량은 초당 1.1K 토큰이다.
- 이 결과는 최고의 지연 시간과 최적화된 일반 디코딩 대비 약 2.15배의 속도 향상을 나타낸다.
- 지정된 시간 내에 일반 디코딩이 완료되지 않는 경우, 우리의 시스템은 HumanEval Pass@First가 43%, Pass@All이 61%에 이르는 시퀀스를 생성할 수 있으며, 이는 단일 시퀀스 추론 디코딩으로는 불가능한 수준이다.
- 디코딩 중 GPU 최대 활용도는 최대 15.8%로, 일반 디코딩의 최고치보다 3배가 넘고, 단일 시퀀스 추론 디코딩의 약 10배에 달한다.

### [MoDE: CLIP Data Experts via Clustering](https://arxiv.org/abs/2404.16030)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16030.png)

Vote: 1

Authors: Wen-Tau Yih, Shang-Wen Li, Saining Xie, Luke Zettlemoyer, Po-Yao Huang, Hu Xu, Shih-Fu Chang, Jiawei Ma

- 대조적 언어-이미지 사전 학습(CLIP)의 성공은 이미지와 캡션 사이의 연결에서 얻은 감독에 의존하지만 웹에서 크롤링된 데이터는 종종 노이즈가 많습니다.
- 연구팀은 데이터 전문가 클러스터링을 통해 CLIP 데이터 전문가 시스템인 '데이터 전문가의 혼합(Mixture of Data Experts, MoDE)'을 학습하여 제시합니다.
- 각 데이터 전문가는 해당 데이터 클러스터에서 훈련되며, 다른 클러스터의 잘못된 부정 정보로 인한 민감성이 감소됩니다.
- 추론 시, 작업 메타데이터와 클러스터 조건 간의 상관 관계를 통해 결정된 가중치를 적용하여 그들의 출력을 집계합니다.
- 상관 관계를 정확하게 추정하기 위해, 한 클러스터 내의 샘플은 의미론적으로 유사해야 하지만, 데이터 전문가의 수는 훈련 및 추론에 있어서 여전히 합리적이어야 합니다.
- 인간 언어의 체계를 고려하여, 보다 세밀한 클러스터 중심을 사용하여 각 데이터 전문가를 거칠게 표현할 것을 제안합니다.
- 실험적 연구는 CLIP 데이터 전문가 네 명이 ViT-B/16을 사용하여 OpenAI의 CLIP 및 OpenCLIP의 ViT-L/14보다 영상 분류에서 우수한 성능을 나타내지만, 훈련 비용은 35% 미만임을 보여줍니다.
- MoDE는 모든 데이터 전문가를 비동기적으로 훈련할 수 있으며, 새로운 데이터 전문가를 유연하게 포함할 수 있습니다.
- 코드는 https://github.com/facebookresearch/MetaCLIP/tree/main/mode에서 이용 가능합니다.

