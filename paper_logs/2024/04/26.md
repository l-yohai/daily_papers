## Daily Papers (2024-04-26)

### [Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding](https://arxiv.org/abs/2404.16710)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16710.png)

Vote: 27

Authors: Saurabh Agarwal, Bilge Acun, Ahmed A Aly, Beidi Chen, Bram Wasti, Carole-Jean Wu, Liangzhen Lai, Ahmed Roman, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Anas Mahmoud, Mostafa Elhoushi

- LayerSkip은 대규모 언어 모델의 추론 속도를 향상시키기 위해 고안된 종단간 솔루션을 제시합니다.
- 훈련 시 초기 레이어에서는 낮은 드롭아웃 비율을, 후반 레이어에서는 높은 드롭아웃 비율을 적용하고, 모든 트랜스포머 레이어가 동일한 출구를 공유하는 초기 출구 손실을 도입합니다.
- 추론 단계에서는 추가적인 보조 레이어나 모듈을 추가하지 않고도 초기 레이어에서의 조기 출구 정확도가 향상됨을 보여줍니다.
- 새로운 자기-추측 디코딩 방식을 통해 초기 레이어에서 출구하고 남은 레이어로 검증 및 수정을 수행함으로써 메모리 사용량을 줄이고 초안과 검증 단계의 공유된 계산 및 활성화의 이점을 활용합니다.
- 다양한 Llama 모델 크기를 사용하여, 처음부터의 사전 훈련, 지속적인 사전 훈련, 특정 데이터 도메인과 작업에 대한 미세 조정 등 다양한 유형의 훈련에서 실험을 수행하였습니다.
- 구현된 추론 솔루션을 통해 CNN/DM 문서 요약에서 최대 2.16배, 코딩에서 1.82배, TOPv2 의미 파싱 작업에서 2.0배의 속도 향상을 보여줍니다.

### [How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://arxiv.org/abs/2404.16821)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16821.png)

Vote: 20

Authors: Jiapeng Luo, Hewei Guo, Zheng Ma, Kongzhi Hu, Zhangwei Gao, Chao Xu, Weiyun Wang, Zhenjiang Jin, Xingjian Wei, Wenwen Tong, Jiaqi Wang, Conghui He, Erfei Cui, Shenglong Ye, Hao Tian, Bin Wang, Ji Ma, Wenjian Zhang, Wei Li, Xiaoyi Dong, +, Zhe Chen, Hang Yan

- 본 보고서에서는 개방형 소스와 기업용 상업 모델 간의 다중 모드 이해 능력 격차를 해소하기 위해 다중 모달 대규모 언어 모델(InternVL 1.5)을 소개합니다.
- 강력한 시각 인코더 개선을 통해 대규모 비전 기반 모델 InternViT-6B의 지속적인 학습 전략을 탐구하고, 이를 다른 LLM에 전송 및 재사용할 수 있게 하여 시각 이해 능력을 향상시켰습니다.
- 동적 고해상도 기법을 도입하여 입력 이미지의 종횡비 및 해상도에 따라 448x448 픽셀의 타일을 1에서 40개까지 나누어 최대 4K 해상도 입력을 지원합니다.
- 고품질 이중 언어 데이터셋을 세심하게 수집하여 일반 장면 및 문서 이미지를 커버하고, 영어와 중국어 질문-답변 쌍으로 주석을 달아 OCR 및 중국어 관련 작업의 성능을 크게 향상시켰습니다.
- InternVL 1.5는 벤치마크 및 비교 연구를 통해 평가되었으며, 개방형 및 상업용 모델과 비교할 때 경쟁력 있는 성능을 보여주며 18개 벤치마크 중 8개에서 최고의 결과를 달성했습니다.
- 관련 코드는 https://github.com/OpenGVLab/InternVL 에서 확인할 수 있습니다.

### [Make Your LLM Fully Utilize the Context](https://arxiv.org/abs/2404.16811)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16811.png)

Vote: 16

Authors: Jian-Guang Lou, Shengnan An, Nanning Zheng, Zexiong Ma, Zeqi Lin

- 많은 최신 대규모 언어 모델들이 긴 입력을 처리할 수 있음에도 불구하고, 긴 맥락에서 정보를 완전히 활용하는 데 어려움을 겪는 '중간에서의 상실' 문제가 있습니다.
- 이 문제는 긴 맥락 훈련 동안 충분한 명시적 지도가 부족하기 때문에 발생한다고 가정하며, 이 문제를 해결하기 위해 정보 집약적(IN2) 훈련 방법을 제안합니다.
- IN2 훈련은 긴 맥락(4K-32K 토큰) 내에 짧은 세그먼트(~128 토큰)에 대한 미세한 정보 인식과 두 개 이상의 짧은 세그먼트의 정보를 통합하고 추론해야 하는 답변을 요구하는 합성 긴 맥락 질문-답변 데이터셋을 활용합니다.
- 이 훈련 방식을 Mistral-7B 모델에 적용하여, 긴 맥락을 활용하는 능력을 평가하기 위해 다양한 맥락 스타일(문서, 코드, 구조화된 데이터 맥락)과 정보 검색 패턴(전방, 후방, 양방향 검색)을 포함하는 세 가지 탐사 작업을 설계한 FILM-7B(FILl-in-the-Middle)를 개발했습니다.
- 탐사 결과, FILM-7B는 32K 맥락 창에서 다양한 위치에서 정보를 견고하게 검색할 수 있는 능력을 보여줍니다.
- 이 외에도 FILM-7B는 실제 긴 맥락 작업에서의 성능을 크게 향상시켰으며 (예: NarrativeQA의 F1 점수 23.5에서 26.9로), 짧은 맥락 작업에서는 비슷한 성능을 유지하고 있습니다 (예: MMLU의 정확도 59.3에서 59.2로).
- 연구 코드는 Github 페이지에서 확인할 수 있습니다: https://github.com/microsoft/FILM.

### [Tele-FLM Technical Report](https://arxiv.org/abs/2404.16645)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16645.png)

Vote: 12

Authors: Zhongyuan Wang, Zhongjiang He, Tiejun Huang, Xuezhi Fang, Xiang Li, Zheng Zhang, Xin Wang, Aixin Sun, Bo Zhao, Xin Jiang, Xuelong Li, Yongxiang Li, Yu Zhao, Zihan Wang, Chao Wang, Yiqun Yao, Yequan Wang, Shuangyong Song, Yuyao Huang, Xinzhang Liu

- 다양한 언어 어플리케이션을 지원하는 LLM(Large Language Models)의 언어 이해 및 생성 능력을 확장하는 데 있어, 50억 개 이상의 매개변수를 효율적으로 확장하는 상세한 오픈 소스 방법론이 부족하다.
- 이 보고서에서는 안정적이고 효율적인 사전 훈련 패러다임과 향상된 사실적 판단 능력을 특징으로 하는 52B 크기의 다국어 대형 언어 모델인 Tele-FLM (FLM-2라고도 함)을 소개한다.
- Tele-FLM은 텍스트 코퍼스에서 BPB 지표를 사용하여 탁월한 다국어 언어 모델링 능력을 보여준다.
- 영어와 중국어 기초 모델 평가에서는 Llama2-70B 및 DeepSeek-67B와 같이 큰 사전 훈련 FLOPs를 포함하는 강력한 오픈 소스 모델과 비교해 볼만하다.
- 모델 가중치 뿐만 아니라 핵심 설계, 엔지니어링 관행, 훈련 세부 사항을 공유하여 학계와 산업계 모두에게 이점을 제공할 것으로 기대한다.

### [Interactive3D: Create What You Want by Interactive 3D Generation](https://arxiv.org/abs/2404.16510)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16510.png)

Vote: 11

Authors: Shaocong Dong, Tianfan Xue, Dan Xu, Zhanpeng Huang, Zibin Wang, Lihe Ding

- 3D 객체 생성 분야는 획기적으로 발전하였으나 사용자의 정확한 제어를 달성하는 데에는 한계가 있으며, 이로 인해 사용자의 기대와 일치하지 않는 결과가 종종 나타납니다.
- 기존 방법은 텍스트 지시에 따른 제어가 제한적이거나 2D 이미지로부터 3D 객체를 재구성하는 두 가지 접근 방식을 주로 사용하여, 사용자 맞춤화의 범위가 제한되고 3D로 변환하는 과정에서 원하지 않는 오류가 발생할 수 있습니다.
- 이 연구에서는 사용자가 생성 과정을 정밀하게 제어할 수 있도록 광범위한 3D 상호작용 기능을 제공하는 새로운 프레임워크인 Interactive3D를 소개합니다.
- Interactive3D는 두 단계로 구성되며, 각각 다른 3D 표현을 사용하는데, 첫 번째 단계에서는 사용자가 직접 상호작용할 수 있는 Gaussian Splatting을 사용합니다.
- 사용자들은 3D 시스템에서 부분 추가 및 삭제, 변형 가능 및 강성 드래깅, 기하학적 변환 및 의미론적 편집을 통해 생성 방향을 중간 단계에서 수정하고 안내할 수 있습니다.
- 이후에 Gaussian 첨가물은 InstantNGP로 변환되며, 두 번째 단계에서 상호 작용적 해시 세밀조정 모듈을 통해 더욱 세부적인 정보를 추가하고 기하학적 세부사항을 추출합니다.
- Interactive3D를 통한 실험 결과는 3D 생성의 조종 가능성과 품질이 크게 향상됨을 보여줍니다.
- 프로젝트 웹 페이지는 https://interactive-3d.github.io/에서 확인할 수 있습니다.

### [NeRF-XL: Scaling NeRFs with Multiple GPUs](https://arxiv.org/abs/2404.16221)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16221.png)

Vote: 10

Authors: Sanja Fidler, Angjoo Kanazawa, Ruilong Li, Francis Williams

- 네트워크가 여러 GPU에 분산되어 처리되어 네트워크의 임의적 크기 확장 및 렌더링이 가능하도록 하는 NeRF-XL 방법을 제시합니다.
- 기존의 여러 GPU를 이용한 방식에서 발견된 문제들을 해결하고, 더 많은 하드웨어 사용을 통해 임의의 매개 변수 수를 가진 NeRF의 훈련과 렌더링을 가능하게 합니다.
- NeRF-XL은 GPU간 통신을 최소화하면서도 수학적으로 고전적인 단일-GPU 사례와 동등한 새로운 분산 훈련 및 렌더링 방식을 기반으로 합니다.
- 넓은 매개 변수 범위에 대한 NeRF의 스케일링 법칙을 최초로 밝히면서, 매개 변수가 많을수록 재구성 품질이 향상되고, GPU가 더 많아질수록 속도가 향상됩니다.
- 가장 큰 오픈 소스 데이터셋인 25km² 도시 지역을 포괄하는 258K 이미지를 포함한 MatrixCity 등, 다양한 데이터셋에서 NeRF-XL의 효과성을 입증합니다.

### [List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs](https://arxiv.org/abs/2404.16375)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16375.png)

Vote: 9

Authors: Jianfeng Gao, Linjie Li, Kevin Lin, Junda Wu, Wanrong Zhu, Lijuan Wang, Zhengyuan Yang, Jianfeng Wang, Jianwei Yang, An Yan, Julian McAuley

- GPT-4V의 시각적 기반 능력을 활성화시키는 Set-of-Mark (SoM) 프롬프팅은 이미지에 삽입된 태그와 시각적 객체를 연결할 수 있게 합니다.
- 이러한 알파뉴메릭으로 표시된 태그들은 텍스트 토큰을 통해 쉽게 참조할 수 있으며, GPT-4V는 뛰어난 성능을 보이지만 다른 다중모달 대규모 언어 모델들은 이러한 시각적 태그를 이해하는 데 어려움을 겪습니다.
- 오픈소스 모델들이 SoM 프롬프팅을 학습할 수 있도록 새로운 학습 패러다임인 "하나씩 항목 나열하기"를 제안합니다. 이 방법은 모델에게 이미지에 배치된 시각적 태그들을 알파뉴메릭 순서대로 열거하고 설명하도록 요청합니다.
- 저희는 맞춤형 데이터셋을 다른 시각적 지시 튜닝 데이터셋과 통합함으로써 기존 다중모달 대규모 언어 모델들에게 SoM 프롬프팅 능력을 갖추게 합니다.
- 새로운 데이터셋은 비교적 작은 규모(태그가 있는 10k-30k 이미지)임에도 불구하고 다중모달 대규모 언어 모델들의 시각적 추론 능력을 크게 향상시키고 환각을 줄입니다.
- 놀랍게도 이러한 개선은 추론 동안 입력 이미지에서 시각적 태그가 생략되어도 지속됩니다.
- "하나씩 항목 나열하기"는 훈련 단계에서 시각적 태그를 사용함으로써 객체-텍스트 정렬을 강화하는 새로운 다중모달 대규모 언어 모델 훈련 패러다임의 잠재력을 시사합니다.
- 또한, SoM의 작동 메커니즘을 이해하기 위해 훈련된 모델을 조사하는 분석을 수행했습니다.
- 연구 코드와 데이터는 https://github.com/zzxslp/SoM-LLaVA 에서 확인할 수 있습니다.

### [ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving](https://arxiv.org/abs/2404.16771)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16771.png)

Vote: 9

Authors: Yiqiang Yan, Wenhui Song, Xiao Dong, Jiehui Huang, Shutao Liao, Hanhui Li, Yuhao Cheng, Shengcai Liao, Long Chen, Jun Zhou, Xiaodan Liang

- 유통 기반 기술이 개인화되고 맞춤화된 얼굴 생성에서 크게 진전을 이루었지만, 기존 방법들은 얼굴 영역을 섬세하게 제어하는 데 한계가 있고, 복잡한 얼굴 세부 정보와 전체 얼굴을 완전히 고려하는 신원 보존 전략이 부족하여 높은 충실도와 정확한 신원 일관성을 달성하는 데 어려움을 겪습니다.
- 이러한 한계를 극복하기 위해, ConsistentID는 단일 참조 이미지만을 사용하여 세밀한 다중 모드 얼굴 프롬프트 하에 다양한 신원을 보존하는 초상화 생성을 위한 혁신적인 방법을 도입합니다.
- ConsistentID는 얼굴 특징, 해당 얼굴 설명, 그리고 전체 얼굴 컨텍스트를 결합한 다중 모드 얼굴 프롬프트 생성기와 얼굴 영역의 신원 일관성을 보존하기 위해 최적화된 신원 보존 네트워크를 포함하는 두 가지 주요 구성 요소로 구성됩니다.
- 이 구성요소들은 얼굴 영역에서 미세하게 조정된 다중 모드 신원 정보를 도입함으로써 신원 보존의 정확성을 크게 향상시킵니다.
- ConsistentID의 훈련을 돕기 위해, 500,000개 이상의 얼굴 이미지를 포함하는 세밀한 초상화 데이터셋 FGID를 제시하며, 이는 기존 공개 얼굴 데이터셋보다 다양성과 종합성이 뛰어납니다.
- 실험 결과에 따르면, ConsistentID는 개인화된 얼굴 생성에서 뛰어난 정밀도와 다양성을 달성하여, 기존 방법들을 MyStyle 데이터셋에서 능가하는 성과를 보여줍니다.
- 또한, ConsistentID는 더 많은 다중 모드 신원 정보를 도입하면서도 생성 중에 빠른 추론 속도를 유지합니다.

### [Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and Human Ratings](https://arxiv.org/abs/2404.16820)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16820.png)

Vote: 7

Authors: Isabela Albuquerque, Aida Nematzadeh, Yasumasa Onoe, Olivia Wiles, Chuhan Zhang, Su Wang, Emanuele Bugliarello, Chris Knutsen, Jordi Pont-Tuset, Ivana Kajić, Cyrus Rashtchian

- 텍스트-이미지(T2I) 생성 모델이 널리 사용되고 있지만, 제시된 프롬프트에 따른 이미지를 생성하지 못할 수 있습니다.
- 이전 연구는 T2I 정렬을 평가하기 위해 지표, 벤치마크, 인간 판정을 수집하기 위한 템플릿을 제안했으나, 이러한 구성 요소의 품질은 체계적으로 측정되지 않았습니다.
- 연구자들이 모델을 비교할 때 사용하는 프롬프트 세트의 신뢰성과 그 평가가 이루어지지 않은 경우가 많습니다.
- 우리는 자동 평가 지표와 인간 템플릿을 평가하기 위한 광범위한 연구를 수행함으로써 이러한 격차를 해소합니다.
- 첫째, 다양한 인간 템플릿에서 모델을 구별할 수 있는 포괄적인 기술 기반 벤치마크를 도입하였습니다.
- 이 벤치마크는 프롬프트를 하위 기술로 분류하여 어떤 기술이 어려운지, 그리고 어떤 복잡성 수준에서 기술이 도전이 되는지를 정확히 파악할 수 있게 해줍니다.
- 둘째, 네 가지 템플릿과 네 가지 T2I 모델을 통해 100K 이상의 평가를 수집하여 프롬프트의 모호성과 지표 및 모델 품질의 차이에서 발생하는 차이점을 이해할 수 있습니다.
- 마지막으로, 새로운 데이터셋, 다른 인간 템플릿 및 TIFA160에서 기존 지표보다 인간 평가와 더 잘 상관관계를 가진 새로운 QA 기반 자동 평가 지표를 도입했습니다.

### [SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension](https://arxiv.org/abs/2404.16790)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16790.png)

Vote: 4

Authors: Ying Shan, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, Bohao Li

- 이미지 내에 포함된 텍스트가 풍부한 다양한 시나리오에서 대다수로 이루어져 있는 현실 세계에서 멀티모달 대규모 언어 모델(MLLM)의 응용이 중요하다는 것을 강조한다.
- 현재 MLLM 벤치마크는 일반 시각적 이해력 평가에 중점을 두고 있어 텍스트가 풍부한 시나리오에서의 MLLM의 능력을 포괄적이고 객관적으로 평가하는 데 한계가 있다.
- 이 연구에서는 텍스트가 풍부한 시각적 이해를 평가하기 위해 특별히 설계된 SEED-Bench-2-Plus 벤치마크를 소개하며, 그것은 차트, 지도, 웹의 세 가지 주요 범주를 포함한다.
- 이 벤치마크는 실제 세계의 텍스트-풍부한 환경을 효과적으로 시뮬레이션 할 수 있다는 점에서 2.3K의 다중 선택형 질문과 정확한 인간 주석을 포함한다.
- 연구진은 34개의 주요 MLLM(예: GPT-4V, Gemini-Pro-Vision, Claude-3-Opus 포함)에 대한 철저한 평가를 수행하여 텍스트가 풍부한 시각적 이해에서의 MLLM의 현재 한계를 강조한다.
- 이 연구가 기존 MLLM 벤치마크에 중요한 추가가 되어 텍스트가 풍부한 시각적 이해에 대한 MLLM의 연구를 촉진하는 데 도움이 되기를 바라며, 데이터셋과 평가 코드는 제공된 웹 링크를 통해 접근할 수 있다.

