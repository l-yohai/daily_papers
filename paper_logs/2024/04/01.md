## Daily Papers (2024-04-01)

### [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2403.19887)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19887.png)

Vote: 47

Authors: Nir Ratner, Barak Lenz, Roman Glozman, Raz Alon, Noam Rozen, Shaked Meirom, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Hofit Bata, Amir Bergman, Opher Lieber, Michael Gokhman, Yoav Shoham, Tomer Asida, Mor Zusman, Erez Shwartz, Yonatan Belinkov, Omri Abend, Avashalom Manevich, Shai Shalev-Shwartz

- Jamba는 새로운 대규모 기반 언어 모델로서, 트랜스포머와 맘바 레이어의 혼합이 특징인 혁신적인 하이브리드 아키텍처를 기반으로 합니다.
- 이 모델은 트랜스포머 및 맘바 블록을 교대로 배치하고, 모델 용량은 증가시키면서도 활성화된 매개변수 사용을 관리가능한 수준으로 유지하고자 전문가들의 혼합(MoE)을 일부 층에 추가합니다.
- 이 유연한 구조는 자원과 목적에 특화된 구성을 가능하게 하며, 실제 구현된 설정에서는 하나의 80GB GPU 내에 배치 가능한 강력한 모델을 만들어냅니다.
- Jamba는 기존의 트랜스포머 대비 높은 처리량과 낮은 메모리 사용을 제공하면서도, 표준 언어 모델 벤치마크와 장문맥 평가에서 최첨단 성능을 보여줍니다.
- 특히, 256K 토큰 컨텍스트 길이까지 뛰어난 결과를 보여주는 것으로 나타났습니다.
- 연구는 트랜스포머와 맘바 레이어를 결합하는 방법, 전문가를 혼합하는 방식 등 다양한 아키텍처 상의 결정들에 대해 다루며, 대규모 모델링에서 이러한 결정들 중 일부는 중요하다는 것을 보여줍니다.
- Jamba의 훈련 및 평가를 통해 드러난 이러한 아키텍처의 다양한 흥미로운 특성들을 기술하고, 이 혁신적인 구조의 추가 탐구를 장려하기 위해 다양한 변형 실행의 체크포인트들을 공개할 계획이라고 밝힙니다.
- 우리는 Jamba의 구현물의 가중치를 자유로운 라이선스 하에 공개할 예정이며, 이를 통해 해당 모델의 접근성을 높입니다.

### [Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs](https://arxiv.org/abs/2403.20041)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.20041.png)

Vote: 23

Authors: Lunxi Yuan, Rui Wang, Sheng Qian, Qin Xie, Jie Lu, Luchang Li

- 휴대전화의 다양한 임무에 사용되는 거대 언어 모델(LLM)의 실시간 장치 배치를 위하여, Transformer-Lite라는 모바일 추론 엔진을 도입하였습니다.
- Transformer-Lite는 기본적인 모델 추론을 위한 상징적 표현 접근법, 연산자 최적화, 실행 우선순위 설정 및 새로운 FP4 양자화 방법인 M0E4을 포함하는 네 가지 최적화 기술을 제안합니다.
- 이 기술들은 LLM의 KV 캐시 복사를 생략하는 하위 텐서 기반 기술과 결합되어, 휴대전화에서의 더 빠른 추론 속도와 지연 감소를 구현합니다.
- Qualcomm과 MTK 프로세서와 호환되는 이 엔진은 2B부터 14B에 이르는 다양한 아키텍처 및 매개변수로 구성된 LLM의 효율성을 입증합니다.
- Transformer-Lite는 ChatGLM2 6B 모델에서는 토큰당 121개의 prefill 속도와 14개의 디코딩 속도를, 더 작은 Gemma 2B 모델에서는 토큰당 330개의 prefill 속도와 30개의 디코딩 속도를 달성하였습니다.
- 이것은 CPU 기반의 FastLLM과 GPU 기반의 MLC-LLM에 비해 prefill 속도에서 10배 이상, 디코딩 속도에서 2~3배의 속도 향상을 제공합니다.

### [Gecko: Versatile Text Embeddings Distilled from Large Language Models](https://arxiv.org/abs/2403.20327)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.20327.png)

Vote: 19

Authors: Blair Chen, Ming-Wei Chang, Sai Meher Karthik Duddu, Michael Boratko, Prateek Jain, Gustavo Hernandez Abrego, Jeremy R. Cole, Kai Hui, Siddhartha Reddy Jonnalagadda, Wen Ding, Iftekhar Naim, Rajvi Kapadia, Xiaoqi Ren, Nithi Gupta, Aditya Kusupati, Yi Luan, Weiqiang Shi, Zhuyun Dai, Daniel Cer, Jinhyuk Lee

- Gecko 라는 새로운 컴팩트하고 다재다능한 텍스트 임베딩 모델을 소개합니다.
- 큰 언어 모델(LLMs)로부터 지식을 추출(distilling)하여 강력한 검색 성능을 달성하는 방법을 사용합니다.
- 먼저 LLM을 사용하여 다양한 합성 쌍 데이터를 생성하는 두 단계의 추출 과정을 거칩니다.
- 그 후에 각 쿼리에 대해 후보 지문들을 검색하고, 동일한 LLM을 사용하여 긍정적인 지문과 어려운 부정적인 지문들을 다시 레이블링하여 데이터 품질을 더욱 개선합니다.
- Gecko의 효율성은 Massive Text Embedding Benchmark (MTEB)에서 입증되었으며, 임베딩 차원이 256인 Gecko는 768 차원을 가진 기존 모델들을 능가합니다.
- 768 차원의 임베딩을 가진 Gecko는 평균 점수가 66.31로 7배 큰 모델과 5배 더 높은 차원의 임베딩과 경쟁합니다.

### [InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds](https://arxiv.org/abs/2403.20309)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.20309.png)

Vote: 10

Authors: Marco Pavone, Danfei Xu, Yue Wang, Xinghao Ding, Zhiwen Fan, Wenyan Cong, Zhangyang Wang, Kevin Wang, Jian Zhang, Georgios Pavlakos, Boris Ivanovic, Kairun Wen

- 새로운 시점 합성(Novel View Synthesis, NVS)의 진전에도 불구하고, 3D 컴퓨터 비전에서 일반적으로 카메라의 내부 및 외부 매개변수를 초기에 추정하는 것이 요구되는데, 이는 밀도있는 시점에서 수행되는 구조 추출 작업(Structure-from-Motion, SfM)이며, 매칭되는 특징이 충분하지 않을 때 느리고 불안정할 수 있습니다.
- 본 연구에서는 점 기반 표현(예: 3D Gaussian Splatting, 3D-GS)과 밀집 스테레오 모델(DUSt3R)의 장점을 통합하여 제약이 없는 설정 하에서 NVS의 복잡하지만 미해결된 문제, 즉 포즈 없는 및 희소 시점의 문제를 해결합니다.
- InstantSplat이라는 프레임워크는 밀집 스테레오 선행 지식을 3D-GS와 통합하여 1분 미만의 시간 안에 희소 시점 및 포즈 없는 이미지에서 대규모 장면의 3D Gaussian을 구현합니다.
- InstantSplat은 선행 학습된 밀집 스테레오 파이프라인에서 파생된 전역 정렬된 3D 점 지도를 이용하여 모든 훈련 시점에 걸쳐 빠르게 초기 장면 구조와 카메라 매개변수를 설정하는 Coarse Geometric Initialization (CGI) 모듈을 포함합니다.
- 이어서 Fast 3D-Gaussian Optimization (F-3DGO) 모듈은 포즈 정규화와 함께 3D Gaussian 속성과 초기화된 포즈를 공동으로 최적화합니다.
- 대규모 야외 Tanks & Temples 데이터셋에 대한 실험은 InstantSplat이 SSIM을 32% 향상시키고 절대 궤적 오류(ATE)를 80% 감소시키는 것을 보여줍니다.
- 이를 통해 InstantSplat은 포즈에 자유롭고 희소한 시점 조건이있는 시나리오에 유용한 솔루션으로 자리매김 할 수 있음을 입증합니다. 프로젝트 페이지: instantsplat.github.io.

### [ReALM: Reference Resolution As Language Modeling](https://arxiv.org/abs/2403.20329)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.20329.png)

Vote: 10

Authors: Prathamesh Saraf, Melis Ozyildirim, Joel Ruben Antony Moniz, Nidhi Rajshree, Yuan Zhang, Soundarya Krishnan, Halim Cagri Ates, Hong Yu

- 참조 해결은 여러 종류의 맥락을 이해하고 성공적으로 다루기 위해 필수적인 중요한 문제입니다.
- 이 맥락에는 이전 대화뿐만 아니라 사용자 화면상의 엔터티나 배경에서 실행되는 엔터티와 같은 대화형이 아닌 엔터티들이 포함됩니다.
- LLMs(Large Language Models)가 다양한 작업에서 매우 강력함을 보여주긴 했지만, 특히 대화형이 아닌 엔터티들에 관한 참조 해결에서는 충분히 활용되지 않았습니다.
- 본 논문은 참조 해결을 언어 모델링 문제로 변환시킬 수 있음을 보여주며, 전통적으로 텍스트만으로 축소하기 어려운 스크린상의 엔터티들 같은 형태의 엔터티들을 포함하여 다양한 타입의 참조들을 해결하기 위한 매우 효과적인 시스템을 만들 수 있음을 보여줍니다.
- 가장 작은 모델에서 화면상 참조에 대해 5% 이상의 절대적인 성능 향상을 포함하여 다른 타입의 참조들에 대해 기존 시스템보다 크게 향상된 결과를 보였습니다.
- 또한, 가장 작은 모델은 GPT-3.5와 GPT-4와 비교하여 GPT-4와 비슷한 성능을 달성했으며, 더 큰 모델들은 GPT-4를 크게 뛰어넘는 성능을 보였습니다.

### [Localizing Paragraph Memorization in Language Models](https://arxiv.org/abs/2403.19851)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19851.png)

Vote: 8

Authors: Niklas Stoehr, Owen Lewis, Mitchell Gordon, Chiyuan Zhang

- 언어 모델이 훈련 데이터의 전체 문단을 기억하고 되뇌이는 데 사용되는 가중치와 메커니즘을 특정할 수 있는지 이 논문은 탐구합니다.
- 기억된 문단에 대한 그래디언트는 비기억 문단에 비해 더 두드러진 공간 패턴을 가지고 있으며, 모델의 하위 레이어에서 더 크다는 것을 발견합니다.
- 또한, 높은 그래디언트를 가진 가중치만 미세 조정함으로써 기억된 예시들을 잊게 할 수 있다는 점을 보여줍니다.
- 문단 기억에서 특별히 중요한 역할을 하는 하위 레이어 주의 집중 헤드(head)를 국소화하고, 이 헤드는 상대적으로 빈도가 낮아 구별력이 높은 토큰에 주의를 집중하는 것으로 나타납니다.
- 접두어에 있는 토큰들이 얼마나 특정 기억에 국소화되어 있는지를 평가하기 위해 토큰을 변형시키고 그로 인한 해독 변경을 측정합니다.
- 몇 개의 구별되는 토큰만으로도 종종 전체 이어지는 말을 변질시킬 수 있으며, 기억된 연속체는 기억되지 않은 것들보다 더 배우기 어렵고 변형도 어렵습니다.

### [DiJiang: Efficient Large Language Models through Compact Kernelization](https://arxiv.org/abs/2403.19928)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19928.png)

Vote: 7

Authors: Hanting Chen, Yuchuan Tian, Xutao Wang, Zhicheng Liu, Yunhe Wang

- 트랜스포머의 계산 부담을 줄이기 위해 선형 주의(linear attention)에 대한 연구가 활발해졌습니다.
- 대규모 언어 모델을 위한 주의 메커니즘 개선 전략은 대규모 재학습을 필요로 하여 비현실적이었습니다.
- 본 논문은 사전 훈련된 기존 트랜스포머를 적은 훈련 비용으로 선형 복잡성 모델로 변환할 수 있는 'DiJiang'이라는 새로운 주파수 도메인 커널화 방법을 제시합니다.
- 가중된 준 몬테카를로 샘플링 방법을 사용함으로써 이론적으로 우수한 근사 효율성을 제공합니다.
- 훈련 계산 복잡성을 더욱 줄이기 위해 이산 코사인 변환(DCT) 연산에 기반한 커널화가 사용됩니다.
- 방대한 실험을 통해 제안된 방법이 원본 트랜스포머와 비교할 수 있는 성능을 달성하면서 훨씬 감소된 훈련 비용과 훨씬 빠른 추론 속도를 보여줍니다.
- DiJiang-7B는 LLaMA2-7B와 비슷한 성능을 보이면서 약 1/50의 훈련 비용만 요구합니다.
- 코드는 https://github.com/YuchuanTian/DiJiang 에서 제공됩니다.

### [Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces](https://arxiv.org/abs/2403.20275)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.20275.png)

Vote: 7

Authors: Jonathan Tremblay, Mauro Comi, Alessio Tonioni, Laurence Aitchison, Nathan F. Lepora, Yijiong Lin, Valts Blukis, Max Yang

- 이 연구는 촉각과 시각 데이터를 접목해 복잡한 표면 재구성과 새로운 시점 합성의 문제를 다루는 Tactile-Informed 3DGS라는 새로운 방법을 제안합니다.
- 촉각 데이터(지역적 깊이 맵)를 다시점 시각 데이터와 통합하여 접촉 지점에서 객체의 기하학적 형태를 정확하게 모델링하기 위해 3D 가우시안 원시체를 최적화합니다.
- 접촉 위치에서 투과율을 감소시키는 프레임워크를 개발함으로써, 균일하게 매끄러운 깊이 맵을 보장하는 세밀한 표면 재구성을 달성합니다.
- 비람버트 표면(예: 반사적 또는 광택 있는 표면)에서 현대 방법들이 정확한 재구성을 실패하곤 하는데, 이 연구에서 제안하는 시각과 촉각 센싱의 결합은 적은 이미지를 사용하면서 더 정확한 기하학적 재구성을 달성합니다.
- 연구팀은 광택 있는 반사 표면을 가진 물체에 대한 평가를 수행하고 접근 방식의 효과를 입증하며, 재구성 품질에서의 중요한 개선을 보여줍니다.

### [Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models](https://arxiv.org/abs/2403.20331)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.20331.png)

Vote: 7

Authors: Hai Li, Yifei Ming, Jingkang Yang, Qing Yu, Jingyang Zhang, Kiyoharu Aizawa, Yixuan Li, Ziwei Liu, Atsuyuki Miyai, Go Irie

- 이 논문은 비전 언어 모델(VLMs)의 신뢰성을 평가하는 새롭고 중요한 도전 과제인 '해결 불가능한 문제 검출(Unsolvable Problem Detection, UPD)'을 소개합니다.
- UPD는 비쥬얼 질문 답변(Visual Question Answering, VQA) 작업에서 해결 불가능한 문제에 직면했을 때 VLM이 답변을 보류하는 능력을 조사합니다.
- 이 검출은 세가지 구별된 설정을 포함하며, 이는 '결석한 답변 검출(Absent Answer Detection, AAD)', '호환되지 않는 답변 세트 검출(Incompatible Answer Set Detection, IASD)', '호환되지 않는 비주얼 질문 검출(Incompatible Visual Question Detection, IVQD)'입니다.
- 광범위한 실험을 통해 GPT-4V, LLaVA-Next-34B를 포함한 대부분의 VLMs이 다양한 정도로 UPD 벤치마크에 어려움을 겪는 것으로 나타났으며, 이는 큰 개선의 여지를 드러냅니다.
- UPD 문제를 다루기 위해 저자들은 훈련이 필요 없는 솔루션과 훈련 기반 솔루션을 모두 탐색하며, 이들의 효율성과 한계에 대한 새로운 통찰을 제공합니다.
- 저자들은 제안된 UPD 설정 내에서의 미래 노력과 함께, 이러한 통찰들이 더 실용적이고 신뢰할 수 있는 VLMs의 발전과 더 넓은 이해를 증진시킬 것을 희망합니다.

### [MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection](https://arxiv.org/abs/2403.19888)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19888.png)

Vote: 6

Authors: Ali Behrouz, Michele Santacatterina, Ramin Zabih

- 딥러닝의 최근 발전은 데이터에 의존하며 대규모 학습이 가능한 트랜스포머에 주로 의존해왔으나, 이들의 주의(attention) 모듈은 입력 크기에 따라 시간과 공간 복잡도가 제곱으로 증가하여 긴 시퀀스 모델링의 확장성에 한계가 있음.
- 이미지 및 다변량 시계열과 같은 다차원 데이터를 위한 효율적이고 효과적인 아키텍처 설계를 위한 최근 시도들은 데이터 독립적이거나 차원 간 통신을 허용하지 못하는 단점이 있음.
- 효율적인 하드웨어 지향 구현을 가진 상태 공간 모델(State Space Models, SSM), 특히 선택적 상태 공간 모델이 긴 시퀀스 모델링에 대한 유망한 가능성을 보임에 따라, 이러한 SSM의 성공을 바탕으로 MambaMixer라는 새로운 아키텍처를 제안함.
- MambaMixer는 데이터에 의존하는 가중치를 사용하며, 토큰과 채널 간의 이중 선택 메커니즘인 Selective Token and Channel Mixer를 사용함.
- 이 아키텍처는 선택적 믹서들을 가중치 평균화 메커니즘을 사용하여 연결하고, 이를 통해 레이어들이 초기 특성에 직접 접근할 수 있도록 함.
- MambaMixer 블록을 기반으로 한 Vision MambaMixer (ViM2) 및 Time Series MambaMixer (TSM2) 아키텍처를 설계하여 다양한 시각 및 시계열 예측 작업에서의 성능을 탐구함.
- 그 결과, 토큰과 채널 간 선택적 혼합의 중요성을 강조하며, ViM2는 이미지넷 분류, 객체 감지 및 의미 분할 작업에서 기존의 유명한 시각 모델들과 경쟁력 있는 성능을 보이며 SSM 기반 시각 모델들을 능가함.
- 시계열 예측에서는 TSM2가 최첨단 방법들과 비교하여 뛰어난 성능을 달성하면서 현저하게 개선된 계산 비용을 보임.
- 이러한 결과들은 트랜스포머, 채널 간 주의, MLP 모두 시계열 예측에서 좋은 성능을 내기에 충분하지만 그중 어느 것도 필수적이지는 않음을 보여줌.

