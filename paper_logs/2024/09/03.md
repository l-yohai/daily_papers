## Daily Papers (2024-09-03)

### [VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters](https://arxiv.org/abs/2408.17253)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.17253.png)

Vote: 22

Authors: Chenghao Liu, Jianling Sun, Lefei Shen, Mouxiang Chen, Zhuo Li, Xiaoyun Joy Wang

- **What's New**: 최근 몇 년간 기초 모델(Foundation models)은 자연어 처리(NLP)와 컴퓨터 비전(CV)에서 큰 혁신을 이루었습니다. 이러한 기초 모델을 통해 다양한 다운스트림 태스크에서 몇 번의 샷 또는 제로 샷(few-shot and zero-shot) 성능을 나타내고 있습니다. 이러한 움직임은 전통적인 하나의 데이터셋에 하나의 모델을 사용하는 프레임워크에서 벗어나, 하나의 미리 학습된 모델로 보편적인 예측을 수행하는 방향으로의 패러다임 전환을 촉진하고 있습니다.
- **Technical Details**: 이 논문에서는 세 번째 아직 많이 탐구되지 않은 가능성 있는 접근 방식을 조사합니다: 미리 학습된 시각 모델을 사용하여 시계열 예측(TSF) 기초 모델을 구축하는 것입니다. 자연 이미지와 시계열 데이터(TS)는 비슷한 연속성, 관찰의 유사성, 정보 밀도에서의 유사성을 공유합니다. 우리는 이 유사성을 활용하여 시각적 마스크드 오토인코더(MAE) 모델을 사용해 TSF 태스크를 패치 수준의 이미지 재구성(image reconstruction) 태스크로 재구성하는 방식을 제안합니다. 이를 통해 제로 샷 예측을 가능하게 합니다.
- **Performance Highlights**: 우리는 제안된 VisionTS를 다양한 TSF 벤치마크에서 평가하였습니다. 그 결과, 별도의 시계열 도메인 적응 없이도 vanilla MAE는 최고 수준의 제로 샷 TSF 기초 모델을 뛰어넘는 성능을 발휘했습니다. 또한, 각 다운스트림 데이터셋에서 한 번만 모델을 미세조정(fine-tuning)할 경우 대부분의 장기 TSF 벤치마크에서 최고 수준(SOTA)의 성능을 발휘했습니다. 이러한 결과는 시계열 데이터와 자연 이미지가 동전의 양면과 같을 수 있음을 시사하며, 시각 모델이 시계열 예측을 위한 무료 점심(free lunch)일 수 있음을 보여줍니다.

### [Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming](https://arxiv.org/abs/2408.16725)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16725.png)

Vote: 14

Authors: Zhifei Xie, Changqiao Wu

- **What's New**: 최근 대형 언어 모델 (Large Language Models)의 발전이 가속화되고 있으며, GPT-4와 같은 강력한 모델들이 등장하고 있습니다. 그러나 현실에서 사용되기 위해 중요한 실시간 음성 상호작용 능력이 부족합니다. 이 문제를 해결하기 위해, 처음으로 오픈소스 멀티모델 대형 언어 모델인 Mini-Omni가 제안되었습니다. Mini-Omni는 텍스트, 오디오, 비디오를 동시에 이해하고 실시간 대화 기능을 제공합니다. 주요 기여는 텍스트 기반 훈련 데이터를 사용하여 최소한의 데이터로 실시간 음성 출력을 가능하게 하는 혁신적인 방법론을 도입한 것입니다.
- **Technical Details**: Mini-Omni 모델은 오디오 입력과 오디오 스트리밍 출력을 완전한 엔드투엔드 방식으로 처리하며, 이에 사용된 방법론은 다음과 같습니다: 1) 말뭉치를 오디오 토큰으로 변환하는 기존의 방법을 응용해 모델의 복잡성을 최소화. 2) 오디오 추론을 단순하게 하기 위해 고품질 오디오 인코더인 SNAC을 선택. 3) 텍스트와 오디오 토큰을 병렬로 생성하는 혁신적인 텍스트 지시 병렬 생성 방법 도입. 4) 최소한의 추가 데이터만으로도 음성 출력 기능을 확장할 수 있는 "Any Model Can Talk" 접근법 제안.
- **Performance Highlights**: Mini-Omni는 전통적인 텍스트-음성 멀티모달 태스크, 예를 들어 텍스트 기반 질문 응답(textQA), 자동 음성 인식(ASR), 텍스트-음성 응답(TTS) 및 음성 기반 질문 응답(speechQA)에서 탁월한 성능을 보였습니다. "batch parallel inference" 방법을 통해 음성 출력의 품질을 보장하면서도 모델의 원래 기능을 유지하는 데 성공했습니다. 또한, 기존 오픈소스 QA 데이터셋의 한계를 극복하기 위해 GPT-4o를 사용해 40만 개 이상의 항목으로 구성된 VoiceAssistant-400K 데이터셋을 소개했습니다.

