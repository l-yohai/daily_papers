## Daily Papers (2024-09-23)

### [Imagine yourself: Tuning-Free Personalized Image Generation](https://arxiv.org/abs/2409.13346)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13346.png)

Vote: 41

Authors: Harihar Subramanyam, Alireza Zareian, Bo Sun, Ankit Jain, Peter Vajda, Haoyu Ma, Animesh Sinha, Peizhao Zhang, Roshan Sumbaly, Zecheng He, Vincent Cheung, Siddharth Shah, Li Chen, Ankit Ramchandani, Felix Juefei-Xu, Anmol Kalia, Ning Zhang

- **What's New**: 새로운 개인화 이미지 생성 모델 'Imagine yourself'를 소개합니다. 기존 모델과 다르게, 사용자별 파인튜닝(finetuning) 없이 모든 사용자가 하나의 모델을 공유할 수 있는 튜닝 프리(tuning-free) 접근 방식을 채택했습니다. 참조 이미지에서 비전 임베딩(vision embedding)을 추출하여 디퓨전 프로세스(diffusion process)에 주입하는 기존 방향과 달리, 다양한 주요 구성 요소를 포함한 혁신적인 방법론을 도입했습니다.
- **Technical Details**: Imagine yourself은 텍스트 프롬프트에 의해 맞춤화된 이미지를 생성합니다. 핵심 요소는 다음과 같습니다: 아이덴티티 보존(identity preservation)을 위한 학습 가능한 비전 인코더(trainable vision encoder)와 마스크된 비전 임베딩(masked vision embedding), 시각적 품질을 향상시키기 위한 다단계 파인튜닝(multi-stage finetune)과 인간 참여 루프(HITL), 텍스트 정렬(text alignment)을 위한 합성 데이터(synthetic data)와 병렬 어텐션(parallel attention)입니다.
- **Performance Highlights**: 대규모 인간 평가를 통해 Imagine yourself는 기존의 SOTA(personalization models)를 크게 능가하며, 아이덴티티 보존, 시각적 품질, 텍스트 정렬 등 모든 측면에서 우수한 성능을 보였습니다. 특히 복잡한 프롬프트에서 텍스트 정렬에서 +27.8%의 향상을 이루었습니다.

### [YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models](https://arxiv.org/abs/2409.13592)

![](/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg)

Vote: 22

Authors: Ashish Patwa, Millon Madhur Das, Abhilash Nandy, Ankit Raj, Niloy Ganguly, Aman Bansal, Yash Agarwal, Pawan Goyal

- **What's New**: 이 논문에서는 기존의 Vision-Language (VL) 모델들이 이미지 속의 풍자를 해석할 수 있는지를 연구합니다. 이를 위해 세 가지 벤치마킹 태스크를 제안합니다: 풍자 이미지 탐지, 풍자 이미지 이해, 풍자 이미지 완성. 각 태스크는 이미지 인식과 언어 이해를 넘어, 상황의 아이러니를 이해하는 능력을 요구합니다.
- **Technical Details**: 본 연구에서는 풍자 이미지 탐지, 풍자 이미지 이해, 풍자 이미지 완성이라는 세 가지 태스크를 통해 VL 모델을 평가합니다. 이를 위해 YesBut라는 고품질 멀티모달 데이터셋을 수집했으며, 1,084개의 풍자 이미지와 1,463개의 비풍자 이미지가 포함됩니다. 각 이미지에는 같은 또는 다른 예술 스타일의 2개의 서브 이미지가 포함되어 있으며, 풍자 이미지는 하나의 시나리오를 묘사하고 다른 하나는 첫 번째 시나리오와 모순되거나 비꼬는 요소를 포함합니다.
- **Performance Highlights**: VL 모델들은 풍자 이미지 탐지 태스크에서 특히 어려움을 겪었습니다. Gemini 모델이 풍자 이미지 이해와 완성 태스크에서 가장 좋은 성능을 보였지만, 여전히 개선의 여지가 큽니다. 추가로 119개의 실제 다양한 풍자 사진 세트를 공개하며, SOTA VL 모델들이 실제 사진에서도 잘 작동하지 않는다는 결론을 내렸습니다.

### [Prithvi WxC: Foundation Model for Weather and Climate](https://arxiv.org/abs/2409.13598)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13598.png)

Vote: 16

Authors: Daniel Salles Civitarese, Will Trojak, Daniela Szwarcman, Rohit Lal, Aditi Sheshadri, Jorge Luis Guevara Diaz, Julian Kuehnert, Sujit Roy, Rajat Shinde, Vishal Gaur, Arlindo Da Silva, Anne Jones, Shraddha Singh, Kumar Ankur, Romeo Kienzler, Amy Lin, Udaysankar Nair, Simon Pfreundschuh, Christopher E Phillips, Johannes Jakubik, Johannes Schmude, Aman Gupta, +

- **What's New**: 새로운 연구에 따르면, Prithvi WxC라는 대규모 AI 모델이 도입되었습니다. 이 모델은 기상 및 기후 예측 분야에서 사용되며, 160개의 대기 변수를 활용해 MERRA-2 데이터 세트를 기반으로 훈련되었습니다. 이는 전통적인 수치예보법보다 계산 비용이 적게 들면서도 높은 정확도를 자랑합니다.
- **Technical Details**: Prithvi WxC는 여러 최신 transformer 아키텍처의 아이디어를 결합한 transformer 기반 딥러닝 아키텍처입니다. 이 모델은 입력 데이터의 지역 및 글로벌 의존성을 효과적으로 처리하며, 더 긴 시퀀스를 효율적으로 처리할 수 있습니다. 재분석 데이터와 관측 데이터를 조립하고, 손실 함수를 최적화하여 기후 예측의 다양한 하위 작업에 적용될 수 있습니다.
- **Performance Highlights**: Prithvi WxC는 제로 샷 평가를 통해 재구성과 예보에 대한 유효성을 확인받았으며, 폭풍 경로 예측, 대기 중력파 플럭스 파라미터화 등의 하위 작업에서도 우수한 성능을 보였습니다. 또한, 지형, 육지 비율 등의 정적 데이터를 활용해 다양한 입력과 목표를 처리할 수 있는 능력을 갖추고 있습니다.

### [MuCodec: Ultra Low-Bitrate Music Codec](https://arxiv.org/abs/2409.13216)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13216.png)

Vote: 14

Authors: Rongzhi Gu, Yaoxun Xu, Zhiwei Lin, Hangting Chen, Shun Lei, Wei Tan, Jianwei Yu, Zhiyong Wu

- **What's New**: 이번 연구는 음악 코덱에 새로운 접근 방식인 MuCodec을 도입하여 음악 압축 및 재구축 작업에서 초저비트레이트의 최저 비트레이트를 달성하면서도 최고 품질의 음악 재구축 기능을 유지하고 있습니다.
- **Technical Details**: MuCodec은 두 가지 주요 측면인 보컬과 배경 음악에 집중하여 MuEncoder라는 특수 피처 추출기를 사용합니다. MuEncoder에서 추출한 피처를 RVQ (Residual Vector Quantization)를 사용하여 이산화한 후, 이를 조건으로 하여 Mel-VAE 피처를 flow-matching 방식을 통해 재구축합니다. Mel 스펙트로그램은 사전학습된 Mel-VAE 디코더를 통해 재구축되며, 최종적으로 HiFi-GAN을 사용하여 음악을 재구축합니다.
- **Performance Highlights**: MuCodec은 주관적 및 객관적 실험 모두에서 저비트레이트 및 고비트레이트 음악 재구축 작업에서 현재까지 최고의 성능을 달성합니다. DAC+GAN 또는 MuEncoder+GAN 방법이 저비트레이트 음악 재구축 작업에서 낮은 성능을 보인 반면, MuCodec은 이를 능가합니다. 실험은 주요 평가 지표로 ViSQOL과 Word Error Rate (WER)를 사용하였습니다.

### [Colorful Diffuse Intrinsic Image Decomposition in the Wild](https://arxiv.org/abs/2409.13690)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13690.png)

Vote: 8

Authors: Chris Careaga, Yağız Aksoy

- **What's New**: 이 논문에서는 기존의 본질적 잔여(intrinsic residual) 모델을 사용하여 자연 사진에서 반사율과 조명 효과를 분해하는 새로운 방법을 소개합니다. 이 방법은 본질적 확산 모델을 사용하는 분해에서 시작하여 단일 색상 음영 및 Lambertian 세계 가정의 제거를 통해 높은 해상도의 확산 반사율과 다채로운 확산 음영을 추정합니다.
- **Technical Details**: 본 연구는 본질적 확산 (intrinsic diffuse) 모델의 한계를 극복하기 위해 설계되었습니다. 기본 모델은 I = A ∗ S (I는 선형 RGB 입력 이미지, A는 3채널 알베도, S는 단일 채널 그레이스케일 음영)로 정의됩니다. 연구진은 이 모델에서 나아가 본질적 잔여 모델, I = A ∗ S + R (R은 비확산 조명 효과를 나타내는 추가 컴포넌트),을 사용하여 현실적인 장면의 복잡한 상호작용을 보다 정확히 모델링하려 했습니다. 이는 픽셀 당 알 수 없는 변수의 수를 4에서 9로 늘려라는 복잡성을 증가시킵니다.
- **Performance Highlights**: 연구진은 제안된 방법이 다양한 자연 장면에서 복잡한 조명 효과를 성공적으로 분해할 수 있음을 보여줬습니다. 이 방법은 퍼픽셀 화이트 밸런싱 및 반사광 제거와 같은 여러 조명 인식 이미지 편집 응용 프로그램에서 뛰어난 성능을 입증하였습니다. 또한, 여러 공통 벤치마크와 자연 장면에서 정성적, 정량적으로 평가된 결과에서도 좋은 성능을 보였습니다.

### [Portrait Video Editing Empowered by Multimodal Generative Priors](https://arxiv.org/abs/2409.13591)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13591.png)

Vote: 8

Authors: Xuan Gao, Shimin Hu, Chenglai Zhong, Haiyao Xiao, Juyong Zhang, Yudong Guo

- **What's New**: 본 논문에서는 3D 포트레이트 비디오 편집 시스템인 PortraitGen을 소개합니다. 이 시스템은 3D 인식을 도입하여 기존 2D 도메인에 국한된 문제를 해결하고, 더 풍부한 3D 정보를 제공함으로써 고품질의 편집을 가능하게 합니다.
- **Technical Details**: 논문은 3D Gaussian Splatting(3DGS) 기법을 사용하여 일관되고 효율적인 렌더링을 수행합니다. SMPL-X(Skinned Multi-Person Linear Model)를 표면에 적용하여 구조적 및 시간적 일관성을 보장합니다. 또한, 학습 가능한 피처 맵을 사용하는 Neural Gaussian Texture 메커니즘을 도입하여 복잡한 스타일의 편집에도 유연하게 대응할 수 있습니다.
- **Performance Highlights**: 실험 결과, PortraitGen은 기존 방법들보다 품질, 효율성, 시간적 일관성 면에서 우수한 성능을 보였습니다. 텍스트 기반 및 이미지 기반 편집, 그리고 채광 변경과 같은 다양한 응용에도 효과적임이 입증되었습니다.

### [V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians](https://arxiv.org/abs/2409.13648)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13648.png)

Vote: 6

Authors: Zhirui Zhang, Kaixin Yao, Liao Wang, Siyuan Xie, Penghao Wang, Lan Xu, Minye Wu, Jingyi Yu

- **What's New**: 신규 연구 'V3'는 동적 3D Gaussian Splatting(3DGS)의 스트리밍 및 렌더링을 모바일 장치에서 가능하게 하는 혁신적인 접근법을 제안합니다. 이 방법은 고급 하드웨어 비디오 코덱을 활용해 효율적인 스트리밍과 디코딩을 지원하며, 2D Gaussian 비디오 형식을 통해 속도와 품질을 모두 최적화합니다.
- **Technical Details**: V3는 각 프레임의 Gaussian 속성을 다차원 2D 비디오 픽셀에 매핑하여, 3D Gaussian 속성(회전, 크기, 위치, 투명도, 색상 및 구면 고조파)을 각 픽셀에서 효율적으로 추출할 수 있습니다. Morton 정렬을 사용하여 3D 공간의 이웃 Gaussian 점들이 2D 표현에서도 이웃으로 유지되도록 하여 공간 일관성을 향상시킵니다. 이러한 방식으로 하드웨어 비디오 코덱과 셰이더를 사용한 실시간 렌더링이 가능합니다.
- **Performance Highlights**: V3는 컴팩트한 동적 Gaussian 표현으로, 2D Gaussian 비디오를 통해 높은 품질의 볼류메트릭 비디오(Volumetric Video) 스트리밍을 모바일 장치에서 실현합니다. 그래픽 처리에서 높은 효율성을 보장하며, 사용자들이 실시간으로 몰입감 있는 볼류메트릭 비디오를 언제 어디서나 감상할 수 있습니다.

### [Temporally Aligned Audio for Video with Autoregression](https://arxiv.org/abs/2409.13689)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13689.png)

Vote: 5

Authors: Ilpo Viertola, Vladimir Iashin, Esa Rahtu

- **What's New**: 이번 연구에서는 비디오 시퀀스에 기반하여 음질이 높은 오디오를 생성하는 새로운 방법을 소개합니다. 기존의 오토레그레시브(autoregressive) 방법을 향상시켜, 오디오와 비디오 간의 높은 시각적-청각적 일치를 달성합니다. 또한, 새로운 데이터셋인 VisualSound를 도입하여 강한 오디오-비디오 상관성을 보장합니다.
- **Technical Details**: 이 모델은 pretrained audio codec을 사용하여 웨이브폼(waveform)을 디스크리트 토큰 시퀀스로 인코딩하고 디코딩합니다. 입력된 비디오로부터 세밀한 시각적, 모션 특징을 추출하고, 이를 고주파수 비율로 업샘플링하여 오디오와 일치시키며, 자연스러운 오디오-비디오 동시 발생(cross-modal feature fusion)을 강화합니다. 세그먼트 AVCLIP(segment AVCLIP)와 같은 고주파수 시각 특성 추출기를 사용하여 보다 세밀한 시각적 이벤트를 포착합니다.
- **Performance Highlights**: 기존의 diffusion 및 rectified flow matching (RFM) 모델과 비교할 때, 이 연구의 오토레그레시브 접근 방식은 오디오와 비디오 간의 높은 일치성을 유지하며 학습 시간이 단축됩니다. 더불어, 시각적 특징과 청각적 토큰을 시간적으로 정렬함으로써 생성된 오디오의 시간적 정렬도가 향상되었습니다. VisualSound 데이터셋을 사용하여 유의미한 오디오-비디오 상관성을 보장하며, 훈련시간과 데이터의 잡음을 줄였습니다.

### [Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments](https://arxiv.org/abs/2409.11276)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11276.png)

Vote: 4

Authors: Sebastian Garcia, Carlos Catania, Maria Rigaki

- **What's New**: 본 연구는 NetSecGame 환경에서 작동하는 레드팀 에이전트로 사용하기 위해 70억 개의 파라미터를 가진 사전 학습된 LLM(Zephyr-7b-β)을 미세 조정하였습니다. 새로운 데이터셋을 생성하여 모델의 환경 이해도를 향상시키고 유효한 행동을 제안하도록 하였습니다. 이 에이전트는 다양한 복잡도의 세 가지 시나리오에서 평가되었으며, 상업용 사전 학습 LLM 및 Q-learning과 같은 RL 기준들과 비교되었습니다.
- **Technical Details**: 넷세크게임(NetSecGame) 환경에서의 에이전트 설계를 위해 Supervised Fine-Tuning 방식이 사용되었으며, Hyper-Parameter Tuning 과정과 데이터셋 생성 방법에 대해 상세하게 설명합니다. 이 연구는 또한 각 에이전트(GPT-4, Zephyr-7b-β, Hackphyr)의 행동을 분석하여 인간과 유사한 계획 능력을 평가합니다.
- **Performance Highlights**: 논문은 세 가지 주요 공헌을 합니다: 첫째, 넷세크게임 환경에서 동작하는 Hackphyr라는 로컬 모델을 만들었습니다. 둘째, 데이터셋 구성 요소가 모델 성능에 미치는 영향을 분석하는 어블레이션 연구를 통해 유의미한 인사이트를 제공합니다. 셋째, 세 가지 LLM 기반 에이전트의 행동 분석을 통해 네트워크 보안 문제를 자율적으로 해결하는 능력을 평가합니다.

### [Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation](https://arxiv.org/abs/2409.12941)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.12941.png)

Vote: 3

Authors: Manaal Faruqui, Satyapriya Krishna, Kalpesh Krishna, Steven Schwarcz, Shyam Upadhyay, Anhad Mohananey, Adam Stambler

- **What's New**: 최근 대형 언어 모델(LLMs)의 발전으로 복잡한 쿼리에 대한 사실 정확성과 정교한 추론이 요구되는 다양한 자연어 처리 작업에서 성능이 크게 향상되었습니다. 이러한 시스템의 성능을 평가하기 위해 우리는 새로운 평가 프레임워크인 FRAMES(Factuality, Retrieval, And reasoning MEasurement Set)를 소개합니다. 이는 LLM이 다중 문서의 정보 검색, 복잡한 제약 조건 간의 추론, 정확한 정보 종합 능력을 엄격하게 테스트하는 것을 목표로 합니다.
- **Technical Details**: FRAMES는 824개의 테스트 샘플로 구성된 데이터 세트로, RAG(Retrieval Augmented Generation) 시스템의 전체적인 성능을 평가합니다. 데이터 수집 과정은 합성 데이터 생성 시도와 인적 주석 작업을 포함했습니다. 합성 데이터 생성 시, 모델이 고도로 도전적인 질문을 생성하지만, 헛소리(hallucinated) 답변의 비율이 높아 인적 주석이 필수적임을 확인했습니다. 인적 주석자는 Wikipedia 기사를 참고하여 복잡한 질문을 만들고 이를 기반으로 다중 제약 추론, 수치 추론, 시간 추론 등의 다섯 가지 유형의 추론을 라벨링했습니다.
- **Performance Highlights**: 주요 성과로는 다음과 같습니다. 첫째, 다중 문서에서 정보를 검색하고 추론하는 LLM의 성능을 종합 평가하는 FRAMES 데이터 세트를 소개했습니다. 둘째, 현재 최첨단 LLM의 사실성, 검색, 추론 작업에 대한 종합적 평가를 제공하여 그들의 강점과 한계를 파악했습니다. 셋째, 다중 단계 검색 및 추론 프레임워크를 제안하여 복잡한 쿼리에 대한 모델 성능을 크게 향상시켰습니다. 이러한 발견은 더욱 강력하고 효율적인 RAG 시스템의 연구 개발로 이어질 것입니다.

### [Minstrel: Structural Prompt Generation with Multi-Agents Coordination for Non-AI Experts](https://arxiv.org/abs/2409.13449)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13449.png)

Vote: 3

Authors: Daling Wang, Sijia Shen, Chaofeng Guan, Ming Wang, Shi Feng, Yijie Huang, Yifei Zhang, Yuanzhong Liu, Xiaocui Yang, Xiaoming Zhang, Xiaoyu Liang

- **What's New**: 연구자들은 LangGPT라는 구조적 프롬프트 설계 프레임워크와 Minstrel라는 다중 에이전트 시스템을 통해 자동화된 프롬프트 생성을 위한 도구를 제안했습니다. LangGPT는 자연어의 유연성과 프로그래밍 언어의 체계성과 재사용성을 결합하여 제안되었으며, Minstrel은 다중 에이전트의 협업을 통해 LangGPT 프롬프트를 자동으로 생성합니다.
- **Technical Details**: LangGPT는 모듈과 요소로 구성된 이중 레이어 구조를 가지고 있습니다. 모듈은 클래스와 유사하게 LLM의 요구 사항을 나타내며, 요소는 모듈 내에서 특정 콘텐츠를 정의합니다. Minstrel은 다중 에이전트 시스템으로 분석 그룹, 설계 그룹, 테스트 그룹으로 구성되어 있으며, 이 그룹들은 사용자 요구에 맞는 프롬프트를 설계하고 테스트 및 최적화합니다. 연구자들은 이 구조적 프롬프트가 LLM을 더 효과적으로 이끌 수 있음을 실험과 사례 연구를 통해 증명했습니다.
- **Performance Highlights**: 실험 결과, Minstrel이 생성한 구조적 프롬프트는 기존의 베이스라인 프롬프트보다 LLM 부트스트래핑에 더 효과적이었으며, 사용자 설문조사에서도 높은 사용 편의성과 만족도가 확인되었습니다. 또한, 일반 사용자가 설계한 프롬프트도 높은 성능을 보였습니다. 이 연구는 프롬프트 설계의 일반화와 재사용성을 높여 학습 비용을 줄이는 데 기여합니다.

### [LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents](https://arxiv.org/abs/2409.11393)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11393.png)

Vote: -

Authors: Hana Chaari, Amine B. Hassouna, Ines Belhaj

- **What's New**: 이번 연구는 LLM 기반 에이전트의 통합 모델링 프레임워크(LLM-Agent-UMF)를 소개한다. 이 프레임워크는 LLM과 같은 대형 언어 모델이 갖추지 못한 외부 지식 접근, 정보 검색, 수학적 추론, 코드 평가 등의 기능을 보완할 수 있도록 설계되었다. 특히, 이 논문은 기존의 에이전트 설계가 직면한 소프트웨어 아키텍처의 복잡성과 비모듈성 문제를 해결하고자 한다.
- **Technical Details**: LLM 기반 에이전트는 주로 언어 모델링, 텍스트 생성, 질문 응답, 감정 분석, 자연어 이해 및 상식 추론에서 뛰어난 성능을 보인다. 그러나 외부 도구와의 인터페이스, 안전성 관리, 메모리 관리 등을 위한 추가 컴포넌트가 필요하다. 본 연구는 이러한 컴포넌트들의 구체적인 구조와 상호작용을 정의하는 LLM-Agent-UMF를 통해 모듈성을 향상시키고, 여러 에이전트 간의 통합 및 재사용성을 높인다.
- **Performance Highlights**: 프레임워크의 성능 평가 결과, 새로운 코어 에이전트(core-agent) 개념 도입으로 모듈의 명확성 및 상호작용이 크게 개선되었다. 또한, 보안 모듈을 포함해 전체 시스템의 안정성과 신뢰성을 높였다. 프레임워크를 기존 솔루션에 적용하여 모듈 식별과 통합 가능성을 평가했으며, 여러 최적화 결과를 통해 다양한 에이전트 구조의 효율성을 검증하였다.

