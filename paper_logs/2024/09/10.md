## Daily Papers (2024-09-10)

### [Towards a Unified View of Preference Learning for Large Language Models: A Survey](https://arxiv.org/abs/2409.02795)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.02795.png)

Vote: 48

Authors: +, Zhifang Sui, Peiyi Wang, Helan Hu, Zeyu Cui, Bowen Yu, Zefan Cai, Liang Chen, Feifan Song, Dayiheng Liu, Daoguang Zan, Ce Zheng, Runxin Xu, Yibo Miao, Qingxiu Dong, Lei Sha, Bofei Gao, Houfeng Wang, Ge Zhang, Zhe Yang, Jian Yang, Wen Xiao, Keming Lu

- **What's New**: ChatGPT111이라는 대형 언어 모델(LLM)의 도입으로 인해 향상된 언어 능력과 전문성이 돋보이는 반면, 사용자에게 공개되기 전에 필요로 하는 '선호도 정렬(preference alignment)' 단계를 거쳐야만 한다는 새로운 인식이 확산되고 있습니다. 이는 모델이 잠재적으로 공격적이거나, 유해하거나, 오해의 소지가 있는 콘텐츠를 생성하지 않도록 하기 위함입니다.
- **Technical Details**: 전통적인 기법들은 강화 학습(RL) 기반의 방법과 감독 학습 기반의 방법으로 나눠져, RLHF(강화 학습을 통한 인간 피드백)와 같은 온라인 강화 학습을 수행하거나 DPO(직접 선호도 최적화)와 같은 오프라인 환경에서 선호도 최적화를 수행합니다. 그러나 이러한 이분법은 연구자들이 선호도 정렬의 핵심을 이해하는 데 방해가 될 수 있습니다. 이 부족함을 보완하기 위해, 우리는 전체적인 이해를 위한 체계적인 프레임워크를 제안하였습니다.
- **Performance Highlights**: 선호도 학습(preference learning)의 다양한 알고리즘 사이의 관계를 통합하며, 연구자들이 특정 분야에서의 추가 탐구를 위한 종합적인 이해와 기초를 제공하고자 합니다. 이를 통해 선호도 정렬의 코어 개념을 명확히 하여, 기존 방법론들이 어떻게 통합될 수 있는지를 설명하려는 시도를 하고 있습니다.

### [MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct](https://arxiv.org/abs/2409.05840)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.05840.png)

Vote: 32

Authors: Heng Tao Shen, Lianli Gao, Fei Huang, Run Luo, Longze Chen, Yuchuan Wu, Yongbin Li, Minzheng Wang, Min Yang, Xiong Liu, Yunshui Li, Xiaobo Xia, Jingkuan Song, Ting-En Lin, Pengpeng Zeng, Haonan Zhang

- **What's New**: MMEvol은 다수의 진화적인 MLLM(iterative evolutions of advanced Multimodal Large Language Models)을 활용하여 다양한 난이도의 오픈 도메인 지시문(open-domain instructions)을 대규모로 자동 생성하는 새로운 프레임워크입니다. 이를 통해 MLLMs의 성능을 크게 향상시키고자 합니다.
- **Technical Details**: MMEvol은 LLaVA-Instruct와 ShareGPT4V 데이터셋을 이용해 시드 지시문(seed instructions) 데이터 163K 샘플을 준비한 후, OpenAI GPT-4o mini API를 사용하여 세 번의 지시문 진화를 수행하여 447K 진화된 샘플을 얻습니다. 세 가지 주요 진화 방향인 세밀한 지각 진화(fine-grained perceptual evolution), 인지적 추론 진화(cognitive reasoning evolution), 상호작용 진화(interactive evolution)를 통해 다양한 형태와 난이도의 지시문을 생성합니다. 이는 시각적 콘텐츠와 관련된 지시 데이터를 생성하여 시각적 환상을 줄이고 지시문의 복잡성과 다양성을 높입니다.
- **Performance Highlights**: 진화된 데이터로 학습된 LLaVA-NeXT 모델은 다양한 비주얼-언어 벤치마크에서 우수한 성능(state-of-the-art)을 달성했습니다. 제안된 방법의 유효성과 효율성은 광범위한 정성적 및 정량적 분석을 통해 검증되었습니다. 각 구성 요소의 기여도는 철저한 분석을 통해 입증되었습니다.

### [OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs](https://arxiv.org/abs/2409.05152)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.05152.png)

Vote: 20

Authors: Jintian Zhang, Jun Zhou, Mengshu Sun, Xiang Chen, Huajun Chen, Ningyu Zhang, Lei Liang, Zhiqiang Zhang, Cheng Peng

- **What's New**: 이 논문은 새로운 신경망 모델을 제안하며, 자율 주행(AI-driven autonomous driving) 기술의 성능을 크게 향상시킬 수 있는 새로운 방법론을 소개합니다. 연구자들은 이를 통해 데이터 처리 및 판단 능력을 높였습니다.
- **Technical Details**: 제안된 모델은 Transformer 구조를 기반으로 하며, 기존의 CNN(convolutional neural networks) 및 RNN(recurrent neural networks)보다 향상된 성능을 보입니다. 특히 다중 센서 데이터를 통합하여 처리하는 데 강점이 있습니다. 이 모델은 Attention 메커니즘을 활용하여 중요한 정보에 집중하며, 이를 통해 더 높은 정확도를 달성합니다.
- **Performance Highlights**: 실험 결과 제안된 모델은 기존 자율 주행 시스템 대비 15% 향상된 정확도를 보여줬습니다. 또한, 반응 시간도 대폭 단축됨을 확인하였습니다. 모델의 효율성은 복잡한 도시 환경에서도 높은 성능을 유지한다는 점에서 특히 주목할 만합니다.

### [MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery](https://arxiv.org/abs/2409.05591)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.05591.png)

Vote: 12

Authors: Peitian Zhang, Kelong Mao, Zhicheng Dou, Zheng Liu, Hongjin Qian

- **What's New**: MemoRAG라는 새로운 프레임워크를 소개합니다. 이 프레임워크는 데이터베이스에서 관련 지식을 검색하여 작업을 연결하는 스마트 인터페이스를 도입합니다. 사용자가 제시한 작업에 대해, MemoRAG는 기억 모듈을 사용하여 검색 단서를 생성합니다.
- **Technical Details**: MemoRAG는 이중 시스템 아키텍처를 채택하고 있습니다. 가벼운 LLM(light LLM)은 메모리 역할을 하며, 무거운 LLM(heavy LLM)은 검색 강화 생성(retrieval-augmented generation)을 수행합니다. 가벼운 LLM은 비용 효율적이며 긴 컨텍스트를 처리할 수 있어, 제한된 계산 예산 내에서 전체 데이터베이스를 수용할 수 있습니다. 또한, 메모리를 미세 조정하여 생성된 단서들이 최적의 검색 품질을 달성할 수 있도록 합니다.
- **Performance Highlights**: MemoRAG는 복잡한 검색 강화 생성 작업들에서도 우수한 성능을 발휘합니다. UltraDomain이라는 벤치마크를 개발하여 평가를 실시했으며, MemoRAG는 기존의 RAG 시스템들이 어려움을 겪는 고난이도 작업에서도 높은 품질의 답변을 생성했습니다. 전통적인 질문-응답(task)에서 MemoRAG의 단서 기반 검색 접근 방식은 여전히 큰 장점을 제공합니다.
- **Implementational Details**: MemoRAG 시스템은 다양한 GPU에서 배포될 수 있습니다. 예를 들어, NVIDIA T4 16GiB로는 최대 68K 토큰의 컨텍스트 길이를 처리할 수 있으며, NVIDIA A100 80GiB로는 최대 백만 토큰까지 처리 가능합니다. 현재 두 개의 메모리 모델, memorag-qwen2-7b-inst와 memorag-mistral-7b-inst가 제공되며, 향후 릴리스에서는 쉽게 사용할 수 있는 인터페이스와 유연한 검색 방법을 제공할 계획입니다.

### [Benchmarking Chinese Knowledge Rectification in Large Language Models](https://arxiv.org/abs/2409.05806)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.05806.png)

Vote: 11

Authors: Tianhe Lu, Jizhan Fang, Yunzhi Yao, Xin Xu, Huajun Chen, Ningyu Zhang

- **What's New**: 이번 연구에서는 LLMs(대형 언어 모델)의 중국어 관련 지식을 수정하여 더 정확하고 신뢰성 있는 콘텐츠를 생성하는 방법을 제안합니다. 특히 새로운 중국어 데이터셋인 CKnowEdit을 개발하여 언어 특유의 문학적 및 문화적 특성을 반영하였습니다.
- **Technical Details**: 기존의 지식 수정 연구는 주로 영어 중심의 문헌을 대상으로 했으며, Wikipedia와 같은 데이터에서 파생된 내용을 사용해왔습니다. 이번 연구에서는 중국어를 대상으로 하여, 중국의 고전 문학, 성어, 속담, 지리 지식 등 총 7가지의 중국어 특화 지식을 포함한 데이터셋을 수집하고 주석을 달았습니다. 이를 통해 모델 예측의 오류를 식별하고, 수동으로 정보의 정확성과 관련성을 확인하여 모델의 지식 베이스를 업데이트했습니다.
- **Performance Highlights**: 새로운 데이터셋 CKnowEdit을 통해 지식 수정 방법의 효능을 평가한 결과, 현존하는 모델들이 중국어와 중국 문화에 대한 이해에서 많은 어려움을 겪고 있다는 점을 밝혀냈습니다. 특히, 단어 수준의 중복과 의미적 벡터 유사성을 사용하여 편집된 모델의 성능을 평가했습니다. 이를 통해 보다 정교한 중국어 지식 수정 접근법의 필요성을 강조했습니다.

### [Paper Copilot: A Self-Evolving and Efficient LLM System for Personalized Academic Assistance](https://arxiv.org/abs/2409.04593)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.04593.png)

Vote: 10

Authors: Guanyu Lin, Tao Feng, Jiaxuan You, Ge Liu, Pengrui Han

- **What's New**: 과학 연구가 전례 없는 속도로 확산됨에 따라 연구자들은 방대한 양의 출판 및 사전 출판 논문을 탐색하고 해석해야 합니다. Paper Copilot은 Retrieval-augmented generation (RAG) 및 Large Language Models (LLMs)을 기반으로 하여 연구자들이 최신 논문에서 유용한 정보를 추출하고 요약할 수 있도록 돕습니다.
- **Technical Details**: Paper Copilot은 사용자 프로필을 기반으로 개인 맞춤형 연구 서비스를 제공합니다. 이는 사용자의 역사적 출판물에서 프로필을 도출하고 최신 연구 주제를 분석하여 아이디어를 제공하며, 연구 채팅 및 조언 서비스를 제공합니다. 논문 데이터베이스는 최신 Arxiv 논문으로 매일 업데이트되며 사용자는 원하는 날짜 범위를 선택하여 논문을 검색할 수 있습니다. 생각 검색(Thought Retrieval) 방법을 사용해 사용자의 과거 질문에 따라 자가 진화(Self-Evolution) 기능을 합니다. 실시간 기능 사전 계산, 다중 스레드 엔진, 빈번한 쿼리 캐싱을 통해 효율적인 배포를 달성했습니다.
- **Performance Highlights**: Paper Copilot은 69.92%의 시간 비용 절감을 통해 효율성을 대폭 향상시켰습니다. 사용자 피드백에 따르면, Paper Copilot은 동일한 양의 정보를 얻는 데 최소 20분을 절약할 수 있습니다. 양적 및 질적 평가를 통해 높은 효율성과 우수한 사용자 경험을 입증했습니다.

### [POINTS: Improving Your Vision-language Model with Affordable Strategies](https://arxiv.org/abs/2409.04828)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.04828.png)

Vote: 7

Authors: Jie Zhou, Le Tian, Yuan Liu, Zhongyin Zhao, Xiao Zhou, Ziyuan Zhuang

- **What's New**: 이번 연구에서는 고전 LLaVA(Liu et al., 2024b)의 최신 발전과 기술을 통합하여, 새로운 강력한 베이스라인을 설정하였습니다. 기존 캡션에 세계 지식과 생성된 캡션을 결합하고, 'Consistent Aspect Ratio Dynamic High Resolution'이라는 새로운 이미지 분할 전략을 도입하여 모델 성능을 향상시켰습니다. 또한, 모델 성능을 극대화하기 위해 'model soup' 기법을 도입하여 다양한 데이터셋으로 미세 조정된 모델 가중치를 병합했습니다.
- **Technical Details**: 본 연구는 세부적으로 세 가지 주요 기술적 혁신을 통합하고 있습니다. 첫째, 불필요한 이미지 왜곡 문제를 해결하기 위해, 고해상도 이미지를 일관된 비율로 유지하면서 분할하는 'Consistent Aspect Ratio Dynamic High Resolution' 방법을 제안했습니다. 둘째, 미리 학습된 데이터셋을 'Perplexity'를 이용해 필터링하여 1백만 개의 데이터 샘플을 선별, 그 샘플을 기반으로 모델을 사전 학습했습니다. 셋째, 'model soup' 기법을 활용해 다양한 데이터셋으로 미세 조정된 모델 가중치를 병합함으로써, 데이터셋 선택이 추가적인 개선을 가져오지 않을 때도 성능을 향상시켰습니다.
- **Performance Highlights**: 새롭게 제안된 POINTS 모델은, 제안된 필터링 및 모델 병합 기법을 통해 기존 모델보다 뛰어난 성능을 보였습니다. 특히, 필터링된 1백만 개 샘플로 학습한 모델이 5배 더 큰 데이터셋을 사용하여 학습한 모델보다 우수한 성능을 보였습니다. 또한, 'model soup' 기법을 적용함으로써 추가적인 데이터셋 선택 없이도 모델 성능을 극대화할 수 있었습니다.

### [Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments](https://arxiv.org/abs/2409.05865)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.05865.png)

Vote: 5

Authors: Julian Mehu, Aaron Edsinger, Zijin Hu, Chris Paxton, Soumith Chintala, Seungjae Lee, Haritheja Etukuru, Lerrel Pinto, Norihito Naka, Nur Muhammad Mahi Shafiullah

- **What's New**: 새로운 논문에서는 Robot Utility Models (RUMs)을 소개합니다. RUMs은 훈련이나 미세 조정(fine-tuning) 없이 새로운 환경에서 바로 적용할 수 있는 로봇 유틸리티 모델로, 전체 시스템 접근 방식(systems-first approach)을 채택하여 데이터를 효율적으로 수집하고 모델 훈련과 배치를 최적화했습니다.
- **Technical Details**: 이 논문에서는 얼굴 인식 커뮤니티에서 개발된 '미세 조정 후 사전 훈련(pretrain-then-finetune)' 전략을 로봇 공학에 적용했습니다. RUMs은 ARKit API가 탑재된 iPhone Pro를 이용한 Stick-v2라는 휴대용 데이터 수집 도구를 사용하여 다양한 환경에서 정밀한 시연 데이터를 대량으로 수집합니다. Multi-modal 행동 학습 알고리즘과 mLLM 기반의 자기 비판 및 재시도 시스템을 사용하여 다양한 환경에서의 모델 성능을 향상시킵니다.
- **Performance Highlights**: 2,950건의 실제 환경 실험(뉴욕, 저지시티, 피츠버그) 결과, 새로운 환경에서 90% 평균 성공률을 기록했습니다. Multi-modal 정책을 사용한 경우 74.4%의 무미세 조정(zero-shot) 성공률을 나타냈으며, mLLM 기반의 자기 비판 및 재시도 시스템을 적용한 후, 성공률이 15.6% 향상되었습니다.

### [UniDet3D: Multi-dataset Indoor 3D Object Detection](https://arxiv.org/abs/2409.04234)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.04234.png)

Vote: 5

Authors: Anton Konushin, Danila Rukhovich, Maksim Kolodiazhnyi, Anna Vorontsova, Matvey Skripkin

- **What's New**: 이 논문에서는 실내 3D 객체 탐지 문제를 해결하기 위한 새로운 접근법을 제안합니다. 주요한 기여는 데이터셋 간의 일관되지 않은 레이블 설정을 해결하고 여러 데이터셋에서 공통적인 라벨 공간을 설정하는 것입니다. 또한, 서로 다른 도메인을 대표하는 데이터를 적절히 선택하고 혼합하는 방법을 제시합니다.
- **Technical Details**: 제안된 방법론은 네 가지 하위 태스크로 나눌 수 있습니다: 1) 데이터 소스 간의 계산 오버헤드를 줄이는 네트워크 아키텍처 설계, 2) 도메인을 대표하는 훈련 데이터셋의 적절한 선택 및 혼합, 3) 여러 데이터셋 간의 일관된 라벨 통합, 4) 모든 도메인에서 견고한 성능을 보장하는 다중 데이터셋 훈련 절차 설정입니다. UniDet3D 모델은 순수한 self-attention 인코더 아키텍처를 채택하며 positional encoding과 cross-attention을 사용하지 않습니다.
- **Performance Highlights**: UniDet3D의 성능은 모든 혼합된 데이터셋의 테스트 분할에서 더 높은 점수를 기록했습니다. 또한, 데이터셋 별로 최고 점수를 얻은 sparse convolutional TR3D (on S3DIS) 및 transformer-based V-DETR (on ScanNet) 방법과 비교하여도 뛰어난 성능을 보였습니다.

### [Open Language Data Initiative: Advancing Low-Resource Machine Translation for Karakalpak](https://arxiv.org/abs/2409.04269)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.04269.png)

Vote: 4

Authors: Mukhammadsaid Mamasaidov, Abror Shopulatov

- **What's New**: 이번 연구는 Karakalpak 언어의 기계 번역을 개선하기 위한 노력의 일환으로, Open Language Data Initiative (OLDI)에서 No Language Left Behind (NLLB) 모델을 미세 조정하여 Karakalpak 번역 모델을 제시합니다. 연구는 현재 지원되지 않는 저자원 언어인 Karakalpak에 대한 기계 번역 기술 개발의 필요성을 강조했습니다.
- **Technical Details**: Karakalpak은 투르크 언어 계열에 속하며, 주로 우즈베키스탄의 Karakalpakstan 자치지역에서 약 90만 명이 사용합니다. Karakalpak은 언어 처리 측면에서 어려움을 겪고 있는 저자원 언어입니다. 연구는 다음의 기여를 통해 번역 성능을 향상시켰습니다: FLORES+ devtest 데이터셋의 Karakalpak 번역, 100,000 문장 쌍의 병렬 코퍼스(우즈베크-Karakalpak, 러시아어-Karakalpak, 영어-Karakalpak), 라틴-키릴 변환 스크립트.
- **Performance Highlights**: 실험에서는 nllb-200-distilled-600M 모델을 사용하였으며, SentencePiece 토크나이저를 약 30만 개의 Karakalpak 문장을 사용하여 교육하였습니다. 이를 통해 토큰 수를 16,000으로 확장하였고, 모델의 성능을 평가하기 위해 다양한 데이터 구성과 토큰을 사용한 세 가지 모델 변형을 개발하였습니다: dilmash-raw, dilmash, dilmash-TIL. 그 결과, 토큰 및 추가 교육 데이터의 조합이 번역 성능에 긍정적인 영향을 미쳤음을 확인했습니다.

### [Insights from Benchmarking Frontier Language Models on Web App Code Generation](https://arxiv.org/abs/2409.05177)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.05177.png)

Vote: 1

Authors: Yi Cui

- **What's New**: 이번 발표에서 (Cui, 2024)는 웹 애플리케이션 코드 생성 성능을 평가하기 위한 벤치마크인 WebApp1K를 도입했습니다. 이 벤치마크를 통해 최첨단 대형 언어 모델(LLMs)의 성능을 평가하고 비교할 수 있습니다. 우리는 16개의 모델이 작성한 코드를 분석하여 그 성과에 대한 통찰을 공유합니다.
- **Technical Details**: 이 보고서는 다음과 같은 구조로 이루어져 있습니다. 2장에서는 각 모델에 대한 테스트 실패에 의해 WebApp1K의 난이도를 분석합니다. 3장에서는 코드 라인(LOC)의 분포를 분석합니다. 4장은 모델들이 만든 오류를 심층 분석합니다. 5장은 관련 작업을 소개하고, 마지막 6장은 발견한 내용을 요약하고 미래 방향을 논의합니다.
- **Performance Highlights**: [{'Benchmark Difficulty': 'WebApp1K 벤치마크의 난이도를 분석한 결과, 대다수 문제는 상대적으로 쉬운 쪽에 분포하고 있습니다. 그러나 38개의 문제는 어느 모델도 해결하지 못했습니다. 전체 160,000개의 솔루션 중에서는 0.1%만이 문법 오류를 가졌으며, Claude 3.5와 Mistral Large 2는 문법 오류가 전혀 없었습니다.'}, {'Line-of-Code (LOC) Analysis': '모든 모델이 React 프레임워크의 모듈화된 디자인을 따르고 있습니다. 모델들이 생성한 중간 LOC는 대부분 35에서 46 사이로 매우 유사합니다. 또한, 코드의 간결도와 정확성(p\u2062a\u2062s\u2062s\u2062@\u20621) 사이에는 강한 상관관계가 없음을 발견했습니다. Violin 차트에서는 높은 성능 모델의 LOC 분포가 bi-modal(쌍봉형태)을 가짐을 볼 수 있습니다.'}, {'Error Analysis': '오류 로그를 분석한 결과, 93%의 오류 로그는 하나의 오류 또는 두 개의 동일한 오류를 포함하고 있습니다. 이는 대부분의 모델들이 단일 버그로 인해 실패했다는 것을 의미합니다. 또한, 어떤 모델도 7가지 오류 유형 중 어느 하나에서도 완전히 자유롭지 않다는 결과를 얻었습니다.'}]

### [Evaluating Multiview Object Consistency in Humans and Image Models](https://arxiv.org/abs/2409.05862)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.05862.png)

Vote: 1

Authors: Tyler Bonnen, Alexei A. Efros, Nancy Kanwisher, Thomas O'Connell, Yutong Bai, Stephanie Fu, Yoni Friedman, Joshua B. Tenenbaum

- **What's New**: 논문은 컴퓨터 비전 모델이 객체의 3D 구조를 어떻게 표현하고 인식하는지를 탐구합니다. 특히, 인간의 시각 능력과 비교하여 이러한 모델이 동일한 수준의 3D 이해를 가지고 있는지를 조사합니다. 이를 위해 인지 과학 실험을 바탕으로 한 새로운 벤치마크를 구축하였습니다.
- **Technical Details**: 이 연구는 zero-shot visual inference (제로샷 시각 추론) 과제를 사용하여 모델과 인간의 3D 시각 능력을 평가합니다. 실험 디자인은 세 개의 이미지 중 두 개는 동일한 객체의 서로 다른 뷰포인트를 보여주고, 나머지 하나는 다른 객체를 포함합니다. 두 가지 주요 과제가 포함되는데, '차이점 찾기(odd-one-out)'와 '샘플 맞추기(match-to-sample)'입니다. 사용된 이미지들은 다양한 객체 타입과 인지 요구를 가지며, 네 가지 주요 데이터셋에서 가져왔습니다.
- **Performance Highlights**: 연구 결과에 따르면, 인간은 충분히 긴 시간을 주어질 때 컴퓨터 비전 모델보다 뛰어난 3D 형태 추론 능력을 보여줍니다. 그러나 이미지가 매우 짧은 시간 (<100ms) 동안만 제공될 경우, 인간과 비전 모델은 유사한 정확도를 보입니다. 이는 인간의 3D 형태 지각이 역동적이고 시간적으로 단계적인 과정임을 시사합니다. 또한, 실험 데이터를 통해 DINOv2, MAE, CLIP 등 여러 비전 모델의 성능과 인간의 성능을 다양한 척도로 비교하였습니다.

