## Daily Papers (2024-09-04)

### [Kvasir-VQA: A Text-Image Pair GI Tract Dataset](https://arxiv.org/abs/2409.01437)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.01437.png)

Vote: 54

Authors: Steven A. Hicks, Sushant Gautam, Andrea Storås, Vajira Thambawita, Cise Midoglu, Michael A. Riegler, Pål Halvorsen

- **What's New**: 의료 진단 분야에서 인공지능(AI)과 머신러닝(ML) 기술의 통합을 통해 복잡한 의료 이미지를 분석하는 것이 점점 더 중요해지고 있습니다. 본 연구에서는 Kvasir-VQA라는 새로운 데이터셋을 소개하며, 이 데이터셋은 기존의 HyperKvasir와 Kvasir-Instrument와 같은 데이터셋에 질문과 답변(Q&A) 주석을 추가하여 확장한 것입니다. 이 주석들은 AI 모델이 문맥을 더 잘 이해하고 복잡한 상황을 해석하며, 진단의 정확성을 높이는데 중요한 역할을 합니다.
- **Technical Details**: Kvasir-VQA 데이터셋은 기존 GI(Gastrointestinal) 이미지 분석 데이터셋에 대해 상세한 Q&A 주석을 포함하고 있습니다(그림 1). 이 주석들은 이미지 캡셔닝(image captioning), VQA (Visual Question Answering), 텍스트 기반 합성 이미지 생성, 객체 탐지(object detection), 그리고 분류(classification)와 같은 다양한 ML 응용 분야를 촉진시키는 것을 목표로 합니다. 데이터셋의 수집 과정과 질문 유형, 이미지 카테고리에 대한 자세한 설명이 포함되어 있습니다.
- **Performance Highlights**: 초기 실험을 통해 Kvasir-VQA 데이터셋에 기반한 이미지 캡셔닝, VQA, 합성 의료 이미지 생성 등의 유용성을 입증하였습니다. 이를 통해 데이터셋이 실제 의료 진단에서 얼마나 효과적으로 사용될 수 있는지를 강조하고 있습니다. 또한, 변형 기반 모델(transformer-based models)이나 Vision Transformer (ViT)와 같은 최신 기술을 활용하여 의료 이미지를 더 잘 이해하고 문맥적으로 정확한 설명을 생성할 수 있게 됩니다. 신경망 기반의 폴립 감지와 캡슐 내시경 분석도 포함되어 있습니다.

### [OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.02060.png)

Vote: 40

Authors: Binyuan Hui, Yuling Gu, Dustin Schwenk, Luca Soldaini, +, Sewon Min, Tim Dettmers, Niklas Muennighoff, Ali Farhadi, Alexander Wettig, Nathan Lambert, Pang Wei Koh, Kyle Lo, Jacob Morrison, Weijia Shi, Oyvind Tafjord, Akshita Bhagia, Shane Arora, Noah A. Smith, Dirk Groeneveld, Pete Walsh, David Wadden, Douwe Kiela

- **What's New**: 새로운 논문에서는 OLMoE라는 오픈 소스 Mixture-of-Experts (MoE) 언어 모델을 소개합니다. 이 모델은 5.1조 개의 토큰으로 사전 훈련되었으며, 총 6.9B(10억) 개의 파라미터 중에서 1.3B 개의 활성화된 파라미터를 사용합니다. 최첨단 성능을 가지면서도 저비용의 추론을 가능하게 합니다.
- **Technical Details**: OLMoE는 NL층의 transformer로 구성된 decoder-only LM입니다. Dense 모델의 feedforward 네트워크(FFN)를 여러 개의 전문가(ne이번ts)로 구성된 MoE 모듈로 대체하였습니다. 각 입력 토큰은 학습된 라우터 네트워크에 의해 선택된 8개의 전문가에 의해 처리되고, dropless token-based 라우팅 전략을 사용합니다. 추가로, 우리는 OLMoE 모델을 부하 분산 손실(Load balancing loss) 및 라우터 z-손실(router z-loss)를 포함하여 훈련합니다.
- **Performance Highlights**: OLMoE-1B-7B는 공개된 1B 모델보다 성능이 뛰어나며, 높은 추론 비용과 메모리 저장이 필요한 dense 모델(예: Llama2-13B)과 비슷한 성능을 보입니다. 명령어 및 선호도 조정을 통해 만든 OLMoE-1B-7B-Instruct는 큰 instruct 모델들을 능가합니다.

### [LongRecipe: Recipe for Efficient Long Context Generalization in Large Languge Models](https://arxiv.org/abs/2409.00509)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.00509.png)

Vote: 28

Authors: Wei Shen, Jinman Zhao, Yan Wang, See-Kiong Ng, Zhiyuan Hu, Zhiwei Jiang, Bryan Hooi, Yuliang Liu, Anh Tuan Luu, Qing Gu, Suyuchen Wang

- **What's New**: 최근 논문에서는 LLMs (Large Language Models)의 긴 문맥 처리를 효율적으로 향상시키기 위해 LongRecipe라는 새로운 프레임워크를 제안했습니다. 이 방법은 주로 position index 변환과 토큰 분석을 통해 긴 문맥 일반화를 최적화합니다.
- **Technical Details**: 기존 BERT 모델과 유사하게, 이 연구에서는 위치 인코딩 벡터를 단어 임베딩 벡터와 직접 결합하여 사용합니다. LongRecipe는 Impactful Token Analysis와 Position Index Transformation을 통해 긴 문맥을 효율적으로 처리합니다. 중요한 토큰을 식별하여 긴 텍스트 코퍼스로부터 짧은 단락을 추출하고, 이들을 사용해 긴 입력을 시뮬레이션합니다. 추가로, pretraining data replay와 model merging을 통해 모델의 긴 문맥 처리를 더욱 최적화합니다.
- **Performance Highlights**: 실험 결과, LongRecipe는 80k 및 128k의 문맥 창(context window)에서 세 종류의 LLMs(Llama3-8B, Mistral-7B, Qwen2-7B)에서 약 5.5%의 평균 성능 향상을 달성했습니다. 또한, GPU 자원 사용량을 크게 줄이면서 거의 동일한 성능을 유지할 수 있었습니다. 일반적인 과제에서도 원래 모델 성능을 거의 그대로 유지하며, GPT-4-Turbo와 유사한 성능을 단 하루의 H100 GPU 훈련으로 달성할 수 있었습니다.

### [FLUX that Plays Music](https://arxiv.org/abs/2409.00587)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.00587.png)

Vote: 18

Authors: Mingyuan Fan, Zhengcong Fei, Changqian Yu, Junshi Huang

- **What's New**: 이 연구에서는 'FluxMusic'라는 새로운 텍스트-음악 생성 모델을 소개합니다. 이 모델은 텍스트 설명을 음향으로 변환하며, 최신 변형기 기반의 확산 모델(transformer-based diffusion model)을 사용하여 텍스트와 음악의 통합된 스트림을 효과적으로 처리합니다.
- **Technical Details**: FluxMusic는 가변 설명기(variable autoencoder)의 잠재 공간(latent VAE space)을 사용하여 메일-스펙트로그램(mel-spectrogram)을 모델링합니다. 모델은 텍스트-이미지 FLUX 모델을 기반으로 하여, 텍스트와 음악 시퀀스를 통합하고, 이 후 텍스트 스트림을 떨어뜨린 후 음악 스트림을 예측하는 구조로 설계되었습니다. 또한, 여러 사전 훈련된 텍스트 인코더(text encoder)를 사용하여 조건부 캡션 특징을 추출하고 유연한 추론을 제공합니다.
- **Performance Highlights**: FluxMusic는 자동 지표와 인간 선호 평가에서 최근 모델들과 견줄 만큼 우수한 성능을 보였습니다. 체계적인 네트워크 설계 분석 및 파라미터 확장에서 FluxMusic의 효율성과 퍼포먼스를 입증하였으며, 결과, 코드 및 모델 가중치를 공개하여 후속 연구를 지원합니다.

### [VideoLLaMB: Long-context Video Understanding with Recurrent Memory Bridges](https://arxiv.org/abs/2409.01071)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.01071.png)

Vote: 18

Authors: Yang Liu, Cihang Xie, Yuxuan Wang, Zilong Zheng

- **What's New**: 최근 GPT4-o와 Project Astra와 같은 대규모 비디오 언어 모델의 발전은 실시간 계획을 위한 실제 환경과의 정교한 상호작용 가능성을 보여주었습니다. 그러나 이러한 모델을 학문적으로 연구하기에는 높은 컴퓨팅 비용과 잘 주석된 공개 비디오-언어 데이터 세트의 부족으로 인해 어려움이 있습니다. 이를 해결하기 위해 우리는 비디오 내의 모든 시각 정보를 삭제하지 않고 순차적으로 인코딩하는 VideoLLaMB라는 혁신적인 프레임워크를 소개합니다.
- **Technical Details**: VideoLLaMB는 비디오 내 모든 시각 정보를 보존하기 위해 Memory Bridge Layers라는 재귀 기억 토큰을 사용하는 프레임워크입니다. 주요 기술 요소로는 SceneTilling 알고리즘을 통해 비디오를 의미적으로 독립된 시퀀스로 나누어, 각 시퀀스 내에서 세부 사항을 희생하지 않고 차원을 줄이는 방법이 포함됩니다. Recurrent memory tokens는 기억 캐시에 주기적으로 새로 고침하여 장기 의존성을 유지합니다.
- **Performance Highlights**: VideoLLaMB는 EgoSchema 및 NexTQA와 같은 긴 형태의 비디오 QA 벤치마크에서 평균 5.5의 정확도 향상을 보였습니다. 또한, 비디오 길이가 최대 8배 길어졌을 때도 성능을 유지했습니다. MVBench 벤치마크에서도 기존의 PLLaVA 모델보다 현저히 뛰어난 성능을 보였습니다. EgoPlan 데이터 세트에서 비디오 계획 작업의 성능을 평가한 결과, VideoLLaMB는 모든 7B 비디오-언어 모델 중 최고의 성능을 보였으며, PLLaVA보다 2.06의 정확도 향상을 보였습니다. 추가적으로, 긴 비디오에서 프레임 검색 능력을 평가한 결과, NIAVH 테스트에서도 기존 방법보다 뛰어난 성능을 나타냈습니다.

### [DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos](https://arxiv.org/abs/2409.02095)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.02095.png)

Vote: 18

Authors: Xiangjun Gao, Yong Zhang, Long Quan, Ying Shan, Xiaodong Cun, Sijie Zhao, Wenbo Hu, Xiaoyu Li

- **What's New**: 이번 연구에서는 DepthCrafter라는 새로운 방법을 제안하여 시간적으로 일관된 고해상도의 깊이 시퀀스를 오픈월드 비디오에 대해 생성합니다. 이는 기존 방법들과 달리 추가 정보를 필요로 하지 않으며, 비디오 확산 모델(application of video diffusion models)을 활용하여 더욱 전반적이고 사실적인 성능을 보입니다.
- **Technical Details**: DepthCrafter는 사전 학습된 이미지-비디오 확산 모델(image-to-video diffusion model)을 활용하여 비디오-깊이 모델(video-to-depth model)을 학습합니다. 이를 위해 현실적 데이터셋과 합성 데이터셋(realistic and synthetic datasets)을 함께 사용하며, 세 단계의 학습 전략(three-stage training strategy)을 채택하여 다양한 길이의 깊이 시퀀스를 생성할 수 있게 합니다. 최대 110프레임까지 처리할 수 있으며, 매우 긴 비디오의 경우도 세그먼트 별로 처리하고 매끄럽게 연결합니다.
- **Performance Highlights**: 다양한 데이터셋에서 제로샷 설정(zero-shot settings)으로 DepthCrafter를 평가한 결과, 기존의 방법보다 현격히 우수한 성능을 보였습니다. 또한 깊이 기반의 시각적 효과(depth-based visual effects)와 조건부 비디오 생성과 같은 다양한 응용에도 활용될 수 있음을 시연하였습니다.

### [Diffusion Policy Policy Optimization](https://arxiv.org/abs/2409.00588)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.00588.png)

Vote: 12

Authors: Hongkai Dai, Justin Lidard, Max Simchowitz, Lars L. Ankile, Anthony Simeonov, Pulkit Agrawal, Benjamin Burchfiel, Anirudha Majumdar, Allen Z. Ren

- **What's New**: 새로운 연구에서는 로봇 학습 정책을 미세조정(fine-tuning)하기 위한 Diffusion Policy Policy Optimization (DPPO) 프레임워크를 제안합니다. 이 프레임워크는 강화 학습(reinforcement learning, RL)에서 인기 있는 정책 그래디언트(policy gradient) 방법과 결합하여 기존의 디퓨전 기반 정책(diffusion-based policies)의 성능을 최적화합니다.
- **Technical Details**: DPPO는 두 계층의 마르코프 결정 과정(Markov Decision Process, MDP)으로 구성된 구조를 사용합니다. 외부 계층은 환경 MDP를, 내부 계층은 디퓨전 MDP를 나타냅니다. 프로시멀 정책 최적화(Proximal Policy Optimization, PPO) 방법을 적용하여 이러한 두 계층 구조에 맞춘 이점 함수(advantage function)를 효율적으로 추정합니다. 또한 디퓨전 프로세스의 마지막 몇 단계만 미세조정하거나 DDIM(Denoising Diffusion Implicit Model)을 대신 미세조정하는 것의 효과를 입증했습니다.
- **Performance Highlights**: DPPO는 여러 대체 방법들, 예를 들어 오프-폴리시 Q-러닝(off-policy Q-learning) 및 가중 회귀(weighted regression) 기반 방법들과 비교했을 때 훈련 안정성과 최종 정책 성능에서 일관되고 두드러진 개선을 보였습니다. 시뮬레이션 및 실제 환경 모두에서 성능을 검증했으며, 특히 소실 신호(sparse reward)와 같은 도전적인 로봇 제어 상황에서도 우수한 성능을 입증했습니다. 시뮬레이션에서 훈련된 정책을 실제 하드웨어에서도 제로-샷으로 성공적으로 배포했고, 기존 방법들에 비해 매우 작은 현실-가상 간의 성능 차이를 보였습니다.

### [Compositional 3D-aware Video Generation with LLM Director](https://arxiv.org/abs/2409.00558)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.00558.png)

Vote: 12

Authors: Anni Tang, Jiang Bian, Hanxin Zhu, Junliang Guo, Tianyu He, Zhibo Chen

- **What's New**: 최근의 대규모 데이터와 생성 모델의 발전 덕분에 다양한 작업에서 주목할 만한 성과들이 나타나고 있습니다. 예를 들어, 웹 스케일 데이터셋으로 사전 훈련된 Large Language Models (LLM)과 디퓨전 모델들은 텍스트에서 이미지 생성을 능숙하게 처리하고 있습니다. 본 논문은 이러한 텍스트-이미지 모델의 강력한 가능성을 비디오 생성으로 확장하고자 하는 새로운 접근법, text-guided compositional 3D-aware video generation (C3V)을 소개합니다.
- **Technical Details**: C3V은 세 가지 주요 단계를 통해 이루어집니다. 첫 번째는 텍스트 프롬프트를 분석하여 장면, 물체, 동작 등 각 개념을 설명하는 서브 프롬프트로 분해하는 것입니다. 두 번째 단계에서는 멀티모달 LLM을 사용해 장면의 크기와 궤적을 설정하지만, 이를 단계별로 나누어 시작점, 종료점, 궤적 등을 예측합니다. 마지막으로, 대규모 시각 데이터의 사전 지식으로 각 객체의 3D 변환 매트릭스를 최적화합니다. 이를 위해 Score Distillation Sampling (SDS) 기법을 사용합니다.
- **Performance Highlights**: C3V은 개별 3D 표현을 통해 개념의 유연한 제어와 상호작용을 자연스럽게 지원하며, 복잡하고 긴 비디오 합성과 시점 제어에 우수합니다. 다양한 실험에서 높은 시각적 품질의 3D-aware 비디오를 생성할 수 있으며, 복잡한 다중 개념 및 관계가 포함된 쿼리에서도 우수한 성능을 보였습니다. 생성된 비디오들은 프로젝트 페이지에 게시되어 있습니다.

### [LinFusion: 1 GPU, 1 Minute, 16K Image](https://arxiv.org/abs/2409.02097)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.02097.png)

Vote: 9

Authors: Songhua Liu, Weihao Yu, Xinchao Wang, Zhenxiong Tan

- **What's New**: 최근 몇 년간 AI 생성 콘텐츠(AIGC)에서 확산 모델(diffusion models)이 큰 발전을 이루었습니다. 이 논문은 고해상도 시각 생성 문제 해결을 위해 새로운 토큰-믹싱 메커니즘을 제안하고 있습니다. 제안된 모델, LinFusion은 선형 복잡도의 토큰-믹서로, 기존의 자기-어텐션(self-attention) 레이어를 대체하고 고해상도 생성 성능을 극대화합니다.
- **Technical Details**: 본 논문에서는 Mamba와 Mamba2 모델의 선형 복잡도(linear complexity)를 확장하여 새로운 토큰 믹서를 제안합니다. 고해상도 입력 상황에서도 일관된 특징 분포를 유지하기 위한 정규화(normalizer)를 도입하였으며, 비인과적(non-causal) 버전의 Mamba를 구현했습니다. LinFusion은 기존의 자기-어텐션 메커니즘을 대체하며, 낮은 메모리와 시간 복잡도를 유지하는 특징을 가집니다. 또한, 제안된 모델은 Stable Diffusion (SD)의 기존 구성 요소와 호환됩니다.
- **Performance Highlights**: 제안된 LinFusion 모델은 50k 반복(iterations)의 훈련만으로도 원래의 SD 모델과 동등하거나 더 나은 성능을 발휘합니다. 또한, 16K 해상도의 이미지를 단일 GPU에서 생성할 수 있으며, 무훈련 추가 구성 요소 주입이 가능합니다. 여러 실험에서 LinFusion이 SD-v1.5, SD-v2.1, SD-XL 모델과의 호환성을 입증했습니다.

### [ContextCite: Attributing Model Generation to Context](https://arxiv.org/abs/2409.00729)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.00729.png)

Vote: 8

Authors: Aleksander Madry, Kristian Georgiev, Benjamin Cohen-Wang, Harshay Shah

- **What's New**: 최근 논문에서는 ContextCite이라는 새로운 방법을 소개합니다. 이는 언어 모델이 생성한 응답의 근거가 되는 컨텍스트의 특정 부분을 직접 식별하는 '컨텍스트 귀속(context attribution)' 문제를 다룹니다. 기존 연구들이 모델이 참조할 소스를 명시적으로 제시하는 것에 초점을 맞췄다면, 이 논문은 모델이 실제로 사용한 정보를 식별하는 방법을 제안합니다.
- **Technical Details**: ContextCite은 대체 모델(surrogate model)을 학습하여 언어 모델의 응답이 특정 컨텍스트의 포함 또는 제외에 따라 어떻게 영향을 받는지 예측하는 방식을 취합니다. 이 대체 모델은 언어 모델의 행동을 충실하게 모델링하고, 소수의 추가 추론 과정을 통해 효율적으로 추정될 수 있습니다. 각 컨텍스트 부분의 중요도를 나타내는 귀속 점수를 할당합니다. 이를 통해 고득점인 부분을 제거하면, 응답에 큰 영향을 미쳐 해당 컨텍스트가 실제로 중요한 것임을 확인할 수 있습니다.
- **Performance Highlights**: ContextCite은 다양한 생성 작업에서 기존 베이스라인을 능가하는 성능을 보였습니다. 이러한 성과를 통해 세 가지 주요 응용 분야에서 유용성을 입증했습니다: 1. 생성된 응답의 정확성을 확인하는 데 도움을 줌. 2. 관련성 높은 정보를 선택하여 응답 품질 개선. 3. 컨텍스트 중독 공격(context poisoning attacks)을 감지하는 데 효과적임을 보여줬습니다.

### [OD-VAE: An Omni-dimensional Video Compressor for Improving Latent Video Diffusion Model](https://arxiv.org/abs/2409.01199)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.01199.png)

Vote: 7

Authors: Zongjian Li, Liuhan Chen, Qian Wang, Xing Zhou, Shenghai Yuan, Bin Lin, Li Yuan, Bin Zhu, Xinghua Cheng

- **What's New**: 이번 논문에서는 OpenAI의 SORA 발표 이후 주목받고 있는 비디오 생성을 위한 새로운 모델 OD-VAE(omni-dimensional compression VAE)를 제안합니다. 이 모델은 비디오의 시간적, 공간적 요소를 함께 압축하여 더 효율적인 비디오 생성을 지원합니다.
- **Technical Details**: 기존의 LVDMs(Latent Video Diffusion Models)들은 Variational Autoencoders(VAEs)를 사용하여 비디오를 압축하고, 압축된 표현에 추가된 노이즈를 예측하는 방식으로 동작합니다. 하지만, 흔히 사용하는 Stable Diffusion VAE(SD-VAE)는 이미지에 특화되어 있어 시간적 중복성을 무시하고, 이는 하드웨어 부담을 가중시킵니다. 따라서 OD-VAE는 3D-Causal-CNN 아키텍처를 사용해 비디오의 시간적·공간적 요소를 효과적으로 압축하고 재구성할 수 있도록 설계되었습니다.
- **Performance Highlights**: OD-VAE는 비디오 재구성 품질과 압축 속도 간의 최적의 균형을 맞추기 위해 네 개의 모델 변형을 도입하고 분석합니다. 또한, SD-VAE의 가중치를 활용한 새로운 타일 초기화와 제한된 GPU 메모리로 임의 길이의 비디오를 처리하기 위한 새로운 temporal tiling 기법을 제안합니다. 폭넓은 실험을 통해 OD-VAE가 기존의 LVDMs 모델들에 비해 효율적이고 효과적임을 입증하였습니다.

### [Accurate Compression of Text-to-Image Diffusion Models via Vector Quantization](https://arxiv.org/abs/2409.00492)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.00492.png)

Vote: 6

Authors: Daniil Pavlov, Denis Kuznedelev, Dan Alistarh, Dmitry Baranchuk, Ruslan Svirschevski, Michael Goin, Vage Egiazarian, Anton Voronov

- **What's New**: 최근 텍스트-투-이미지(T2I) 확산 모델(diffusion models)이 사용자 텍스트 프롬프트를 매우 현실적인 이미지로 변환하는 데 혁신적인 발전을 이뤘습니다. 이번 연구는 대규모 텍스트-투-이미지 확산 모델을 효과적으로 압축할 수 있는 새로운 포스트-트레이닝 양자화(PTQ) 방법을 제시합니다.
- **Technical Details**: 연구는 증가하는 모델 크기와 데이터셋의 중요성에 주목하며, 대규모 확산 모델의 양자화 방법으로 '추가 양자화(AQ)'에 기반한 기법을 탐구합니다. AQ는 벡터 양자화방식을 활용하여 모델을 4-8비트로 압축할 수 있으며, 이를 통해 메모리 사용과 예측 속도 문제를 해결할 수 있습니다. 본 연구에서는 SDXL 모델(2.6B 파라미터)을 3/3/3 비트로 압축하였으며, 진행 중인 작업에서 칼리브레이션과 코드북 튜닝을 통합한 후 미세조정을 통해 성능 격차를 줄였습니다.
- **Performance Highlights**: 연구는 압축된 모델이 동일한 비트 폭에서 스칼라 양자화(scalar quantization) 방법보다 뛰어난 품질의 텍스트-이미지 생성을 제공함을 입증하였습니다. PTQ와 결합하여 거의 손실 없는 4비트 압축을 달성한 SDXL-Turbo 모델에서도 유사한 결과가 나타났습니다. 다양한 자동화된 지표와 인간 평가를 통해 같은 비트 폭에서 이 접근법이 기존 방법을 능가하는 것을 확인했습니다.

### [GenAgent: Build Collaborative AI Systems with Automated Workflow Generation -- Case Studies on ComfyUI](https://arxiv.org/abs/2409.01392)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.01392.png)

Vote: 4

Authors: Di Huang, Xiangyuan Xue, Lei Bai, Zeyu Lu, Wanli Ouyang

- **What's New**: 이 논문은 이미지 생성에 대한 새로운 접근법을 제안합니다. 최신 모델을 통해 시각적 품질을 크게 향상시키는 기술을 제시합니다.
- **Technical Details**: 이 연구는 새로운 아키텍처를 사용하며, 두 가지 모델을 결합한 인코더-디코더(Encoder-Decoder) 구조를 가지고 있습니다. 또한, VQ-VAE(Vector Quantized-Variational AutoEncoder)와 GAN(Generative Adversarial Networks)를 결합하여 이미지를 생성합니다.
- **Performance Highlights**: 제안된 모델은 기존의 방법들보다 시각적 품질이 뛰어나며, 여러 벤치마크 데이터셋에 대해 우수한 성능을 보였습니다.

### [Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation](https://arxiv.org/abs/2409.01055)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.01055.png)

Vote: 3

Authors: Hongfa Wang, Qi Tian, Junkun Yuan, Wei Liu, Yue Ma, Hongmei Wang, Shaobo Min, Qihua Chen, Qifeng Chen, Wenzhe Zhao

- **What's New**: Follow-Your-Canvas라는 새로운 방법을 제안하여 더 높은 해상도의 비디오 확장을 가능하게 합니다. 이 방법은 공간 창(spatial window)를 통해 작업을 분산시키고, 생성된 레이아웃과 원본 비디오의 일관성을 유지합니다.
- **Technical Details**: Follow-Your-Canvas는 다음 두 가지 주요 설계를 기반으로 합니다. 첫째, 공간 창을 사용하여 outpainting 작업을 더 작고 쉬운 하위 작업으로 나누며, 둘째, 인코더 모듈과 상대 영역 임베딩(Relative Region Embedding)을 도입해 생성된 공간 레이아웃을 정렬합니다. 또한 Gaussian weights를 사용하여 창을 부드럽게 합칩니다.
- **Performance Highlights**: Follow-Your-Canvas는 512×512 해상도의 비디오를 1152×2048로 확장하면서도 높은 품질을 유지합니다. 이는 기존 방법보다 월등히 우수한 결과로, 특히 FVD 점수가 928.6에서 735.3으로 향상되었습니다. Spatial-temporal consistency도 높아져 전체 레이아웃과의 일관성이 보장됩니다.

### [Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders](https://arxiv.org/abs/2409.00391)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.00391.png)

Vote: 3

Authors: Aaron Elkins, Aman Chadha, Adrian Kieback, Georgios Ioannides

- **What's New**: DAAMAudioCNNLSTM 및 DAAMAudioTransformer 모델을 통한 우울증 진단 시스템을 새롭게 소개합니다. 이 모델들은 음성 데이터를 통해 정확하고 실시간으로 우울증을 감지할 수 있도록 설계되었으며, 특히 Density Adaptive Attention Mechanism (DAAM)을 적용하여 높은 성능과 설명 가능성을 제공합니다.
- **Technical Details**: DAAMAudioCNNLSTM은 약 280K 파라미터를 가지는 경량 하이브리드 모델로, multi-head DAAM을 활용하여 음성 데이터의 중요한 부분을 동적으로 조정합니다. DAAMAudioTransformer는 Transformer 기반 모델로, 약 1.1M 파라미터를 가지며, DAIC-WOZ 데이터셋에서 state-of-the-art F1 macro score 0.72를 달성했습니다. 두 모델 모두 학습 가능한 매개변수를 통해 입력 오디오 인코딩을 변환하고, 중요한 특징들에 집중하여 우울증을 감지합니다.
- **Performance Highlights**: 우리의 DAAMAudioCNNLSTM 및 DAAMAudioTransformer 모델은 기존의 최첨단 모델 성능을 능가하며, 추가적인 데이터 없이도 우울증을 높은 정확도로 감지합니다. 특히 DAIC-WOZ 데이터셋에서 새로운 벤치마크를 수립했으며, 의료 전문가들이 진단 프로세스를 더 잘 이해할 수 있도록 뛰어난 설명 가능성을 제공합니다.

### [PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action](https://arxiv.org/abs/2409.00138)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.00138.png)

Vote: 1

Authors: Tianshi Li, Yanchen Liu, Diyi Yang, Weiyan Shi, Yijia Shao

- **What's New**: 최근 아카이브(arXiv)에 게시된 새로운 논문은 대규모 자연어 처리 모델(NLP, Natural Language Processing)인 'GPT-4'의 발전과 그 응용에 대해 다루고 있습니다. 이 연구는 최신 딥러닝(Deep Learning) 기술을 사용해 언어 모델의 성능을 획기적으로 개선한 내용을 담고 있습니다.
- **Technical Details**: 이번 연구에서 사용된 GPT-4 모델은 트랜스포머(Transformer) 아키텍처를 기반으로 하며, 수십억 개의 매개변수(parameters)와 다중 레이어(multiple layers)를 포함하고 있습니다. 학습 과정에서는 대량의 텍스트 데이터를 활용하였고, 다양한 언어적 과제에서 모델의 일반화 능력을 증명하였습니다. 특히, 어텐션 메커니즘(Attention Mechanism)을 최적화하여 성능을 극대화하였습니다.
- **Performance Highlights**: 연구 결과에 따르면, GPT-4 모델은 이전 버전인 GPT-3보다 다양한 벤치마크(benchmark) 테스트에서 월등한 성능을 보였습니다. 특히, 자연어 이해와 생성, 그리고 다중 작업(multitasking) 환경에서 뛰어난 결과를 나타냈습니다. 예를 들어, 인간 수준의 논리적 사고와 문맥 추론능력을 보여 주었으며, 다양한 언어적 지식 테스트에서도 높은 점수를 기록했습니다.

### [Know When to Fuse: Investigating Non-English Hybrid Retrieval in the Legal Domain](https://arxiv.org/abs/2409.01357)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.01357.png)

Vote: 1

Authors: Antoine Louis, Gerasimos Spanakis, Gijs van Dijck

- **What's New**: 이번 연구는 정보 검색에서 두 가지 기본적인 매칭 패러다임을 결합하여 검색 품질을 향상시키는 방법을 탐구합니다. 기존 연구에서는 BM25와 단일 벡터 밀집(bi-encoder) 시스템을 주로 결합했지만, 이번 연구에서는 더 넓은 범위의 검색 모델을 조사하고, 특히 프랑스어 법적 도메인에서의 잠재적인 시너지를 탐구합니다.
- **Technical Details**: ['다양한 도메인-제너럴 검색 모델을 법적 검색에 적용한 효과를 조사합니다.', '전문화된 검색기(fusion)가 도메인 내에서의 성능에 미치는 영향을 탐구합니다.', '학습된 검색기, 특히 프랑스어 SPLADE와 ColBERT 모델을 공개합니다.', '비지도 방식의 BM25, 단일 벡터 Dense 모형, 다중 벡터 Dense 모형, 단일 벡터 Sparse 모형, 그리고 Cross-Attention 모델을 활용합니다.', '후기 결합(late fusion) 기술을 사용하여 예측 후 결과를 집계합니다.', '프랑스 텍스트 랭킹 데이터셋인 mMARCO-fr과 LLeQA를 사용하여 실험을 진행합니다.']
- **Performance Highlights**: ['적재 효율성 및 추론시의 컴퓨팅 메모리 효율성을 평가합니다.', '저장 공간 및 검색 지연 시간(latency)을 측정하여 실제 배포 가능성을 측정합니다.', '하이브리드 조합 모델이 법적 도메인에서의 일반화 능력을 보여줍니다.']

### [The MERIT Dataset: Modelling and Efficiently Rendering Interpretable Transcripts](https://arxiv.org/abs/2409.00447)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.00447.png)

Vote: 1

Authors: A. J. Lopez-Lopez, I. de Rodrigo, J. Boal, A. Sanchez-Cuadrado

- **What's New**: 이번 아카이브(arxiv)에서 발표된 논문은 새로운 머신러닝(ML, Machine Learning) 모델을 소개합니다. 이 모델은 데이터 증강(data augmentation) 기법을 활용하여 기존 시스템의 성능을 크게 향상시킵니다. 특히, 이미지 처리(image processing)와 자연어 처리(NLP, Natural Language Processing) 분야에서 적용 가능성을 보였습니다.
- **Technical Details**: 논문은 Transformer 아키텍처를 기반으로 한 새로운 알고리즘을 제안합니다. 이 알고리즘은 멀티-헤드 어텐션(multi-head attention) 메커니즘을 사용하여 다양한 데이터 포인트 간의 관계를 더욱 정밀하게 분석할 수 있습니다. 추가적으로, 모델 훈련(training) 과정에서 Hyperparameter Optimization 기법을 활용하여 최적의 성능을 달성합니다.
- **Performance Highlights**: 제안된 모델은 다양한 벤치마크 데이터셋(benchmark datasets)에서 기존 최고 성능(state-of-the-art)의 모델들을 능가하였습니다. 특히, ImageNet과 COCO 데이터셋에서 각각 5%와 3%의 성능 향상을 보이며, BLEU 점수에서도 2.5 포인트 증가를 기록했습니다. 이는 실제 응용 프로그램에서 딥러닝 모델의 효율성과 정확도를 높이는 데 중요한 기여를 할 것으로 예상됩니다.

