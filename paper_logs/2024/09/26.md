## Daily Papers (2024-09-26)

### [Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models](https://arxiv.org/abs/2409.17146)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.17146.png)

Vote: 38

Authors: Christopher Clark, Jiasen Lu, Kyle Lo, YenSung Chen, Matt Deitke, Huong Ngo, Ajay Patel, Rose Hendrix, Rohun Tripathi, Mark Yatskar, Mohammadreza Salehi, Chris Callison-Burch, Andrew Head, Jae Sung Park, Yue Yang, Niklas Muennighoff, +, Favyen Bastani, Taira Anderson, Erin Bransom, Sangho Lee, Kiana Ehsani, Luca Soldaini

- **What's New**: Molmo (Multimodal Open Language Model)라는 새로운 최첨단 비전-언어 모델(VLM)이 출시되었습니다. 이 모델은 독립적으로 사전 훈련된 비전 인코더와 언어 모델을 연결하여 학습된 것입니다. 또한, 기존의 독점 시스템에 의존하지 않고 높은 품질의 고밀도 이미지 설명 데이터셋을 수집하여 학습되었습니다.
- **Technical Details**: Molmo VLM의 아키텍처는 간단하고 표준적인 디자인을 따르며, 이미지 엔코더와 Transformer 기반의 언어 모델을 결합합니다. 사전 처리기, ViT 이미지 인코더, 비전 토큰을 언어 모델 입력 차원에 프로젝션하는 커넥터, 디코더-온리 Transformer LLM으로 구성됩니다. Molmo 모델들은 OpenAI의 ViT-L/14 336px CLIP 모델과 OLMo-7B, OLMoE-1B-7B, Qwen2 7B, Qwen2 72B 등의 다양한 언어 모델 선택지를 사용합니다.
- **Performance Highlights**: Molmo 패밀리 모델들은 11개의 학문적 벤치마크에서 평가되었으며, 사용자 선호도에 따른 인간 평가에서도 높은 순위를 차지했습니다. 가장 효율적인 MolmoE-1B 모델은 학문적 벤치마크와 사용자 선호도 모두에서 GPT-4V와 거의 동일한 성능을 보였습니다. Molmo-7B-O 및 Molmo-7B-D 모델은 학문적 벤치마크 및 사용자 선호도에서 GPT-4V와 GPT-4o 사이의 성능을 기록했습니다. 최고의 성능을 보인 Molmo-72B 모델은 GPT-4o에 비해 학문적 벤치마크 점수는 높았으며, 인간 선호도에서는 두 번째로 높은 순위를 차지했습니다.

### [Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale](https://arxiv.org/abs/2409.17115)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/628f6e5ab90dde28ef57d293/AxNzR2nvrND6Rf3RPkYMk.jpeg)

Vote: 34

Authors: Junlong Li, Pengfei Liu, Fan Zhou, Qian Liu, Zengzhi Wang

- **What's New**: 이번 논문에서는 모델 적응을 위한 SFT 데이터 생성을 위한 자세한 프롬프트(prompt) 설계에 대해 설명합니다. 새로운 포맷 스코어링 프롬프트를 추가하여 웹 문서의 형식과 구조에 중점을 두어 평가합니다. 또한, Llama-3-70B-Instruct와 Llama-3.1-405B-Instruct를 사용하여 데이터 주석(annotation)을 수행합니다.
- **Technical Details**: 기존의 FineWeb-Edu에서는 교육 점수가 2222 이상인 경우에만 문서를 유지하는 방식을 사용했지만, 이는 너무 많은 토큰을 제거했습니다. 이를 해결하기 위해 ProX에서는 형식 점수를 포함하여 필터링 기준을 완화하였습니다. 전체 코퍼스의 약 23%에서 최대 28%까지의 토큰을 유지하며, FineWeb-Edu가 약 9%를 유지하는 것과 비교하여 훨씬 많은 고유 토큰을 보존합니다.
- **Performance Highlights**: 우리의 전략은 더 많은 토큰을 유지하면서도 데이터 품질을 유지하는 데 성공적입니다. 0.7 billion 파라미터 모델에서의 초기 실험 결과, 우리의 큐레이션 데이터를 사용한 모델이 유사한 다운스트림 성능을 나타냈습니다. 이는 대규모 전처리에 적합한 전략임을 입증합니다. F1 score를 기반으로 한 문서 필터링 성능 평가도 포함되어 있습니다.

### [Boosting Healthcare LLMs Through Retrieved Context](https://arxiv.org/abs/2409.15127)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.15127.png)

Vote: 11

Authors: Jordi Bayarri-Planas, Ashwin Kumar Gururajan, Dario Garcia-Gasulla

- **What's New**: 대규모 언어 모델(LLMs)이 많은 문자 관련 작업의 기본 솔루션으로 자리 잡았지만, 사실 정확성(factual accuracy)에 대한 우려가 계속되는 상황입니다. 이를 개선하기 위한 접근법으로, 컨텍스트 검색 시스템(context retrieval system)을 활용한 모델 출력을 최적화하는 방법이 주목받고 있습니다. 이 연구는 특히 의료 다중 선택 질문 답변(MCQA) 영역에서 개방형 모델(open models)들이 어떻게 최적화된 컨텍스트 검색 시스템을 통해 성능을 향상시킬 수 있는지 탐구하고 있으며, 다중 선택 질문 외에도 열린 형태의 답변(open-ended answers) 생성에도 적용할 수 있도록 제안하고 있습니다.
- **Technical Details**: 연구에서는 Medprompt 디자인을 기반으로 한 컨텍스트 검색 시스템을 최적화하는 방법론이 자세히 설명됩니다. 이 시스템의 핵심 구성 요소로는 '선택 항목 섞기(choice shuffling)', '앙상블 수(ensembles)', '데이터베이스(database)', '임베딩 모델(embedding model)', '재순위 매기기 모델(reranking model)' 등이 있으며, 각 요소의 역할과 영향력을 평가합니다. 또한, 성능 평가를 위해 4개의 주요 의료 다중 선택 질문 데이터셋(MedQA, MedMCQA, CareQA, MMLU)을 사용합니다. 실험에 사용된 주요 모델은 'Llama3-Aloe-8B-Alpha'로 8조 파라미터를 갖춘 최신 개방형 LLM이며, 이를 위해 NVIDIA H100 GPU를 활용합니다.
- **Performance Highlights**: 현재 프라이빗 모델(GPT-4, MedPalm-2)이 의료 MCQA 영역에서 최적의 성능을 보이고 있는 반면, 개방형 모델은 성능 면에서 뒤처지고 있습니다. 하지만 최적화된 컨텍스트 검색 시스템을 적용한 개방형 모델들이 프라이빗 모델에 근접한 성능을 보일 가능성을 제시하고 있습니다. 특히 선택 항목 섞기, 앙상블 수 조정, 데이터베이스 다양화 등 다양한 방법을 적용하여 모델 성능을 향상시킬 수 있음을 발견했으며, 이를 통해 실세계 임상 시나리오에서의 활용성을 높일 수 있습니다.

### [AIM 2024 Sparse Neural Rendering Challenge: Dataset and Benchmark](https://arxiv.org/abs/2409.15041)

![](/avatars/4234203c77a0e6f594f3de26bfe8c649.svg)

Vote: 8

Authors: Radu Timofte, Sibi Catley-Chandar, Thomas Tanay, Eduardo Pérez-Pellitero, Michal Nazarczuk, Richard Shaw

- **What's New**: 최신 신경 렌더링 (neural rendering) 기술의 발전은 다양한 컴퓨터 비전 과제에서 상당한 진전을 이뤘습니다. 이러한 렌더링 기술은 주로 밀도 높은 뷰 입력 (dense-view input data)에 초점을 맞추었지만, 드문 데이터 설정의 도전과 기회를 간과했습니다. 이를 해결하기 위해 SpaRe라는 Sparse Rendering 데이터셋과 벤치마크를 소개합니다. 이는 드문 뷰 신경 렌더링(sparse-view neural rendering)의 최첨단을 발전시키고 평가하기 위해 설계된 새로운 데이터셋입니다. SpaRe는 AIM 2024 Sparse Neural Rendering Challenge를 위해 개발되었습니다.
- **Technical Details**: SpaRe 데이터셋은 3 또는 9개의 입력 이미지를 이용해 드문 뷰포인트에서 테스트를 수행하며, 훈련 세트에는 밀도 높은 뷰 커버리지를 제공합니다. 각 장면은 하나의 3D 객체를 중심으로 최대 64개의 카메라 뷰와 16개의 조명 설정 (lighting configurations)을 포함하고 있으며, 1600×1200 해상도로 렌더링됩니다. SpaRe는 정밀한 카메라 포즈와 고해상도 사실적인 이미지를 제공합니다. 또한, 실제와 합성 데이터를 모두 포함하여 실제 세계의 조명, 반사, 텍스처와 같은 복잡한 요소를 균형 있게 반영합니다.
- **Performance Highlights**: SpaRe 벤치마크 플랫폼은 고해상도 이미지를 사용하고, 숨겨진 실제 검증 세트를 통해 평가를 표준화합니다. 이는 reproducibility 및 reliability를 보장합니다. SpaRe 데이터셋은 고품질의 사실적인 3D 자산 기반의 102개의 합성 장면을 포함하며, 다양한 복잡성, 재료, 텍스처 및 occlusions을 고려하여 드문 신경 렌더링 알고리즘 성능을 종합적으로 평가할 수 있도록 설계되었습니다. SpaRe는 연구 커뮤니티가 드문 데이터 환경에서 더욱 효율적이고 효과적인 신경 렌더링 방법을 개발할 수 있도록 가치 있는 자원을 제공합니다.

### [DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion](https://arxiv.org/abs/2409.17145)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.17145.png)

Vote: 6

Authors: Zheng-Jun Zha, Ailing Zeng, Lei Zhang, Jianan Wang, Yukun Huang, Xihui Liu

- **What's New**: 드림왈츠-G(DreamWaltz-G)는 텍스트 기반 3D 아바타 생성을 위한 새로운 제로샷 학습 프레임워크입니다. 이 프레임워크의 핵심 요소는 'Skeleton-guided Score Distillation (SkelSD)'와 'Hybrid 3D Gaussian Avatars (H3GA)'로, 이를 통해 안정적인 최적화와 표현력 있는 애니메이션이 가능합니다.
- **Technical Details**: SkelSD는 기존 방법과 달리 인체 골조 기준들을 3D 아바타 표현뿐 아니라 디퓨전 모델에도 주입하여 3D 인체 구조와 일치하는 더 안정적인 'Score Distillation Sampling (SDS)'를 가능하게 합니다. H3GA는 3D Gaussian Splatting, neural implicit fields, parameterized meshes 요소들을 혼합하여 SDS 최적화, 실시간 렌더링, 손가락 움직임과 얼굴 표정을 포함한 표현력 있는 애니메이션을 지원합니다.
- **Performance Highlights**: 드림왈츠-G는 텍스트 기반으로 애니메이션 가능한 3D 아바타 생성을 두 단계로 수행합니다. 첫 번째 단계에서는 텍스트 설명에 따라 기본 3D 아바타를 생성하고, 두 번째 단계에서는 이 아바타에 SMPL-X를 기반으로 리깅하여 정확하게 애니메이션되도록 합니다. 실험 결과 기존의 텍스트 기반 3D 아바타 생성 방법에 비해 우수한 생성과 애니메이션 품질을 기록했습니다.

### [Synchronize Dual Hands for Physics-Based Dexterous Guitar Playing](https://arxiv.org/abs/2409.16629)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.16629.png)

Vote: 6

Authors: Pei Xu, Ruocheng Wang

- **What's New**: 최근 컴퓨터 그래픽스와 VR/AR 기술의 발전으로 가상 음악 공연이 등장하면서 더 몰입감 있는 라이브 음악 경험을 제공하게 되었다. 그러나 가상으로 시뮬레이션된 캐릭터가 악기를 사람처럼 물리적으로 그럴듯하게 제어하는 것은 여전히 도전 과제다. 본 논문에서는 기타를 연주하는 가상의 손과 손가락의 동작을 합성하는 새로운 접근 방식을 제안한다.
- **Technical Details**: 본 연구는 기타 연주를 데이터 기반 접근 방식을 통해 사람처럼 물리적으로 가능한 동작을 생성하는 데 중점을 둡니다. 기타 연주는 두 손이 이질적인 작업(자기 자리를 누르고 줄을 튕기고 정교한 핑거피킹 패턴을 연주)에 동기화되어야 하는 어려운 과제입니다. 본 연구에서는 두 손을 개별 에이전트로 간주하여 학습 효율성을 높이고, 물리 기반 제어와 추가 모방 학습을 결합하여 접촉이 많은 작업에서 발생하는 자연스럽고 물리적으로 그럴듯한 동작을 보장한다.
- **Performance Highlights**: 당사의 접근 방식은 고품질의 단일 핸드 제어를 학습하면서도 빠르게 동기화된 동작을 생성할 수 있으며, 연구 결과는 실험 섹션에서 정성적 및 정량적으로 평가되었습니다. 추가적인 제거 연구는 양손 제어를 위한 훈련 계획의 타당성을 뒷받침하고, 기타 연주 외의 작업에도 적용할 잠재력을 강조합니다.

### [Game4Loc: A UAV Geo-Localization Benchmark from Game Data](https://arxiv.org/abs/2409.16925)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg)

Vote: 3

Authors: Zhuoyue Tan, Liaoni Wu, Yuxiang Ji, Boyong He

- **What's New**: 본 연구에서는 UAV(드론) 비주얼 지오로컬라이제이션(geo-localization)의 새로운 벤치마크인 GTA-UAV 데이터를 소개합니다. 이 데이터셋은 기존의 완벽한 매칭 대신, 부분적으로 일치하는 드론-위성 뷰 이미지 쌍을 포함하여 현실적인 시나리오를 반영합니다. 또한, 우리는 가중치 정보 대조 학습법(weighted contrastive learning method)인 weighted-InfoNCE를 개발하여 모델이 부분 일치 패러다임을 학습할 수 있도록 합니다.
- **Technical Details**: 기존 연구들은 대부분 드론-위성 이미지 쌍을 한 쌍씩 완벽하게 매칭하여 데이터셋을 구성했습니다. 그러나 실제 환경에서는 이러한 완벽한 매칭이 희박하기 때문에, 본 연구에서는 부분적으로 일치하는 이미지 쌍을 구성하고자 했습니다. 이를 위해 상용 비디오 게임을 활용하여 다양한 비행 고도 및 각도의 드론-위성 이미지 쌍을 시뮬레이션하고 수집했습니다. 총 33,763개의 드론 뷰 이미지가 다양한 경관 (도시, 산, 사막, 숲, 들판, 해안)을 포함한 지역에서 수집되었습니다.
- **Performance Highlights**: GTA-UAV 데이터셋과 weighted-InfoNCE 학습 방법을 통해, 네트워크는 다른 뷰에서 부분적으로 일치하는 샘플 간의 임베딩 거리를 줄여, 검색 및 로컬라이제이션 작업을 가능하게 했습니다. 실험 결과는 제안된 데이터셋과 방법이 실제 과제에서 그 잠재력과 일반화 능력을 갖추고 있음을 보여주었습니다.

### [Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors](https://arxiv.org/abs/2409.17058)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.17058.png)

Vote: 3

Authors: Wenqi Ren, Renjing Pei, Aiping Zhang, Xiaochun Cao, Zongsheng Yue

- **What's New**: 새로운 연구 논문에서 지속적인 복잡한 문제인 이미지 초해상도(Super-Resolution, SR) 문제를 해결하기 위해 고성능의 단일 스텝 초해상도 확산 네트워크인 S3Diff를 제안하였습니다.
- **Technical Details**: S3Diff는 텍스트-이미지(T2I) 생성 모델의 사전 학습된 생성 능력을 활용하여, LR(Low-Resolution) 이미지의 저해상도 정보를 최대한 활용하여 고해상도(HR, High-Resolution) 이미지를 생성합니다. 특히, SD-Turbo의 효율적인 소수 단계 추론과 강력한 생성 능력을 기반으로 하며, Low-Rank Adaptation (LoRA)모듈과의 통합을 통해 모델 파라미터를 데이터 의존적으로 수정하는 혁신적인 접근 방식을 채택하였습니다.
- **Performance Highlights**: S3Diff는 기존 방법들에 비해 단일 전방 패스를 통해 고품질의 HR 이미지를 생성하는 데 있어 추론 시간을 크게 단축하고, 필요한 학습 파라미터를 최소화하였습니다. 이는 특히 실제 환경에서 고효율의 초해상도 작업을 가능하게 합니다.

### [NoTeeline: Supporting Real-Time Notetaking from Keypoints with Large Language Models](https://arxiv.org/abs/2409.16493)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.16493.png)

Vote: 2

Authors: Xiaodi Alice Tang, Jeffrey P. Bigham, David Chuan-en Lin, Abdus Samee, Faria Huq

- **What's New**: 새로운 연구에서 NoTeeline이라는 시스템을 소개합니다. NoTeeline은 '마이크로노트(micronotes)' 방식을 통해 효과적인 노트 작성 과정을 최적화합니다. 이 시스템은 사용자가 간단하고 적은 노력으로 마이크로노트를 작성하면, 이를 전체 노트로 자동 확장해줍니다.
- **Technical Details**: NoTeeline은 매우 간략하게 작성된 마이크로노트를 바탕으로, 전체 노트로 확장하기 위해 비디오 전사 및 이전 사용자가 작성한 예시를 활용합니다. 마이크로노트는 몇 단어와 비공식 표기법으로 구성되어 있어, 비디오 시청 중 최소한의 방해로 필수 정보를 밀도 있게 포착할 수 있습니다.
- **Performance Highlights**: 사용자 연구를 통해 NoTeeline이 전통적인 노트 작성 도구보다 훨씬 더 빠르고 쉽게 노트를 작성하고 합성할 수 있음을 확인했습니다. NoTeeline은 93.2%의 사실적 정확성으로 정확한 노트를 생성하며, 작성 시간을 최대 43.9%까지 줄여줍니다.

### [TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans](https://arxiv.org/abs/2409.16666)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.16666.png)

Vote: 1

Authors: Nikolaos Sarafianos, Bindita Chaudhuri, Rakesh Ranjan, Aggelina Chatziagapi, Dimitris Samaras, Amit Kumar

- **What's New**: 컴퓨터 비전과 그래픽스 분야에서 포토리얼리스틱한 4D 인간을 합성하는 것은 오랜 연구 과제였습니다. 본 논문에서는 단안 비디오(monocular videos)를 통해 전체 몸의 동작, 손의 움직임, 얼굴 표정을 결합하는 TalkinNeRF를 제안합니다. 이는 모노큘러 비디오에서 학습되는 통합된 동적 네트워크(dynamic NeRF)로, 전체 몸의 움직임과 세밀한 손가락 움직임 및 얼굴 표정을 합성합니다. 이는 다른 발표 포즈에서 애니메이션을 수행할 수 있고, 다중 정체성 학습이 가능하여 트레이닝 시간을 크게 단축하고 일반화 성능을 높입니다.
- **Technical Details**: 기존의 NeRF 기반 접근법은 다중 뷰(multiple views)를 필요로 했으나, TalkinNeRF는 단안 비디오만으로도 4D 인간의 움직임을 학습합니다. 각 비디오 프레임에 대해 파라메트릭 모델(parametric model)을 맞추고, 몸 포즈, 손 포즈, 얼굴 표정의 파라미터를 추출하여, 각각의 모듈로 조건화합니다. 손의 복잡한 움직임을 포착하기 위해 추가적인 변형 필드(deformation field)를 학습하며, 각 주제에 대한 개별적인 아이덴티티 코드를 학습하여 다중 정체성 학습이 가능합니다. 이를 통해 새로운 포즈에서도 강건한 성능을 보이며, 전체 학습 시간을 크게 단축합니다.
- **Performance Highlights**: TalkinNeRF는 처음으로 단안 비디오만으로 전체 몸의 동작, 손의 포즈, 얼굴 표정을 동시에 학습할 수 있는 통합된 동적 네트워크를 제안합니다. 이는 지금까지의 연구들보다 질감 높은 합성 영상을 제공합니다. 특히, 세밀한 손가락의 움직임과 얼굴 표정을 통해 대화 상황에서 더 실감 나고 자연스러운 디지털 휴먼을 생성하는데 뛰어난 성능을 보입니다. 또한, 새로운 아이덴티티에 대한 일반화 성능도 우수하여, 짧은 비디오만으로도 고품질의 애니메이션을 생성할 수 있습니다.

### [HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale](https://arxiv.org/abs/2409.16299)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.16299.png)

Vote: -

Authors: Phong X. Nguyen, Huy Nhat Phan, Nghi D. Q. Bui

- **What's New**: 최근 몇 년 동안, 대형 언어 모델(LLM)은 코드 생성, 자동 완성, 버그 수정 및 리팩토링과 같은 다양한 코딩 작업을 지원하는 데 놀라운 능력을 보여주었습니다. 그러나 소프트웨어 공학(SE) 작업이 점점 더 복잡해지면서, 실제 소프트웨어 개발의 복잡성을 처리할 수 있는 더 정교한 솔루션이 필요해지고 있습니다. 이에 대응하기 위해 HyperAgent라는 일반 목적의 멀티 에이전트 시스템을 제안합니다. 이 시스템은 LLM의 고급 추론 및 생성 기능을 활용하여 다양한 SE 작업을 자동화하고 간소화할 수 있습니다.
- **Technical Details**: HyperAgent는 계획(Planner), 탐색자(Navigator), 코드 편집자(Code Editor), 실행자(Executor) 등 네 가지 주요 에이전트로 구성되어 있습니다. 이 네 가지 에이전트는 소프트웨어 엔지니어가 일상 업무에서 따르는 일반적인 워크플로우에 따라 설계되었습니다. 각 작업 단계는 문서 검토와 이해, 기능 현지화, 코드 편집 및 추가, 실행 및 테스트로 나뉩니다. HyperAgent는 다양한 작업에 쉽게 적응하고 최소한의 구성 변경으로 새로운 모듈을 시스템에 추가할 수 있는 일반화 가능성, 다양한 복잡도 수준의 작업을 효율적으로 관리할 수 있는 효율성, 실제 시나리오에서 확장 가능성을 갖추고 있습니다.
- **Performance Highlights**: HyperAgent는 다양한 소프트웨어 엔지니어링 벤치마크에서 우수한 성능을 보여주었습니다. 예를 들어, GitHub 이슈 해결에서는 SWE-bench 벤치마크에서 AutoCodeRover와 SWE-Agent를 능가하였고, 저장소 수준의 코드 생성에서는 RepoExec 벤치마크에서 WizardLM2 및 GPT-3.5-Turbo보다 뛰어난 성과를 보였습니다. 또한, 결함 위치 지정 및 프로그램 수리에서는 Defects4J 데이터셋에서 최첨단 성과를 달성하였습니다. HyperAgent는 여러 프로그래밍 언어에서 다양한 SE 작업을 오프 더 쉘프에서 처리할 수 있도록 설계된 첫 번째 시스템으로, 실세계의 소프트웨어 개발 시나리오에서 혁신적인 도구로 자리잡을 가능성을 보여주었습니다.

