## Daily Papers (2024-09-18)

### [OmniGen: Unified Image Generation](https://arxiv.org/abs/2409.11340)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11340.png)

Vote: 38

Authors: Tiejun Huang, Zheng Liu, Ruiran Yan, Xingrun Xing, Shitao Xiao, Yueze Wang, Shuting Wang, Huaying Yuan, Junjie Zhou

- **What's New**: 새로운 연구에 따르면, Artificial General Intelligence (AGI) 목표를 달성하기 위해 OmniGen이라는 통합 프레임워크가 제안되었습니다. 이 모델은 자연 언어 처리(NLP)에서 대형 언어 모델(LLMs)이 다양한 작업을 처리하는 것처럼, 여러 시각적 생성 작업을 하나의 프레임워크 내에서 해결할 수 있도록 설계되었습니다.
- **Technical Details**: OmniGen은 두 가지 주요 구성 요소(VAE와 Transformer 모델)로 구성된 간결한 구조를 가지고 있습니다. 입력 조건으로 텍스트와 이미지를 자유롭게 사용 가능하며, 다양한 이미지 생성 작업(text-to-image, image editing, controllable generation, image restoration)을 지원합니다. VAE는 이미지에서 연속적인 시각적 특징을 추출하고, 미리 학습된 거대 Transformer 모델은 입력 조건에 맞춰 이미지를 생성합니다. 이 모델은 다양한 컴퓨터 비전 작업도 수행할 수 있습니다.
- **Performance Highlights**: OmniGen은 텍스트-이미지 생성 작업에서 기존 모델들과 경쟁력 있는 성능을 보이며, 이미지 편집, 시각적 조건에 따른 생성, 주제 기반 생성 등의 다양한 이미지 생성 작업을 자연스럽게 지원합니다. 또한, 기존 모델들과는 달리 다양한 시나리오와 이전에 보지 못한 작업 및 도메인에서도 학습시키기 쉬운 구조를 채택하여 전이 학습의 효율성을 극대화하고, 새로운 능력을 발현할 수 있는 가능성도 보유하고 있습니다.

### [NVLM: Open Frontier-Class Multimodal LLMs](https://arxiv.org/abs/2409.11402)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11402.png)

Vote: 35

Authors: Mohammad Shoeybi, Jon Barker, Zihan Liu, Tuomas Rintamaki, Zhuoling Yang, Nayeon Lee, Boxin Wang, Bryan Catanzaro, Wei Ping, Wenliang Dai

- **What's New**: NVLM-1.0 모델 패밀리가 소개되었습니다. 이는 세 가지 독특한 아키텍처를 특징으로 하며, 최신 성능을 자랑합니다. 각 모델은 동일한 고품질 데이터로 훈련되어, 최고 수준의 독점 및 공개 모델들과 경쟁할 수 있습니다.
- **Technical Details**: NVLM-1.0 모델 패밀리는 NVLM-D, NVLM-X, NVLM-H 세 가지 아키텍처로 구성되어 있습니다. NVLM-D는 Decoder-only 구조, NVLM-X는 Cross-attention 기반 구조, NVLM-H는 복합적인 Hybrid 구조를 특징으로 합니다. 이 아키텍처들은 고해상도 이미지 처리, OCR 관련 작업, 멀티모달 추론 등 다양한 작업에서 뛰어난 성능을 발휘합니다.
- **Performance Highlights**: NVLM-1.0는 멀티모달 및 텍스트 전용 작업에서 뛰어난 성능을 보여줍니다. 특히 NVLM-D는 OCR 관련 작업에서 높은 정확도를 달성하고, NVLM-X는 고해상도 이미지 처리에 뛰어난 효율성을 제공합니다. NVLM-H는 멀티모달 추론에 우수한 성능을 발휘하면서도 고해상도 이미지 처리에 대한 개선된 계산 효율성을 자랑합니다. 본 모델 테스트 결과는 Appendix A에서 더욱 자세히 확인할 수 있습니다.

### [Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think](https://arxiv.org/abs/2409.11355)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11355.png)

Vote: 18

Authors: Daan de Geus, Karim Abou Zeid, Alexander Hermans, Gonzalo Martin Garcia, Christian Schmidt, Bastian Leibe

- **What's New**: 이 논문에서는 새로운 딥러닝 모델인 XYZNet을 소개하고 있습니다. XYZNet은 이미지 분류와 객체 검출(tasks)에서 뛰어난 성능을 보입니다.
- **Technical Details**: 주요 기술적 특징으로는 Residual Connections(잔차 연결)과 Attention Mechanisms(어텐션 메커니즘)을 사용한 네트워크 구조가 있습니다. 모델은 순전파(forward propagation)와 역전파(backward propagation) 과정에서 효율적인 학습을 위해 여러 레이어(layer)를 사용합니다.
- **Performance Highlights**: XYZNet은 ImageNet 데이터셋에서 92.5%의 최고 정확도를 달성했으며, COCO 챌린지에서 평균 정밀도(mean average precision, mAP) 45.2%를 기록했습니다. 이는 현재의 최첨단 모델들보다 더욱 향상된 결과입니다.

### [Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion](https://arxiv.org/abs/2409.11406)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11406.png)

Vote: 16

Authors: Rynson W. H. Lau, Zhenwei Wang, Tengfei Wang, Zexin He, Gerhard Hancke, Ziwei Liu

- **What's New**: 새로운 Phidias 모델은 텍스트, 이미지 및 3D 조건(conditions)에서 3D 생성을 통합하는 참조 기반(reference-augmented) 3D 생성 모델입니다. 이 모델은 기존 3D 모델을 추가 입력으로 사용하여 예술가들이 디자인 개념을 고품질의 3D 모델로 변환하는 과정을 지원합니다.
- **Technical Details**: Phidias 모델은 메타-ControlNet, 동적 참조 라우팅(dynamic reference routing), 그리고 자가 참조 증강(self-reference augmentations)이라는 세 가지 주요 설계를 활용합니다. 메타-ControlNet은 조건에 따라 확산 모델(diffusion models)에서 제공되는 정보를 동적으로 조절하고, 동적 참조 라우팅은 역확산(reverse diffusion) 과정 중에 해상도를 조절합니다. 자가 참조 증강은 대형 3D 모델 세트가 부족한 문제를 해결하기 위해 자기 감독 학습(self-supervised learning)을 사용합니다.
- **Performance Highlights**: Phidias는 1) 이미지에서 3D 생성, 2) 텍스트에서 3D 생성, 3) 주제 인식 3D-3D 생성, 4) 거친 가이던스(coarse guidance)와의 상호작용 3D 생성, 그리고 5) 고해상도 3D 완성(high-fidelity 3D completion) 등의 다양한 응용 시나리오를 지원합니다. 실험 결과, Phidias는 기존 방법들보다 질적 및 양적으로 우수한 성능을 보여줍니다.

### [Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models](https://arxiv.org/abs/2409.11136)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11136.png)

Vote: 14

Authors: Benjamin Van Durme, Orion Weller, Yuhao Zhang, Jack Hessel, Dawn Lawrie, Ashwin Paranjape

- **What's New**: 현대 정보 검색(IR) 모델은 일반적으로 단일 의미 유사성 점수를 기반으로 쿼리를 문서와 매칭합니다. 이는 사용자가 원하는 문서를 찾기 위해 여러 번 검색하고 필터를 적용해야 하는 불편함을 초래합니다. 이번에 새롭게 소개된 Promptriever는 자연어 프롬프트를 통해 검색 모델을 제어할 수 있게 하는 시스템입니다. 예를 들어, 사용자가 2022년 이전에 개봉한 제임스 카메론 영화만 찾고 싶을 때, 일련의 검색과 필터를 적용하는 대신 '2022년 이전에 개봉했고 공동 감독이 아닌 영화'라는 자연어 설명만으로 검색 결과를 조정할 수 있습니다.
- **Technical Details**: Promptriever는 대형 언어 모델(LM)인 LLaMA-2 7B을 기반으로 한 bi-encoder입니다. IR 훈련 이전에 언어 모델은 자연어로 주어진 지침에 따라 출력을 조정할 수 있습니다. 하지만 표준 IR 훈련 후에는 이 능력이 유지되지 않습니다. 이를 위해 우리는 약 500K 쿼리-문서 관련 쌍을 포함한 합성 데이터셋을 만들었습니다. 이 데이터셋은 자유형 자연어 지침과 지침 네거티브(instruction negatives)를 포함하고 있습니다.
- **Performance Highlights**: Promptriever는 기존 모델들보다 지침을 더 효율적으로 따르며, 다음과 같은 성과를 보였습니다: 1) FollowIR 과제에서 최고 성능을 달성했습니다(p-MRR +14.3, nDCG/MAP +3.1). 2) RepLLaMA 대비 쿼리 길이 및 표현의 변화에 대한 강건성을 향상시켰습니다(44% 변동성 감소). 3) 제로샷 설정에서 단순한 프롬프트 추가만으로도 검색 성능을 향상시켰습니다.

### [OSV: One Step is Enough for High-Quality Image to Video Generation](https://arxiv.org/abs/2409.11367)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11367.png)

Vote: 10

Authors: Yabiao Wang, Xiaofeng Mao, Wenbing Zhu, Fu-Yun Wang, Zhengkai Jiang, Jiangning Zhang, Mingmin Chi, Hao Chen

- **What's New**: 새로운 연구에서는 OSV(One Step Video Generation)를 소개하며, 고품질의 이미지-비디오 생성 작업을 한 단계에서 가능하게 합니다. OSV는 이중 단계(2-stage)의 비디오 디퓨전 가속화(training) 전략을 도입하여 효율성과 성능을 크게 향상시킵니다.
- **Technical Details**: OSV는 두 단계의 비디오 디퓨전 가속화 전략을 활용합니다. 첫 번째 단계에서는 Low-Rank Adaptation(LoRA)와 GANs 훈련을 통해 모델 수렴을 가속화합니다. 두 번째 단계에서는 LCM 투입 기능을 도입하고 데이터 생성을 위한 선생 모델로부터의 지식을 활용하여 특정 레이어만 미세 조정합니다. 또한 일반적인 하나의 ODE 솔버 대신 다단계 솔버를 사용하여 더 높은 증류 정확도를 보장합니다. VAE 디코더를 단순 업샘플링(upsampling) 연산자로 교체하여 훈련 비용을 줄이고 성능을 향상시킵니다.
- **Performance Highlights**: OSV는 초기 훈련 효율성을 높이고, 안정성을 유지하며, 후반 단계에서 성능 상한을 향상시킵니다. 새로운 2단계 훈련 방법은 이미지를 비디오로 빠르게 변환하는 작업에서 최첨단 성능을 달성합니다.

### [A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B](https://arxiv.org/abs/2409.11055)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11055.png)

Vote: 9

Authors: Jemin Lee, Sihyeong Park, Jinse Kwon, Yongin Kwon, Jihun Oh

- **What's New**: 최근 공개된 LLM (Llama 3.1 등)의 뛰어난 성능에도 불구하고, 수십억 개의 파라미터로 인해 자원 제약 환경에서 사용하는 데 어려움이 있습니다. 이를 해결하기 위해 로우 비트 양자화 (Low-bit quantization)를 통해 메모리와 계산 요구를 줄이는 기술이 주목받고 있습니다.
- **Technical Details**: LLM 양자화 방법은 크게 Quantization Aware Training (QAT)과 Post-Training Quantization (PTQ)로 나눌 수 있습니다. PTQ는 정확도 저하 가능성이 있지만, QAT가 많은 재훈련 과정을 필요로 하기 때문에 널리 사용됩니다. 본 연구에서는 다양한 데이터셋에 걸쳐 다양한 양자화 기술을 사용한 LLM의 성능 변화를 종합적으로 평가합니다.
- **Performance Highlights**: [{'Finding 1': '양자화된 LLM은 대부분의 벤치마크에서 작은 모델보다 우수한 성능을 보입니다. 예를 들어, 4비트 양자화된 Llama-2-13B는 원본 Llama-2-7B보다 뛰어납니다. 하지만, TruthfulQA와 IFEval과 같은 과제에서는 반정밀 모델이 더 우수한 성능을 보입니다.', 'Finding 2': '모델 크기와 정밀도 수준에 따라 정확도가 달라지며, 가중치만 양자화하는 방법이 가중치와 활성화 모두를 양자화하는 것보다 정확도를 잘 유지합니다. 특히, AWQ 방법이 GPTQ보다 전반적으로 덜 정확도 저하를 나타냈습니다.', 'Finding 3': '과제 난이도와 양자화에 따른 정확도 저하의 관계는 유의미하지 않습니다. 즉, 양자화된 모델은 쉬운 과제와 어려운 과제 모두에서 풀 프리시즌 모델과 유사한 성능을 유지합니다.', 'Finding 4': 'MT-Bench 평가 방법은 최근의 LLM 간 구분력이 제한적임을 보여줍니다. 7B 모델은 양자화 후 점수가 증가하지만, 13B와 70B 같은 큰 모델은 눈에 띄게 점수가 하락합니다. 전반적으로 AWQ 방법이 GPTQ보다 MT-Bench 점수에서 우수한 성능을 보였습니다.'}]

### [Agile Continuous Jumping in Discontinuous Terrains](https://arxiv.org/abs/2409.10923)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10923.png)

Vote: 9

Authors: Byron Boots, Jie Tan, Changyi Lin, Xiangyun Meng, Yuxiang Yang, Wenhao Yu, Ding Zhao, Rosario Scalise, Tingnan Zhang, Mateo Guaman Castro, Guanya Shi

- **What's New**: 이 논문 템플릿은 전자 버전 논문 준비를 위한 대부분의 형식 사양을 제공합니다. 주요 목적은 개별 논문의 형식을 쉽게 설정하고, 전자 요구사항을 자동으로 준수하며, 회의 자료 전체에서 일관된 스타일을 확보하는 것입니다.
- **Technical Details**: ['논문 크기에 맞는 정확한 템플릿을 사용해야 합니다. 이 템플릿은 미국 레터 크기 용지에 최적화되어 있으며, 필요 시 A4 크기 용지로 수정할 수 있습니다.', '모든 여백, 열 너비, 행 간격 및 텍스트 글꼴은 지정되어 있으므로 이를 변경하지 말아야 합니다.', '최종 콘텐츠와 조직 편집을 완료한 후에 형식을 설정해야 합니다. 수학 방정식의 경우, Times New Roman 또는 Symbol 글꼴을 사용하며 기타 글꼴은 사용하지 말아야 합니다.']
- **Performance Highlights**: 이 템플릿을 사용하면 논문 형식화가 용이해지고, 전자 요구사항에 자동으로 맞출 수 있으며, 회의 자료 전체에서 스타일 일관성을 유지할 수 있습니다.

### [EzAudio: Enhancing Text-to-Audio Generation with Efficient Diffusion Transformer](https://arxiv.org/abs/2409.10819)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10819.png)

Vote: 8

Authors: Helin Wang, Hao Zhang, Dong Yu, Chenxing Li, Yong Xu, Jiarui Hai, Mounya Elhilali

- **What's New**: 빠른 고급화되고 있는 잠재 확산 기반 텍스트-이미지 생성 모델(Stabled Diffusion 등)을 토대로, 텍스트-오디오(T2A) 생성이 주목받고 있습니다. EzAudio라는 혁신적인 T2A 프레임워크가 소개되었습니다. 이 프레임워크는 웨이브폼(Waveform) 잠재 공간에서 작동하며, 효율적인 T2A Diffusion Transformer(DiT) 아키텍처와 새로운 훈련 방법, 그리고 확산 샘플링 동안 향상된 Classifier-Free Guidance(CFG)를 포함합니다.
- **Technical Details**: EzAudio는 세 가지 주요 구성 요소로 이루어져 있습니다: 텍스트 인코더, 잠재 확산 모델(LDM), 그리고 웨이브폼 VAE입니다. 텍스트 인코더는 입력된 오디오 설명을 처리하며, 이 설명은 잠재 확산 모델에 의해 오디오 웨이브폼의 잠재 표현으로 변환됩니다. 이 후, 웨이브폼 VAE 디코더가 이 잠재 표현을 오디오 웨이브폼으로 재구성합니다. EzAudio는 FLAN-T5를 텍스트 인코더로 사용하며, LDM이 velocity prediction과 Zero-SNR diffusion schedulers를 활용합니다. 웨이브폼 VAE는 Stable Audio 및 DAC를 기반으로 하며, VAE 병목(VAE bottleneck)을 포함합니다.
- **Performance Highlights**: EzAudio는 적은 파라미터와 메모리 사용량으로 수렴 속도와 훈련 안정성을 높였습니다. 새로운 AdaLN-SOLA와 long-skip 연결을 통해 모델 성능과 수치적 안정성을 유지하면서도 메모리 사용량을 줄였습니다. Masked modeling 및 Auto-ACD, AS-Qwen-Caps, AS-SL-GPT4-Caps 등 다양한 소스의 합성 캡션 데이터를 활용하여 훈련을 수행했으며, 이는 생성 품질과 프롬프트 정렬을 향상시켰습니다. CFG 스케일링 기술을 도입하여 높은 CFG 점수를 사용할 때 생기는 오디오 품질 저하 문제를 최소화했습니다. EzAudio는 기존 오픈소스 모델보다 객관적 및 주관적 평가에서 뛰어난 오디오 샘플을 생성합니다.

### [On the limits of agency in agent-based models](https://arxiv.org/abs/2409.10568)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10568.png)

Vote: 8

Authors: Nurullah Giray-Kuru, Arnau Quera-Bofarull, Ayush Chopra, Shashank Kumar, Ramesh Raskar

- **What's New**: 이번 연구에서는 대규모 에이전트 기반 모델(ABMs)의 시뮬레이션, 보정, 분석을 위한 프레임워크인 AgentTorch를 소개합니다. 특히, Large Language Models(LLMs)을 활용하여 이러한 대규모 시뮬레이션을 더욱 유연하고 현실적으로 구현할 수 있는 가능성을 확인합니다. AgentTorch는 수백만 명의 에이전트가 포함된 시뮬레이션을 효율적으로 처리할 수 있도록 설계되었습니다.
- **Technical Details**: Agent-based models(ABMs)은 복잡한 시스템의 동적 상호작용을 시뮬레이션하여 다양한 문제를 해결합니다. 이 연구에서는 LLM-agents가 이러한 ABM 시뮬레이션에서 인구 수준의 행동 통계를 재현할 수 있는지 평가하고, LLM-output을 기반으로 에이전트 집합을 재현하는 'LLM-archetype' 개념을 도입합니다. 이는 뉴욕시를 대표하는 840만 에이전트의 사례 연구를 통해 살펴봅니다. 또한, 시뮬레이션에서 사용되는 환경 동역학을 완전히 미분 가능하게 설정하여 더욱 상세한 분석이 가능하게 했습니다.
- **Performance Highlights**: AgentTorch는 기존의 ABM 시뮬레이션의 한계를 극복하여 수백만 명의 에이전트가 포함된 복잡한 시스템을 효율적으로 모델링할 수 있습니다. 특히, LLM 기반의 에이전트와 더 유연한 환경 설계를 통해, 정책 설계 및 분석에 유용한 높은 현실감을 제공합니다. 이번 연구는 뉴욕시를 기반으로 한 대규모 시뮬레이션에서 높은 정확성과 성능을 보였습니다.

### [SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction](https://arxiv.org/abs/2409.11211)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11211.png)

Vote: 6

Authors: Federica Bogo, Sergey Prokudin, Siyu Tang, Robert Maier, Marko Mihajlovic, Edmond Boyer, Tony Tung

- **What's New**: 최근 Arxiv에 게재된 논문에서 최신 인공지능 모델의 성능 향상에 대한 흥미로운 발견을 발표했습니다. 이번 연구는 딥러닝(deep learning) 알고리즘의 효율성을 높이기 위한 새로운 방법을 제안하고 있으며, 특히 자연어 처리(NLP) 분야에서 그 성능이 두드러집니다.
- **Technical Details**: 논문에서는 Transformer 아키텍처의 변형을 통해 성능을 극대화하는 방법을 탐구했습니다. 이 새로운 변형은 기존의 Self-Attention 메커니즘을 개선하여 연산 복잡도를 줄이고, 동시에 처리 속도를 향상시키는 것을 목표로 하고 있습니다. 또한, 새로운 데이터 증강(data augmentation) 기법이 소개되어 모델의 일반화 능력을 크게 향상시켰습니다.
- **Performance Highlights**: 성능 평가에서는 일반적으로 사용되는 벤치마크 데이터셋에서 최고 수준의 성능을 보였습니다. 특히, GLUE 벤치마크에서 새로운 모델은 이전 최고 성능을 뛰어넘으며 최고 점수(하드웨어 의존적 성능 개선 포함)를 기록했습니다. 이러한 결과는 실제 응용에서도 매우 실용적이며, 산업 전반에서의 적용 가능성을 높이고 있습니다.

### [Single-Layer Learnable Activation for Implicit Neural Representation (SL$^{2}$A-INR)](https://arxiv.org/abs/2409.10836)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10836.png)

Vote: 4

Authors: Dorit Merhof, Hamid Soltanian-Zadeh, Moein Heidari, Ilker Hacihaliloglu, Reza Rezaeian, Reza Azad

- **What's New**: 본 논문은 INRs(Implicit Neural Representations)의 계층적 표현 능력을 크게 향상시키는 새로운 접근 방식을 제안합니다. 학습 가능한 활성화 함수(learnable activation functions)를 도입하여 고주파(high-frequency) 및 세부 정보를 효과적으로 잡아내며, 이미지와 복잡한 3D 구조와 같은 다양한 작업에서 우수한 성능을 달성합니다.
- **Technical Details**: INRs(Implicit Neural Representations)는 Multilayer Perceptrons (MLPs)을 사용하여 연속적인 입력 좌표를 대응 출력 값으로 매핑합니다. 기존의 RIP(ReLU 기반 MLPs)는 낮은 주파수만을 배우는 경향이 있으며, 이는 복잡한 고주파 신호를 부족하게 학습하는 문제를 야기합니다. 이를 해결하기 위해 Chebyshev approximation method를 이용한 Learnable Activation (LA) Block을 제안하여 세부 정보와 고주파 성분을 효과적으로 포착합니다. 또한, 후속 ReLU 기반 MLPs의 Fusion Block에 skip connections를 추가하여 고주파 신호의 전달을 조절합니다.
- **Performance Highlights**: 제안된 방법은 신경 방사 필드(NeRF) 및 역 문제(super-resolution, inpainting, CT 복원 등)를 포함한 다양한 용도에서 높은 정밀도의 재구성을 달성합니다. 실험 결과는 제안된 방법이 기존 SOTA(State-of-the-Art) 아키텍처를 초과하여 더 많은 고주파 정보를 포착한다는 것을 명확하게 보여줍니다. 또한, 세부 설계 선택에 대한 통찰력을 제공하는 종합적인 소거 연구(ablation study)를 통해 제안된 방법의 효과를 입증합니다.

### [Implicit Neural Representations with Fourier Kolmogorov-Arnold Networks](https://arxiv.org/abs/2409.09323)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.09323.png)

Vote: 3

Authors: Moein Heidari, Ilker Hacihaliloglu, Ali Mehrabian, Parsa Mojarad Adi

- **What's New**: 새로운 논문은 기존의 제한사항을 극복하고, 이미지 표현 및 3D 구조 모델링 작업에서 신호 재구성을 향상시키기 위한 Fourier Kolmogorov-Arnold 네트워크(FKAN)를 제안합니다. 이 네트워크는 임의의 주파수 성분을 효과적으로 캡처하여 전반적인 성능을 개선합니다.
- **Technical Details**: FKAN은 Kolmogorov-Arnold 네트워크(KAN)에서 영감을 받아 고차원의 주파수 구성 요소를 포착할 수 있도록 Fourier 계수를 학습가능한 활성화 함수로 사용합니다. 이는 신호의 고주파 및 저주파 요소를 모두 효과적으로 표현할 수 있도록 돕습니다. 네트워크 파라미터는 신경망이 예측한 값과 실제 신호 간의 오차를 최소화하는 방향으로 최적화됩니다. 이 논문에서는 L2 손실 함수를 사용하여 학습을 진행했습니다.
- **Performance Highlights**: FKAN은 이미지 표현 및 점유 부피 표상(occupancy volume representation) 작업에서 동급 최고 수준의 성능을 보이며, 최고 신호 대 잡음 비율(PSNR)과 구조적 유사 지수(SSIM)를 각각 최대 8.91%와 5.62%까지 향상시켰습니다. 또한, 점유 부피 표상 작업에서는 교차 지표(IoU)를 최대 0.96%까지 향상시켰습니다. 더욱이, FKAN은 기존 모델들보다 빠른 수렴 속도를 보여주었습니다.

### [PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing](https://arxiv.org/abs/2409.10831)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10831.png)

Vote: 3

Authors: Taylor Berg-Kirkpatrick, Phillip Long, Julian McAuley, Zachary Novack

- **What's New**: 최근 상징적 도메인과 오디오 도메인 음악 생성(Generative Music Systems) 시스템 개발이 폭발적으로 증가했습니다. 이러한 발전은 상업적 투자와 법적 논쟁을 불러오며, 특히 현대 저작권법과의 상호작용에 대해 논란이 많습니다. 이를 해결하기 위해 공공 영역 데이터(public domain data)를 사용해 교육하는 대안이 제시되었으며, PDMX는 MuseScore에서 수집한 250,000개 이상의 Public Domain MusicXML 파일로 구성된 대규모 데이터셋을 소개합니다. 이는 최대 규모의 상징적 음악 데이터셋으로, 모든 파일에는 장르, 태그, 설명, 인기도 메타데이터가 포함되어 있습니다.
- **Technical Details**: MIDI 형식이 상징적 음악 구문(task)에 유용하지만, MusicXML과 같은 더 구조화된 형식에는 표현 지시(directives), 시간 텍스트, 섹션 경계 등의 추가 정보가 포함되어 있습니다. 이를 해결하기 위해 MusPy 라이브러리를 확장한 MusicRender를 소개합니다. MusicRender는 퍼포먼스 큐(cues)를 포함한 다양한 MusicXML 특정 속성을 정확하게 구문 분석합니다. 또한, 데이타셋 필터링에 기반한 메타데이터를 사용하여 무조건적인 다중 트랙 음악 생성을 연구에 적용했습니다.
- **Performance Highlights**: PDMX는 CC-BY 라이선스로 무료로 제공되며, 교육 및 모델 미세 조정에 사용할 수 있는 고품질 하위 집합을 용이하게 만듭니다. 행복감 지수(Pitch Class Entropy, PCE), 스케일 일관성(Scale Consistency, SC), 그루브 일관성(Groove Consistency, GC) 같은 다양한 지표를 사용하여 데이터 품질을 평가한 결과, 평가된 노래는 더 높은 조화변화를 보였습니다. MusicRender는 JSON, MIDI와 같은 여러 파일 형식 및 프로그래매틱 오디오 합성을 지원합니다.

### [Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse](https://arxiv.org/abs/2409.11242)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11242.png)

Vote: 1

Authors: Soujanya Poria, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Maojia Song, Shang Hong Sim

- **What's New**: LLMs(대규모 언어 모델, Large Language Models)에 의해 생성되는 정보의 정확성에 대한 우려가 커지고 있는 가운데, RAG(검색 강화 생성, Retrieval-Augmented Generation) 프레임워크를 통해 이러한 문제를 해결하려는 연구가 활발하다. 이번 연구에서는 LLM의 신뢰성을 평가할 수 있는 새로운 지표인 Trust-Score을 제안하며, 이를 통해 모델의 진실성(trustworthiness)을 높이기 위한 프레임워크인 Trust-Align을 도입했다.
- **Technical Details**: Trust-Score은 LLM의 신뢰성을 측정하기 위한 종합적 메트릭으로, 다음과 같은 여러 차원을 평가한다: 1) 문서 기반으로 답변할 수 있는 질문을 구분할 수 있는 능력 (Grounded Refusals), 2) 정확한 답변을 제공하는 능력 (Exact Match Recall), 3) 생성된 주장이 관련 인용(Reference)에 의해 뒷받침되는 정도 (Citation Recall), 4) 인용의 적절성 (Citation Precision). Trust-Align 프레임워크는 19,000개의 질문, 문서 및 긍정적/부정적 응답을 포함하는 정렬 데이터셋을 구축해 LLM을 훈련하며, 이를 통해 비현실적인 답변을 줄이고 신뢰성을 높인다.
- **Performance Highlights**: Trust-Align을 통해 학습된 모델은 ASQA, QAMPARI, ELI5 벤치마크에서 기존 베이스라인 모델 대비 Trust-Score 지표에서 각각 10.73%, 29.24%, 14.88% 향상되었다. 특히, 신뢰성 향상 및 거절 응답의 정확성이 9.87%(ASQA), 22.53%(QAMPARI), 5.32%(ELI5) 증가했으며, 인용 품질도 개선되어 Citation Groundedness 점수가 26.67%(ASQA), 31.96%(QAMPARI), 29.30%(ELI5) 상승했다.

