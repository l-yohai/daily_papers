## Daily Papers (2024-09-16)

### [InstantDrag: Improving Interactivity in Drag-based Image Editing](https://arxiv.org/abs/2409.08857)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08857.png)

Vote: 11

Authors: Daehyeon Choi, Joonghyuk Shin, Jaesik Park

- **What's New**: 최근 빠른 ODE 솔버, 일관성 증류, ODE 궤적 직선화, 적대적 학습 등 다양한 연구가 있었지만, 실제 이미지를 편집하는 데는 여전히 느리고 덜 효율적입니다. 이에 대한 해결책으로, InstantDrag를 제안합니다. 이 방법은 최적화 없이 이미지를 빠르게 편집하고, 사용자 입력 마스크와 텍스트 프롬프트의 필요성을 제거합니다.
- **Technical Details**: InstantDrag는 이미지와 드래그 지시를 입력받아 작동하는 파이프라인입니다. 드래그 편집 작업을 모션 생성과 모션 조건부 이미지 생성으로 분리하고, 각각의 작업은 서로 다른 생성 모델에 할당됩니다. FlowGen는 사용자 입력으로부터 고밀도 옵티컬 플로를 생성하고, FlowDiffusion는 최적화 없이 높은 품질의 편집을 수행합니다. 이 방법은 문장 인코더를 제거하여 아키텍처를 단순화하고 중복된 컴포넌트를 없앴습니다.
- **Performance Highlights**: InstantDrag는 실험 결과에서 실제 이미지에 대해 약 75배 더 빠르고, 최대 5배 더 적은 GPU 메모리를 소비합니다. 이를 통해 포토리얼리스틱(photo-realistic) 편집을 빠르게 수행할 수 있습니다. 또한 사용자 입력 마스크나 텍스트 프롬프트가 필요하지 않습니다.

### [Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos](https://arxiv.org/abs/2409.08353)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08353.png)

Vote: 6

Authors: Yuheng Jiang, Lan Xu, Zhehao Shen, Chengcheng Guo, Yu Hong, Yize Wu, Yingliang Zhang, Jingyi Yu

- **What's New**: 디지털 세계와 현실 세계의 경계가 점점 희미해짐에 따라 3D 및 4D 콘텐츠가 빠르게 발전하면서 사회적 기대와 디지털 환경 전반의 응용 프로그램을 변화시키고 있습니다. 그중에서도 볼륨매트릭 비디오(volumetric videos)는 시청자에게 여섯 가지 자유도(six degrees of freedom)를 제공하여 사용자들이 가상 환경 내에서 자유롭게 탐험할 수 있게 합니다. 본 논문에서는 DualGS라는 새로운 가우시안 기반(Dual Gaussians) 볼륨매트릭 비디오 표현 방식을 제안하여 더 높은 시간 일관성과 추적 정확도를 달성하고 효과적인 압축 전략을 가능하게 합니다. 이 접근 방식은 프레임당 약 350KB의 저장 공간만 필요로 하여 저장 효율성을 크게 향상시킵니다.
- **Technical Details**: DualGS는 모션과 외모를 분리하고 계층화된 방식으로 표현하는 Dual Gaussians를 활용합니다. 이를 통해 추적 정확도와 시간 일관성을 높이고, 가우시안 속성의 명시적 분리를 통해 최적화 과정을 개선합니다. 첫 번째 프레임에서 Joint Gaussians를 무작위로 초기화하고, 이를 기반으로 Skin Gaussians가 초기화됩니다. 각 Skin Gaussian은 여러 Joint Gaussians에 앵커되어 위치와 회전의 보간을 가능하게 합니다. 프레임별 인간 성능 추적을 위해 coarse-to-fine 최적화 전략이 사용됩니다. Coarse 정렬 단계에서는 Joint Gaussians에만 최적화를 수행하며, Fine 정교화 단계에서는 자세한 위치와 외모를 미세 조정합니다. 이 과정은 명시적 DualGS 표현 덕분에 모션과 외모 속성을 효과적으로 분리 및 압축할 수 있습니다.
- **Performance Highlights**: DualGS 접근 방식은 원래 3DGS에 비해 최대 120배 압축률을 달성합니다. 여러 4D 자산을 실시간 렌더링을 위해 VR 환경에 원활하게 통합할 수 있게 하여, 사용자들이 라이브 공연을 가상으로 경험할 수 있습니다. 이 방식은 다양한 도전적인 케이스에서도 높은 렌더링 품질과 시간 일관성을 지속적으로 제공합니다.

### [A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis](https://arxiv.org/abs/2409.08947)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08947.png)

Vote: 5

Authors: Alban Gauthier, George Drettakis, Julien Phillip, Jean-Francois Lalonde, Yohan Poirier-Ginter

- **What's New**: 최근 Radiance fields(복사장)이 3D 장면 캡처에 혁신을 가져왔습니다. 이 논문에서는 Diffusion Models(확산 모델)을 활용하여 단일 저주파 조명 조건에서 전체 장면을 실시간으로 재조명할 수 있는 새로운 방법을 제안합니다. 이를 통해 복사장을 재조명이 가능한 장으로 변환할 수 있습니다.
- **Technical Details**: 기존의 방식은 다중 조명 데이터를 포착하려면 무거운 캡처 설정이 필요하거나, 신경망을 사용하여 합성 데이터를 예측하는 방식이었습니다. 그러나 이 논문에서는 사전 훈련된 Diffusion Model을 주조명 방향으로 미세 조정하고, 이를 사용하여 다중 조명 데이터를 생성합니다. 그런 다음 생성된 데이터셋을 사용하여 3D Gaussian Splatting을 기반으로 한 새로운 재조명 가능한 복사장을 훈련시킵니다.
- **Performance Highlights**: 제안된 방법은 단일 조명 조건에서 다중 뷰 데이터셋을 실제 실내 장면으로 확대하여 실시간으로 재조명을 가능하게 합니다. 이를 통해 조명 방향과 카메라 뷰를 제어할 수 있으며, 생성된 조명 간의 불일치를 처리하는 소형 Multi-Layer Perceptron와 보조 피처 벡터를 사용합니다. 실제 및 합성 실내 장면 모두에서 실시간으로 구현된 재조명 결과를 보여줍니다.

### [DrawingSpinUp: 3D Animation from Single Character Drawings](https://arxiv.org/abs/2409.08615)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08615.png)

Vote: 5

Authors: Miu-Ling Lam, Hongbo Fu, Jie Zhou, Chufeng Xiao

- **What's New**: DrawingSpinUp는 아마추어 캐릭터 드로잉에서 3D 애니메이션을 생성하는 첫 번째 3D-인식 애니메이션 시스템을 도입했습니다. 이는 기존의 2D 애니메이션 방법을 넘어 3D 움직임과 뷰포인트 자유도를 제공합니다.
- **Technical Details**: DrawingSpinUp는 윤곽선(contour lines)을 인식하고 별도로 처리하여 사전 학습된 이미지-to-3D 모델의 재구성 사전(reconstruction prior)에 적응시키는 전략으로 작동합니다. 먼저 네트워크를 설계하여 캐릭터 드로잉에서 윤곽선을 제거하고, 제거된 영역에 텍스처를 복원합니다. 그런 다음, 사전 학습된 이미지-to-3D 모델을 사용하여 텍스처가 적용된 기하학적 구조를 3D 모션 리타게팅 대신 프록시로 재구성합니다. 마지막으로 프레임별 윤곽선을 렌더링하는 지오메트리-인식 스타일화 네트워크를 제안하여 내부 텍스처를 입력 이미지의 스타일과 일치시키고, 뼈대 기반의 슬림 변형 알고리즘을 개발하여 세밀한 구조를 정제합니다.
- **Performance Highlights**: DrawingSpinUp는 Amateur Drawing Dataset에서 다양한 스타일의 120개의 아마추어 캐릭터 드로잉 샘플을 사용하여 테스트되었습니다. 광범위한 실험 결과와 지각적 사용자 연구에서 단일 캐릭터 드로잉에서 만족스러운 3D 재구성과 생생한 3D 애니메이션을 생성하며, 기존의 2D 및 3D 애니메이션 방법보다 우수한 성능을 보였습니다.

### [Apollo: Band-sequence Modeling for High-Quality Audio Restoration](https://arxiv.org/abs/2409.08514)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08514.png)

Vote: 4

Authors: Yi Luo, Kai Li

- **What's New**: 이번 논문에서는 신규 오디오 복원 모델인 Apollo를 제안합니다. 이 모델은 고차 샘플링률 오디오 복원을 위해 설계되었으며, 다양한 압축률에 따른 오디오 품질을 복원할 수 있습니다.
- **Technical Details**: Apollo는 주파수 대역 분할 모듈, 주파수 대역 시퀀스 모델링 모듈, 주파수 대역 재구성 모듈의 세 가지 주요 모듈로 구성됩니다. 주파수 대역 시퀀스 모델링 모듈에서는 Roformer와 TCN(Temporal Convolutional Network)을 사용하여 더 효율적인 오디오 복원이 가능합니다. 모델은 STFT(Short-Time Fourier Transform)를 사용해 시간-주파수 도메인으로 변환 후, 역 STFT(iSTFT)를 통해 파형(waveform)을 복원합니다.
- **Performance Highlights**: Apollo 모델은 MUSDB18-HQ와 MoisesDB 데이터셋에서 SR-GAN과 같은 최첨단 모델들과 비교하여 뛰어난 성능을 보였습니다. 특히, 여러 악기와 보컬이 혼합된 복잡한 시나리오에서 뛰어난 복원 성능을 발휘했습니다. 또한, 스트리밍 오디오 애플리케이션에서도 효율성이 검증되어 실시간 고품질 오디오 복원에 잠재력을 보여줍니다.

### [Mamba-YOLO-World: Marrying YOLO-World with Mamba for Open-Vocabulary Detection](https://arxiv.org/abs/2409.08513)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08513.png)

Vote: 3

Authors: Mingmin Chi, Qingdong He, Haoxuan Wang, Jinlong Peng, Hao Yang, Yabiao Wang

- **What's New**: 이번 연구에서는 YOLO 기반의 새로운 Open-Vocabulary Detection(OVD) 모델인 Mamba-YOLO-World를 소개합니다. 기존 YOLO-World의 한계를 해결하기 위해 MambaFusion Path Aggregation Network(MambaFusion-PAN)를 도입하여 성능과 범용성을 크게 향상시켰습니다.
- **Technical Details**: Mamba-YOLO-World 모델은 YOLOv8과 Darknet Backbone, CLIP Text Encoder, MambaFusion-PAN을 통해 다양한 크기의 이미지와 텍스트 피처를 통합합니다. 제안된 MambaFusion-PAN은 State Space Model(SSM)에 기반한 페러럴 및 시리얼 피처 융합 메커니즘을 사용하여 다중 모달 피처를 효율적으로 융합합니다. SSM은 입력 시퀀스의 길이에 비례하여 복잡도가 증가하지 않도록 설계되어, O(N+1)의 낮은 복잡도로 글로벌 가이드 수용 필드를 제공합니다. 또한, Mamba-YOLO-World는 텍스트 대 이미지, 이미지 대 텍스트, 그리고 다시 텍스트 대 이미지로 구성된 3단계 피처 융합 과정을 진행합니다.
- **Performance Highlights**: 제안된 Mamba-YOLO-World는 기존 YOLO-World에 비해 정확도와 일반화 성능에서 크게 앞서는 동시에 유사한 파라미터 수와 FLOPs를 유지합니다. 또한, 기존의 최첨단 OVD 방법들보다 적은 파라미터와 FLOPs로도 우수한 성능을 입증하였습니다.

### [Click2Mask: Local Editing with Dynamic Mask Generation](https://arxiv.org/abs/2409.08272)

![](/avatars/4f728a5b70c9fe4a64e80e2b643ca620.svg)

Vote: 2

Authors: Dani Lischinski, Omer Regev, Omri Avrahami

- **What's New**: 새로운 논문에서 제시된 Click2Mask는 사용자의 상호작용을 단순화하고, 정밀한 마스크(detailed mask)나 설명적인 프롬프트 없이도 원하는 위치에 콘텐츠를 추가할 수 있는 혁신적인 접근법입니다. 한 지점을 클릭하는 것만으로 마스크가 동적으로 생성되고, Blended Latent Diffusion(BLD) 과정과 Alpha-CLIP를 사용해 진화하며, 명확하고 맥락적으로 적절한 지역 편집을 가능하게 합니다.
- **Technical Details**: Click2Mask는 사용자가 제공하는 하나의 지점을 기반으로 동적으로 진화하는 마스크를 생성합니다. 이 마스크는 Blended Latent Diffusion(BLD) 과정과 Alpha-CLIP를 통해 가이드되며, 기존 오브젝트의 경계에 얽매이지 않고 자유롭게 콘텐츠를 추가할 수 있습니다. 이로 인해 세분화(segmentation) 기반의 방법이나 세밀한 마스크 생성을 요구하는 기존 방법들 대비, 사용자 부담을 크게 줄였습니다.
- **Performance Highlights**: 실험 결과, Click2Mask는 사용자 노력을 줄이면서도 최첨단 방법들과 비교해 경쟁력 있는 또는 보다 우수한 결과를 보여주었습니다. 특히, 기존의 방법들이 겪는 지역 제한이나 세그먼트 경계 문제를 극복하고, 자유로운 형태의 오브젝트 추가를 실현하였습니다.

