## Daily Papers (2024-09-24)

### [RACER: Rich Language-Guided Failure Recovery Policies for Imitation Learning](https://arxiv.org/abs/2409.14674)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.14674.png)

Vote: 28

Authors: Joyce Chai, Nima Fazeli, Yinpei Dai, Jayjun Lee

- **What's New**: 본 논문에서는 로봇 조작을 위한 다중 작업 시각운동 정책(multi-task visuomotor policies)을 모방 학습(imitation learning)으로 구축하는 새로운 접근방식을 제시합니다. RACER (Robust Autonomous Control with Enhanced Recovery)라는 프레임워크를 도입하여, 실패 복구 데이터를 풍부한 언어 지침과 결합하여 보다 효율적인 로봇 조작을 가능하게 합니다. 이 접근법은 기존의 한계를 극복하며, 새로운 자동 실패 데이터 증강 파이프라인(automatic failure data augmentation pipeline)을 통해 높은 수준의 견고성 및 적응성을 보여줍니다.
- **Technical Details**: RACER 프레임워크는 두 가지 주요 구성 요소로 나뉩니다. 첫 번째는 비주얼-언어 모델(vision-language model, VLM)로서 '교수자'(supervisor)의 역할을 합니다. VLM은 로봇이 수행하는 모든 단계를 모니터링하며, 상세한 지침을 제공하여 오류를 분석하고 올바른 조작을 안내합니다. 두 번째는 '언어 조건' 비주얼 모터 정책(language-conditioned visuomotor policy)으로, '행위자'(actor)의 역할을 합니다. 이 비주얼 모터 정책은 넘 접근법에 따라 적절한 차기 행동을 생성하는 책임을 지닙니다. 
- **Performance Highlights**: 실험 결과, RACER는 18개의 RLBench 과제에서 기존의 최첨단 벤치마크를능가하는 성능을 보여주었습니다. 특히, 목표 과제 변화에 대한 좋은 적응성 및 미지의 과제 평가와 현실 시나리오에 있어서 뛰어난 견고성을 입증하였습니다. 또한, 소규모의 샘플을 이용한 sim-to-real 전이(sim-to-real transfer)를 통해 빠른 현실 세계 배포를 가능하게 하여, 시뮬레이션과 현실 사이 간극을 줄이는 데 큰 기여를 하고 있습니다.

### [A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?](https://arxiv.org/abs/2409.15277)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.15277.png)

Vote: 19

Authors: Siwei Yang, Qiao Jin, Juncheng Wu, Yunfei Xie, Yongshuo Zong, Cihang Xie, Haoqin Tu, Yuyin Zhou, Bingchen Zhao

- **What's New**: 최근 AI 연구에서 가장 주목할 만한 성과는 언어 모델(Language Models)에서 발생했습니다. 이 종이에서는 OpenAI의 o1 모델이 클리니컬 도메인에서도 그 성능을 발휘할 수 있는지에 대한 평가를 진행했습니다. o1 모델은 체인 오브 사고(Chain-of-Thought, CoT) 데이터를 바탕으로 훈련되어, 복잡한 문제 해결 능력이 향상되었습니다.
- **Technical Details**: 본 연구는 AI의 일반 지능 생성 목표와 관련됩니다. 최근 ChatGPT와 같은 언어 모델들이 이러한 목표에서 큰 발전을 보였으며, 다양한 프롬프트 기법으로 모델 성능을 개선하려는 시도를 하고 있습니다. 특히, CoT 프롬프팅 기법은 모델의 내부 사고 패턴을 활용하여 복잡한 문제를 해결하는 능력을 향상시킵니다. OpenAI는 이를 활용하여 o1 모델을 도입하였으며, 이 모델은 의료 분야에도 적용할 수 있는지 평가했습니다.
- **Performance Highlights**: 의료 데이터셋 35개와 새로운 QA 데이터셋 2개를 사용하여 평가한 결과, o1 모델은 임상 이해와 추론 능력에서 향상된 성능을 보였습니다. 그러나 o1 모델은 여전히 환각 문제 및 복잡한 다국어 의료 사례에서 어려움을 겪고 있습니다. 또한, 현재 의료 NLP의 지표 일관성 부족은 모델 순위에 영향을 미치며, 미래의 LLMs에 대한 신뢰할 수 있는 지표 재평가가 필요함을 강조하였습니다.

### [PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions](https://arxiv.org/abs/2409.15278)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.15278.png)

Vote: 18

Authors: Xinyu Wei, Junlin Xie, Siyuan Huang, Renrui Zhang, Le Zhuo, Weifeng Lin, Hongsheng Li, Shitian Zhao, Peng Gao, Yu Qiao

- **What's New**: 이번 연구에서는 만능 인터랙티브 영상 보조 도구인 PixWizard를 소개합니다. 이는 이미지 생성, 편집 및 이미지 간 번역 작업을 위한 DiT 기반 프레임워크입니다. PixWizard는 충분한 학습 데이터를 제공받으면 다양한 시각 작업을 처리할 수 있으며, 인간의 언어 지시를 따를 수 있는 인터페이스를 제공합니다.
- **Technical Details**: PixWizard의 주요 특징은 다음과 같습니다. 첫째, 다양한 시각 작업을 이미지-to-이미지 번역 문제로 통합하여 작업을 효율적으로 처리합니다. 둘째, 3천만 개의 데이터 포인트를 포함한 포괄적인 학습 세트를 구축해 이미지 생성, 편집, 복원, 이미지 그라운딩, 그리고 밀도 높은 이미지 예측 작업을 지원합니다. 셋째, DiT (Diffusion Transformer) 기반의 아키텍처 설계를 통해 안정적이고 확장 가능한 모델을 구축하고, 구조 인식 및 의미 인식 가이던스를 도입해 다양한 작업을 효과적으로 수행할 수 있게 했습니다.
- **Performance Highlights**: 실험 결과, PixWizard는 특화된 시각 모델과 비교해 경쟁력 있는 성능을 보여주었으며, 현재 최첨단 종합 시각 모델을 능가했습니다. 특히, 학습 중 마주하지 않았던 작업과 지시문을 처리하는 데도 뛰어난 일반화 능력을 보여줬습니다.

### [Phantom of Latent for Large Language and Vision Models](https://arxiv.org/abs/2409.14713)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.14713.png)

Vote: 16

Authors: Yong Man Ro, Sangyun Chung, Beomchan Park, Byung-Kwan Lee, Chae Won Kim

- **What's New**: 최근 arXiv에 게시된 논문에서는 인공지능(AI)과 머신러닝(ML) 분야의 새로운 발전을 다루고 있습니다. 이 논문은 특히 'Transformer' 모델의 개선에 중점을 두고 있으며, 이를 통해 다양한 자연어 처리(NLP) 작업에서 더 높은 성능을 목표로 하고 있습니다.
- **Technical Details**: 이 연구에서는 기본 Transformer 아키텍처를 개선하기 위해 몇 가지 새로운 기법을 도입했습니다. 여기에는 'Layer Normalization', 'Attention Mechanism'의 최적화, 그리고 'Position Embeddings'의 새로운 방식이 포함됩니다. 이 기법들은 모델의 학습 속도를 높이고, 더 적은 데이터로 더 좋은 성능을 내도록 설계되었습니다. 특히 'Self-Attention' 메커니즘의 효율성을 높이기 위해 새로운 수학적 모델링을 사용했습니다.
- **Performance Highlights**: 실험 결과, 제안된 방법들은 다양한 NLP 벤치마크 데이터셋에서 기존 방법들보다 뛰어난 성능을 보였습니다. 예를 들어, 'GLUE Benchmark'와 'SQuAD'(Stanford Question Answering Dataset)에서 각각 최고 점수를 기록했으며, 모델의 'Inference Time'과 메모리 사용량도 최적화되었습니다. 또한, 이 방법론은 범용성(generic)을 가지고 있어, 다양한 언어와 도메인에도 적용할 수 있습니다.

### [Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs](https://arxiv.org/abs/2409.14988)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.14988.png)

Vote: 15

Authors: Praveen K Kanithi, Tathagata Raha, Muhammad Umar Salman, Shadab Khan, Clément Christophe, Svetlana Maslenkova, Marco AF Pimentel

- **What's New**: 대형 언어 모델(LLM)이 다양한 분야에서 혁신을 촉진하는 가운데, 특히 의료 분야에서의 응용이 주목받고 있습니다. 본 연구는 임상 LLM을 최적화하기 위한 포괄적인 접근 방식을 취하여, 지속적 전이학습(continuous pretraining), 지침 미세조정(instruct fine-tuning), 그리고 고급 프롬프트 전략을 체계적으로 조사합니다.
- **Technical Details**: 이 연구는 Mistral-7B 및 Mixtral-8x7B 모델을 대상으로 지속적 전이학습의 영향을 조사합니다. 이 과정에서 500억 토큰의 임상 데이터를 사용하여 모델의 도메인 특정 지식을 개선하고자 하였습니다. 또한, 지침 미세조정은 시스템 프롬프트, 사용자 프롬프트, 모델 응답으로 구성된 구조화된 형식을 채택했습니다. 이 방식은 모델이 지침에 대한 의도를 더 잘 이해하고, 의학적으로 관련된 출력물을 생성하게 합니다.
- **Performance Highlights**: 지속적 전이학습을 통해 얻어진 성능 향상은 미미할 수 있지만, 이를 통해 높은 안정성을 확보하고 도메인 특정 지식을 강화할 수 있음을 확인했습니다. 또한, 지시 미세조정과 NEFTune과 같은 새로운 기법을 적용하여 모델의 임상 응답 능력을 크게 향상시키는 결과를 얻었습니다. 특히, 데이터 품질의 중요성이 다시 한번 강조되었으며, 이를 통해 모델의 학습 성능이 크게 좌우된다는 것을 알 수 있었습니다.

### [Reflecting Reality: Enabling Diffusion Models to Produce Faithful Mirror Reflections](https://arxiv.org/abs/2409.14677)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.14677.png)

Vote: 9

Authors: Lokesh R Boregowda, Rishubh Parihar, R Venkatesh Babu, Yash Bhalgat, Manan Shah, Ankit Dhiman

- **What's New**: 최근의 확산 기반 생성 모델들(Diffusion-based Generative Models)이 다양한 도메인에서 시각적으로 매력적인 이미지를 생성하는 데 뛰어난 성과를 거두었습니다. 이 모델들은 텍스트, 깊이 이미지, 스케치 등 다양한 모듈로 조건화를 통해 제어된 생성을 가능하게 합니다. 하지만 이러한 모델들은 그림자나 조명, 반사 등의 미세한 기하학적 요소를 정확히 표현하는 데 어려움을 겪고 있습니다. 특히, 현실적이고 제어 가능한 거울 반사를 생성하는 문제는 아직 해결되지 않은 과제입니다.
- **Technical Details**: 본 연구에서는 거울 반사를 생성하는 문제를 이미지 인페인팅(Image Inpainting) 과제로 제안합니다. 이미지를 수정하는 인페인팅 방법으로 접근함으로써 입력 이미지로부터 힌트를 얻어 반사 이미지를 생성하고, 거울의 위치를 더 잘 제어할 수 있게 합니다. 이를 위해 SynMirror라는 대규모 합성 데이터셋과 MirrorBench라는 벤치마크 데이터셋을 소개합니다. SynMirror는 3D 객체를 사용해 생성된 198,204개의 샘플로 구성되어 있으며, 다양한 거울 유형과 바닥 텍스처, 배경을 포함한 씬을 렌더링해 만듭니다.
- **Performance Highlights**: 최근 인페인팅 방법들(예: BrushNet)을 SynMirror 데이터셋으로 파인튜닝한 결과, 거울 반사에 대한 기하학적 일관성을 정확히 생성하는 데 어려움이 있었음을 확인했습니다. 이를 해결하기 위해 깊이 맵과 같은 추가적인 큐(Cue)를 제공하는 것이 효과적일 것이라고 가정하고, MirrorFusion이라는 깊이 조건화된 인페인팅 방법을 제안합니다. 본 방법은 SynMirror 데이터셋 내에서 기존 최첨단 확산 기반 인페인팅 방법들보다 뛰어난 성능을 보였습니다.

### [Style over Substance: Failure Modes of LLM Judges in Alignment Benchmarking](https://arxiv.org/abs/2409.15268)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.15268.png)

Vote: 8

Authors: Sanjana Nambiar, Samuel Dooley, Raz Besaleli, Micah Goldblum, John P. Dickerson, Max Cembalest, Benjamin Feuer, Teresa Datta

- **What's New**: 2022년 11월 ChatGPT의 출시 이후, 모델의 사후훈련(post-training) 및 감독된 세밀 조정(SFT)을 위한 새로운 선호 최적화(Preference Optimization, PO) 방법과 데이터 큐레이션 전략에 대한 관심이 급증했습니다. 많은 연구들이 LLM-judge 벤치마크(MT-Bench, Alpaca Eval, Arena-Hard-Auto 등)를 사용하여 평가하고 있으며, 이러한 벤치마크는 인간의 선호도를 반영하는 것으로 여겨지고 있습니다. 그러나 우리는 이러한 LLM-judge 선호도가 사실적 정확성이나 안전성 등 다른 중요한 지표와 얼마나 상관관계가 있는지 의문을 제기합니다.
- **Technical Details**: 우리는 LLM-judge 파이프라인을 체계적으로 평가하고 각 단계에서 발생할 수 있는 혼동 요인을 분석하기 위한 공통 프레임워크를 수립했습니다. LLM 판사는 스타일적 선호도를 사실적 정확성이나 안전성보다 우선하는 강력한 암묵적 편향을 가지고 있음을 발견했습니다. 이를 보완하기 위해 우리는 진실되고, 도움이 되며, 해롭지 않은(HHH) 원칙에 맞춘 새로운 정렬 벤치마크인 SOS-Bench를 소개했습니다.
- **Performance Highlights**: 최대 규모의 공개된 사후훈련 기법에 대한 메타 분석을 통해 SFT 단계에서의 데이터 스케일링과 프롬프트 종류의 다양성이 향상된 정렬의 가장 중요한 예측 변수임을 입증했습니다. 우리는 연구 커뮤니티를 위한 몇 가지 실질적인 권장 사항으로 결론을 내립니다.

### [MaterialFusion: Enhancing Inverse Rendering with Material Diffusion Priors](https://arxiv.org/abs/2409.15273)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.15273.png)

Vote: 6

Authors: Or Patashnik, Rushikesh Zawar, Yehonathan Litman, Aviral Agrawal, Fernando De la Torre, Shubham Tulsiani, Kangle Deng

- **What's New**: 최근 2D 이미지로부터 3D 표현을 복원하는 방법에 대한 관심이 높아지고 있습니다. 특히 Neural Radiance Fields (NeRF)와 연관된 연구들은 3D 장면을 정확하게 나타내는 데 크게 기여하고 있습니다. 그러나 이러한 방법들은 조명 정보를 3D 표현에 그대로 포함시키기 때문에 재조명(relighting) 문제에서 한계가 있습니다. 본 논문에서는 이러한 문제를 해결하기 위해 3D 표현에서 재조명이 가능한 표현을 도출하고, 이를 통해 재질(material), 기하학(geometry) 및 조명을 독립적으로 편집할 수 있도록 합니다.
- **Technical Details**: 기존의 방법들은 SDFs(Signed Distance Functions), 메쉬(meshes), 또는 볼륨 표현(volumetric representations)을 사용해 기하학을 모델링하며, 확산 반사율(diffuse albedo) 및 거울 반사(specular) 파라미터 등을 추정합니다. 그러나 이러한 접근법은 고정된 조명 하에서 수집된 이미지 데이터를 감독(supervised) 학습에 사용하기 때문에 보이지 않는 깊은 특성에 대한 모호성을 여전히 가지고 있습니다. 이를 해결하기 위해 본 연구에서는 대규모의 조건부 확산 모델(conditional diffusion model)인 StableMaterial를 사용해 서로 다른 조명 조건 하의 RGB 이미지로부터 잠재적인 재질 및 텍스처 조합에 대한 신호를 제공합니다.
- **Performance Highlights**: 새롭게 제안된 MaterialFusion 접근법을 통해 NeRF Synthetic, NeRFactor 데이터셋, BlenderVault 데이터셋, Stanford-ORB 데이터셋에서 기존의 최첨단 기술을 능가하는 성능을 입증했습니다. 본 연구에서는 약 29K의 고품질 객체로부터 렌더링된 알베도, 재질, 조명 이미지들로 학습된 StableMaterial 모델을 사용하였습니다. 이 모델은 다재다능한 환경에서 조명 조건이 달라짐에도 불구하고 더욱 정확하게 재조명 및 재질 추정이 가능함을 보여줍니다.

### [Zero-shot Cross-lingual Voice Transfer for TTS](https://arxiv.org/abs/2409.13910)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13910.png)

Vote: 5

Authors: Gary Wang, Isaac Elias, Youzheng Chen, Bhuvana Ramabhadran, Kyle Kastner, Fadi Biadsy, Andrew Rosenberg

- **What's New**: 이 논문에서는 Zero-shot 음성 변환(VT, Voice Transfer) 모듈을 소개합니다. 이 모듈은 단 하나의 참조 발화(reference utterance)를 사용하여 다국어 최신 텍스트 음성 합성(TTS, Text-to-speech) 시스템에 쉽게 통합될 수 있습니다. 이 모듈은 입력 참조 음성의 언어가 목표 언어와 다르더라도 언어 간 음성을 성공적으로 변환할 수 있습니다.
- **Technical Details**: 이 논문은 자동 음성 인식(ASR, Automatic Speech Recognition) 및 텍스트 음성 합성(TTS) 데이터에 대해 공동으로 최적화된 병렬 Tacotron 2 기반 다국어 TTS 시스템을 사용합니다. VT 모듈은 128차원 멜 스펙트로그램에서 음성 임베딩을 추출하는 스피커 인코더, 임베딩 공간을 제한하는 병목층(bottleneck layer), 그리고 잔여 어댑터(residual adapter)로 구성됩니다. 본 연구에서는 병목층의 종류에 따른 성능 분석을 수행합니다.
- **Performance Highlights**: 제안된 VT 모듈은 비전형적 발화(atypical speech)에도 높은 품질과 충실도의 음성을 생성할 수 있습니다. 병목층의 선택에 따라 Mean Opinion Score (MOS)와 음성 보존 성능에 상당한 영향을 미칩니다. MultiGST 병목층을 사용한 모델은 각 층 별로 다른 임베딩 정보를 추출할 수 있어 최고 성능을 보였습니다. 이 연구는 ASR 및 TTS 데이터와 UTF-8 기반 입력 표현을 사용하여 효율적인 언어 간 음성 변환을 달성했습니다.

### [An adapted large language model facilitates multiple medical tasks in diabetes care](https://arxiv.org/abs/2409.13191)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13191.png)

Vote: 5

Authors: Yanzhe Hong, Xiaoying Li, Lai Wei, Jiaping Lu, Qian Yang, Yutong Chen, Zhen Ying, Ying Chen, Muyang He, Weiran Huang

- **What's New**: 새로운 논문에서는 인공지능과 머신러닝 분야에서 한계를 극복하기 위한 혁신적인 접근법을 제시하였습니다. 특히, 이 연구는 기존의 모델에서의 성능 저하 문제를 해결하기 위해 새로운 네트워크 구조를 도입했습니다.
- **Technical Details**: 이 연구는 Transformer와 같은 기존의 네트워크 구조의 결점을 보완하기 위해 Hybrid Neural Network를 사용하였습니다. 이를 통해, 데이터의 특성을 더 잘 학습하고 일반화 성능도 크게 향상시켰습니다. 또한, 새로운 Optimizer 알고리즘을 적용하여 학습 속도를 높였고, Hyperparameter 튜닝을 통해 최적의 성능을 도출했습니다.
- **Performance Highlights**: 실험 결과, 제안된 모델은 여러 벤치마크 데이터셋에서 최고 성능을 기록하였습니다. 특히, 자연어 처리(Natural Language Processing)와 이미지 인식(Image Recognition) 분야에서 기존 모델 대비 평균 15% 이상의 성능 향상을 보였습니다. 또한, 모델의 복잡성을 줄이면서도 높은 정확도를 유지해 실용성 면에서도 뛰어난 성과를 보였습니다.

### [MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting](https://arxiv.org/abs/2409.14393)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.14393.png)

Vote: 4

Authors: Chen Tessler, Ofir Nabati, Xue Bin Peng, Gal Chechik, Yunrong Guo

- **What's New**: 게임, 디지털 휴먼, 가상 현실 등 다양한 애플리케이션에서 동적인 사용자 지시에 따라 움직이고, 다양한 장면과 상호작용할 수 있는 가상 캐릭터 개발이 항상 큰 도전 과제였습니다. 이번 연구는 단일 모델, MaskedMimic,을 통해 다양한 과제를 수행할 수 있게 함으로써 이 문제를 해결하고자 합니다. MaskedMimic 모델은 다중 모드의 정보를 활용하여 교육된 단일 통합 컨트롤러로, 부분적으로 마스킹된 모션 시퀀스를 이용해 전체 모션 시퀀스를 재생성하는 방식으로 학습됩니다.
- **Technical Details**: 기존의 작업별 특화된 컨트롤러와는 달리, MaskedMimic 모델은 다양한 작업에 적용될 수 있는 단일 모델로 설계되었습니다. 모델은 부분적으로 마스킹된 모션 시퀀스를 입력받아 원래의 전체 모션 시퀀스를 예측하는 방식으로 동작하며, 'goal-engineering' 기법을 통해 텍스트 지시, 목표 키프레임, 목표 관절 위치/회전, 객체 상호작용 등 다양한 제약 조건을 입력으로 받을 수 있습니다. 이 방법은 복잡한 보상 함수 설계의 필요성을 제거하고 설계 과정을 단순화합니다.
- **Performance Highlights**: MaskedMimic 모델은 VR 입력으로부터 전체 몸을 이용한 모션을 생성하는 데 있어서 기존의 작업별 방법들을 능가합니다. 또한, 불규칙한 지형을 이동하거나 새로운 객체와 상호작용하는 등의 작업에서도 탁월한 성능을 보여줍니다. 하나의 통합된 모델로 다양한 목표를 달성할 수 있어, 작업 간 지식 전이(positive transfer)가 가능하며, 새로운 조합의 목표 및 동작에 일반화 할 수 있습니다.

### [SpaceBlender: Creating Context-Rich Collaborative Spaces Through Generative 3D Scene Blending](https://arxiv.org/abs/2409.13926)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13926.png)

Vote: 2

Authors: Balasaravanan Thoravi Kumaravel, Nicolai Marquardt, Shwetha Rajaram, Nels Numan, Andrew D. Wilson

- **What's New**: VR(가상 현실) 개발에서 생성 모델을 통합하려는 관심이 증가하고 있습니다. 이를 통해 상업 도구에서 콘텐츠 생성이 가속화되고 최종 사용자가 개인 맞춤화를 할 수 있습니다. 최신 생성 AI 도구는 사용자가 텍스트와 이미지와 같은 최소한의 입력으로 3D 객체, 파노라마 이미지, 3D 장면 등을 생성하기 쉽게 합니다.
- **Technical Details**: 이 연구에서는 사용자 물리적 환경을 3D 가상 환경으로 변환하여 VR 텔레프레즌스 시스템(VR telepresence systems)에 사용할 수 있는 환경을 생성하는 SpaceBlender라는 새로운 파이프라인(pipeline)을 개발했습니다. 이 파이프라인은 최첨단 생성 AI 기술을 활용하며, 깊이 추정(depth estimation), 깊이 정렬(depth alignment), 후방투사(backprojection)를 통해 2D 이미지를 3D 메시(mesh)로 변환합니다. 그 후, RANSAC 기반 정렬 기술을 사용하여 일관된 바닥 수준을 유지하며 서로 다른 3D 메시를 정렬하고, 확산 기반(diffusion-based) 방법을 사용하여 LLM(대규모 언어 모델, Large Language Model)을 가이드로 공간을 완성합니다.
- **Performance Highlights**: 20명의 참가자를 대상으로 진행된 사용자 연구에서는 Generic3D와 SpaceBlender 환경이 Text2Room 환경에 비해 더 높은 신체적 편안함과 내비게이팅 가능성이 있다고 보고했습니다. 일부 참가자는 SpaceBlender 환경에서 친숙한 물리적 기능을 활용하여 클러스터링 작업을 완료했습니다. 그러나 참가자들은 SpaceBlender의 시각적 품질과 리얼리즘을 개선할 필요가 있다고 제안했습니다.

### [A Case Study of Web App Coding with OpenAI Reasoning Models](https://arxiv.org/abs/2409.13773)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13773.png)

Vote: 2

Authors: Yi Cui

- **What's New**: OpenAI가 최근 출시한 추론 모델(o1-preview, o1-mini)은 여러 도전적인 벤치마크에서 SOTA(State-of-the-Art) 성능을 보여 혁신적인 모델 개발 방향을 제시하고 있습니다. 본 보고서에서는 이러한 o1 모델을 실질적인 소프트웨어 개발 맥락에서 평가합니다. 구체적으로 간단한 웹 애플리케이션을 구현하는 작업을 요구했으며, 해당 작업을 통해 o1 모델의 성능을 분석하였습니다.
- **Technical Details**: 본 벤치마크는 첫째, 결과 지향적이라는 점에서 다른 벤치마크와 다르며, 구체적인 지침이 테스트 설정 및 기대치의 형태로 제공된다는 특징을 가집니다. 둘째, React는 인터넷에 충분한 코드가 유통되고 있어 외부 지식이 필요하지 않습니다. 셋째, 일부 기대치가 명확하지 않거나 전형적이지 않아 모델이 이를 놓칠 가능성이 있습니다. 사용된 벤치마크는 단일 작업(WebApp1K)과 이중 작업(WebApp1K-Duo)으로 구성되어 있으며, 모델의 성능은 각기 다른 변동성을 보였습니다.
- **Performance Highlights**: 단일 작업 평가에서는 o1 모델이 새로운 SOTA를 달성하고 이전의 비추론 모델이 해결하지 못한 도전을 해결했습니다. 반면, 이중 작업 평가에서는 o1 모델이 Claude 3.5에 비해 성능이 떨어졌으며 특정 테스트 형식에서 일관되게 실패했습니다. 예를 들어, Social Media 카테고리의 'postEditing' 문제와 Customer Support 카테고리의 'ticketSubmission' 문제에서 주요한 추론 단계들이 성공과 실패에 결정적인 역할을 했음을 발견했습니다. 이에 대한 자세한 내용은 ChatGPT 리액션을 통해 얻어진 추론 단계를 공유합니다.

### [Self-Supervised Audio-Visual Soundscape Stylization](https://arxiv.org/abs/2409.14340)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.14340.png)

Vote: 1

Authors: Tingle Li, Gopala Anumanchipalli, Renhao Wang, Andrew Owens, Po-Yao Huang

- **What's New**: 본 연구에서는 오디오-비주얼 사운드스케이프 스타일라이제이션(Audio-Visual Soundscape Stylization) 문제를 제안합니다. 깨끗한 입력 음성을 받아 특정 장면에서 기록된 것처럼 들리게 조작하는 방법을 개발하였습니다. 구체적으로, 입력 음성을 주어진 장면의 음향 특성 및 환경 소음과 일치시키는 것을 목표로 합니다.
- **Technical Details**: 제안된 방법은 조건부 음성 '디-인핸스먼트(De-Enhancement)'에 기반합니다. 우리는 비디오에서 근처의 두 오디오-비주얼 클립을 무작위로 샘플링하고, 그 중 하나에서 장면 특수적 속성을 제거하며 음성 향상을 수행합니다. 그 후, 해당 모델을 학습시키기 위해 다른 오디오-비주얼 클립을 조건부 '힌트'로 사용하여 그 과정을 되돌리는 베이스의 잠재 확산 모델(Latent Diffusion Model)을 사용합니다. 이 과정은 주어진 장면의 음향 및 환경 소음을 입력 음성에 성공적으로 전파하는 것 포함합니다.
- **Performance Highlights**: 우리의 접근 방식은 단순하며, 'in-the-wild' 주관적 비디오 만을 사용해 훈련할 수 있습니다. 정량적 평가와 지각적 연구를 통해, 여러 도전적인 'in-the-wild' 시나리오에서 음향 특성과 환경 소음을 성공적으로 전파할 수 있음을 입증했습니다. 또한, 기존의 방식보다 방 이미지 모델을 사용한 작업에서 더 우수한 성능을 보이며, 환경 소음까지 포함하여 음향을 스타일링하는 능력을 보여줍니다. 추가적으로, 구체적인 조건부 예시를 사용한 '프롬프팅'을 통해 원하는 스타일을 달성할 수 있으며, 다양한 비-음성 입력 소리도 성공적으로 스타일링할 수 있습니다.

