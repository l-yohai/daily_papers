## Daily Papers (2024-09-11)

### [GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering](https://arxiv.org/abs/2409.06595)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.06595.png)

Vote: 29

Authors: Sacha Muller, Bilel Omrani, Gautier Viaud, António Loison

- **What's New**: 이 논문은 기초 정보에 기반한 질문 응답(grounded question answering) 시스템의 평가 방법에 대한 새로운 접근법을 제안합니다. 기존의 평가 기준들이 일부 실험에서 잘 동작하지 않음을 언급하고, 다양한 실패 모드를 체계적으로 분석하여 새로운 자동 평가 파이프라인을 제안합니다. 또한 GPT-4를 심판으로 활용하고, Prometheus 및 기타 오픈소스 평가자들의 한계를 확인했습니다.
- **Technical Details**: 제안된 시스템은 GroUSE(Grounded QA Unitary Scoring of Evaluators)라는 새로운 메타-평가 벤치마크를 사용하여 평가됩니다. GroUSE는 144개의 세밀하게 큐레이션된 유닛 테스트로 구성되어 있으며, 다양한 실패 모드를 감지하고 구분하는 능력을 평가합니다. 또한 GPT-4의 평가 데이터를 기반으로 Llama-3 모델을 미세 조정하여 평가 캡퍼빌리티를 크게 향상시켰습니다.
- **Performance Highlights**: 논문에서는 새로운 평가 파이프라인이 기존 자동 평가 프레임워크보다 높은 오류 검출 정확도를 달성했다고 보고합니다. 특히, GPT-4의 평가 기준과 높은 일치도를 보여줬으며, 오픈소스 평가자들이 감지하지 못하는 실패 모드도 효과적으로 감지할 수 있음을 확인했습니다. 또한, Llama-3를 GPT-4의 평가 데이터를 이용해 미세 조정했을 때, 평가 성능이 크게 향상되었습니다.

### [LLaMA-Omni: Seamless Speech Interaction with Large Language Models](https://arxiv.org/abs/2409.06666)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.06666.png)

Vote: 24

Authors: Shaolei Zhang, Shoutao Guo, Yang Feng, Qingkai Fang, Yan Zhou, Zhengrui Ma

- **What's New**: ChatGPT와 같은 큰 언어 모델(LLMs)은 일상 대화에서 사람들을 돕는 강력한 도구로 자리 잡았습니다. 최근에는 GPT-4o(OpenAI, 2024)를 통해 음성으로도 상호작용할 수 있게 되었으며, 이는 사용자 경험을 크게 향상시켰습니다. 그러나 대부분의 LLM은 현재 텍스트 기반의 상호작용만을 지원하여, 텍스트 입력 및 출력이 이상적이지 않은 상황에서는 제한적일 수밖에 없습니다. 이에 따라 LLM 기반의 음성 상호작용 모델을 구축하는 데 있어서 오픈 소스 커뮤니티의 탐구가 부족한 상황입니다.
- **Technical Details**: LLaMA-Omni 모델은 저지연 및 고품질의 상호작용을 위해 설계되었습니다. 이 모델은 음성 인코더(encoder), 음성 어댑터(adaptor), LLM, 그리고 스트리밍 음성 디코더(decoder)로 구성됩니다. 사용자의 음성 명령은 음성 인코더와 어댑터를 통해 인코딩되어 LLM에 입력되며, LLM은 이 음성 명령으로부터 텍스트 응답을 직접 디코딩합니다. 음성 디코더는 비자기회귀적(non-autoregressive) 스트리밍 Transformer(Ma et al., 2023)로, 이는 LLM의 출력 히든 상태를 입력으로 받아 연결주의 시계열 분류(CTC; Graves et al., 2006a)를 사용하여 음성 응답의 이산 단위를 예측합니다. LLaMA-Omni는 동시에 높은 품질의 텍스트와 음성 응답을 226ms의 낮은 지연 시간으로 생성할 수 있습니다.
- **Performance Highlights**: 실험 결과, LLaMA-Omni는 이전의 음성-언어 모델들보다 훈련 데이터와 컴퓨팅 자원을 크게 절약하면서도 매우 높은 품질의 텍스트와 음성 응답을 동시에 생성하는 능력을 보여줍니다. InstructS2S-200K 데이터셋을 구축하여, 기존의 텍스트 지시 데이터를 재작성하고 음성 합성을 수행하여 성능을 입증하였습니다. 또한 SpeechGPT(Zhang et al., 2023)와 비교하여 LLaMA-Omni는 필수적인 훈련 데이터와 컴퓨팅 리소스를 크게 줄였으며, 최신 LLMs 기반의 강력한 음성 상호작용 모델을 효율적으로 개발할 수 있음을 입증했습니다.

### [INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding](https://arxiv.org/abs/2409.06210)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.06210.png)

Vote: 23

Authors: Hoigi Seo, Se Young Chun, Ji Ha Jang

- **What's New**: 새롭게 제안된 연구인 INTRA (INTeraction Relationship-aware weakly supervised Affordance grounding)에 대해 소개합니다. 이는 약한 감독 하의 애포던스 추론(affordance grounding)을 위한 새로운 방법으로, 기존 연구들이 겪었던 여러 문제를 해결하고자 기획되었습니다. 이 방법은 대규모 언어 모델(LLM)과 비전-언어 모델(VLM)의 텍스트 인코더를 활용하여 기존 언어 정보를 애포던스에 대한 학습에 효과적으로 활용합니다. 이를 통해 새로 등장하는 물체나 상호작용에 대한 제로샷 추론(zero-shot inference)을 가능하게 합니다.
- **Technical Details**: INTRA는 기존의 약한 감독 하의 애포던스 추론 방법들이 겪는 문제를 해결하기 위해, 새로운 표현 학습(representation learning) 접근법을 취하고 있습니다. 특히, 인간의 관찰 학습처럼 exocentric 이미지(외부 시점에서 촬영된 이미지)만을 사용하여 모델을 학습시키는 점이 특징입니다. 이 접근법은 기존에 필요했던 egocentric 이미지(주체의 시점에서 촬영된 이미지)와의 짝을 지어 학습시키는 부담을 줄여줍니다. 더욱이, 텍스트 동의어 증가, 텍스트 조건 애포던스 맵 생성 모듈, 그리고 상호작용 관계-안내 대조 학습(contrastive learning)을 통해 다양한 데이터셋에서 뛰어난 확장성과 성능을 제공합니다.
- **Performance Highlights**: INTRA는 AGD20K, IIT-AFF, CAD, UMD와 같은 다양한 데이터셋에서 질적 및 양적으로 우수한 성능을 보여주었습니다. 기존의 약한 감독 하의 애포던스 추론 방법을 뛰어넘는 성과를 나타내며, 특히 제로샷 추론(평가중 처음 보는 상호작용)에 대해 뛰어난 성능을 입증하였습니다.

### [SongCreator: Lyrics-based Universal Song Generation](https://arxiv.org/abs/2409.06029)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.06029.png)

Vote: 14

Authors: Helen Meng, Zhiyong Wu, Jingcheng Wu, Max W. Y. Lam, Hangyu Liu, Boshi Tang, Feng Liu, Shun Lei, Yixuan Zhou, Shiyin Kang

- **What's New**: 음악 생성에 있어 가사에서부터 고품질의 노래를 생성하는 새로운 AI 시스템 'SongCreator'를 소개합니다. SongCreator는 가사 기반의 보컬과 반주를 조화롭게 조율할 수 있으며, 작곡과 편곡 능력을 학습하여 다양한 음악 생성 작업에 적용될 수 있습니다.
- **Technical Details**: SongCreator는 언어 모델(LM)과 잠재 확산 모델(LDM)로 구성된 시스템입니다. 이를 위해 두 개의 디코더를 사용하여 보컬과 반주 정보를 별도로 모델링하고, 이 두 시퀀스 간의 영향을 캡처하는 동적 양방향 크로스-어텐션 모듈을 활용합니다. 또한, UniLM과 GLM에서 영감을 받아 설계된 특수 어텐션 마스크 전략을 통해 다양한 형태의 노래 생성 작업을 통일된 방식으로 처리할 수 있습니다.
- **Performance Highlights**: SongCreator는 가사에서부터 보컬 및 반주를 조화롭게 생성하며, 노래 편집, 반주에서 보컬 생성 등 다양한 음악 생성 작업을 모두 수행할 수 있습니다. 이는 기존의 음악 생성 모델보다 높은 음악성 및 품질을 자랑합니다.

### [Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis](https://arxiv.org/abs/2409.06135)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.06135.png)

Vote: 13

Authors: Binjie Mao, Qi Yang, Pengfei Yan, Ying Guo, Cheng Zhen, Zili Wang, Pengfei Gao, Xing Nie, Shiming Xiang

- **What's New**: 새로운 연구는 자동 비디오 폴리(automatic video foley) 기술에 초점을 맞추고 있으며, 'Draw an Audio'라는 더 컨트롤 가능한 오디오 합성 프레임워크를 제안하고 있습니다. 이 프레임워크는 콘텐츠, 시간적, 음량의 일관성을 동시에 해결하는 것을 목표로 합니다.
- **Technical Details**: 'Draw an Audio'는 비디오, 텍스트, 그림으로 그린 비디오 마스크 및 음량 신호와 같은 여러 지침을 입력으로 받아들입니다. 이는 'Animate-Anything'(Dai et al. 2023)에서 영감을 받아 Mask-Attention Module(MAM)과 Time-Loudness Module(TLM)을 도입하였으며, 이를 통해 생성된 오디오와 입력 비디오 간의 의미적 일치를 향상시킵니다. 특히, 손으로 그린 음량 신호를 입력으로 지원하여 오디오의 특정 음량 변화를 생성할 수 있습니다.
- **Performance Highlights**: 'Draw an Audio'는 제안된 데이터셋뿐만 아니라 다른 챌린지 데이터셋에서도 탁월한 성능을 보여주었습니다. 이는 콘텐츠, 시간적 및 음량 일관성이 뛰어난 오디오를 생성하여 폴리 디자이너의 요구를 더 효과적으로 충족시킵니다.

### [SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation](https://arxiv.org/abs/2409.06633)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.06633.png)

Vote: 10

Authors: Ran Yi, Hongrui Huang, Yabiao Wang, Jiangning Zhang, Teng Hu, Lizhuang Ma

- **What's New**: 이 논문에서는 최신 딥러닝(Deep Learning) 모델을 사용하여 기존의 방법보다 효과적으로 문제를 해결하는 새로운 접근 방식을 제안합니다. 특히, 자연어 처리(NLP)와 컴퓨터 비전 분야에서 두각을 나타내고 있습니다.
- **Technical Details**: 이 연구는 Transformer 기반의 아키텍처를 중심으로 이루어졌습니다. 모델의 주요 구성 요소는 Attention 메커니즘으로, 이는 입력 데이터의 중요한 부분을 효과적으로 캡처하는 데 도움을 줍니다. 또한, 대규모 데이터셋을 활용해 학습을 진행하였으며, 이를 통해 일반화 성능을 크게 향상시켰습니다.
- **Performance Highlights**: 제안된 모델은 여러 벤치마크 테스트에서 높은 성능을 기록했습니다. 특히, 이미지 인식과 텍스트 분류 작업에서 기존의 최첨단 기술보다 더 우수한 결과를 도출했습니다.

### [LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation](https://arxiv.org/abs/2409.06703)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.06703.png)

Vote: -

Authors: Kamal Gupta, Vatsal Agarwal, Abhinav Shrivastava, Archana Swaminathan, Anubhav Gupta, Shishira R. Maiya

- **What's New**: 이번 연구에서는 multiview 이미지에서 view-invariant 3D 객체 latent embedding을 학습하여 객체의 관절 상태(articulations)를 모델링하는 새로운 방법을 제안합니다. 이로써 대규모 사전 훈련된 모델이나 비디오 없이도 다양한 객체 상태를 학습할 수 있습니다.
- **Technical Details**: LEIA라는 end-to-end 방법론을 도입하여, 여러 상태에서 캡처된 multiview 이미지만으로 객체의 새로운 상태를 생성합니다. 이 방법은 하이퍼네트워크(hypernetwork)를 사용하여 NeRF (Neural Radiance Fields) 가중치를 예측합니다. 이 가중치는 test time에 새로운 객체 상태를 생성하는 데 사용됩니다.
- **Performance Highlights**: LEIA는 ground-truth 3D 데이터, 모션 정보 또는 관절 코드 없이도 복잡한 관절을 효과적으로 캡처할 수 있습니다. 이 방법은 단일 및 다중 관절, 그리고 다양한 종류의 모션에 대해 강건한 성능을 보이며, 이전 연구들과 비교해 제약 없이 스케일이 가능함을 입증했습니다.

