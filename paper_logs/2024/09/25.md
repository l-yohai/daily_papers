## Daily Papers (2024-09-25)

### [HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models](https://arxiv.org/abs/2409.16191)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.16191.png)

Vote: 28

Authors: Songyang Zhang, Liqun He, Ge Zhang, Yutao Mou, Haoran Que, Jian Yang, Zekun Moore Wang, Junran Peng, Zhaoxiang Zhang, Wenge Rong, Jiaheng Liu, Wangchunshu Zhou, Feiyu Duan, Kai Chen

- **What's New**: 이 논문은 대규모 언어 모델(Large Language Models, LLMs)의 긴 텍스트 생성 능력을 평가하기 위해 새로운 벤치마크, HelloBench를 도입했습니다. 최근 LLM들이 다양한 자연어 처리(NLP) 작업에서 인상적인 성과를 내고 있지만, 이들의 긴 텍스트 생성 능력을 평가하는 포괄적인 벤치마크의 부족이 지적되었습니다.
- **Technical Details**: HelloBench는 Bloom's Taxonomy에 기반하여 기억(remember), 이해(understand), 적용(apply), 분석(analyze), 평가(evaluate), 창작(create) 등 6단계의 인지 수준으로 LLM의 긴 텍스트 생성 능력을 평가합니다. 각 단계는 오픈형 질문 응답(QA), 요약, 채팅, 텍스트 완성, LLM-as-a-Judge, 휴리스틱 텍스트 생성과 같은 구체적인 작업으로 분류됩니다.
- **Performance Highlights**: ['GPT-4o 및 Claude-3.5-Sonnet 같은 현재 성능이 좋은 LLM들은 4000단어 이상의 긴 텍스트를 생성하는 데 어려움을 겪고 있습니다.', '일부 오픈소스 LLM들은 긴 텍스트를 생성할 수 있지만, 생성된 텍스트에서는 심각한 반복과 품질 저하가 발생합니다.', 'LLM의 장기 문맥 이해 능력과 긴 텍스트 생성 능력 사이에 부정적인 상관관계가 확인되었습니다.', 'HelloEval은 기존의 ROUGE, BLEU, PPL 등의 메트릭 및 다양한 LLM-as-a-Judge 평가 방법보다 인간 평가와의 상관성이 가장 높게 나타났습니다.']

### [MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling](https://arxiv.org/abs/2409.16160)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.16160.png)

Vote: 19

Authors: Miaomiao Cui, Yifang Men, Liefeng Bo, Yuan Yao

- **What's New**: 새로운 논문에서는 영화 제작, 가상현실, 애니메이션 분야에 큰 잠재력을 가진 캐릭터 비디오 합성(character video synthesis)을 위한 혁신적이고 새로운 방법을 제안합니다. 이번 연구는 사용자가 간단한 입력을 통해 다양한 속성(캐릭터, 모션, 장면)을 제어할 수 있는 영상 합성을 가능하게 합니다. 이 방법을 통해 임의 캐릭터, 새로운 3D 모션, 실제 상호작용 장면에 확대 적용할 수 있도록 설계하였습니다. 저자는 이를 'MIMO'라고 명명했습니다.
- **Technical Details**: 제안된 방법은 공간적 해체 모델링(spatial decomposed modeling)을 통해 2D 비디오를 3D 인식 형태로 디코딩하고 표현합니다. 3D 깊이 정보를 기반으로 비디오 클립을 장면(scene), 사람(human), 가림(occlusion) 등 3가지 공간 구성 요소로 계층화하여 분해합니다. 이러한 구성 요소는 최신 VAE 인코더를 통해 임베딩되어, 확산 디코더(diffusion decoder)에 조건으로 제공됩니다. 이를 통해 네트워크는 다양한 속성의 제어 가능한 합성과 주요 객체, 전경 및 배경의 3D 인식 조성을 학습합니다.
- **Performance Highlights**: 제안된 프레임워크는 간단한 사용자 입력만으로도 다양한 속성을 제어할 수 있는 현실감 있는 캐릭터 비디오를 합성할 수 있습니다. 또한 기존 모델과 달리, 공간적 속성을 자동으로 분리하고 3D 인식 합성을 통해 장면 상호작용까지 가능하게 하여 복잡한 3D 모션과 자연스러운 객체 상호작용을 처리할 수 있습니다.

### [Making Text Embedders Few-Shot Learners](https://arxiv.org/abs/2409.15700)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.15700.png)

Vote: 18

Authors: Zheng Liu, Yingxia Shao, Shitao Xiao, MingHao Qin, Defu Lian, Chaofan Li, Kun Luo, Jianlyu Chen

- **What's New**: 이 연구에서는 대형 언어 모델(LLMs)의 인컨텍스트 학습(ICL) 능력을 활용하여 적응성이 뛰어난 텍스트 임베딩을 생성하는 방법을 제안합니다. 구체적으로, 쿼리 프롬프트에 작업별 예시를 포함함으로써 ICL 전략을 사용해 특정 도메인에 더 적합하고 다양한 컨텍스트에 일반화될 수 있는 임베딩을 생성합니다.
- **Technical Details**: 전통적인 임베딩 모델이 다양한 작업을 처리하는 데 제한이 있다는 점을 극복하기 위해, ICL 전략을 통한 few-shot contrastive 학습을 도입하였습니다. 구체적으로, 쿼리-패시지 쌍에 대해 작업 정의와 예시 템플릿을 포함한 입력을 생성하여 더 일반화된 임베딩을 만들었습니다. 
- **Performance Highlights**: 제안된 모델 bge-en-icl은 MTEB 및 AIR-Bench 벤치마크에서의 성능이 향상되었으며, 다중언어 임베딩 모델 bge-multilingual-gemma2와 경량화된 리랭커 bge-reranker-v2.5-gemma2-lightweight도 공개되었습니다. 특히, 모델의 단순한 아키텍처와 ICL 전략의 결합만으로도 뛰어난 성과를 달성했습니다. 또한, 모든 모델 체크포인트와 데이터셋, 학습 스크립트를 공개하여 연구 커뮤니티에서 쉽게 접근할 수 있도록 했습니다.

### [OmniBench: Towards The Future of Universal Omni-Language Models](https://arxiv.org/abs/2409.15272)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.15272.png)

Vote: 16

Authors: Xingwei Qu, Emmanouil Benetos, Jinjie Shi, Ruibin Yuan, Hangyu Guo, Zhaoxiang Zhang, Xiangzhou Wang, Ge Zhang, Yizhi Li, Jian Yang, Xinyue Zhang, Zachary Liu, Wenhao Huang, Siwei Wu, Jiaheng Liu, Yinghao Ma, Kang Zhu, Yiming Liang, Zhenzhu Yang, Chenghua Lin

- **What's New**: 인공지능의 급속한 발전으로 인해 이미지, 오디오, 텍스트와 같은 다양한 데이터 유형을 처리하고 해석할 수 있는 멀티모달 대형 언어 모델(multimodal large language models, MLLMs)이 출현했습니다. 이러한 모델들은 환경으로부터 종합적인 컨텍스트를 학습하며 인간과 유사한 이해력을 모방하는 것을 목표로 합니다. 그러나 이런 모델들이 동시에 세 가지 모달리티를 처리하고 추론하는 능력은 아직 완전히 탐구되지 않았습니다. 이러한 필요를 해결하기 위해, 우리는 OmniBench라는 혁신적인 유니버설 멀티모달 벤치마크를 도입하여 MLLMs의 시각, 음향, 텍스트 입력을 동시에 인식하고 해석하며 추론하는 능력을 엄격하게 평가합니다.
- **Technical Details**: OmniBench는 고품질의 인간 주석 데이터(human annotations)를 사용하여 개발되었으며, 주석자들이 제공한 답변 근거를 포함하여 평가의 신뢰성을 극대화했습니다. OmniBench는 MLLMs의 '옴니-이해 및 추론 능력(omni-understanding and reasoning ability)'을 평가하기 위해 고안되었습니다. 모델은 주어진 이미지와 오디오로부터 요소를 인식하고 텍스트 지시에 따라 멀티모달 객체 간의 의미와 관계를 해석하며, 이를 통해 모든 모달리티의 보완적 정보로부터 추론하고 답변을 제공합니다.
- **Performance Highlights**: OmniBench를 사용한 초기 결과는 기존 MLLMs의 '옴니-이해 능력'에 중요한 한계가 있음을 드러냅니다. 공개된 OLMs은 세 가지 모달리티로 훈련되었지만, 특정 경우 이미지와 오디오가 함께 제공될 때 지시사항을 따르기 어려워합니다. 텍스트를 이미지 및 오디오의 대안으로 사용하는 설정에서는 VLMs와 ALMs이 상대적으로 나은 결과를 보였지만 여전히 초기 수준에 머물러 있습니다. 반면, 비공개 모델은 전체 평가 설정에서 성능 저하가 없는 것으로 나타났습니다. 이러한 결과는 멀티모달 시스템 개선의 필요성을 확인하고, OmniBench가 연구를 유도하는 중요한 도구임을 강조합니다.

### [Present and Future Generalization of Synthetic Image Detectors](https://arxiv.org/abs/2409.14128)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.14128.png)

Vote: 15

Authors: Enrique Lopez-Cuena, Dario Garcia-Gasulla, Pablo Bernabeu-Perez

- **What's New**: 이 논문은 최신 Synthetic Image Detection (SID)의 현황과 미래 전망을 분석합니다. 구체적으로, 여러 훈련 조건이 모델 일반화에 미치는 영향을 분석하여 강력한 탐지기를 구축하기 위한 가이드를 제공합니다. 이러한 가이드라인을 활용하여 데이터 소스, 모델 변동 및 모델 연식에 따른 일반화를 연구합니다. 뿐만 아니라 최신 생성기에서 생성된 데이터를 사용하여 최첨단 탐지기 모델들에 대한 벤치마크도 제공합니다.
- **Technical Details**: 이 연구는 ResNet-18 백본을 사용하는 탐지기 구조로 고정하여, 일반화 모델의 진화와 실제 사용을 집중적으로 다룹니다. 대부분의 탐지기는 이미지 전체보다는 특정 부분을 분석하는데, 이는 이미지의 특정 부분이 조작 흔적이 더 뚜렷할 수 있기 때문입니다. 탐지기는 GLCM(회색 레벨 공변 행렬)에서 가장 큰 대조를 보이는 패치를 기반으로 훈련됩니다. 또한, COCO와 같은 실제 이미지 데이터셋과 최신 생성 모델에서 만든 합성 이미지 데이터셋을 사용하여 탐지기를 훈련합니다.
- **Performance Highlights**: 연구 결과, 현재의 탐지기는 단독으로 사용될 때 합성 콘텐츠 탐지에 불충분합니다. 이에 따라 탐지기 앙상블(enemble)의 사용과 생성기 전용 탐지기의 훈련이 제안됩니다. 형식 및 해상도, 데이터셋 편향으로 인한 성능 차이가 확인되었으며, 각기 다른 데이터 소스에 대한 일반화를 잘 수행합니다.

### [MonoFormer: One Transformer for Both Diffusion and Autoregression](https://arxiv.org/abs/2409.16280)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.16280.png)

Vote: 13

Authors: Jingdong Wang, Haocheng Feng, Yuxing Song, Errui Ding, Chuyang Zhao, Xinyan Xiao, Yifan Sun, Wenhao Wang

- **What's New**: MonoFormer라는 새로운 접근법을 소개합니다. 이 방법은 하나의 transformer를 활용하여 autoregression과 diffusion 모두를 처리할 수 있도록 설계되었습니다. 기존에는 텍스트 생성과 이미지 생성을 위해 각각 다른 모델이 필요했지만 MonoFormer는 둘 다 하나의 모델에서 처리할 수 있습니다.
- **Technical Details**: Diffusion 모델은 probabilistic 접근 방식으로, 노이즈를 점차 제거하며 데이터를 생성합니다. Autoregressive 모델은 텍스트 데이터를 순차적으로 예측하는 방식입니다. 두 모델을 통합하기 위해 MonoFormer는 텍스트 토큰 임베딩이나 이미지 인코딩과 같은 연속적인 임베딩을 입력으로 받아 텍스트 예측 및 이미지 디코딩을 위한 출력 임베딩을 생성합니다.
- **Performance Highlights**: MonoFormer는 두 가지 태스크, 즉 텍스트-텍스트 생성과 텍스트-이미지 생성에서 실험을 진행해, 현재 최첨단 방법들과 비교할만한 이미지 생성 성능을 보였습니다. 동시에, 텍스트 생성 능력도 유지하고 있습니다.

### [Seeing Faces in Things: A Model and Dataset for Pareidolia](https://arxiv.org/abs/2409.16143)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.16143.png)

Vote: 11

Authors: Anne Harrington, Simon Stent, Vasha DuTell, William T. Freeman, Mark Hamilton, Ruth Rosenholtz, Jennifer Corbett

- **What's New**: 이번 연구는 'Faces in Things'라는 5000개의 파레이돌리아(pareidolia) 얼굴 이미지를 포함한 주석 데이터셋을 도입했습니다. 사용자는 파레이돌리아 현상을 연구하기 위한 최첨단 컴퓨터 비전 모델을 개발할 수 있으며, 이는 인간의 시각 시스템과 기계의 시각 시스템을 비교 연구하는 데 도움을 줍니다.
- **Technical Details**: 'Faces in Things' 데이터셋은 LAION-5B 데이터셋에서 추출된 이미지로 구성되어 있으며, 텍스트 검색을 통해 파레이돌리아 이미지 후보를 선정했습니다. 주석 작업에는 VGG Image Annotation 도구가 사용되었으며, 실제 인간 또는 동물의 얼굴을 포함하지 않도록 샘플을 제거했습니다. 여러 이미지 증강 기법(image augmentation techniques)과 추가 학습 데이터 소스를 사용해 다양한 실험을 수행했습니다.
- **Performance Highlights**: 이번 연구는 최신 WIDER FACE 탐지 벤치마크로 학습된 신경망 모델이 파레이돌리아 얼굴을 잘 탐지하지 못한다는 것을 보여주었습니다. 그러나 'Faces in Things' 데이터를 사용해 모델을 미세 조정(fine-tuning)하면 파레이돌리아 얼굴 탐지 성능이 크게 향상될 수 있다는 강력한 기준선을 설정할 수 있음을 입증했습니다. 또한 동물 얼굴 탐지로 미세 조정된 모델이 파레이돌리아 얼굴 탐지 성능을 크게 향상시킬 수 있음을 발견했습니다.

### [EuroLLM: Multilingual Language Models for Europe](https://arxiv.org/abs/2409.16235)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.16235.png)

Vote: 10

Authors: Nuno M. Guerreiro, Duarte M. Alves, José Pombal, João Alves, Pierre Colombo, Ricardo Rei, José G. C. de Souza, Manuel Faysse, Patrick Fernandes, Amin Farajian, Pedro Henrique Martins, Barry Haddow, Mateusz Klimaszewski, André F. T. Martins, Alexandra Birch

- **What's New**: 유럽연합의 모든 언어와 기타 관련 언어에 대해 텍스트를 이해하고 생성할 수 있는 대형 언어 모델(LLM)을 개발하려는 EuroLLM 프로젝트를 소개합니다. 초기 모델은 EuroLLM-1.7B와 EuroLLM-1.7B-Instruct로 구성됩니다.
- **Technical Details**: 유럽과 기타 관련 언어들을 위한 데이터를 수집하고 필터링하여 LLM을 훈련시킵니다. 데이터의 주요 출처는 웹 데이터, 병렬 데이터, 코드/수학 데이터, 고품질 데이터로 나뉘어집니다. 또한 다국어 토크나이저를 개발하고, 하이퍼파라미터 설정 후 모델을 예비 훈련하고 자연어 지침을 따르도록 미세 조정합니다.
- **Performance Highlights**: 유럽 언어와 기타 다국어에 대해 성능을 예측하는 멀티링구얼, 조인트 스케일링 법칙을 사용하여 모델의 성능을 평가합니다. 이를 통해 병렬 데이터의 비율과 고품질 데이터 반영 여부 등을 결정하고, 다양한 언어에 대한 토큰 할당량을 효율적으로 관리합니다.

### [MaskBit: Embedding-free Image Generation via Bit Tokens](https://arxiv.org/abs/2409.16211)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.16211.png)

Vote: 8

Authors: Xiaohui Shen, Mark Weber, Daniel Cremers, Lijun Yu, Liang-Chieh Chen, Xueqing Deng, Qihang Yu

- **What's New**: 최근 class-conditioned 이미지 생성 및 text-to-image 생성 분야에서는 Masked Transformer 모델이 auto-regressive 모델과 diffusion 모델에 대한 강력한 대안으로 부상하고 있습니다. 이 연구는 기존 VQGAN 모델의 문제점을 보완하고 성능을 크게 개선한 VQGAN+ 모델을 제안합니다. 또한, embedding-free generation 메커니즘으로서 새로운 MaskBit 프레임워크를 개발하여 ImageNet 256×256 이미지 생성 벤치마크에서 state-of-the-art 성능을 달성했습니다.
- **Technical Details**: Masked Transformer 모델은 대개 discrete tokenizer(VQGAN)을 사용하여 이미지를 latent space로 변환하고, Transformer 모델이 masked token sequence에서 이미지를 생성하는 두 단계 프레임워크를 구성합니다. 그러나 기존 VQGAN 기반 토크나이저의 세부적인 발전은 잘 다루어지지 않았습니다. 이에 연구팀은 개선된 VQGAN+ 모델을 개발하면서 중요한 구성 요소들에 대한 세밀한 ablation 연구를 진행하였습니다. 또한, 새로운 Lookup-Free Quantization(LFQ)를 도입하여 효율적이고 구조적으로 의미가 높은 bit tokens를 생성했고, 이 bit token을 사용하여 MaskBit 생성 모델을 제안하였습니다.
- **Performance Highlights**: 제안된 VQGAN+ 모델은 원래 VQGAN 보다 Reconstruction FID를 7.94에서 1.66으로 크게 개선했습니다. MaskBit 모델은 305M 파라미터로 ImageNet 256×256 이미지 생성에서 1.52 FID를 달성하며 컴팩트한 생성 모델임에도 불구하고 state-of-the-art 성능을 보였습니다.

### [Improvements to SDXL in NovelAI Diffusion V3](https://arxiv.org/abs/2409.15997)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.15997.png)

Vote: 6

Authors: Eren Doğan, Alex Birch, Juan Ossa, F. Johnson

- **What's New**: 최근 이미지 생성 모델 중 확산 기반 모델(Diffusion based models)이 큰 인기를 얻고 있습니다. 그 중 하나인 Stable Diffusion은 오픈 소스로 공개되어 많은 관심을 받았습니다. Stability AI는 SDXL이라는 확장 버전을 출시했으며, 이를 기반으로 한 NovelAI Diffusion V3 모델을 개발하고 여러 훈련 개선을 추가했습니다.
- **Technical Details**: NovelAI Diffusion V3는 SDXL을 기반으로 여러 개선 사항을 도입했습니다. 먼저, 훈련 매개변수를 ϵ-추정(ϵ-prediction)에서 𝐯-추정(𝐯-prediction)으로 변경하여 Zero Terminal SNR(ZTSNR)을 지원했습니다. 이는 SDXL이 순수 노이즈에서 이미지를 예측할 수 있게 해줍니다. 또한, 컬러 변화 제거 및 샘플 품질의 빠른 수렴과 같은 다양한 장점을 제공합니다. SDXL 훈련 중 발생한 노이즈 스케줄 문제를 해결함으로써 이미지를 항상 적절한 밝기로 생성하게 했습니다. 이를 위해, ZTSNR 노이즈 스케줄을 도입하여 SDXL이 퓨어 노이즈 수준에서도 학습할 수 있도록 했습니다.
- **Performance Highlights**: NovelAI Diffusion V3는 높은 SNR 단계에서의 예측 안정성 및 이미지 품질을 개선했습니다. 기본 설정에서 발생하는 컬러 변화를 제거하고, 샘플의 품질을 더 빠르게 수렴하게 만들었습니다. 또한, 모델이 퓨어 노이즈에서 예측할 수 있도록 개선하여, 비목적성 특성 생성을 줄였습니다.

### [Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation](https://arxiv.org/abs/2409.16283)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.16283.png)

Vote: 6

Authors: Carl Doersch, Homanga Bharadhwaj, Ted Xiao, Dorsa Sadigh, Dhruv Shah, Shubham Tulsiani, Fei Xia, Abhinav Gupta, Sean Kirmani, Debidatta Dwibedi

- **What's New**: Gen2Act는 웹 데이터를 활용하여 로봇 조작 작업의 일반화를 가능하게 하는 언어 조건부 로봇 조작 시스템입니다. 사람 비디오 생성 모델을 활용하여 언어 설명에 따라 사람 비디오를 생성하고, 이 비디오를 기반으로 로봇이 작업을 수행하는 방식입니다.
- **Technical Details**: Gen2Act는 두 단계로 구성됩니다. 첫 번째는 비디오 예측 모델(Video prediction model)을 사용해 작업을 수행하는 사람의 비디오를 생성합니다. 두 번째는 생성된 사람 비디오를 기반으로 로봇 행동을 추론하는 인간-로봇 번역 모델(Human-to-robot translation model)을 사용합니다. 또한, 저희는 궤적 예측 보조 손실(Track prediction auxiliary loss)을 통해 시점의 움직임 정보를 추출해내고, 행동 예측을 최적화하도록 정책을 학습합니다.
- **Performance Highlights**: Gen2Act는 새로운 객체 유형과 로봇 상호작용 훈련 데이터에 보이지 않은 새로운 움직임 유형에 대한 일반화에서 가장 경쟁력 있는 베이스라인(s)보다 약 30% 높은 성공률을 보입니다. 또한 '커피 만들기'와 같은 긴 시퀀스를 포함하는 활동을 수행할 때의 우수한 성능을 시연합니다.

### [Reward-Robust RLHF in LLMs](https://arxiv.org/abs/2409.15360)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.15360.png)

Vote: 4

Authors: Xingzhou Lou, Jian Xie, Dong Yan, Yu Wang, Yiping Zhang, Yuzi Yan, Chao Yu, Jialian Li, Yuan Shen

- **What's New**: 최신 연구에서는 보상 신뢰성(RLHF, Reinforcement Learning from Human Feedback) 프레임워크를 개선하기 위해 보상 견고성(Robustness)을 도입하여 모델의 성능과 견고성을 균형 있게 최적화하는 새로운 방법을 제안하고 있습니다. 또한 Bayesian Reward Model Ensembles(BRME)를 활용하여 보상 모델의 불확실성을 평가하고 이를 통해 보다 정확한 보상 신호를 제공하는 방법을 도입했습니다.
- **Technical Details**: 제안된 방법은 두 가지 주요 단계를 포함합니다. 첫째, 다중 헤드(Multi-Head) 보상 모델을 통해 각 헤드(head)가 Gaussian 분포의 평균과 표준 편차를 출력하며 최종 보상을 샘플링합니다. BRME는 평균 제곱 오차(MSE, Mean Square Error) 손실로 훈련됩니다. 이는 전통적인 최대 우도 추정(MLE, Maximum Likelihood Estimation) 손실을 사용하는 보상 모델보다 보상 배포의 커버리지와 정확도가 뛰어납니다. 두번째로, 새로운 최적화 목표 Jλ(θ)=λJperform(θ)+(1−λ)Jrobust(θ)을 도입하여 성능과 견고성을 함께 고려한 최적화를 수행합니다. 이 최적화 목표는 명목 성능을 측정하는 Jperform와 보상 모델의 불확실성 내에서 최악의 성능을 평가하는 Jrobust를 포함합니다.
- **Performance Highlights**: 제안된 보상 견고성 RLHF 프레임워크는 16개의 널리 사용되는 벤치마크에서 표준 RLHF보다 일관되게 우수한 성능을 보였습니다. 장기 훈련 과정에서는 안정성과 성능이 더 강력하게 나타나며 전통적인 방법보다 평균 정확도가 약 4% 더 높았습니다. 이 연구는 또한 보상 모델의 본질적인 불완전성을 강조하고, 이론적 분석을 통해 제안된 방법의 우수성을 입증합니다. 보상 신호의 왜곡(under-scoring)이 과대평가(over-scoring)보다 훈련 과정에서 더 바람직함을 보였습니다.

### [DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control](https://arxiv.org/abs/2409.12192)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.12192.png)

Vote: 4

Authors: Lerrel Pinto, Zichen Jeff Cui, Aadhithya Iyer, Siddhant Haldar, Hengkai Pan

- **What's New**: 이번 연구에서는 Dynamics Pretraining for Visuo-Motor Control (DynaMo)라는 새로운 자가 지도 방식(self-supervised method)을 소개합니다. 이는 제한된 양의 도메인 내 데이터를 통해 시각적 표현을 사전 학습(pretraining)하여 시각-운동(visuo-motor) 제어 성능을 향상시키는 방법입니다.
- **Technical Details**: DynaMo는 인코더(encoder)를 동시적으로 학습하여 역(inverse) 및 순(forward) 역학 모델을 학습합니다. 여기서, 진짜 액션 데이터(ground truth actions) 접근 없이 학습이 이루어집니다. 주로 대비 학습(contrastive learning)이나 마스킹 자동 인코딩(masked autoencoding)과 같은 기법이 사용되지만, 이는 주로 프레임 단위의 데이터 처리를 가정합니다. DynaMo는 시퀀스 구조를 이용한 동적 예측 목표(dynamics prediction objective)를 사용하여 이러한 한계를 극복합니다.
- **Performance Highlights**: DynaMo는 기존 최첨단(self-supervised representations) 대비 하위 정책 성능(downstream policy performance)을 39% 향상시키는 결과를 보였습니다. 특히 Block Pushing 및 Push-T와 같은 높은 난이도의 폐회로 제어 작업과 실제 로봇 실험에서 뛰어난 성능을 보여주었습니다. DynaMo는 다양한 정책 형태에 적합하며, 사전 학습된 가중치를 미세 조정하거나 제한된 실세계 데이터로도 효과적으로 작동합니다. 전체 데이터셋과 평가 코드, 그리고 학습된 정책의 비디오는 공개될 예정입니다.

### [SLIMER-IT: Zero-Shot NER on Italian Language](https://arxiv.org/abs/2409.15933)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.15933.png)

Vote: 3

Authors: Marco Maggini, Leonardo Rigutini, Andrew Zamai, Andrea Zugarini

- **What's New**: 이 논문에서는 이탈리아어에 대한 Zero-Shot Named Entity Recognition (NER) 평가 프레임워크를 제안하고, SLIMER 모델을 이탈리아어로 미세 조정한 SLIMER-IT를 도입했습니다. 이번 연구는 기존의 많은 NER 연구들이 영어에 집중되어 있어 이탈리아어와 같은 다른 언어에 대한 연구의 필요성을 강조합니다.
- **Technical Details**: SLIMER-IT는 SLIMER 모델을 이탈리아어로 번역한 후, 정의와 가이드라인을 포함한 Instruction-Tuning 기법을 사용하여 최적화되었습니다. 모델은 gpt-3.5-turbo-1106을 사용하여 이탈리아어에 맞는 정의와 가이드라인을 자동 생성했습니다. 평가 프레임워크는 다양한 일반화 수준을 독립적으로 측정하기 위해 설계되었으며, 이탈리아어 데이터를 사용하여 실험을 진행했습니다.
- **Performance Highlights**: SLIMER-IT는 미세 조정된 다른 최신 모델들과 비교했을 때, 보지 못한 엔티티 태그를 라벨링하는 데 있어 뛰어난 성능을 보였습니다. 실험 결과, SLIMER-IT는 특히 위키뉴스(WikiNews) 데이터셋에서 도메인 내부 및 외부 평가에서 우수한 성능을 입증했습니다. 또한 Multinerd-IT 데이터셋을 사용한 실험에서도 보지 못한 NER 태그를 잘 처리하는 것으로 나타났습니다.

### [Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts](https://arxiv.org/abs/2409.16040)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.16040.png)

Vote: 3

Authors: Xiaoming Shi, Dianqi Li, Ming Jin, Qingsong Wen, Zhou Ye, Shiyu Wang, Yuqi Nie

- **What's New**: 이번 발행된 논문은 Time-MoE라는 새로운 시계열 예측 모델을 소개합니다. Time-MoE는 mixture-of-experts (MoE) 아키텍처를 활용하여 모델 크기를 확장하면서도 높은 예측 정확도를 유지합니다.
- **Technical Details**: Time-MoE는 오토-리그레시브 (auto-regressive) 방식으로 작동하는 디코더-전용 트랜스포머 모델입니다. 이 모델은 점별 토큰화 및 인코딩된 입력 시계열 데이터를 처리하며, 대규모 시계열 데이터(9개 도메인, 3000억 이상의 데이터 포인트)에서 학습되었습니다. 또한 멀티-태스크 학습을 통해 여러 해상도에서 예측할 수 있습니다.
- **Performance Highlights**: Time-MoE 모델은 총 2.4억 파라미터를 가지며, 동일한 계산 예산의 밀집 모델 대비 예측 오류를 평균 23% 및 25% 줄였습니다. Time-300B라는 대규모 오픈 액세스 시계열 데이터 컬렉션과 함께, 이 모델은 여러 실제 벤치마크에서도 최고 성능을 보였습니다.

### [RRM: Robust Reward Model Training Mitigates Reward Hacking](https://arxiv.org/abs/2409.13156)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13156.png)

Vote: 1

Authors: Daniel Sohn, Junru Wu, Yang Gao, Bilal Piot, Rishabh Joshi, Tianqi Liu, Zhen Qin, Mohammad Saleh, Jeremiah Liu, Lichang Chen, Tianhe Yu, Yuan Liu, Abe Ittycheriah, Jiaming Shen, Aviral Kumar, Anastasiia Makarova, Wei Xiong, Jie Ren

- **What's New**: 이번 연구는 기존의 보상 모델(reward model) 학습 방식이 맥락에 기인한 실제 선호 신호와 맥락에 구애받지 않는 인공적인 요소들을 구분하기 어렵다는 문제를 다루고 있습니다. 이를 해결하기 위해 인간 선호 모델링을 위한 인과 그래프(causal graph)를 제안하고, 데이터 증강(data augmentation)을 통해 보상 모델이 학습한 인공적인 요소를 완화합니다.
- **Technical Details**: 연구진은 보상 모델 학습을 인과적 개념틀(causal framework)로 전환하였으며, 인과 규칙에 기반한 데이터 증강 기법을 도입하였습니다. 이 기법은 다른 예제의 응답을 활용하여 선택된 응답과 거부된 응답의 인공적인 요소를 효과적으로 균형 맞추는 역할을 합니다. 이를 통해 실제 품질을 비추는 신호만을 학습하도록 도와줍니다.
- **Performance Highlights**: 연구 결과들은 이러한 견고한 보상 모델을 기반으로 훈련된 정책들이 기존의 기준 보상 모델을 기반으로 한 정책들보다 일관되게 뛰어난 성능을 보임을 나타냅니다. 이는 인간의 선호 신호를 보다 정확하게 반영하고, 보상 해킹 문제를 완화하는 데 기여합니다.

### [Tabular Data Generation using Binary Diffusion](https://arxiv.org/abs/2409.13882)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.13882.png)

Vote: 1

Authors: Slava Voloshynovskiy, Vitaliy Kinakh

- **What's New**: 이번 연구에서는 머신러닝에서 민감하고 소중한 또는 부족한 실제 데이터를 다룰 때 중요한 임무인 합성 테이블 데이터(synthetic tabular data) 생성을 다룹니다. 우리는 제안된 방법을 통해 일반적인 테이블 데이터를 이진(binary) 표현으로 변환하고, 이진 데이터를 위해 설계된 생성 모델인 Binary Diffusion을 소개합니다. Binary Diffusion은 XOR 연산을 활용하여 노이즈를 추가 및 제거하며, 광범위한 전처리 및 복잡한 노이즈 파라미터 튜닝이 필요 없습니다.
- **Technical Details**: Binary Diffusion 모델의 핵심은 XOR 연산을 사용하여 데이터를 노이즈와 결합하거나 노이즈를 제거하는 방식입니다. 이는 특히 이진 데이터 처리에 적합합니다. 우리는 테이블 데이터를 다루기 위해 컬럼별로 고유한 전처리가 필요 없는 이진 표현으로 변환하였습니다. 연속형 데이터는 min-max 정규화를 거친 후 32비트 부동소수점 인코딩을 통해 이진 형태로 변환되고, 범주형 데이터는 이진 인코딩을 사용합니다.
- **Performance Highlights**: Binary Diffusion 모델은 Travel, Adult Income, Diabetes 데이터셋에서 기존 최첨단 모델들을 능가하는 성능을 보였습니다. 또한, 이 모델의 파라미터 수가 2M 이하로 상대적으로 작고, 대규모 데이터셋에 대한 사전 훈련을 필요로 하지 않아 빠른 훈련 및 샘플링 기능을 제공합니다.

