## Daily Papers (2024-09-13)

### [Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale](https://arxiv.org/abs/2409.08264)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08264.png)

Vote: 24

Authors: Arthur Bucker, Dan Zhao, Lawrence Jang, Francesco Bonacci, Dillon Dupont, Kazuhito Koishida, Sara Abdali, Rogerio Bonatti, Yinheng Li, Justin Wagle, Zack Hui

- **What's New**: WindowsAgentArena는 Windows OS 생태계에서 멀티모달 에이전트의 개발 및 테스트를 진전시키기 위해 새롭게 도입된 벤치마크입니다. 이 벤치마크는 Linux 작업에 집중하는 OSWorld와 달리 Windows 전용으로 개발되었습니다. WindowsAgentArena는 다양한 앱과 웹 도메인에 걸친 154개의 현실적인 멀티스텝 작업을 제공합니다.
- **Technical Details**: 이 벤치마크는 상태 공간, 관찰 공간(Observation Space), 액션 공간(Action Space), 전이 함수(Transition Function), 보상 함수(Reward Function)로 구성된 부분 관측 마르코프 결정 과정(POMDP)으로 에이전트 행동을 공식화합니다. 또한 Docker 컨테이너와 Azure 가상 머신을 활용하여 평가 작업을 병렬화하여 가속화할 수 있습니다.
- **Performance Highlights**: 새로운 멀티모달 에이전트 Navi는 WindowsAgentArena에서 19.5%의 성공률을 기록하며, 특히 Set-of-Marks 프롬팅과 시스템 접근성 트리 및 픽셀 기반 요소 탐지기를 결합하여 좋은 성과를 보였습니다. 또한 Navi는 Mind2Web 벤치마크에서도 경쟁력 있는 성능을 입증했습니다.

### [DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?](https://arxiv.org/abs/2409.07703)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.07703.png)

Vote: 17

Authors: Dong Yu, Liqiang Jing, Xiaoyang Wang, Zhehui Huang, Wenlin Yao, Kaixin Ma, Xinya Du, Hongming Zhang, Wenhao Yu

- **What's New**: 최근의 Large Language Models (LLMs)와 Large Vision-Language Models (LVLMs)는 다양한 시각 및 언어 작업에서 큰 성공을 거두었으나, 실생활 응용 프로그램과의 통합 부족으로 인해 일상적인 작업 수행에는 한계가 있다. 이를 극복하기 위해, 상호작용 지능 시스템 내에 LLMs와 LVLMs의 통합이 점점 더 주목받고 있다. 여기서 두드러진 예로는 데이터 과학 에이전트(data science agents)로, 이는 대량의 데이터를 분석하여 정보에 기반한 결정을 돕는다.
- **Technical Details**: 우리는 데이터 분석(466)과 데이터 모델링(74) 작업을 포함한 총 540개의 작업으로 구성된 포괄적인 데이터 과학 벤치마크를 도입했다. 이 벤치마크는 ModelOff와 Kaggle과 같은 데이터 과학 대회에서 가져온 실제 시나리오를 반영하며, 다양한 도구와 다중 모달 작업 배경을 이해할 수 있다. 이를 통해 복잡한 작업 처리가 가능해진다.
- **Performance Highlights**: 우리는 최신의 LLMs와 LVLMs, 및 에이전트들을 평가하여 이들이 대부분의 작업을 해결하지 못함을 발견했다. 예를 들어, 실험에서 가장 성능이 좋은 에이전트도 데이터 분석 작업에서 34.12%의 정확도를 기록했으며, Relative Performance Gap (RPG) 모델링 작업에서는 34.74%를 기록했다. 이는 현존하는 접근 방식들에 비해, 더욱 복잡한 실제 데이터 과학 작업의 도전에 대한 높은 기준을 제시함을 시사한다.

### [Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers](https://arxiv.org/abs/2409.04109)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.04109.png)

Vote: 17

Authors: Diyi Yang, Tatsunori Hashimoto, Chenglei Si

- **What's New**: LLM(대형 언어 모델, Large Language Models)의 급격한 개선은 과학적 작업에서 새로운 응용을 가능하게 했습니다. 이번 연구는 LLM이 인간 전문가와 비교할 만한 새로운 연구 아이디어를 생성할 수 있는지를 평가합니다.
- **Technical Details**: 본 연구는 크게 세 가지 실험 구성 요소로 진행됩니다: 아이디어 자체, 아이디어를 전달하는 작성물, 작성물의 전문가 평가입니다. 연구 주제는 Bias(편향성), Coding(코딩), Safety(안전성), Multilinguality(다중 언어), Factuality(사실성), Math(수학), Uncertainty(불확실성) 등으로 구성되었으며, 인간과 LLM 모두에 동일한 템플릿과 예시가 제공되었습니다.
- **Performance Highlights**: 총 300건의 리뷰를 통해 AI가 생성한 아이디어가 인간 전문가의 아이디어보다 더 새롭다고 평가받았습니다(p<0.05). 다만, 실현 가능성에서는 약간의 단점이 있었습니다. 또한, LLM 에이전트의 한계로 인해 아이디어 다양성이 부족했습니다.

### [IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation](https://arxiv.org/abs/2409.08240)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08240.png)

Vote: 10

Authors: Bing Ma, Xinchao Wang, Xuefeng Su, Yinwei Wu, Xianpan Zhou, Kai Ma

- **What's New**: 이 연구는 'Instance Feature Generation (IFG)'이라는 새로운 과제를 제안합니다. 이는 텍스트-이미지 변화의 주된 도전 과제인 위치 정확도와 특징 생성 정확도를 동시에 해결하기 위한 것입니다. 이를 위해, IFAdapter라는 새로운 컴포넌트를 도입하여 상세한 인스턴스 설명을 가진 데이터셋을 생성하고, 이를 통해 더 세밀한 인스턴스 특징을 생성할 수 있습니다.
- **Technical Details**: 1. 세부 인스턴스 설명을 생성하기 위해 최신 비전-언어 모델(VLMs)을 사용합니다. 2. 두 가지 구성 요소를 도입합니다: 'Appearance Tokens (외관 토큰)'와 'Instance Semantic Map (인스턴스 의미 맵)'. Appearance Tokens는 학습 가능한 쿼리를 사용하여 세밀한 인스턴스 특징 정보를 추출하고, Instance Semantic Map은 인스턴스 특징을 공간적 위치와 연결하여 인스턴스 간의 혼란을 방지합니다. 3. 이러한 구성 요소는 주로 크로스 어텐션 레이어의 하위 집합에만 통합되어, 다양한 모델에 재학습 없이 플러그-앤-플레이 방식으로 적용할 수 있습니다.
- **Performance Highlights**: 제안된 IFAdapter는 COCO-IFG 벤치마크에서 평가되었으며, 기존 방법보다 우수한 성능을 보였습니다. 이는 정량적, 정성적 평가 모두에서 입증되었습니다. 특히, 인스턴스의 위치 정확도와 필드 생성 정확도에서 상당한 향상을 이루었습니다. 또한, 새로운 벤치마크와 검증 파이프라인을 도입하여 모델 성능을 정확히 평가할 수 있는 구조를 마련했습니다.

### [TextBoost: Towards One-Shot Personalization of Text-to-Image Models via Fine-tuning Text Encoder](https://arxiv.org/abs/2409.08248)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08248.png)

Vote: 8

Authors: Kunhee Kim, Hyunjung Shim, NaHyeon Park

- **What's New**: 최신 대규모 텍스트-이미지 변환 모델의 발전으로 인해 자연어 프롬프트를 이용한 다양한 이미지를 생성할 수 있게 되었습니다. 이러한 성공에 기반하여, 사용자가 특정 개념을 포함하는 이미지를 개인화할 수 있는 연구가 활발히 진행되고 있습니다. 그러나 기존 방법들은 고품질 출력을 위해 최소 3333~5555장의 참조 이미지가 필요하며, 단일 참조 이미지만 제공되었을 때는 사용자 텍스트 프롬프트를 효과적으로 반영하기 어려웠습니다. 이 논문에서는 단일 참조 이미지만 사용하여 고품질 개인화를 달성하기 위한 새로운 접근 방식을 제안합니다.
- **Technical Details**: 본 연구에서는 기존 접근 방식들이 이미지 모듈을 직접 미세 조정하는 것과는 달리, 텍스트 인코더(text encoder)를 독점적으로 미세 조정하는 새로운 접근 방식을 제안합니다. 이를 위해 세 가지 혁신적인 기술을 도입했습니다: (1) 과적합을 줄이고 주제 관련 및 비관련 기능을 분리하는 '증강 토큰(augmentation token)' 도입, (2) 텍스트 인코더가 언어 이동을 방지하고 다양한 텍스트 프롬프트에 대한 일반적인 능력을 유지하도록 하는 '지식 보존 손실(knowledge preservation loss)', (3) 효율적인 훈련을 촉진하기 위한 'SNR-가중치 타임스텝 샘플링(SNR-weighted timestep sampling)'. 이러한 방법들을 통해 TextBoost라는 접근 방식을 제안하며, 텍스트 프롬프트를 통해 창의적인 제어와 고품질 개인화를 달성할 수 있음을 실험적으로 입증했습니다.
- **Performance Highlights**: TextBoost는 단일 참조 이미지만을 사용하여 다양한 실세계 응용 프로그램에서 고품질, 다양성 높은 출력을 생성할 수 있음을 입증했습니다. 본 접근 방식은 메모리와 저장 공간 효율성 면에서도 우수하며, 0.7M의 파라미터와 5.1MB의 저장 공간만 필요해 보다 광범위한 실제 응용 프로그램에 적용할 수 있습니다.

### [DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with Diffusion Priors](https://arxiv.org/abs/2409.08278)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08278.png)

Vote: 7

Authors: Thomas Hanwen Zhu, Tomas Jakab, Ruining Li

- **What's New**: 새로운 연구인 DreamHOI를 통해 주어진 텍스트 설명을 바탕으로 기존 3D 인간 모델이 현실적으로 3D 객체와 상호작용할 수 있도록 하는 방법을 제시합니다. 이 방법론은 영화 및 비디오 게임 제작, 제품 광고 등 다양한 산업에 큰 영향을 미칠 수 있습니다.
- **Technical Details**: DreamHOI는 텍스트-이미지 확산 모델 (diffusion models)에서 가이던스를 추출하여 HOI synthesis를 구현합니다. 이를 위해 듀얼 암시적-명시적 표현 (dual implicit-explicit representation) 방식을 사용합니다. 암시적 표현은 신경 방사장(NeRF)으로, 명시적 표현은 입력된 스킨드 인간 메쉬입니다. NeRF를 다중 뷰 렌더링을 통해 스킨드 메쉬의 뼈 회전을 예측하는 리그레서 (regressor)로 변환합니다. 또한 최적화 동안 아티큘레이션 파라미터를 유지하기 위해 주기적으로 NeRF와 명시적 표현 간 전환하는 방식으로 진행됩니다.
- **Performance Highlights**: DreamHOI는 단일 뷰 및 다중 뷰 텍스트-이미지 확산 모델의 가이던스를 결합하여, 다양한 3D 인간-객체 상호작용을 생성하는 데 높은 효과를 보입니다. 광범위한 정성적 (qualitative) 및 정량적 (quantitative) 실험을 통해 이 방식을 입증했습니다.

### [Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources](https://arxiv.org/abs/2409.08239)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08239.png)

Vote: 7

Authors: Jakob Foerster, Maria Lomeli, Jane Dwivedi-Yu, Nicola Cancedda, Roberta Raileanu, Alisia Lupidi, Jason Weston, Carlos Gemmell

- **What's New**: 이 논문에서는 Source2Synth라는 방법을 제안합니다. 이 방법은 외부 실세계 데이터를 기반으로 합성 데이터를 생성하는 일반적인 접근 방식입니다. 이를 통해 인간 주석에 의존하지 않고도 LLM이 복잡한 작업에서 성능을 향상시킬 수 있습니다. 특히 웹에서 다중 홉 질문(multi-hop question)과 SQL을 도구로 사용하는 테이블 기반 질문 응답(tabular question answering)에서 성능 향상을 달성했습니다.
- **Technical Details**: Source2Synth는 데이터 생성(Data Generation) 단계, 데이터 정제(Data Curation) 단계, 모델 미세 조정(Model Finetuning) 단계의 세 가지 단계로 구성됩니다. 데이터 생성 단계에서는 현실적인 정보에 기반하여 합성 데이터를 생성하기 위해 웹의 테이블이나 위키피디아 문서와 같은 데이터 소스를 선택합니다. 생성된 합성 데이터는 후속 단계에서 모델을 미세 조정하는데 사용됩니다. 데이터 정제 단계에서는 생성된 합성 데이터를 두 개의 조각으로 나누고 첫 번째 조각을 사용해 모델을 미세 조정하여 중간 모델을 만듭니다. 이 모델을 사용해 두 번째 조각을 정제하여 최종 미세 조정을 수행합니다.
- **Performance Highlights**: Source2Synth는 복잡한 작업에서 인간 주석 없이도 성능을 개선할 수 있는 확장 가능한 데이터 생성 방법을 제공합니다. 다중 홉 질문과 테이블 질문 응답에서 실험한 결과, 해당 작업에서 우수한 성능을 보였습니다.

### [FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally](https://arxiv.org/abs/2409.08270)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08270.png)

Vote: 6

Authors: Xinchao Wang, Qiuhong Shen, Xingyi Yang

- **What's New**: 본 논문에서는 3D Gaussian Splatting (3D-GS) 기법을 활용한 3D 장면의 새로운 렌더링 및 재구성 방법을 소개합니다. 이 방법은 다수의 유색 3D 가우시안을 활용하여 고충실도의 3D 장면을 표현하며, 객체 인식 및 장면 조작 등의 응용을 위해 2D 마스크로부터 3D 가우시안 분할을 수행하는 간단하지만 전역 최적 솔버를 제안합니다.
- **Technical Details**: 기존의 방법들은 3D 가우시안의 라벨링을 위해 반복적 경사 하강법을 사용했지만, 이는 느린 수렴 속도와 비효율성을 가지는 단점이 있었습니다. 이에 반해, 제안된 방법은 3D-GS의 렌더링 처리 과정을 선형화하여 이러한 문제를 해결했습니다. 2D 이미지로부터 재구성된 3D 가우시안의 누적 기여를 선형 함수로 단순화하여 문제를 선형 정수 프로그래밍 문제로 변환하고, 알파 합성(Alpha Composition) 항을 이용하여 닫힌 형태로 문제를 해결합니다.
- **Performance Highlights**: 제안된 솔버는 약 30초 내로 최적화를 완료하여 기존 방법들보다 약 50배 빠른 속도를 자랑합니다. 또한, 잡음에 강하며 다양한 장면의 3D 분할에 있어 높은 성능을 입증하였습니다. 이를 통해 객체 제거 및 인페이팅과 같은 응용 작업에서도 우수한 성능을 보였습니다.

### [Can OOD Object Detectors Learn from Foundation Models?](https://arxiv.org/abs/2409.05162)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.05162.png)

Vote: 4

Authors: Xin Wen, Xiaojuan Qi, Shizhen Zhao, Jiahui Liu, Yingxian Chen

- **What's New**: 이번 연구에서는 다양한 시각 개념의 데이터를 활용하여 모델의 신뢰성을 강화하고 OOD(Object Out-Of-Distribution) 객체 감지를 개선하기 위해 텍스트-이미지 생성 모델을 조사했습니다. SyncOOD는 텍스트-이미지 생성 모델로부터 의미 있는 데이터를 자동으로 수집하는 프로세스를 도입하여, 최소한의 인적 노동으로 고품질 OOD 데이터를 생성합니다.
- **Technical Details**: SyncOOD는 기초 모델(Foundation Models)을 활용하여 OOD 샘플을 수집하고, Stable Diffusion을 사용하여 고품질의 제어 가능한 이미지 편집을 수행합니다. 이 과정에서 대형 언어 모델(LLM)은 새로운 개념을 상상하며, SAM(Segment Anything Model)을 이용해 바운딩 박스를 정제합니다. 이러한 자동화된 데이터 수집 파이프라인을 통해 ID와 OOD 데이터 간의 경계 설정을 최적화하며, 다양한 컨텍스트에서 모델의 성능을 검증합니다.
- **Performance Highlights**: 우리의 방법론은 여러 벤치마크 실험에서 기존 최첨단 방법들을 크게 능가하는 성능을 보였습니다. 텍스트-이미지 생성 모델의 잠재력을 풀어내어 OOD 객체 감지에서 뛰어난 성능을 입증했으며, 이는 최소한의 합성 데이터를 사용하면서도 높은 품질의 OOD 데이터를 제공함으로써 가능해졌습니다.

### [PiTe: Pixel-Temporal Alignment for Large Video-Language Model](https://arxiv.org/abs/2409.07239)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.07239.png)

Vote: 4

Authors: Han Zhao, Min Zhang, Siteng Huang, Yang Liu, Donglin Wang, Pengxiang Ding

- **What's New**: 이번 연구에서는 대규모 비디오-언어 모델(LVidLM)의 새로운 접근법인 PiTe를 소개합니다. PiTe는 비디오 콘텐츠의 정교한 분석을 위해 픽셀 수준에서 비전과 언어를 복잡하게 정렬하는 방식을 도입합니다. 이 모델을 통해 제로샷(Zero-shot) 설정에서의 비디오 이해 능력을 크게 향상시킬 수 있습니다.
- **Technical Details**: PiTe는 영상과 텍스트 간의 정밀한 정렬을 위해 Trajectories(객체 이동 경로)를 사용합니다. 이를 위해 자동 주석 파이프라인을 구축하여 PiTe-143k라는 대규모 비디오-언어 데이터셋을 생성했습니다. 이 데이터셋에는 개별 객체의 이동 경로와 비디오 캡션이 포함되어 있습니다.
- **Performance Highlights**: PiTe는 제로샷 비디오 질문 응답(Question Answering), 시간적 고착(Temporal Grounding), 밀도 캡션(Dense Captioning) 과제에서 현존하는 최고 수준의 성능을 보여줍니다.

