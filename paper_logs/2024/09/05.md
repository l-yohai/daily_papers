## Daily Papers (2024-09-05)

### [Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency](https://arxiv.org/abs/2409.02634)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.02634.png)

Vote: 40

Authors: Jianwen Jiang, Gaojie Lin, Jiaqi Yang, Chao Liang, Tianyun Zhong, Yanbo Zheng

- **What's New**: 최근 비디오 합성 분야에서 GAN과 확산 모델(Diffusion Model)의 빠른 발전으로 인해 인간 비디오 합성 기술이 실용성에 근접하게 되었습니다. 특히, 오디오 기반의 초상화 비디오 합성이 큰 주목을 받고 있습니다. 이 논문은 Loopy라는 새롭고 템플릿이 필요 없는 오디오 기반 초상화 확산 모델을 제안하여 자연스러운 모션 패턴을 생성하고자 합니다.
- **Technical Details**: Loopy는 두 가지 주요 모듈로 구성됩니다: inter-/intra-temporal 모듈과 오디오-모션 잠재 변수(Audio-to-Motion Latents) 모듈. inter-/intra-temporal 모듈은 클립 간의 시간 관계를 모델링하는 별도 레이어와 클립 내의 시간 관계를 모델링하는 원래 시간 모듈로 구성됩니다. 오디오-모션 잠재 변수 모듈은 오디오와 얼굴 모션 관련 특징(랜드마크, 머리 움직임 분산, 표정 움직임 분산)을 기반으로 모션 잠재 변수를 생성하여 디노이징 네트워크의 조건으로 활용됩니다.
- **Performance Highlights**: Loopy는 공공 데이터셋에서의 실험을 통해 모션의 자연스러움과 비디오 합성의 견고성을 효과적으로 개선했음을 입증했습니다. 또한, 여러 유형의 이미지와 오디오에 대해 모델의 성능을 평가하여 기존 방법보다 더 생동감 있고 안정적인 합성 결과를 달성했습니다.

### [LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture](https://arxiv.org/abs/2409.02889)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.02889.png)

Vote: 33

Authors: Chen Zhang, Dingjie Song, Shunian Chen, Benyou Wang, Xidong Wang

- **What's New**: LongLLaVA라는 새로운 시스템이 소개되었습니다. 이 시스템은 멀티이미지 시나리오에서 MLLM(Multimodal Large Language Models)의 컨텍스트 확장을 다룬 첫 번째 하이브리드 아키텍처입니다. 특히, 이 시스템은 Mamba-Transformer 하이브리드 아키텍처를 통해 성능과 효율성의 균형을 맞추고자 합니다.
- **Technical Details**: LongLLaVA의 아키텍처는 Transformer와 Mamba를 결합하여 이미지 토큰을 2D 풀링으로 압축하는 효율적인 이미지 표현 방법을 제안합니다. 데이터 구성에서 시간 및 공간 종속성 구분을 위한 특수 포맷을 설계했고, 단일 이미지 정렬(Single-image Alignment), 단일 이미지 지시 튜닝(Single-image Instruction-tuning), 다중 이미지 지시 튜닝(Multi-image Instruction-tuning)의 세 단계 방법을 통해 멀티모달 적응을 점진적으로 향상시켰습니다.
- **Performance Highlights**: LongLLaVA는 VNBench 과제에서 검색, 카운팅, 정렬 작업에서 뛰어난 성능을 보이며, Needle-In-A-Haystack 평가에서 1,000개의 이미지를 대상으로 단일 80GB GPU에서 거의 100% 정확도를 달성했습니다. 이는 개방형 소스 MLLMs가 다중 이미지 작업에서 성능 저하와 높은 추론 비용 문제를 해결하는 데 큰 기여를 합니다.
- **Scaling Up the Image Number**: MLLMs의 입력 길이는 이미지 수 증가에 따라 급격히 늘어나며, 이는 모델 성능 저하와 높은 추론 비용 문제를 야기합니다. 이를 해결하기 위해 새로운 하이브리드 아키텍처를 제안하여 장기 컨텍스트 시나리오에서 효율성 및 성능을 최적화했습니다. 예를 들어, Int8 정밀도로 단일 A100 80GB GPU가 최대 140K 토큰을 처리할 수 있습니다. 또한, Temporal Expansion과 Spatial Expansion을 통해 MLLMs의 응용범위를 확대하고자 합니다.

### [LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA](https://arxiv.org/abs/2409.02897)

![](/avatars/14e5794307e4672b1b51d26b31227e0f.svg)

Vote: 28

Authors: Danqing Liu, Lei Hou, Yuxiao Dong, Ling Feng, Wanjun Gu, Xin Lv, Shulin Cao, Juanzi Li, jiajie Zhang, Yushi Bai, Minhao Zou

- **What's New**: 최근 몇 년 동안 긴 맥락의 대형 언어 모델(LLMs)의 발전이 크게 진행되어, 100,000 토큰을 초과하는 긴 텍스트를 기반으로 정보 추출 및 요약 등의 다양한 사용자 질문을 처리할 수 있게 되었습니다. 그러나 현재의 긴 맥락 LLMs는 생성된 문장을 뒷받침하는 특정 맥락의 인용(citations)을 제공하지 않아 사용자들이 모델 출력의 신뢰성을 확인하기 어렵게 만듭니다. 이를 해결하기 위해, 본 연구는 정교한 문장 수준의 인라인 인용을 생성할 수 있는 Coarse to Fine(CoF) 파이프라인을 제안하고, 이를 통해 LongCite-45k라는 대규모 데이터셋을 생성합니다.
- **Technical Details**: [{'description': '본 연구는 긴 맥락의 질문 응답과 인용(Long-context Question Answering with Citations, LQAC)을 평가하기 위해 LongBench-Cite라는 자동 벤치마크를 제안합니다. 이 벤치마크는 LLM들의 성능을 평가하며, 기존 모델들이 불충분한 성능을 보인다는 것을 나타냅니다.'}, {'description': 'Coarse to Fine(CoF) 파이프라인은 기존의 LLM을 활용해 긴 텍스트에서 정교한 문장 수준의 인용이 포함된 QA 예제를 자동으로 생성합니다. CoF는 (1) 긴 텍스트에서 질의와 응답을 생성, (2) 응답을 기반으로 고급 인용이 포함된 청크를 추출, (3) 관련 문장을 식별해 정교한 인용을 생성, (4) 인용이 부족한 예제를 삭제하는 4단계로 구성됩니다.'}, {'description': '이를 통해 생성된 LongCite-45k 데이터셋을 활용하여 GLM-4-9B와 Llama3.1-8B 두 최신 모델을 미세 조정하여 LongCite-9B와 LongCite-8B 모델을 개발합니다. 이 모델들은 128,000 토큰의 맥락 윈도우를 지원하며 정밀한 인라인 인용과 정확한 응답을 한 번에 생성할 수 있습니다.'}]
- **Performance Highlights**: [{'description': 'LongBench-Cite에서 평가한 결과, LongCite-9B와 LongCite-8B 모델은 기존의 상용 모델보다 훨씬 뛰어난 인용 품질을 나타냈습니다. 예를 들어, GPT-4보다 인용 F1 점수에서 6.4%/3.6% 우수하며, 더 세밀한 인용을 생성할 수 있습니다.'}, {'description': 'LQAC 데이터를 활용한 SFT(Self-Fine-Tuning)는 LLM의 환각 문제를 완화하고, 모델이 맥락 정보를 보다 균형있고 포괄적으로 활용할 수 있게 하여 응답의 정확성을 높였습니다.'}]

### [MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/abs/2409.02813)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.02813.png)

Vote: 18

Authors: Shengbang Tong, Yuxuan Sun, Yu Su, Ge Zhang, Ming Yin, Tianyu Zheng, Xiang Yue, Huan Sun, Wenhu Chen, Yubo Wang, Kai Zhang, Graham Neubig, Yuansheng Ni, Botao Yu

- **What's New**: 최근 다중모달 대형 언어 모델(Multi-modal Large Language Models, MLLMs)의 발전은 텍스트와 시각 정보를 결합한 복잡한 추론 작업을 해결하는 데 놀라운 진전을 이루었습니다. 이에 대응하는 새로운 벤치마크, MMMU-Pro를 소개합니다. MMMU-Pro는 기존의 MMMU 벤치마크보다 더 정교하고 도전적인 평가를 제공하여 모델의 실제 다중모달 이해 및 추론 능력을 평가하는 데 중점을 둡니다.
- **Technical Details**: MMMU-Pro는 세 단계의 엄격한 구축 과정을 거쳐 개발되었습니다. 먼저, 텍스트 전용 언어 모델로 대답할 수 있는 질문을 필터링하고, 후보 선택지를 증가시켜 추론을 통해 정답을 맞추기 어렵게 만들었습니다. 마지막으로 스크린샷이나 사진에 질문을 포함하여 '비전 전용 입력 설정'을 도입했습니다. 이는 모델이 실제 인간처럼 텍스트와 이미지를 동시에 이해하게 합니다.
- **Performance Highlights**: MMMU-Pro를 통해 실험한 결과, 기존의 MMMU 벤치마크와 비교했을 때 모든 테스트된 모델에서 16.8%에서 26.9%의 성능 저하를 관찰했습니다. 이는 현재 최신 모델들의 진정한 다중모달 이해와 추론 능력에 한계가 있음을 보여줍니다. 또한, CoT(Chain of Thought) 프롬프트는 일반적으로 성능을 향상시키지만, OCR 프롬프트는 대부분의 모델에서 큰 영향을 미치지 않는 것으로 나타났습니다.

### [Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining](https://arxiv.org/abs/2409.02326)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.02326.png)

Vote: 12

Authors: Rajhans Samdani, Hojae Han, Yuxiang Wei

- **What's New**: 이번에 발표된 연구에서는 Arctic-SnowCoder-1.3B라는 고성능의 소형 코드 모델을 소개합니다. 이 모델은 새로운 3단계 훈련 방법론을 통해 데이터 품질을 점진적으로 향상시킴으로써 개발되었습니다. Arctic-SnowCoder-1.3B는 StarCoderBase-3B를 뛰어넘으며, Phi-1.5-1.3B를 BigCodeBench 벤치마크에서 36% 앞서 뛰어난 성능을 보였습니다.
- **Technical Details**: Arctic-SnowCoder-1.3B의 훈련은 3단계로 나누어 진행되었습니다. 첫 번째 단계는 400B의 원시 코드 데이터를 사용하여 초반 훈련을 진행하는 단계로, 500B 토큰에 대해 일반 훈련이 이루어졌습니다. 두 번째 단계에서는 BERT 기반의 품질 평가기를 사용해 12.5B 고품질 코드 파일이 선별되었으며, 이를 4번 반복하여 50B 토큰을 통해 추가 훈련을 진행했습니다. 마지막 단계에서는 Llama-3.1-70B를 사용해 생성된 2B 합성 데이터를 활용해 최종적으로 5B 토큰에 대해 향상된 훈련이 이루어졌습니다. 모델의 아키텍처는 Llama-2 기반으로 구성되었으며, 주요 파라미터로는 hidden_dim 2048, num_layers 24, vocab_size 64000 등이 포함되었습니다.
- **Performance Highlights**: Arctic-SnowCoder-1.3B는 555B 토큰으로 훈련되었으며, BigCodeBench 벤치마크에서 Phi-1.5-1.3B를 36% 앞서는 성과를 보였습니다. 특히, HumanEval+ 벤치마크에서 28.0 점수를 기록하며 StarCoderBase-3B 및 StarCoder2-3B(각각 27.4 점수)를 뛰어넘었습니다. 또한, 다양한 벤치마크에서 동일한 크기의 다른 모델들과 비교해도 동등하거나 우수한 성능을 보였습니다.

### [FastVoiceGrad: One-step Diffusion-Based Voice Conversion with Adversarial Conditional Diffusion Distillation](https://arxiv.org/abs/2409.02245)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.02245.png)

Vote: 6

Authors: Yuto Kondo, Kou Tanaka, Takuhiro Kaneko, Hirokazu Kameoka

- **What's New**: FastVoiceGrad는 다단계(diffusion-based VC) 모델에서 단일 단계(step)로 음향 변환을 수행하는 새로운 방법입니다. 이는 기존의 VoiceGrad 모델을 개선하여 더 빠르고 효율적인 음향 변환을 가능하게 합니다.
- **Technical Details**: FastVoiceGrad는 복합적인 적대적 조건부 확산 증류(adversarial conditional diffusion distillation, ACDD)를 사용하는데, 이는 다단계 교사 모델을 단일 단계로 증류하는 과정에서 GAN과 diffusion 모델의 능력을 활용합니다. 이 방법은 초기 상태를 재고하고 음성 변환을 위한 최적의 설정을 탐구하여 효율적인 모델 구조를 형성합니다.
- **Performance Highlights**: 실험 결과는 FastVoiceGrad가 동일한 단일 단계 간 반전 확산(reversed diffusion) 과정에서 VoiceGrad를 능가함을 보여줍니다. 30단계의 반전 확산 과정을 사용하는 VoiceGrad와 비교할 때도 동등하거나 우수한 성능을 나타냈습니다. 또한, DiffVC와 비교하여 성능은 동등하거나 뛰어나며, 추론 속도에서도 큰 개선을 보였습니다.

### [Political DEBATE: Efficient Zero-shot and Few-shot Classifiers for Political Text](https://arxiv.org/abs/2409.02078)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.02078.png)

Vote: 4

Authors: Kayla Kahn, Ryan Yank Wang, Michael Burnham, Rachel X. Peng

- **What's New**: 이 논문에서는 GPT-4와 GPT-4o 모델을 이용한 텍스트 분류기(classifier)를 설계하여, 주어진 가설(hypothesis)이 텍스트 내용과 일치하는지 판단합니다. 주목할 부분은, 이 모델은 0과 1의 이진 출력만을 생성하도록 설계되었습니다.
- **Technical Details**: 제안된 시스템은 '역할부여 메시지(system_message)'를 통해 모델에게 분류기의 역할을 부여하며, 온도(temperature)를 0으로 설정하여 출력의 확실성을 보장합니다. 또한 logit-bias를 {15:100, 16:100}로 설정하여 특정 클래스의 출력을 강하게 유도합니다. 학습 매개변수(training parameters)로는 선형 학습률 스케줄러(lr_scheduler_type='linear'), 20회의 학습 에포크(num_train_epochs=20), FP16 정밀도(fp16=True), 그리고 데이터로더 작업자 수(dataloader_num_workers=12) 등을 사용합니다.
- **Performance Highlights**: 훈련과 평가 시 각각의 장비 당 배치 사이즈(per_device_train_batch_size=8, per_device_eval_batch_size=8)를 유지하고, gradient accumulation을 통해 효율성을 극대화합니다. 예열 비율(warmup_ratio=0.06)과 가중치 감소(weight_decay=0.01)를 통해 모델의 일반화 성능을 증진시킵니다.

### [Affordance-based Robot Manipulation with Flow Matching](https://arxiv.org/abs/2409.01083)

![](/avatars/2bea6235aa832715d5e27c798216c0dc.svg)

Vote: 4

Authors: Michael Gienger, Fan Zhang

- **What's New**: 이 논문에서는 새로운 로봇 조작(Manipulation) 문제 해결 방법으로 시각-언어 모델(Vision-Language Model, VLM)의 발전을 활용하는 방안을 제시합니다. 특히, 텍스트 조건 기반 프롬프트 학습을 통해 다양한 작업 시나리오에서 조작 가능성(Affordances)을 학습하는 새로운 접근 방식을 도입했습니다. 이 방법은 로봇 조작 정책 학습을 위해 흐름 맞추기(Flow Matching) 기법을 확장한 첫 번째 시도입니다.
- **Technical Details**: 제안된 방법은 크게 세 가지로 요약할 수 있습니다: 1) 사전 학습된 VLM을 미세 조정하는 파라미터 효율적인 프롬프트 튜닝(Parameter-Efficient Fine-Tuning, PEFT) 방법을 사용, 2) 로봇 조작 정책 학습을 위한 시각적 조작 가능성을 활용하는 새로운 흐름 맞추기 기법 채택, 3) 일상 생활 활동(Activities of Daily Living)을 포함한 10가지 작업에 대한 실제 데이터셋 구축 및 평가. 이 방법은 VLM을 효율적으로 텍스트 조건 기반의 프롬프트를 학습시켜 강력한 시각적 이해력을 유지하면서 명령과 관련된 조작 가능성을 추출합니다. 이를 위해 대형 프리트레인된 모델에 학습 가능한 프롬프트를 추가하여 최적화합니다.
- **Performance Highlights**: 실험 결과, 프롬프트 튜닝 방법이 다양한 데이터 스케일, 시각-언어 융합 아키텍처, 프롬프트 변형에 걸쳐 다른 미세 조정 프로토콜과 비교해 경쟁력 있는 성능을 발휘하며, 때로는 그 이상의 성능을 보여주었습니다. 또한, 흐름 맞추기 정책이 로봇 행동 클로닝(Behavior Cloning) 기법들 가운데 안정된 학습 및 생성 품질에서도 높은 성능을 달성함을 입증했습니다.

