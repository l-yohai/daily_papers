## Daily Papers (2024-09-02)

### [SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding](https://arxiv.org/abs/2408.15545)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15545.png)

Vote: 13

Authors: Sihang Li, Mingjun Xu, Hengxing Cai, Jin Huang, Linfeng Zhang, Guolin Ke, Yaorui Shi, Xiaochen Cai, Jiaxi Zhuang, Xiang Wang

- **What's New**: 논문은 과학 문헌 이해를 위한 최신 모델 SciLitLLM을 소개합니다. 기존의 두 가지 접근법인 과학적 명령을 이용한 미세 조정(fine-tuning)과 과학 자료에 기반한 사전 학습(pre-training)의 단점을 해결하기 위해, 이 논문에서는 지속적 사전훈련(CPT)과 감독된 미세 조정(SFT)을 결합한 하이브리드 전략을 제안하고 있습니다.
- **Technical Details**: 이 논문은 두 가지 주요 요구 사항을 강조합니다: 고품질 CPT 코퍼스와 다양한 과학적 명령. 첫 번째 단계에서는 73k 권의 교과서와 625k 편의 학술 논문으로 구성된 대규모 코퍼스를 사용하여 텍스트를 추출하고 이를 정제합니다. 이후, 새로운 명령 생성 방법을 통해 다양한 과학적 명령을 생성하여 모델을 미세 조정합니다. 또한, Llama3-7B-Instruct 모델을 사용하여 PDF 파싱에서 발생하는 오류를 수정하고, 텍스트 품질 분류기를 통해 저품질 텍스트를 필터링합니다.
- **Performance Highlights**: 이 논문에서 제안된 SciLitLLM-7B와 4-bit 양자화된 SciLitLLM-72B 모델은 기존의 LLM들과 비교하여 과학 문헌 이해 벤치마크에서 뛰어난 성능을 보였습니다. SciAssess와 SciRIFF 벤치마크에서 각각 평균 3.6%와 10.1%의 성능 향상을 기록했으며, 7B 모델은 70B 파라미터를 가진 Llama3와 Qwen2 모델보다도 뛰어난 성능을 나타냈습니다.

### [UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal Models in Multi-View Urban Scenarios](https://arxiv.org/abs/2408.17267)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.17267.png)

Vote: 9

Authors: Conghui He, Tianyi Bai, Jinhua Yu, Haote Yang, Dairong Chen, Songyang Zhang, Weijia Li, Baichuan Zhou, Dahua Lin, Junyan Ye

- **What's New**: 최근 연구 커뮤니티에서는 다양한 벤치마크에서 뛰어난 능력을 보여주는 Large Multimodal Models(LMMs)를 개발하는 데 큰 관심을 보이고 있습니다. 이 모델들은 특히 도시 환경에서 인간 중심의 AI 모델로서 일상생활을 돕는 역할을 목표로 하고 있으며, UrBench 벤치마크를 통해 그러한 LMMs를 평가하고 개발하는 새로운 방법론을 도입했습니다.
- **Technical Details**: UrBench는 도시 환경에서 LMMs의 성능을 종합적으로 평가하기 위해 설계된 다중 작업 및 다중 보기 벤치마크입니다. Geo-Localization, Scene Understanding, Scene Reasoning, Object Understanding 등 4가지 차원에 걸쳐 총 14개의 작업을 포함하고 있으며, 이를 위해 크로스-뷰 감지 및 매칭 알고리즘을 활용하여 annotation을 생성하는 새로운 벤치마크 큐레이션 파이프라인이 도입되었습니다.
- **Performance Highlights**: UrBench를 통해 21개의 인기 있는 LMMs를 평가한 결과, 대부분의 작업에서 현재 모델들이 인간 전문가보다 성능이 저조하며, 여러 도시 뷰에서의 일관되지 않은 행동을 보였습니다. 이는 현재 LMMs가 복잡한 도시 환경에서 인간 중심의 어시스턴트 역할을 하는 데 한계가 있음을 보여줍니다.

### [CoRe: Context-Regularized Text Embedding Learning for Text-to-Image Personalization](https://arxiv.org/abs/2408.15914)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15914.png)

Vote: 9

Authors: Yun Pang, Jian Yin, Feize Wu, Qing Li, Baoquan Zhao, Junyi Zhang, Xudong Mao, Lianyu Pang

- **What's New**: 새로운 연구에서는 텍스트-이미지 개인화(text-to-image personalization)를 위한 텍스트 정렬(text alignment)을 개선하는 방법을 제안합니다. 이 방법은 CLIP 텍스트 인코더(encoder)를 활용하여 프롬프트(prompt)에 대한 더 정확한 의미론적 이해를 목표로 합니다. 새로운 개념의 텍스트 임베딩(text embedding)을 학습하는 대신, 프롬프트 내 새로운 개념 주변의 컨텍스트 토큰(context tokens)을 조정하여 텍스트 임베딩을 개선합니다.
- **Technical Details**: 제안된 방법은 Context Regularization(CoRe)이라고 불리며, 이는 새로운 개념의 텍스트 임베딩을 정규화하여 프롬프트의 의미론적 이해를 증진시키는 것입니다. CoRe는 CLIP 텍스트 인코더의 출력 임베딩과 컨텍스트 토큰의 주의(attention) 맵에 제약을 가하여, 새로운 개념을 포함한 프롬프트가 기존 슈퍼 카테고리(super-category) 토큰을 포함한 프롬프트와 비슷한 출력 결과를 낼 수 있도록 합니다. CoRe는 이미지 생성 없이 어떤 프롬프트에도 적용될 수 있어 범용성을 높였습니다.
- **Performance Highlights**: 이 방법은 다양한 최첨단 개인화 방법과의 비교를 통해 높은 수준의 정체성(identity) 보존과 텍스트 정렬을 달성함을 입증했습니다. 특히 높은 시각적 변화를 요구하는 프롬프트에서 특히 우수한 성능을 보였습니다. 얼굴 개인화에서도 탁월한 성능을 보였으며, 최근의 다른 얼굴 개인화 방법들과 비교하여 더 잘 보존된 얼굴 이미지를 생성했습니다.

### [The VoxCeleb Speaker Recognition Challenge: A Retrospective](https://arxiv.org/abs/2408.14886)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.14886.png)

Vote: 6

Authors: Arsha Nagrani, Andrew Brown, Daniel Garcia-Romero, Jaesung Huh, Joon Son Chung, Jee-weon Jung, Andrew Zisserman

- **What's New**: VoxCeleb Speaker Recognition Challenges (VoxSRC)는 2019년부터 2023년까지 매년 개최된 연례 도전 과제 시리즈입니다. 이 챌린지는 화자 인식 및 분할 분야에서 새로운 연구를 탐색하고 촉진하며, 최신 상태를 측정하고 조정하고, 커뮤니티에 무료 및 오픈 소스 데이터를 제공하는 것을 목표로 했습니다.
- **Technical Details**: VoxSRC의 주요 작업은 화자 검증(speaker verification)과 화자 분할(speaker diarisation)입니다. 화자 검증 작업에서는 두 발화가 동일한 화자인지 확인하며, 화자 분할 작업에서는 다중 화자 세그먼트를 대상으로 '누가 언제 말하는지'를 라벨링합니다. 도전 과제는 주로 웹에서 수집된 제약 없는 음성 데이터를 사용하며, 데이터는 시끄럽고 다양하며 때로는 매우 짧고 일시적인 발화로 구성됩니다. 또한, 경쟁자는 평가 항목에 따라 실수 예측 점수를 제출합니다.
- **Performance Highlights**: VoxCeleb2 dev 세트를 사용하는 'Speaker verification – closed' 트랙에서는 참가자가 공개된 데이터를 사용하여 최고 성능을 달성하도록 장려되었습니다. 'Speaker verification – open' 트랙에서는 추가 데이터 사용이 허용되었으며, 'Self-supervised speaker verification' 트랙에서는 레이블 없는 데이터를 사용한 자가 학습(Self-supervised learning) 접근 방식을 조사했습니다. 'Semi-supervised domain adaptation' 트랙에서는 소량의 라벨이 있는 데이터를 사용하여 다른 언어 도메인에 모델을 적응시키는 능력을 평가했습니다.

### [CrossViewDiff: A Cross-View Diffusion Model for Satellite-to-Street View Synthesis](https://arxiv.org/abs/2408.14765)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.14765.png)

Vote: 5

Authors: Conghui He, Huaping Zhong, Jun He, Weijia Li, Zhimeng Zheng, Dahua Lin, Zilong Huang, Junyan Ye

- **What's New**: CrossViewDiff라는 새로운 cross-view diffusion 모델을 제안하여 위성 이미지에서 스트리트 뷰(street-view) 이미지를 생성하는 성능을 향상시켰습니다.
- **Technical Details**: 기하학적 구조와 텍스처 컨트롤(controls)을 위성 이미지에서 추출하는 구조 및 텍스처 매핑 모듈을 설계하고, CrossView Control Guided Denoising Process를 통해 구조와 텍스처 일관성을 강화했습니다. 또한 텍스트, 지도 데이터, 건물 높이 데이터 및 다중 시간 위성 이미지 등 다양한 데이터 소스를 활용하였습니다.
- **Performance Highlights**: 세 가지 공공 데이터셋에서 실험한 결과, CrossViewDiff는 SSIM에서 평균 9.0%, FID에서 39.0%, GPT 기반 점수에서도 35.5% 향상된 성능을 보였습니다.

### [InkubaLM: A small language model for low-resource African languages](https://arxiv.org/abs/2408.17024)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.17024.png)

Vote: 5

Authors: Eric Peter Wairagala, Jenalea Rajab, Pelonomi Moiloa, Bonaventure F. P. Dossou, Vukosi Marivate, Jessica Ojo, Benjamin Rosman, Jade Abbott, Aremu Anuoluwapo, Atnafu Lambebo Tonja, Fadel Thior

- **What's New**: 이번 논문에서는 InkubaLM이라는 새로운 소형 다국어 모델을 소개합니다. InkubaLM은 수십억 개의 파라미터를 지닌 대형 언어 모델(LLM)과는 달리, 비교적 작은 크기로도 강력한 성능을 발휘하도록 설계되었습니다. 특히 아프리카 언어를 지원하는 첫 오픈 소스 소형 모델이라는 점에서 의미가 큽니다.
- **Technical Details**: InkubaLM은 문장 분류, 감정 분석, 개체명 인식(NER), 품사 태깅(POS), 질문 응답과 같은 다양한 NLP 작업을 수행할 수 있도록 설계되었습니다. 또한, 이 모델은 단일 코드 포맷, 공통 어휘, 언어 간 공유 표현을 통해 성능을 극대화하고 있습니다. 게다가 두 개의 데이터셋을 동반하여 더 나은 성능을 자랑합니다.
- **Performance Highlights**: 논문에서 발표된 InkubaLM은 고자원 언어만을 지원하는 기존의 LLM들과 달리, 아프리카 언어를 효과적으로 지원하도록 설계되어 있습니다. 이는 특히 데이터가 부족한 저자원 언어에서도 높은 성능을 보인다는 점에서 중요한 의미를 가집니다. InkubaLM은 이러한 언어 불균형을 해결하는 첫걸음이 될 것입니다.

### [VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers](https://arxiv.org/abs/2408.17131)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.17131.png)

Vote: 4

Authors: Zeyu Wang, Shuaiting Li, Kejie Huang, Hong Gu, Juncan Deng, Kedong Xu

- **What's New**: 최근 진보된 텍스트-투-이미지 확산 모델(text-to-image diffusion models)의 성과에 기반해, 확산 트랜스포머 모델(DiTs)이 등장했습니다. DiTs는 뛰어난 성능을 보여주고 있으며, OpenAI의 SoRA가 그 대표적인 응용 사례입니다. DiTs는 여러 트랜스포머 블록을 순차적으로 쌓아 올리는 아키텍처로, 더 많은 파라미터를 사용하여 유연한 확장이 가능합니다.
- **Technical Details**: DiTs는 많은 파라미터와 고도의 계산 복잡성을 갖추고 있지만, 이렇게 큰 모델을 배포하는 데에는 상당한 비용이 듭니다. 예를 들어, DiT XL/2 모델을 사용해 256×256 해상도의 이미지를 생성하는 데에는 약 17초가 소요되며, NVIDIA A100 GPU에서 10^5 Gflops가 필요합니다. 또한, SoRA 비디오 생성 모델의 경우 약 30억개의 파라미터를 가지고 있어, 제한된 자원을 가진 엣지 디바이스에 배포하기 어렵습니다.
- **Performance Highlights**: VQ4DiT는 DiT 모델을 매우 낮은 비트 폭으로 양자화(post-training vector quantization)하는 새로운 방식으로, K-Means 알고리즘을 통해 각 층의 가중치 서브벡터를 코드북(codebook)에 맵핑합니다. 이 방법은 코드북과 할당을 동시에 보정하여, 양자화된 모델의 성능을 부동소수점 모델과 비교해 경쟁력을 유지하게 합니다. 이를 통해, VQ4DiT는 ImageNet 벤치마크에서 전체 정밀 모델에 비견될만한 평가 결과를 얻었습니다.

### [Automatic Differential Diagnosis using Transformer-Based Multi-Label Sequence Classification](https://arxiv.org/abs/2408.15827)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15827.png)

Vote: 4

Authors: Mohammad Ashrafuzzaman Khan, Abu Adnan Sadi, Lubaba Binte Saber

- **What's New**: 이 연구는 Transformer 기반의 아키텍처를 활용하여 차별적 진단(differential diagnosis)을 수행하는 방법을 제안합니다. 기존의 NLP 모델들이 단일 진단을 목표로 하는 반면, 본 연구는 환자의 나이, 성별, 의료 역사, 증상 등을 이용해 다양한 잠재적 질병 목록을 출력하는 다중 레이블 분류(multi-label classification) 문제로 접근합니다.
- **Technical Details**: 본 연구는 공개된 DDXPlus 데이터셋을 사용하여 Transformer 모델을 미세 조정(fine-tuning)하는 방식으로 차별적 진단을 수행합니다. DDXPlus 데이터셋은 49개의 질병 카테고리와 관련된 환자 데이터를 포함하고 있습니다. 데이터셋 처리 기술을 통해 환자의 상황을 텍스트 보고서로 변환하고, 그 보고서로 Transformer 모델을 학습시킵니다. 또한, 데이터 변환 모듈을 제안하여 의학 용어를 다양화하고 모델의 강인성을 향상시킵니다.
- **Performance Highlights**: 모델의 강인성과 제한 사항을 평가하기 위해 추가 테스트 케이스를 설계하고 세 가지 행동 테스트를 통해 제안된 접근 방법의 성능을 추가로 평가합니다. 기존 연구에 비해 이 연구는 진단 초기 단계에서 환자의 증상에 따른 잠재적 질병 목록을 제공하여 의사가 추가 테스트를 통해 최종 진단을 보다 정확하게 내릴 수 있도록 지원합니다.

### [Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever](https://arxiv.org/abs/2408.16672)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16672.png)

Vote: 3

Authors: Michael Günther, Mohammad Kalim Akram, Han Xiao, Saba Sturua, Bo Wang, Rohan Jha

- **What's New**: 최근 유능한 사전 학습된 언어 모델(Pre-trained Language Models, PLMs)이 등장함에 따라 뉴럴 검색(Neural Retrieval)이 많은 관심을 받고 있습니다. 기존의 싱글-벡터 인코더 모델과 다르게, Jina-ColBERT-v2는 다언어 성능을 개선하고 효율성을 높이는 여러 새로운 기능들을 제안합니다.
- **Technical Details**: 이번 연구에서는 다양한 약한 감독 데이터(weakly supervised data)를 활용한 대규모 대조 튜닝(contrastive tuning)과 작은 규모의 감독 증류(supervised distillation)를 포함한 2단계 학습 방식을 제안합니다. 또한, 학습된 토큰 임베딩 크기를 추론 단계에서 최소한의 성능 저하로 선택할 수 있도록 여러 크기의 선형 프로젝션 헤드(linear projection heads)를 도입하였으며, 플래시-어텐션 최적화된 백본(flash-attention optimized backbone)인 Jina-XLM-RoBERTa를 사용했습니다.
- **Performance Highlights**: Jina-ColBERT-v2는 영어 및 다언어 벤치마크에서 경쟁력 있는 검색 성능을 보였습니다. 다양한 고자원 및 저자원 언어를 포함한 데이터를 학습한 결과, 도메인 외(out-of-domain) 다언어 성능도 향상되었습니다. 또한, 제안된 훈련 방식이 실제로 성능 향상에 기여하는지에 대한 여러 통제 실험 결과를 제시하였습니다.

### [VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images](https://arxiv.org/abs/2408.16176)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16176.png)

Vote: 3

Authors: Medha Sawhney, Yasin Bakis, Abhilash Neog, Charles Stewart, Henry L. Bart, Bahadir Altintas, Wasila Dahdul, Harish Babu Manogaran, Yu Su, Anuj Karpatne, M. Maruf, Matthew J. Thompson, Hilmar Lapp, James P. Balhoff, Arka Daw, Kazi Sajeed Mehrab, Tanya Berger-Wolf, Mridul Khurana, Paula M. Mabee, Josef C. Uyeda, Elizabeth G. Campolongo, Wei-Lun Chao

- **What's New**: 생물학적 이미지의 양이 급증하면서 생물체의 구조, 생태 및 진화를 연구하는 학문에서 이미지 분석의 중요성이 커지고 있습니다. 최근에 다양한 텍스트 및 이미지 작업을 동시에 해결할 수 있는 대규모 기반 모델(VLMs)이 등장하면서, 이러한 VLMs가 생물학적 특성(traits)을 이미지에서 자동으로 식별하고 추론할 수 있는지 평가가 필요해졌습니다. 이 연구는 12개의 최신 VLMs를 이용하여 생물학적 특성 식별, 종류 분류 등 5가지 과학적으로 관련된 작업에서 그들의 성능을 평가합니다.
- **Technical Details**: 연구에서는 사전 학습된 최신 VLMs (예: GPT-4V(ision), GPT-4o 등)을 사용하여 어떤 작업에서의 성능과 시각적 추론 능력을 테스트하기 위해 VLM4Bio 벤치마크 데이터셋을 개발하였습니다. 이 데이터셋은 약 30,000장의 물고기, 새, 나비 이미지를 기반으로 한 약 469,000개의 질문-답변 쌍으로 구성됩니다. 주요 작업은 종 분류(species classification), 특성 식별(trait identification), 특성 지칭(trait referring), 특성 위치 지정(trait grounding), 특성 개수 세기(trait counting)입니다.
- **Performance Highlights**: 연구 결과, 다양한 VLMs가 종 분류, 특성 식별 등의 예측 정확도 측면에서는 우수한 성능을 보였지만, 시각적 단서와 생물학적 지식을 바탕으로 추론하는 능력에서는 여전히 한계가 있습니다. 특히, VLMs가 특성 지칭과 위치 지정 작업에서 제대로 된 결과를 도출하지 못하는 경우가 종종 발생하였습니다. 이 연구는 또한 프롬프팅(prompting)의 효과와 VLM의 추론 과정에서 발생하는 환각(hallucination)을 분석하여, VLM의 생물 학적 추론 능력에 대한 새로운 통찰을 제공하였습니다.

### [CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation](https://arxiv.org/abs/2408.14572)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.14572.png)

Vote: 3

Authors: Muhammad Fawi

- **What's New**: 이번 연구는 CURLoRA라는 새로운 접근 방식을 소개합니다. CURLoRA는 큰 언어 모델(LLM)들을 파인 튜닝하는 과정에서 재앙적 망각(catastrophic forgetting)을 완화하기 위해 CUR 행렬 분해 방법을 사용합니다.
- **Technical Details**: CURLoRA 접근 방식은 CUR 행렬 분해를 사용하여 사전에 훈련된 가중치 행렬을 분해한 후 U 행렬만 파인 튜닝합니다. 이 방법은 파라미터 공간을 제한하여 훈련 가능한 파라미터 수를 최소화합니다. CUR 분해는 본래의 행렬에서 실제 열과 행을 사용하여 해석 가능성을 높입니다. 이 접근 방식은 열의 통계적 중력을 기반으로 중요한 열과 행을 선택합니다.
- **Performance Highlights**: CURLoRA는 기존의 LoRA가 가진 재앙적 망각 문제를 효과적으로 완화하며, 필요한 훈련 가능한 파라미터 수를 줄이는 데 성공적임을 보였습니다. 이는 특히 자원 제한 환경에서 모델 적응을 더욱 효율적이고 실행 가능하게 만듭니다.

### [GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs](https://arxiv.org/abs/2408.15300)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15300.png)

Vote: 2

Authors: Evgeny Burnaev, Egor Shvetsov, Egor Venediktov, Aleksandr Zuev, Viktor Moskvoretskii, Mariya Krylova, Maxim Zhelnin

- **What's New**: 최신 연구는 LLMs (Large Language Models)의 성능 향상을 위해 GIFT-SW라는 새로운 Parameter Efficient Fine-Tuning(PEFT) 방법을 제안합니다. 이 방법은 중요한 가중치들(salient weights)의 일부만 업데이트하고, 덜 중요한 가중치에는 노이즈를 주입합니다.
- **Technical Details**: GIFT-SW는 사전에 학습된 LLM의 중요한 가중치 열만을 미세 조정(fine-tune)하고, 나머지 가중치에는 Gaussian 노이즈를 주입하여 학습 중에 고정합니다. 또한, 민감도 메트릭(sensitivity metrics)을 일반화하여 중요한 가중치 열을 식별하는데 사용됩니다.
- **Performance Highlights**: GIFT-SW는 최신 PEFT 방법 및 전체 미세 조정 베이스라인 대비 대부분의 zero-shot 작업에서 더 우수한 성능을 발휘합니다. GIFT-SW는 LLaMA 모델에서 전체 파라미터의 3%만 미세 조정하면서도 TÜLU2 모델과 비교해 거의 동일한 정확도를 달성하며, 10배 적은 계산 자원을 사용합니다.

### [SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section](https://arxiv.org/abs/2408.16444)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16444.png)

Vote: 2

Authors: Thiago Soares Laitz, Roberto Lotufo, Gustavo Bartz Guedes, Jayr Pereira, Rodrigo Nogueira, Thales Sales Almeida, Leandro Carísio Fernandes

- **What's New**: 이 논문은 SurveySum이라는 새로운 데이터셋을 소개합니다. SurveySum은 여러 과학기술 논문을 요약하여 학술 조사 섹션을 생성하는 데 중점을 두고 있습니다. 이 데이터셋은 분야별 요약 도구의 부재를 해소하고, 과학적 조사 문서의 여러 섹션을 자동으로 생성하기 위한 특정 파이프라인을 제시합니다.
- **Technical Details**: SurveySum 데이터셋은 인공지능, 자연어 처리 및 머신러닝 분야에서 광범위한 주제를 포함하는 6개의 설문에서 텍스트와 인용을 추출하여 작성됩니다. s2orc-doc2json을 사용하여 과학 논문을 JSON 형식으로 변환하고 인용을 추출한 다음, 다양한 API와 크롤러를 사용하여 전체 텍스트를 검색했습니다. 최종적으로 전체 텍스트와 인용을 수동으로 검증해 데이터 품질을 보장했습니다.
- **Performance Highlights**: SurveySum 데이터셋은 79개의 섹션에서 평균 7.38개의 논문과 관련이 있습니다. 이 파이프라인의 단계는 (1) 제목과 관련 논문을 정의, (2) 텍스트를 청크로 나누고 검색 알고리즘을 사용해 관련 청크를 검색, (3) 대형 언어 모델(LLM)을 이용해 섹션 텍스트를 생성하는 것입니다.

### [Large-Scale Multi-omic Biosequence Transformers for Modeling Peptide-Nucleotide Interactions](https://arxiv.org/abs/2408.16245)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16245.png)

Vote: 2

Authors: Beakal Lemeneh, Shivanand P. Lad, Robert J. Steele, Sully F. Chen, Eric Oermann

- **What's New**: 바이오인포매틱스 분야에서 중요한 목표는 뉴클레오타이드와 펩타이드 서열의 주요 정보를 추출하는 모델을 개발하는 것입니다. 특히 고속 시퀀싱 (high throughput sequencing) 기술의 발전으로 인해 이 목표의 중요성은 더욱 커졌습니다. Transformer 아키텍처를 기반으로 한 자연어 처리 (NLP) 모델들이 최근 큰 성과를 내며 이러한 목표를 달성하는 데 기여하고 있습니다. 본 연구에서는 뉴클레오타이드와 펩타이드 서열을 동시에 모델링하는 멀티-오믹스 트랜스포머 바이오서열 모델 (MOMs)을 제안하며, 이를 OmniBioTE (omni-biosequence transformer encoders)라고 명명하였습니다.
- **Technical Details**: OmniBioTE 모델은 뉴클레오타이드와 펩타이드 서열을 큰 데이터 세트에서 프리트레이닝(pretraining)하여 공동 표현 공간을 학습합니다. 이를 통해 단백질-뉴클레오타이드 결합 예측, 변이의 결합 친화도 영향을 예측하는 등의 작업에 높은 정확성을 보입니다. 또한 이 모델은 첫 번째 서열만으로도 구조적 정보를 자연스럽게 학습하는 능력을 보여줍니다. 동일한 컴퓨팅 자원으로 뉴클레오타이드나 펩타이드 서열만을 사용하여 학습한 모델들과 비교했을 때 다중 서열을 포함하여 학습해도 성능 저하가 거의 없는 것으로 나타났습니다.
- **Performance Highlights**: OmniBioTE 모델은 다운스트림 멀티-오믹스 작업에서 놀라운 성능을 보였습니다. 예를 들어, 모델은 단백질-뉴클레오타이드 상호작용의 ΔG와 ΔΔG를 높은 정확도로 예측하며, 펩타이드 잔기와 뉴클레오타이드의 결합 위치도 정확하게 예측합니다. 또한 단일-오믹스 작업에서도 성능 저하가 거의 없으며, 모델이 분자 생물학의 중심 교리(Central Dogma)를 이해하는 능력을 보여줍니다.

### [ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution](https://arxiv.org/abs/2408.15993)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15993.png)

Vote: 2

Authors: Anahita Bhiwandiwalla, Sungduk Yu, Brian L. White, Matthew Lyle Olson, Tung Nguyen, Vasudev Lal, Musashi Hinck

- **What's New**: 이번 연구는 기후 변화 탐지 및 귀속(D&A)을 위한 데이터셋 'ClimDetect'를 소개합니다. 이 데이터셋은 CMIP6 모델 앙상블에서 역사적 및 미래 기후 시나리오를 기반으로 한 816,000개 이상의 일일 스냅샷을 포함하고 있습니다. ClimDetect는 기후 변화 신호를 감지할 수 있는 모델 개발을 촉진하기 위해 설계되었습니다.
- **Technical Details**: ClimDetect는 다양한 기후 반응 변수 및 기후 강제 변수(target variables)를 대표하는 데이터를 포함합니다. 데이터셋은 총 28개의 다른 기후 모델과 142개의 모델 앙상블 실행을 포함하며, 역사적 시나리오(1850-2014)와 SSP2-4.5 및 SSP3-7.0 시나리오를 다룹니다. ClimDetect는 모델의 다양성 및 균형성을 강조하며, 여러 기후 강제 시나리오를 제공하여 모델 학습의 폭을 넓힙니다. Vision Transformers (ViT)와 같은 최신 머신러닝 아키텍처가 기후 데이터를 분석하는 데 사용되었습니다.
- **Performance Highlights**: Vision Transformers (ViT)를 사용한 연구 방법은 기존의 통계적 기법보다 더 섬세한 패턴을 발견할 수 있는 가능성을 보여줍니다. 기후 변화 탐지 및 귀속(D&A)에 대한 최신 연구들과 비교할 때, ClimDetect 데이터셋은 입력 변수를 표준화하고, 연구 간의 일관성과 비교 가능성을 촉진합니다. 이 데이터셋은 기후 변동성 및 모델 간의 예측 성능을 향상시키기 위한 벤치마크 역할을 제공합니다.

### [Iterative Graph Alignment](https://arxiv.org/abs/2408.16667)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16667.png)

Vote: 1

Authors: Hardeep Singh Arora, Fangyuan Yu, Matt Johnson

- **What's New**: 이번 연구는 대규모 언어 모델(LLM)의 한계를 극복하기 위해 Iterative Graph Alignment (IGA) 알고리즘을 제안합니다. IGA는 논리적인 그래프와 답변을 생성하여 학습 모델에 적응적이고 정렬된 학습 과정을 제공하며, 이는 모델이 답을 이해하는 '왜'와 '무엇'을 모두 포함합니다.
- **Technical Details**: 본 연구에서는 압축(compression)이 단순 암기(memorization)로 이어질 수 있는 문제를 지적합니다. 이를 해결하기 위해 Iterative Graph Prompting (IGP)와 Self-Aligned Incremental Learning (SAIL) 접근법을 도입했습니다. IGP는 시각적 언어 모델(Vision-Language Model)을 사용하여 논리적인 그래프를 생성하고, 이를 통해 병렬 처리가 가능하며 전역 인식을 활용합니다. SAIL은 '제안 및 확인' 매커니즘을 통해 정렬된 답변과 정렬되지 않은 답변을 식별합니다.
- **Performance Highlights**: 실증 평가 결과, IGP를 적용한 Claude Sonnet 3.5 모델의 규칙 정렬이 기준 프롬프팅 대비 73.2% 향상되었으며, IGA로 미세 조정된 Llama3-Instruct 모델은 기준 대비 86.2% 향상되었습니다. 이는 IGP를 통해 Claude Sonnet 3.5와 동등한 성능을 보였습니다.

