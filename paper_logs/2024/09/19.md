## Daily Papers (2024-09-19)

### [Qwen2.5-Coder Technical Report](https://arxiv.org/abs/2409.12186)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.12186.png)

Vote: 63

Authors: Junyang Lin, Jian Yang, Xuancheng Ren, Zeyu Cui, Jiaxi Yang, Kai Dang, Xingzhang Ren, Jingren Zhou, Fei Huang, Rui Men, Bowen Yu, An Yang, Lei Zhang, Tianyu Liu, Jiajun Zhang, Binyuan Hui, Dayiheng Liu

- **What's New**: Qwen2.5-Coder 모델 시리즈가 새롭게 선보였습니다. CodeQwen1.5의 후속 모델로, 다양한 규모의 모델 크기에 대해 최고 수준의 성능을 자랑합니다. Qwen2.5-Coder는 오픈소스로 제공되어 코딩 에이전트 및 코딩 어시스턴트 애플리케이션 분야에서의 연구와 혁신을 촉진하는 데 기여할 예정입니다.
- **Technical Details**: Qwen2.5-Coder는 Qwen2.5 아키텍처와 동일한 아키텍처를 사용하는지만, 각각 1.5B와 7B 파라미터 모델이 존재합니다. 두 모델은 층(layer) 수와 헤드 크기(128)가 동일하지만, 히든 사이즈와 쿼리 및 키-값 헤드 수에서 차이를 보입니다. Qwen2.5-Coder는 고유한 코드 이해를 위한 특수 토큰을 도입하여 다양한 코드 데이터 구조를 처리할 수 있습니다.
- **Performance Highlights**: Qwen2.5-Coder는 총 5.5조 토큰을 사용해 사전 학습되었으며, 데이터 처리 및 훈련 기술을 통해 코드 관련 10개 이상의 벤치마크에서 최첨단 성능을 달성했습니다. 매우 큰 모델을 능가하는 성능을 보이며, 실세계 애플리케이션에서의 광범위한 채택을 촉진할 수 있을 것입니다.

### [Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](https://arxiv.org/abs/2409.12191)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.12191.png)

Vote: 39

Authors: Keqin Chen, Shuai Bai, Sinan Tan, Yang Fan, Chang Zhou, Mengfei Du, Jinze Bai, Dayiheng Liu, Junyang Lin, Shijie Wang, Xuancheng Ren, Wenbin Ge, Jingren Zhou, Zhihao Fan, Rui Men, Xuejing Liu, Peng Wang, Kai Dang, Jialin Wang

- **What's New**: 이번 Qwen2-VL 시리즈는 세 가지 크기의 모델(Qwen2-VL-2B, Qwen2-VL-7B, Qwen2-VL-72B)로 구성되며 각각 20억, 70억, 720억 개의 파라미터를 갖추고 있습니다. 독자적인 여러 해상도와 비율을 이해할 수 있다는 점, 20분 이상의 비디오를 이해할 수 있는 능력, 다양한 장치에서 자율적으로 작동할 수 있는 강력한 에이전트 기능과 다국어 지원 등이 주요 발전 사항입니다.
- **Technical Details**: Qwen2-VL 시리즈는 675M 파라미터의 Vision Transformer (ViT)을 사용하여 이미지와 비디오 입력을 처리합니다. 동적 해상도로 이미지를 처리할 수 있는 나이브 다이내믹 해상도 지원과 2D Rotary Position Embedding (RoPE) 도입 등이 주요 기술적 개선사항입니다. 또한, 기존의 1D-RoPE 대신 Multimodal Rotary Position Embedding (M-RoPE)를 사용하여 텍스트, 이미지, 비디오와 같은 다중 모달 입력의 위치 정보를 효과적으로 모델링합니다.
- **Performance Highlights**: Qwen2-VL은 DocVQA, InfoVQA, RealWorldQA, MTVQA, MathVista 등의 비주얼 벤치마크에서 최고 성능을 기록하며, 20분 이상의 비디오 콘텐츠를 높은 품질로 이해하고 질문 응답, 대화, 콘텐츠 생성 등 다양한 작업을 수행할 수 있습니다. 다국어 지원 기능으로 영어와 중국어 이외에도 한국어, 일본어, 아랍어, 유럽 언어 등 다양한 언어를 이해할 수 있게 되었습니다.

### [LLMs + Persona-Plug = Personalized LLMs](https://arxiv.org/abs/2409.11901)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11901.png)

Vote: 15

Authors: Yutao Zhu, Shuting Wang, Erxue Min, Zhicheng Dou, Yu Lu, Shuaiqiang Wang, Dawei Yin, Jiongnan Liu, Xiaochi Wei

- **What's New**: 이번 연구에서는 대규모 언어 모델(LLMs)을 개인화하는 새로운 접근법인 페르소나 플러그(PPlug) 모델을 제안했습니다. PPlug 모델은 사용자 임베딩 모듈을 통해 사용자 개인의 패턴을 학습하여 LLM이 개인화된 출력을 생성하도록 돕습니다.
- **Technical Details**: PPlug 모델은 사용자 행동 인코더와 입력 인식 개인 집계기를 포함합니다. 사용자 행동 인코더는 사용자의 각 행동을 벡터 형식으로 변환합니다. 그런 다음 입력 인식 개인 집계기가 현재 작업 입력과 관련성을 기반으로 벡터를 종합하여 사용자 전용 임베딩을 생성합니다. 이 사용자 임베딩은 고정된 LLM에 추가되어 개인화된 출력을 생성하는 데 사용됩니다.
- **Performance Highlights**: Public language model personalization (LaMP) 벤치마크에서 수행된 실험 결과, PPlug 모델이 기존 개인화 LLM 모델 대비 1.4%에서 35.8%까지 성능을 향상시켰습니다.

### [To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning](https://arxiv.org/abs/2409.12183)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.12183.png)

Vote: 14

Authors: Xinyu Zhao, Dongwei Jiang, Zayne Sprague, Xi Ye, Greg Durrett, Juan Diego Rodriguez, Prasann Singhal, Fangcong Yin, Manya Wadhwa, Kyle Mahowald

- **What's New**: 이 연구는 최근 문헌에서 보고된 CoT(Chain-of-thought) 기법과 직접 답변(DA) 방식의 성능을 체계적으로 메타 분석하고, 20개의 데이터셋과 14개의 최신 대형 언어 모델(LLMs)을 사용한 실험을 통해 CoT가 언제, 왜 도움이 되는지를 평가합니다.
- **Technical Details**: CoT는 수학적, 논리적 또는 알고리즘적 추론이 필요한 문제에서 주로 도움이 됩니다. 본 연구는 CoT가 문제를 해결하는 두 단계(계획과 실행) 중 실행 단계에서 특히 효과적이라는 것을 발견했습니다. 하지만 CoT와 도구 증강(tool augmentation)을 결합한 경우, CoT만 사용하는 것보다 더 나은 성능을 보였습니다.
- **Performance Highlights**: CoT는 수학적 문제에서는 95% 이상의 성능 향상을 가져왔지만, 비수학적 문제에서는 거의 도움이 되지 않거나 오히려 성능을 저하시켰습니다. MMLU 데이터셋 분석 결과, CoT의 성능 향상은 수학 관련 질문에서만 나타났습니다. 비수학적 질문에 대해서는 CoT가 도움을 줄 수 있는 특징을 찾지 못했습니다.

### [A Controlled Study on Long Context Extension and Generalization in LLMs](https://arxiv.org/abs/2409.12181)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.12181.png)

Vote: 12

Authors: Justin T. Chiu, Siyu Ren, Yi Lu, Jing Nathan Yan, Zhiyong Wu, Alexander M. Rush, Fei Yuan, Wenting Zhao, Songlin Yang

- **What's New**: 최근 몇 년간 대형 언어 모델(LLMs)의 사전 학습 데이터 규모가 크게 증가하였습니다. 하지만, 긴 컨텍스트 윈도우를 완전히 학습하는 것은 여전히 어렵습니다. 이 논문에서는 컨텍스트 확장(context extension) 방법을 제안하여 모델의 능력을 비교하고 평가하고자 합니다.
- **Technical Details**: {'Pretraining Data': '사전 학습 데이터 규모 최대 15T 토큰까지 확장 (AI@Meta, 2024).', 'Challenges': '긴 컨텍스트 윈도우 학습의 구현 어려움 (Liu et al., 2023a).', 'Context Extension': '기존 표준 시퀀스로 사전 학습한 후, 긴 컨텍스트 길이로 적응. 다양한 attention 타입과 포스트 트레이닝 적응 기술 사용 (Chen et al., 2023a; Peng et al., 2023).', 'Metrics': '새로운 메트릭인 long context perplexity와 retrieval accuracy 도입 (Chen et al., 2023a, b; Han et al., 2023).'}
- **Performance Highlights**: {'Intrinsic Metrics': '퍼플렉서티(perplexity)와 다운스트림 과제 성능 간 강한 상관관계 발견. 일부 효율적 attention 방법은 퍼플렉서티와 다운스트림 과제 성능 간 균형 부족.', 'Approximate Methods': '대부분의 벤치마크에서 정확도 트레이드오프 발생. Hyperparameter가 민감하고 일반 트레이닝 레시피로는 성능 저하.', 'Exact Attention': '정확 attention을 사용한 지속적 미세조정 방법이 전반적으로 우수한 성능 발휘. 특히 Dynamic NTK(eMozilla, 2023) 방법이 최고 성능을 보임.', 'Extrapolation': '긴 컨텍스트로의 외삽 여전히 도전적 문제.'}

### [Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey](https://arxiv.org/abs/2409.11564)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11564.png)

Vote: 12

Authors: Shi-Xiong Zhang, Hanyang Zhao, Anirban Das, Sambit Sahu, Wenpin Tang, Genta Indra Winata, David D. Yao

- **What's New**: 최신 연구는 인간 피드백을 활용하여 생성 모델을 인간의 선호에 맞추는 단계가 중요하다는 점을 강조합니다. 이는 모델이 인간의 언어와 글쓰기를 더 잘 모방할 수 있게 합니다. 이러한 연구들은 사람들이 명령을 잘못 해석하는 문제와 생성되는 컨텐츠의 안전성 문제를 다루고 있습니다.
- **Technical Details**: 이 논문은 강화학습(Reinforcement Learning, RL) 문제로 규정된 선호 조정(Preference Tuning) 기법을 설명합니다. 정책 모델(policy model)은 입력 프롬프트(x)와 결과 시퀀스(y)를 생성하는데, 보상 모델(Reward Model, RM)은 입력과 타겟을 처리하여 선호도를 반영한 보상(rθ⁢ (y|x))을 계산합니다. 텍스트 기반 작업의 경우 입력은 텍스트 시퀀스이며 출력은 텍스트 어휘의 확률 분포입니다. 비전-텍스트 기반 작업에서는 입력이 텍스트 시퀀스이고 출력이 생성된 이미지입니다.
- **Performance Highlights**: 이 논문은 다양한 접근 방식을 통해 생성 모델의 성능을 향상시키는 데 초점을 맞추고 있으며, 이는 생성된 컨텐츠의 품질이 더 높아지고, 인간의 피드백을 통해 보다 인간 친화적인 결과를 만들어낼 수 있음을 보여줍니다. 특히, 강화학습을 활용한 방법들이 많은 성공을 거두고 있습니다.

### [GRIN: GRadient-INformed MoE](https://arxiv.org/abs/2409.12136)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.12136.png)

Vote: 10

Authors: Wenxiang Hu, Liyuan Liu, Chen Liang, Vishrav Chaudhary, Young Jin Kim, Shuohang Wang, Hao Cheng, Xiaoxia Wu, Xiaodong Liu, Jilong Xue, Weizhu Chen, Zeqi Lin, Yelong Shen, Chenruidong Zhang, Masahiro Tanaka, Jianfeng Gao, Hany Awadalla

- **What's New**: SparseMixer-v2는 두 가지 핵심 구성 요소를 갖추고 있습니다: 이산 변수 샘플링을 통한 TopK 근사 및 확장 가능한 방식으로 그래디언트 추정. 특히, SparseMixer-v2는 기존 MoE (Mixture of Experts) 훈련에서 흔히 쓰이는 전문가 라우팅에 대한 잡음 추가를 명시적인 전문가 샘플링으로 대체하려고 합니다.
- **Technical Details**: SparseMixer-v2는 Top1 MoE의 특수 케이스로부터 시작하여 TopK MoE로 확장됩니다. 여기서 Top1 (𝒛)을 MaskedSoftmax (𝒛)를 통한 샘플링으로 근사합니다. MaskedSoftmax (⋅)는 랜덤성과 샘플링 공간의 희소성을 제어하는 하이퍼 파라미터 r을 도입합니다. 훈련 시에는 SparseMixer-v2* 및 SparseMixer-v2라는 두 가지 변형을 제안하였습니다.
- **Performance Highlights**: 실험 결과, MaskedSoftmax (⋅)는 기존 MoE 훈련에서 사용되는 잡음 추가와 유사한 성능을 보였습니다. SparseMixer-v2는 그래디언트 추정에서 더 정확한 방법을 이용하여 기존보다 더 나은 성능을 기대할 수 있습니다.

### [Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models](https://arxiv.org/abs/2409.12139)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.12139.png)

Vote: 8

Authors: Yanni Hu, EverestAI, Chengwei Tian, Laipeng He, Quanlei Yan, Bin Lin, Sijin Chen, Chen Wang, Yuguang Yang, Jingjing Yin, Yiting Lin, Zhicheng Wang, Yuan Feng, Pengfei Tan, Tianwei He, Jixun Yao, Wendi He, Ruoye Xie, Jianhao Ye

- **What's New**: 최근 대형 언어 모델(LLMs), 뉴럴 코덱(neural codecs), 확산 및 플로우 모델(diffusion and flow models)에서 중요한 발전이 있었습니다. 이러한 혁신은 제로샷 텍스트-투-스피치(TTS) 및 보이스 컨버전(VC) 분야에서 고품질의 음성을 생성할 수 있게 하여 사용자 인터랙션을 보다 자연스럽고 몰입감 있게 만들었습니다. 이 맥락에서, 책 오디오북 제작을 지원하기 위해 사용자 맞춤형 음성 콘텐츠 생성을 허용하는 Takin AudioLLM 모델 시리즈가 제안되었습니다.
- **Technical Details**: Takin AudioLLM 시리즈는 Takin TTS, Takin VC, 그리고 Takin Morphing으로 구성되어 있으며, 각각의 모델은 고유한 기능을 가지고 있습니다. Takin TTS는 고충실도의 저대역폭 뉴럴 스피치 코덱과 다단계 멀티태스크 학습 전략을 사용하여 높은 자연스러움을 유지할 수 있는 음성을 생성합니다. Takin VC는 음색 특성과 자율 학습 기반의 콘텐츠 표현을 결합하여 스피커 유사성과 명료성을 향상시킵니다. Takin Morphing은 멀티-레퍼런스 음색 인코더와 프로소디 인코더를 사용하여 다양한 사용자 맞춤형 오디오북을 생성할 수 있습니다.
- **Performance Highlights**: Takin TTS는 복잡한 실제 환경에서도 강력하고 효과적인 성능을 보이며, 다양한 애플리케이션에 고품질의 자연스러운 음성을 생성합니다. Takin VC는 타깃 스피커의 음색을 정확히 재현하여 높은 원음 유사성을 유지하고, Takin Morphing은 다양한 미지의 스피커 음색과 프로소디 스타일을 결합하여 개인화된 콘텐츠를 생성하는 데 뛰어납니다.

### [Vista3D: Unravel the 3D Darkside of a Single Image](https://arxiv.org/abs/2409.12193)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.12193.png)

Vote: 5

Authors: Qiuhong Shen, Xingyi Yang, Xinchao Wang, Michael Bi Mi

- **What's New**: 최신 연구 논문에서는 Vista3D라고 불리는 새로운 프레임워크를 제안합니다. Vista3D는 단일 이미지에서 3D 객체의 'Darkside'(숨겨진 면)를 재구성하는 데 초점을 맞추고 있습니다.
- **Technical Details**: Vista3D는 크게 두 가지 주요 단계로 구성됩니다: Coarse Phase와 Fine Phase. Coarse Phase에서는 3D Gaussian Splatting 기법을 사용하여 기본적인 지오메트리와 텍스처를 신속하게 생성합니다. Fine Phase에서는 이 초기 지오메트리를 서명 거리 필드(Signed Distance Fields, SDF)로 변환하고, FlexiCubes와 같은 고급 차별화 가능한 아이소서피스 기법을 사용하여 더욱 정교화합니다. 또한 Disentangled Texture Representation과 Angular-based Composition 기법을 통해 다양한 각도에서의 텍스처 예측을 강화합니다.
- **Performance Highlights**: Vista3D는 단일 이미지로부터 다양한 3D 객체를 5분 내에 효율적으로 생성할 수 있으며, 생성된 3D 객체는 일관성과 다양성의 균형을 유지합니다. 본 연구는 3D 객체의 일관성과 텍스처의 다양성을 동시에 보장하는 기법을 통해 높은 충실도의 3D 격자(메시)를 생성할 수 있음을 입증합니다.

### [Towards Diverse and Efficient Audio Captioning via Diffusion Models](https://arxiv.org/abs/2409.09401)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.09401.png)

Vote: 5

Authors: Chenxing Li, Manjie Xu, Xinyi Tu, Wei Liang, Ruibo Fu, Yong Ren, Dong Yu

- **What's New**: 오디오 캡셔닝(audio captioning) 분야에서 중요한 진전이 이루어졌습니다. 기존의 Autoregressive (AR) 모델을 넘어, 새로운 비-자기회귀형(Non-Autoregressive, NAR) 디퓨전 기반 모델인 DAC(Diffusion-based Audio Captioning)가 제안되었습니다. DAC는 오디오 데이터를 보다 다양하고 효율적으로 설명할 수 있는 모델로, 기존의 AR 모델을 능가하는 성능을 보여줍니다.
- **Technical Details**: DAC는 Denoising Diffusion Probabilistic Model (DDPM)을 기반으로 하며, 주요 단계는 아래와 같습니다. 텍스트 설명은 토큰화 후 불연속적인 토큰 임베딩으로 변환되고, 임베딩 함수에 의해 연속적인 잠재 공간으로 맵핑됩니다. 오디오 콘텐츠는 Mel Spectrogram으로 변환된 다음 사전 훈련된 오디오 인코더를 통해 인코딩됩니다. 디퓨전 모델은 텍스트 벡터에 잡음을 추가하고, 역방향 과정에서는 각 단계에서 추가된 잡음을 예측하여 디노이징합니다. 최종적으로 이는 언어 모델 헤드를 통해 다시 불연속적인 토큰으로 디코딩됩니다.
- **Performance Highlights**: 평가 결과, DAC는 기존의 최첨단(SOTA) 기법들과 비교하여 생성 품질 면에서 경쟁력을 갖추고 있을 뿐만 아니라 생성 다양성과 속도 면에서도 전통적인 AR 방법을 능가합니다. DAC의 성능은 단순히 일반적인 평가 지표를 넘어 semantic metrics인 CLAP 및 GPT4-eval을 통하여도 입증되었습니다. 이는 다양한 모달리티 간의 변환 작업에도 적용될 수 있는 통합된 모델의 가능성을 시사합니다.

### [SoloAudio: Target Sound Extraction with Language-oriented Audio Diffusion Transformer](https://arxiv.org/abs/2409.08425)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08425.png)

Vote: 3

Authors: Helin Wang, Jiarui Hai, Yen-Ju Lu, Karan Thakkar, Najim Dehak, Mounya Elhilali

- **What's New**: 본 논문은 복잡한 음향 환경에서 특정 소리를 추출하는 능력을 갖춘 고유한 솔루션, SoloAudio를 제안합니다. 이는 특히 TSE(타겟 사운드 추출) 작업에서 두드러진 성능을 발휘하며, 오디오 및 언어 단서를 모두 지원하는 새로운 Transformer 모델을 도입하였습니다. SoloAudio는 텍스트-오디오(T2A) 생성 모델을 통해 합성된 고품질 오디오를 추가적인 학습 데이터로 활용합니다.
- **Technical Details**: SoloAudio는 오디오 변이형 자동 인코더(VAE)의 잠재 공간에서 확산 과정을 적용하는 Transformer 백본을 활용합니다. 또한 CLAP 모델을 통해 음성과 언어 공간 간의 연결을 지원하며, 필요한 경우 제로-샷 예측 기능도 갖추고 있습니다. 확산 드노이징 확률 모델(DDPM)과 결합된 이 방법은 음원 분리 성능을 크게 향상시키기 위해 수정된 확산 스케줄러 및 속도 예측 방식을 사용합니다.
- **Performance Highlights**: FSD Kaggle 2018 데이터셋을 사용한 실험 결과, SoloAudio는 최신 방법들에 비해 매우 우수한 성능을 보였습니다. 특히, 도메인을 벗어난 데이터와 이전에 보지 못한 음향 이벤트에 대한 제로-샷 및 소수-샷 학습 능력이 탁월합니다. 또한, 실제 데이터에 대한 주관적 평가에서 사용자들은 SoloAudio가 추출한 오디오를 선호하는 경향을 보였습니다.

### [RoMath: A Mathematical Reasoning Benchmark in Romanian](https://arxiv.org/abs/2409.11074)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11074.png)

Vote: 1

Authors: Adrian Cosma, Ana-Maria Bucur, Emilian Radoi

- **What's New**: 이번 연구에서는 수학적 추론을 위한 새로운 루마니아어 벤치마크 스위트인 RoMath222GitHub (https://github.com/cosmaadrian/romath333Huggingface: https://hf.co/datasets/cosmaadrian/romath)이 제안되었습니다. RoMath는 RoMath-Baccalaureate, RoMath-Competitions, RoMath-Synthetic 세 가지 데이터셋으로 구성되어 있으며, 각 데이터셋은 고유한 특징과 난이도를 가지고 있습니다.
- **Technical Details**: RoMath는 총 76,910개의 문제 문장을 포함하고 있으며, OCR을 이용한 반자동 워크플로우를 통해 문제를 수집 및 정리했습니다. 특히, 높은 수준의 문제를 포함하여 다양한 난이도의 문제를 제공합니다(예: 선형 대수학, 미적분, 기하학, 확률 등). RoMath-Baccalaureate는 5,777개의 문제, RoMath-Competitions는 1,133개의 문제, RoMath-Synthetic은 70,000개의 문제로 구성되어 있습니다. LLM을 판사로 사용하는 평가 절차를 제안하여 솔루션의 정확성을 적절히 평가할 수 있도록 했습니다.
- **Performance Highlights**: 단순히 문제 문장을 번역하는 것만으로는 성능이 크게 저하된다는 점을 강조하며, 영어 외의 다른 언어에 대한 전용 리소스의 필요성을 강조합니다. RoMath는 비영어 LLM의 향상된 추론 능력 개발을 자극하고, 외부 도구의 통합을 촉진하는 것을 목표로 합니다.

### [CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark](https://arxiv.org/abs/2409.11363)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11363.png)

Vote: 1

Authors: Benedikt Stroebl, Zachary S. Siegel, Nitya Nagdir, Arvind Narayanan, Sayash Kapoor

- **What's New**: 본 연구는 연구에서 분석한 여러 분야의 논문들에 대해 컴퓨팅 재현성(computational reproducibility)을 자동화할 수 있는 AI 에이전트의 가능성을 평가합니다. CORE-Bench라는 새로운 벤치마크를 도입하여, 컴퓨터 과학, 사회 과학 및 의학 분야에서 Python과 R 코드베이스를 가진 90개의 논문에서 파생된 270개의 작업을 포함합니다.
- **Technical Details**: CORE-Bench는 CodeOcean.com 저장소에서 취합된 논문의 재현성 여부를 검증한 후, 세 가지 다른 난이도의 작업을 만듭니다. 이러한 작업은 코딩, 셸 상호 작용, 데이터 검색 및 도구 사용과 같은 다양한 기술을 평가합니다. 또한, 두 가지 에이전트인 AutoGPT와 이를 기반으로 한 CORE-Agent를 평가하였으며, 각각의 성능을 통해 AI 에이전트가 특정 작업에 쉽게 적응할 수 있음을 보여줍니다.
- **Performance Highlights**: 평가 결과, 일반적인 에이전트인 AutoGPT는 가장 쉬운 작업에서 60%의 정확도를 나타냈으나, 가장 어려운 작업에서는 21%로 떨어졌습니다. 이러한 결과는 컴퓨팅 재현성을 자동화할 가능성을 시사하지만, 여전히 개선이 필요함을 보여줍니다. 이를 위해, 본 연구에서는 정확하고 효율적으로 에이전트를 평가할 수 있도록 수백 개의 병렬 가상 머신에서 작업을 수행할 수 있는 평가 하네스를 함께 제공합니다.

### [fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction](https://arxiv.org/abs/2409.11315)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.11315.png)

Vote: 1

Authors: Xuelin Qian, Yanwei Fu, Jianxiong Gao, Yun Wang, Jianfeng Feng, Yuqian Fu

- **What's New**: 뇌의 3D 시각 능력을 모델링하는 새로운 연구 과제인 Recon3DMind(마음에서 3D 객체 재구성)을 도입했습니다. 이 과제는 fMRI 신호로부터 뇌가 지각하는 3D 시각 정보를 디코딩하고 재구성하는 것을 목표로 합니다. 또한, 기존 데이터셋의 한계를 보완하기 위해 fMRI-Objaverse라는 확장된 데이터셋을 소개하며, 이를 사용해 MinD-3D라는 혁신적인 프레임워크를 제안합니다.
- **Technical Details**: 이 연구에서는 뇌의 3D 시각 정보를 더 잘 이해하기 위해 fMRI 신호로부터 공간 구조(spatial structure)와 의미적 특성(semantic features)을 모두 추출하는 향상된 fMRI Feature Extractor를 개발했습니다. 3D 객체의 정확한 재구성을 위해 세 단계로 구성된 MinD-3D 프레임워크를 사용했습니다. 첫 단계에서 Encorder를 사용해 공간적 특징을 추출하고, 두 번째 단계에서 Transformer 기반 Diffusion Model을 통해 시각적 활동을 디코딩, 마지막 단계에서 latent-adapted Argus 모델을 통해 3D 모델을 재구성합니다.
- **Performance Highlights**: 우리 모델의 성능을 평가하기 위해 새로 설계된 벤치마크를 사용했습니다. 모델은 의미적 및 구조적 수준에서 3D 표현을 생성하는 능력을 종합적으로 평가하며, 결과적으로 기본 모델들을 능가하는 성과를 보였습니다. 또한, fMRI-3D 데이터셋과 MinD-3D가 추출한 특징을 심층적으로 분석하여, 뇌의 시각 정보 처리와 일치하는 표현을 생성하는 데 그 신뢰성을 확인했습니다.

### [Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2409.12001)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.12001.png)

Vote: 1

Authors: Arnu Pretorius, Jonathan P. Shock, Louise Beyers, Callum Rhys Tilbury, Claude Formanek

- **What's New**: 이 논문은 멀티 에이전트 시스템(Multi-Agent Systems) 문제에 대한 새로운 접근을 제안합니다. 이러한 시스템은 교통 관리, 전력망 최적화, 위성 통신 등 다양한 현실 세계의 복잡한 문제를 해결하는 데 사용됩니다. 특히, 오프라인 강화학습(Offline Reinforcement Learning)을 이용하여 큰 기존 데이터셋을 활용해 최적의 제어 정책을 학습하는 방법을 다룹니다.
- **Technical Details**: 멀티 에이전트 강화학습(MARL) 연구는 불충분한 시뮬레이터 및 데이터셋으로 인해 어려움을 겪고 있습니다. 논문의 저자들은 협력 시나리오에서 데이터 생성 및 사용에 대한 표준화된 지침과 80여 개의 다양한 환경-시나리오-품질 조합으로 구성된 데이터셋을 제공합니다. 또한, 데이터셋의 특성을 이해할 수 있는 도구와 API를 제안합니다.
- **Performance Highlights**: 오프라인 MARL 분야는 아직 초기 단계에 있으며, 데이터의 품질과 내용이 학습 성능에 큰 영향을 미친다는 사실을 발견했습니다. 이를 통해, 데이터셋을 보다 잘 처리할 수 있는 새로운 방법들이 제안되며, 이는 궁극적으로 실제 세계에 더 유용한 정책을 만드는 데 기여할 것입니다.

### [BERT-VBD: Vietnamese Multi-Document Summarization Framework](https://arxiv.org/abs/2409.12134)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.12134.png)

Vote: 1

Authors: Thien Van Luong, Trang Mai Xuan, Tuan-Cuong Vuong

- **What's New**: 자동 추출 방식과 생성 방식(extractive and abstractive approaches)을 결합한 베트남어 다중 문서 요약(Multi-Document Summarization, MDS) 프레임워크를 제안합니다. 이 프레임워크는 두 개의 주요 구성 요소를 지니며, Sentence-BERT(SBERT)와 VBD-LLaMA2-7B-50b와 같은 딥러닝 모델을 활용하여 중요한 내용을 유지하면서도 읽기 쉬운 요약 문서를 생성합니다. 우리의 실험 결과는 향후 연구와 현재 베트남어 모델과의 비교에서 우수함을 보였습니다.
- **Technical Details**: 제안된 모델은 두 가지 구성 요소가 있는 파이프라인 구조로 되어 있으며, 첫 번째는 추출 요약(extractive summarization)이고 두 번째는 생성 요약(abstractive summarization)입니다. 추출 요약 구성 요소는 원본 문서에서 중요한 내용을 식별하고 추출하는 역할을 합니다. 이후 생성 요약 구성 요소는 추출된 내용을 기반으로 새로운 요약 문장을 생성합니다. 데이터 전처리 과정은 베트남어의 복잡성을 고려하여 특수한 전처리 단계를 포함하며, 이 과정에는 소문자 변환, 의미 없는 단어 제거, 비알파벳 문자 제거, 문장 분할과 단어 분할이 포함됩니다.
- **Performance Highlights**: 제안된 프레임워크는 VN-MDS 데이터셋을 사용해 평가되었으며, 그 결과는 기존 모델을 능가하는 성능을 보였습니다. 특히, 추출 및 생성 방식을 결합하여 요약 성능을 극대화하였으며, 이 프레임워크는 각 문장과 단어 임베딩을 추가하여 정보 보유력을 향상시켰습니다.

### [Measuring Human and AI Values based on Generative Psychometrics with Large Language Models](https://arxiv.org/abs/2409.12106)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.12106.png)

Vote: 1

Authors: Yuanyi Ren, Guojie Song, Xin Zhang, Hanjun Fang, Haoran Ye, Yuhang Xie

- **What's New**: 연구진은 기존의 자기 보고(self-report) 설문지를 통한 가치 측정 도구들의 한계를 극복하고자 Generative Psychometrics for Values (GPV)를 소개했습니다. GPV는 대형 언어 모델(LLMs) 기반의 데이터 주도(value-driven) 가치 측정 패러다임으로, 텍스트에서 맥락화된 가치 인식을 추출하여 다양한 가치 시스템에 맞게 해석합니다. 이를 통해 역사적 데이터나 주관적인 데이터를 다루는 능력이 부족한 기존 도구들의 한계를 극복합니다.
- **Technical Details**: GPV는 Llama 3를 기반으로 미세 조정된 ValueLlama를 이용합니다. 이 모델은 기존의 심리 측정 항목과 비교했을 때 우수한 성능을 보입니다. GPV는 감지된 가치 인식을 기반으로 가치를 추출하여 상위 퍼포먼스를 보이며, 텍스트를 가치 인식으로 분석하는 데 큰 기여를 합니다. 또한, 17개의 LLM과 4개의 가치 이론을 바탕으로 LLM의 가치를 측정하는 실험을 진행했습니다. LLM의 수많은 자유 형식 출력(free-form outputs)을 기반으로 가치 측정 가능성을 확인했으며, Schwartz 등의 가치 이론과 비교하여 상황 의존적인 가치 측정을 가능하게 합니다.
- **Performance Highlights**: Human-authored blogs에 GPV를 적용하여 심리적 도구들보다 안정성과 타당성 측면에서 우월함을 입증했습니다. 또한, GPV를 통해 LLM의 가치 측정이 스케일 가능하고 맥락에 따른 가치 측정이 가능함을 보여주었으며, Schwartz의 가치 이론 뿐만 아니라 다른 가치 시스템(VSM 등)이 더 나은 예측력을 제공할 수 있다는 새로운 발견도 제시했습니다. ValueLlama는 최신의 일반 및 특정 작업 LLM들을 뛰어넘는 성능을 보입니다.

