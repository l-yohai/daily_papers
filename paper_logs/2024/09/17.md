## Daily Papers (2024-09-17)

### [Seed-Music: A Unified Framework for High Quality and Controlled Music Generation](https://arxiv.org/abs/2409.09214)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.09214.png)

Vote: 23

Authors: Andrew Shaw, Qingqing Huang, Dongya Jia, Yiqing Lu, Xingxing Li, +, Bochen Li, Ye Bai, Zhuo Chen, Duc Le, Xiaohong Dong, Hui Li, Zhongyi Huang, Jitong Chen, Haonan Chen, Wei-Tsung Lu, Shouda Liu, Yi Deng, Lamtharn Hantrakul, Feihu La, Janne Spijkervet, Chumin Li, Weituo Hao

- **What's New**: Seed-Music는 보컬과 악기 음악 생성, 가창 목소리 합성, 가창 목소리 변환, 음악 편집 등을 지원하는 새로운 음악 생성 모델입니다. 이 모델은 음악 제작 과정에서 초보자와 전문가 모두에게 도움을 줄 수 있는 도구를 제공합니다. 우리가 제안하는 통합 프레임워크는 여러 사용자의 요구를 충족시키기 위해 Auto-regressive (AR)와 diffusion 접근 방식을 결합합니다.
- **Technical Details**: Seed-Music는 다음 세 가지 주요 부분으로 구성된 통합 프레임워크를 소개합니다. 1) 원시 오디오를 고도로 압축된 표현으로 변환하는 표현 학습 모듈, 2) 다양한 사용자 제어에 따라 표현을 예측하는 생성 모듈, 3) 표현으로부터 파형을 생성하기 위해 훈련된 렌더링 모듈. 우리의 프레임워크는 다양한 시나리오를 위해 오디오 토큰(auxiliary tokens), 상징적 토큰(symbolic tokens), vocoder latents의 세 가지 표현 방식을 설정합니다.
- **Performance Highlights**: Seed-Music는 고품질 보컬 음악 생성을 위한 Auto-regressive language model (LM) 기반 접근 방식을 도입했습니다. 뮤직 오디오 편집의 미세한 부분을 위해 diffusion 기반 접근 방식을 제시하였으며, 단 10초의 노래 또는 음성 녹음만으로 제로 샷 가창 목소리 변환을 가능하게 하는 새로운 방법도 제안했습니다. 이 모델은 다양한 사용자 요구와 상황을 지원할 수 있도록 설계되었습니다.

### [RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval](https://arxiv.org/abs/2409.10516)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10516.png)

Vote: 12

Authors: Baotong Lu, Bailu Ding, Di Liu, Meng Chen, Qianxi Zhang, Fan Yang, Qi Chen, Chen Chen, Kai Zhang, Yuqing Yang, Lili Qiu, Zhenhua Han, Huiqiang Jiang, Chengruidong Zhang

- **What's New**: 최신 트랜스포머 기반의 대형 언어 모델(LLM)들이 긴 문맥 처리에서 주목할 만한 성능을 보여주고 있습니다. 예를 들어, Gemini 1.5 Pro 모델은 1000만 토큰의 컨텍스트 윈도우를 지원합니다. 그러나 이러한 긴 문맥을 처리하는 동안 효율성 문제를 겪게 되며, 이를 해결하기 위해 KV 캐싱(KV Caching) 기법이 널리 사용되고 있습니다. 본 논문에서는 ANNS(Approximate Nearest Neighbor Search) 기반의 새로운 어텐션 메커니즘인 RetrievalAttention을 소개하고, 이 방법이 기존의 정적이거나 휴리스틱한 방식보다 더 높은 정확도를 유지하면서도 효율성을 개선할 수 있음을 증명합니다.
- **Technical Details**: LLM의 긴 문맥 처리는 주로 어텐션 메커니즘의 점차적인 복잡성 증가로 인해 효율성에 문제가 생깁니다. KV 캐싱 기법은 이러한 문제를 해결하기 위해 과거의 Key와 Value 토큰을 저장해 불필요한 계산을 줄이지만, GPU 메모리 사용량이 상당히 증가하고, 컨텍스트 크기에 비례한 추론 지연 문제를 야기합니다. 본 논문에서 제시된 RetrievalAttention은 ANNS를 활용해 동적 희소성(dynamic sparsity)을 효율적으로 이용하고, query 벡터와 key 벡터 간의 분포 차이를 고려해 최적화된 벡터 인덱싱을 수행합니다. 이 방법은 CPU 메모리에 대부분의 KV 벡터를 오프로드하여 리소스 활용도를 최적화하며, 연산 결과를 CPU와 GPU의 벡터 인덱스를 통해 결합하여 보다 효율적인 추론을 가능하게 합니다.
- **Performance Highlights**: RetrievalAttention은 다양한 벤치마크 테스트(예: ∞-Bench, RULER)에서 정확성을 유지하면서도 성능을 크게 향상시키는 것으로 나타났습니다. 특히 128K 컨텍스트에서 4090 GPU를 사용할 때, 기존의 정확한 KNN과 전통적인 ANNS 인덱싱 기반의 검색 방법에 비해 각각 4.9배, 1.98배의 디코딩 지연 시간 감소를 달성했습니다. 또한, RetrievalAttention은 8B 수준의 모델을 단일 4090 GPU(24GB) 상에서 실행할 수 있는 최초의 솔루션으로, 기존 정확도를 거의 손상시키지 않으며 허용 가능한 지연 시간을 유지합니다.

### [Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types](https://arxiv.org/abs/2409.09269)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.09269.png)

Vote: 7

Authors: Aman Chadha, Vinija Jain, Neelabh Sinha

- **What's New**: 이 논문에서는 VQA(Visual Question-Answering) 문제에 대한 새로운 평가 프레임워크를 제안합니다. 기존의 VQA 연구들은 이미지의 다양한 성격과 외부 지식의 필요성 등으로 인해 많은 도전에 직면해 왔습니다. 최근 VLM(Vision-Language Models)의 발전 덕분에 대부분의 VLM들이 특정 VQA 문제에서도 우수한 성능을 보이고 있습니다. 제안된 평가 프레임워크는 실제 응용 환경에서 VLM을 평가할 수 있는 표준화된 패러다임을 제공합니다.
- **Technical Details**: 이 논문은 VQA 테스크 유형, 애플리케이션 도메인, 지식 유형 등 세 가지 측면에서 VLM을 비교할 수 있는 구조를 개발합니다. VQA 벤치마크(VQAv2, OK-VQA, ChartQA 등)를 기반으로 한 데이터셋을 만들고, 각 태스크를 해당 측면들로 분류 및 레이블링합니다. 또한, GPT-4o 기반의 멀티모달 평가 메트릭인 GoEval을 제안하여, 인간의 판단과 더 밀접하게 일치하는 평가 결과를 제공합니다.
- **Performance Highlights**: 우리의 분석을 통해 현재 공개된 8종의 VLM의 10가지 변종을 평가하여, 다양한 태스크에서 가장 적합한 모델들을 식별했습니다. VLM 성능은 높지 않게 상관관계가 있었지만, 각 모델이 다양한 태스크에서 다른 강점과 약점을 가지고 있음을 확인했습니다. 우리 프레임워크를 통해 특정 응용 요구에 따라 가장 적합한 VLM을 선택하는 것이 그 성능을 보완할 수 있도록 도와줍니다.

### [ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds](https://arxiv.org/abs/2409.09213)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.09213.png)

Vote: 6

Authors: Chandra Kiran Reddy Evuru, Ramani Duraiswami, Dinesh Manocha, Oriol Nieto, Sonal Kumar, Sreyan Ghosh

- **What's New**: Zero-shot audio classification(ZSAC)에서 새로운 접근법을 제안합니다. CLAP 기반으로 캡션(data augmentation) 증강하는 'ReCLAP'을 소개합니다. 다양한 방법으로 해당 카테고리를 재설명하여 모델의 성능을 높이는 것을 목표로 하고 있습니다.
- **Technical Details**: CLAP은 audio-caption 쌍 간의 대조 목표를 학습하여 음성 및 텍스트 표현을 공유합니다. ReCLAP은 대조적 학습을 따르지만, '캡션 증강(capti on augmentation)'을 사용합니다. 학습 데이터의 각 오디오 샘플에 대해 원래 캡션을 여러 가지 방법으로 재작성하여 캡션을 증가시키고, 이 방법으로 학습 데이터를 늘려 모델의 성능을 높입니다.
- **Performance Highlights**: ReCLAP은 다양한 ZSAC 벤치마크에서 1%-55%의 성능 향상을 달성했습니다. 또한, 기존의 손으로 작성된 템플릿 프롬프트를 넘어 각 카테고리에 대해 사용자 정의 프롬프트(custom prompts)를 생성하여 정확도를 높였습니다.

### [Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models](https://arxiv.org/abs/2409.06277)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.06277.png)

Vote: 5

Authors: Wenyang Hu, Bryan Kian Hsiang Low, Fei Richard Yu, Yao Shu, See-Kiong Ng

- **What's New**: 최근 대규모 언어 모델 (LLMs)이 다양한 현실 세계 애플리케이션에서 필수 도구로 자리 잡고 있습니다. 번역, 요약과 같은 자연어 처리 작업부터 코드 생성 및 의사결정 시스템과 같은 복잡한 작업에 이르기까지 LLMs의 규모와 다양성은 실질적으로 매우 가치가 있지만, 페더레이티드 설정에서 미세 조정(fine-tuning)하는 데에는 상당한 도전 과제를 안겨줍니다. 이 문제를 해결하기 위해 Ferret이라는 새로운 페더레이티드 학습(FL) 방법을 제안합니다.
- **Technical Details**: Ferret는 LLMs의 페더레이티드 풀 파라미터 튜닝을 확장 가능하고 효율적으로 수행하기 위해 설계된 최초의 1차(First-order) FL 방법이며, 공유된 랜덤성을 이용합니다. Ferret는 다음 세 가지 주요 측면을 통해 이를 달성합니다. 첫째, 각 클라이언트에서 1차 최적화 방법을 사용하여 계산 효율이 높은 로컬 업데이트를 수행합니다. 둘째, 업데이트를 저차원 공간으로 투영하여 기존의 1차 최적화 기반 FL보다 통신 비용을 크게 줄입니다. 셋째, 공유된 랜덤성을 사용하여 저차원 공간에서 로컬 업데이트를 재구성하여 빠른 수렴과 경쟁력 있는 모델 정확도를 보장합니다.
- **Performance Highlights**: Ferret는 이론적 분석과 실험을 통해 다른 기존 방법보다 우수한 확장성 및 경쟁력 있는 모델 정확도를 제공함을 입증했습니다. 다양한 실험을 통해 Ferret가 기존의 방법을 일관되게 능가하며, 현실적인 대규모 페더레이티드 환경에서 LLMs를 배포하는 데 있어 바람직한 솔루션으로 자리매김할 수 있음을 보였습니다.

### [One missing piece in Vision and Language: A Survey on Comics Understanding](https://arxiv.org/abs/2409.09502)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.09502.png)

Vote: 4

Authors: Emanuele Vivoli, Dimosthenis Karatzas, Marco Bertini, Artemis LLabres, Andrey Barsky, Mohamed Ali Souibgui

- **What's New**: 이번 연구는 최근의 AI 기술을 활용해 **Comics Understanding(만화 이해)**를 심도 있게 탐구한 논문을 소개합니다. 만화의 시각적 및 언어적 요소를 동시에 분석하여 다양한 문제를 해결하고자 하는 연구가 늘어나고 있습니다. 특히 LoCU(Layer of Comics Understanding) 프레임워크를 도입하여 만화 데이터를 분석하는 체계적인 접근 방식을 제안합니다.
- **Technical Details**: 만화는 주요하게 여러 모달리티(visual and textual, 시각적 및 언어적 요소)를 포함하고 있어 AI 모델이 해결해야 할 과제가 복잡합니다. 연구는 다양한 분석 작업(태깅 및 증강, 그라운딩 및 분할, 검색 및 수정, 이해, 생성 및 합성)을 레이어별로 분류하여 접근합니다. 특히, **Multimodal Vision-Language understanding** 접근 방식을 통해 만화를 분석하는 방법론을 설명합니다.
- **Performance Highlights**: 이 연구는 이전의 전통적인 머신러닝 방법론을 넘어 **Deep Learning** 및 **Modern Foundational Models**로 진화하면서 만화 객체 탐지 및 스타일 분류에서 뛰어난 성능을 보였습니다. 또한, 대규모 다중 모달 데이터셋에 대한 학습을 통해 더 일반화된, 다중 과제를 수행할 수 있는 모델을 개발했습니다. 이러한 모델은 판넬, 캐릭터, 텍스트 박스 탐지 등 다양한 작업에서 높은 정확도를 달성했습니다.

### [jina-embeddings-v3: Multilingual Embeddings With Task LoRA](https://arxiv.org/abs/2409.10173)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10173.png)

Vote: 3

Authors: Feng Wang, Han Xiao, Markus Krimmel, Andreas Koukounas, Georgios Mastrapas, Isabelle Mohr, Michael Günther, Nan Wang, Saba Sturua, Bo Wang, Mohammad Kalim Akram

- **What's New**: 최근 아카이브에 올라온 논문은 jina-embeddings-v3라는 새로운 텍스트 임베딩 모델을 소개합니다. 이 모델은 다국어 데이터, 긴 문맥 검색, 여러 작업에서의 높은 성능을 위해 최적화된 5억 7천만 파라미터를 보유하고 있습니다. 평가 결과, OpenAI와 Cohere의 최신 임베딩보다 영어 작업에서 우수한 성과를 보였으며, 다국어 작업에서도 뛰어난 성능을 자랑합니다. LLM 기반 임베딩(e.g., e5-mistral-7b-instruct)과 비교하여 훨씬 더 비용 효율적이며, 실제 애플리케이션에서 사용하기에 적합합니다.
- **Technical Details**: jina-embeddings-v3는 XLM-RoBERTa 모델을 백본으로 사용하며, 다음과 같은 주요 특징이 있습니다: LoRA(Low-Rank Adaptation) 어댑터를 사용한 작업별 최적화, 합성 데이터를 통한 검색 실패 사례 보완, Matryoshka Representation Learning, 인스트럭션 튜닝(Instruction Tuning), 긴 문맥 검색 지원이 포함됩니다. 이 모델은 상대적 위치 인코딩 방법을 사용하여 긴 텍스트 시퀀스도 효과적으로 처리할 수 있도록 설계되었습니다.
- **Performance Highlights**: MTEB 벤치마크에서 jina-embeddings-v3는 이전 버전(jina-embeddings-v2)과 이중어 모델(mBERT, XLM-R)보다 뛰어난 성능을 입증했습니다. 특히 영어 작업에서 OpenAI 및 Cohere의 최신 임베딩 모델을 능가했으며, 모든 다국어 작업에서도 multilingual-e5-large-instruct보다 우수한 성능을 보였습니다. 또한, 파라미터 수 약 12배, 출력 차원 수 약 4배 큰 e5-mistral-7b-instruct 모델보다 원가 효율적이며, 실제 환경에서의 사용에 적합합니다.

### [On the Diagram of Thought](https://arxiv.org/abs/2409.10038)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10038.png)

Vote: 3

Authors: Yang Yuan, Andrew Chi-Chih Yao, Yifan Zhang

- **What's New**: 이 논문에서는 Diagram of Thought (DoT)라는 새로운 프레임워크를 소개합니다. 이는 논리적 추론을 단일 대형 언어 모델(LLM) 내에서 방향성 비순환 그래프(DAG)로 모델링합니다. DoT는 자연어 비평을 포함하여 이진 신호보다 더 풍부하고 정보가 많은 피드백을 제공합니다.
- **Technical Details**: DoT는 세 가지 역할을 사용하는 auto-regressive next-token prediction을 통해 LLM 내부에서 제안자(<proposer>), 비평가(<critic>), 요약자(<summarizer>)로 원활하게 전환하도록 합니다. 이러한 역할 전환을 통해 LLM은 외부 개입 없이 아이디어를 제안하고 비평하며, 요약하는 과정을 수행합니다. 또한, 이 논문은 DoT의 이론적 기초를 토포스 이론(Topos theory) 내에서 공식화하여 논리적 일관성과 타당성을 보장합니다.
- **Performance Highlights**: DoT는 다중 LLM 협업이나 외부 제어 메커니즘의 필요성을 제거하여 배포를 간소화하고 기존의 LLM 훈련 패러다임과 밀접하게 연관됩니다. 이는 복잡한 reasoning 작업에서 LLM의 능력을 향상시키며, 제안, 비평, 요약의 피드백 루프를 통해 더 깊은 이해와 효과적인 수정이 가능합니다.

### [Policy Filtration in RLHF to Fine-Tune LLM for Code Generation](https://arxiv.org/abs/2409.06957)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.06957.png)

Vote: 2

Authors: Wei Shen, Chuheng Zhang

- **What's New**: ICLR 2025 대회의 논문 제출 방식에 대한 세부 지침이 공개되었습니다. 모든 논문은 전자적으로 https://openreview.net/을 통해 제출되어야 하며, ICLR 웹사이트에서 추가 지침을 확인할 수 있습니다. 최종적으로 논문이 승인되면, 'iclrfinalcopy'를 삽입하여 카메라 레디 요구 사항에 맞게 형식을 조정해야 합니다.
- **Technical Details**: 논문 양식은 NeurIPS 포맷의 변형입니다. LaTeX2e를 사용하여 스타일 파일 iclr2025_conference.sty와 iclr2025_conference.bst를 적용해야 합니다. 제출된 논문의 본문은 최대 10페이지로 제한되며, 인용 문헌은 그 페이지 제한에 포함되지 않습니다. Figures, tables, acknowledgments 및 references에 대한 지침을 특히 주의 깊게 따라야 합니다.
- **Formatting Instructions**: 본문은 Times New Roman 10포인트 글꼴로 작성되어야 하며, 간격은 11포인트로 설정해야 합니다. 첫 번째 수준의 제목은 작은 대문자 (small caps)로 12포인트 크기여야 하며, 두 번째와 세 번째 수준의 제목은 10포인트 크기여야 합니다. 인용은 natbib 패키지를 사용하여 작성해야 하며, figure 번호와 캡션은 그림 뒤에 위치해야 합니다.
- **Performance Highlights**: PDF 또는 PostScript 파일은 반드시 'US Letter' 크기로 준비해야 합니다. 특히 그림과 테이블의 경우 손으로 위치를 조정하지 말고 graphicx 패키지의 \includegraphics 명령어를 사용하는 것이 권장됩니다. 논문의 마지막 부분에는 공헌자 표시와 감사 표시 섹션을 추가할 수 있으며, 이 섹션들은 선택적입니다.

### [AudioBERT: Audio Knowledge Augmented Language Model](https://arxiv.org/abs/2409.08199)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08199.png)

Vote: 2

Authors: Suho Yoo, Hyunjong Ok, Jaeho Lee

- **What's New**: 우리는 AuditoryBench를 소개합니다. 이는 언어 모델의 청각 지식을 평가하기 위한 첫 벤치마크 데이터셋입니다. 두 가지 주요 과제를 제안했습니다: 동물 소리 인식 및 소리 피치 비교. 이를 통해 언어 모델이 청각적 상식 지식이 부족하다는 사실을 발견했습니다.
- **Technical Details**: AuditoryBench는 LAION-Audio-630K를 기반으로 구축되었습니다. 우리는 LLM(대규모 언어 모델)을 사용하여 텍스트 정보를 처리하고 데이터를 계층적으로 범주화했습니다. 또한, 우리는 텍스트-오디오 대조 학습(contrastive learning) 모델인 CLAP를 사용하여 관련 오디오를 검색하고, 이를 통해 오디오 임베딩을 생성하여 언어 모델에 주입하는 AudioBERT 프레임워크를 제안했습니다. AudioBERT는 LoRA를 사용하여 모델을 미세 조정합니다.
- **Performance Highlights**: AuditoryBench를 사용한 실험에서 BERT, Gemma, LLaMA 모델 모두 예측 정확도가 낮았습니다. 그러나 AudioBERT는 동일한 테스트 세트에서 예측 정확도를 40% 이상 향상시켰습니다.

### [Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records](https://arxiv.org/abs/2409.07012)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.07012.png)

Vote: 1

Authors: Edward Choi, Junu Kim, Tackeun Kim, Daeun Kyung

- **What's New**: 본 연구에서는 지난 CXR 이미지와 전자 건강 기록(EHR) 데이터를 결합하여 미래의 CXR 이미지를 예측하는 새로운 과제를 제안합니다. EHRXDiff라 명명된 본 연구의 모델은 이러한 과제를 해결하기 위해 최초로 latent diffusion 기반의 접근 방식을 도입했습니다.
- **Technical Details**: 우리의 모델은 이전의 CXR 이미지(I_prev)와 이에 따라 나오는 일련의 의료 이벤트(S_event)로부터 목표 CXR 이미지(I_trg)를 예측합니다. 이 과정에서 VAE 인코더와 두 개의 CLIP 인코더를 사용하며, 이미지와 테이블 형태의 의료 데이터를 함께 활용합니다. 모델은 세부 단면에서 주요 장기들의 형태와 병변의 위치 및 크기를 포착하는 VAE 인코더와 데이터를 통합하는 어댑터 모듈을 포함하고 있습니다.
- **Performance Highlights**: EHRXDiff 모델은 기존의 기본 모델들과 비교하여 환자의 상태 변화를 시간에 따라 추적하는데 있어서 탁월한 성능을 보였습니다. 이는 의료 전문가들이 환자 상태의 미래 변화를 시뮬레이션하고 예측할 수 있는 강력한 도구로서의 가능성을 보여줍니다.

### [LLM-Powered Grapheme-to-Phoneme Conversion: Benchmark and Case Study](https://arxiv.org/abs/2409.08554)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08554.png)

Vote: -

Authors: Zahra Dehghanian, Hamid R. Rabiee, Mahta Fetrat Qharabagh

- **What's New**: 최근 연구에서는 FastSpeech2와 같은 최신 Text-to-Speech (TTS) 모델의 핵심 구성 요소로 자리매김한 Grapheme-to-Phoneme (G2P) 변환에 대한 개선 사항을 제시하였습니다. 특히 페르시아어의 문맥에 따라 소리 값이 달라지는 단어와 같은 언어적 특성을 고려하기 위해 대규모 언어 모델(LLM)을 활용한 다양한 프롬프트 기술 및 후처리 방법을 도입하였습니다.
- **Technical Details**: 이 연구에서는 페르시아어 문장을 평가하기 위한 벤치마킹 데이터셋을 새롭게 소개하고, 기존의 LLM을 추가 학습 없이 최대한 활용할 수 있는 방법을 제시했습니다. 특히, 첫 번째 LLM 기반 G2P 도구를 선보였으며, 혁신적인 프롬프트 및 후처리 기술을 통해 LLM의 음성 정확도를 향상시켰습니다. 'Sentence-Bench'라는 문장 레벨 벤치마킹 데이터셋과 Ezafe 예측 메트릭을 도입해 문장 내 도전 과제를 평가했습니다. 또한, 'Kaamel-Dict'라는 페르시아어 G2P 사전도 공개했습니다.
- **Performance Highlights**: 네이브 음성 접근법을 통한 초기 모델에서는 평균 Phoneme Error Rate(PER)가 31.61%으로 나타났으나, in-context learning 방식을 활용한 후에 PER이 15.79%로 감소하여 성능을 크게 향상시켰습니다. 이에는 예시와 언어적 규칙을 포함하여 모델의 학습을 돕는 방법이 포함되었으며, 이러한 접근 방식이 더 나은 성능을 가져왔습니다.

### [Breaking reCAPTCHAv2](https://arxiv.org/abs/2409.08831)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.08831.png)

Vote: -

Authors: Andreas Plesner, Roger Wattenhofer, Tobias Vontobel

- **What's New**: 이번 연구는 Google의 reCAPTCHAv2 시스템에 대한 분석을 통해 딥러닝 모델을 사용한 자동화된 봇 차단 기술의 효과를 평가하고자 합니다. 특히, YOLO 모델을 활용하여 reCAPTCHAv2 캡차를 100% 해결할 수 있었으며, 이는 이전 연구에서 달성한 68-71%의 성공률을 크게 상회합니다.
- **Technical Details**: Google의 reCAPTCHAv2는 자동화된 위협에 대항하는 중요한 방어선으로, 사용자 경험과 보안 간의 우수한 균형을 자랑하는 이미지 기반 캡차 기술입니다. 이번 연구에서는 YOLO v8 모델을 사용하여 이미지 분할 및 분류 작업을 수행했으며, 고도화된 딥러닝 기술로 인간과 봇의 수행 차이를 분석하였습니다. 또한, 브라우저 쿠키와 이력 데이터가 인간과 봇을 구분하는 데 큰 역할을 한다는 점을 발견했습니다.
- **Performance Highlights**: 우리는 reCAPTCHAv2 캡차를 100% 해결하는 데 성공했으며, 이는 이전 연구의 68-71% 성공률보다 훨씬 높은 수치입니다. 또한, 인간과 봇이 캡차를 해결하는 데 필요한 도전 과제 수에 큰 차이가 없음을 확인했으며, 오히려 봇이 인간보다 더 우수할 수 있다는 증거를 발견했습니다. 이러한 결과는 캡차를 무력화할 수 있는 딥러닝 기술의 진보를 보여줍니다.

### [beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems](https://arxiv.org/abs/2409.10309)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.10309.png)

Vote: -

Authors: Vojtěch Vančura, Pavel Kordík, Milan Straka

- **What's New**: 이 논문에서는 추천 시스템 도메인에서 텍스트 기반 보조 정보를 활용하여 상호작용 데이터를 직접 학습하는 새로운 문장 Transformer 모델 프레임워크, beeFormer를 소개합니다. 이는 특히 cold-start, zero-shot, 그리고 time-split 추천 시나리오에서 기존 모델들을 뛰어넘는 성능을 보여줍니다. beeFormer은 다양한 도메인 간의 지식 전이 능력을 갖추고 있으며, 다양한 도메인에서의 결합 데이터셋으로 학습 시, 도메인에 구애받지 않는 추천에 있어서 성능을 향상시킵니다.
- **Technical Details**: beeFormer은 ELSA 모델에서 사용하는 A 매트릭스를 직접 최적화하는 대신, sentence Transformer 모델을 사용하여 A 매트릭스를 생성하고 Transformer 모델의 파라미터를 최적화합니다. 이 접근 방식은 고효율 배치 크기로 인해 Transformer 훈련 과정에서 발생하는 문제를 극복하기 위해 gradient checkpointing, gradient accumulation, negative sampling 기술을 적용합니다. 또한, 본 논문에서는 LLM을 사용하여 생성된 항목 설명을 사용된 모든 데이터셋에 대해 공개했습니다.
- **Performance Highlights**: 실험 결과, beeFormer를 통해 학습된 sentence Transformer 모델은 cold-start, zero-shot, 그리고 time-split 추천 시나리오에서 모든 기준 모델을 능가하는 성능을 보였습니다. beeFormer는 학습된 모델을 production 시스템에 sentence Transformers 라이브러리를 통해 쉽게 배포할 수 있는 장점을 제공하며, 도메인에 무관하고 다중 모달 추천 시스템을 향한 가능성을 열었습니다.

