## Daily Papers (2024-09-09)

### [How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data](https://arxiv.org/abs/2409.03810)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.03810.png)

Vote: 20

Authors: Zhuoma Gongque, Zhexu Wang, Yejie Wang, Mengdi Zhang, Muxi Diao, Yanxu Chen, Keqing He, Heyang Xu, Guanting Dong, Xunliang Cai, Weiran Xu, Dayuan Fu, Jingang Wang, Yujia Fu

- **What's New**: 이 논문은 기존의 코드 인스트럭션 튜닝(code instruction tuning) 데이터셋을 분석하여 고품질의 데이터를 효율적으로 선별하고 이를 통해 뛰어난 성능을 발휘하는 새로운 모델, XCoder를 소개합니다.
- **Technical Details**: 코드 인스트럭션 튜닝은 기존 모델의 능력을 고품질이지만 소규모 데이터셋을 이용하여 원하는 방향으로 맞추는 데 중점을 둡니다. 이 논문에서는 좋은 코드 데이터를 다양성, 복잡성 및 품질 측면에서 정의하고, 데이터 효율적인 인스트럭션 튜닝(paradigm of data-efficient instruction tuning)을 제안합니다. 복잡성은 evolved complexity scorer를 사용하여 예측하며, 품질은 여러 테스트 케이스의 통과율을 기반으로 평가하고, 다양성은 데이터 풀과의 거리로 측정합니다.
- **Performance Highlights**: XCoder는 LLaMA3 모델을 기반으로 하며, 실험 결과 HumanEval과 LiveCodeBench 벤치마크에서 WizardCoder, Magicoder, StarCoder2-Instruct 및 OpenCodeInterpreter와 같은 최첨단 모델과 유사한 성능을 발휘합니다. 특히, XCoder-8B는 40K 샘플만으로도 뛰어난 성능을 보여주었으며, XCoder-70B는 오픈 소스 모델 중 최고 성능을 달성했습니다.

### [Configurable Foundation Models: Building LLMs from a Modular Perspective](https://arxiv.org/abs/2409.02877)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.02877.png)

Vote: 17

Authors: Feng Yao, Ao Zhang, Yufei Huang, Zexuan Zhong, Zhengyan Zhang, Guanyu Lin, +, Yingfa Chen, Jingbo Shang, Huimin Chen, Weilin Zhao, Yankai Lin, Chenglei Si, Chenyang Song, Xiaozhi Wang, Shuo Wang, Xu Han, Chenyang Zhao, Yuge Tu, Chaojun Xiao, Khai Hao Moo, Dazhi Jiang, Zhiyuan Liu

- **What's New**: 이번 논문에서는 대규모 사전 학습 언어 모델(LLMs)을 구성 블록(bricks) 기반으로 모듈화하여 효율성을 극대화하는 접근 방안을 탐구합니다. 이를 통해 디바이스에서 LLMs의 활용을 가능하게 하고, 도메인 간 지식 충돌을 최소화하며, 지속적인 학습과 적응을 개선할 수 있는 방향을 제시합니다.
- **Technical Details**: LLMs의 모듈화 접근법은 프리 트레이닝(pre-training)과 포스트 트레이닝(post-training) 과정에서 기능을 갖춘 모듈을 '블록' 단위로 구성하는 것입니다. 프리 트레이닝 단계에서는 셀프 슈퍼바이즈드 러닝을 통해 다양한 지식을 습득하며, 포스트 트레이닝에서는 도메인 지식과 특정 태스크 능력을 추가로 얻게 됩니다. 논문에서는 이러한 과정을 통해 LLMs의 내부 파라미터가 모듈화되는 경향을 설명합니다.
- **Performance Highlights**: 모듈화 접근법의 주요 이점으로는 특정 명령어에 따른 기능성 블록 선택을 통해 계산 효율성을 극대화할 수 있다는 점과, 신규 블록의 구축이나 기존 블록의 업데이트를 통해 LLMs의 적응성과 확장성을 높일 수 있다는 점입니다. 이는 전체 모델의 지속적인 학습보다 훨씬 비용 효율적이며, 다양한 도메인과 시나리오에 대응할 수 있는 유연성을 제공합니다.

### [Open-MAGVIT2: An Open-Source Project Toward Democratizing Auto-regressive Visual Generation](https://arxiv.org/abs/2409.04410)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.04410.png)

Vote: 14

Authors: Yixiao Ge, Ying Shan, Zhuoyan Luo, Fengyuan Shi, Yujiu Yang, Limin Wang

- **What's New**: 이 논문에서는 자연어 생성에서 우수한 성능을 보이는 대규모 언어 모델(Large Language Models, LLMs)을 기반으로 시각적 생성 분야에서도 유사한 효과를 얻기 위해 노력한 결과를 소개합니다. 특히, 최신 MAGVIT-v2 모델의 Lookup-Free Quantizer를 재현하고, 이를 통해 더 나은 시각적 생성 품질을 달성하는 방법을 연구합니다.
- **Technical Details**: Open-MAGVIT2는 강력한 시각 토크나이저와 자기회귀 트랜스포머(auto-regressive transformer)를 통합하는 두 단계로 구성됩니다. 시각 토크나이저는 입력된 시각 신호를 이산 토큰 표현으로 매핑하고, 이러한 벡터양자화된 시퀀스를 자기회귀 트랜스포머에 입력하여 토큰 간의 관계를 모델링하고 시각적 합성을 수행합니다. 특히, CNN 기반의 인코더-토크나이저-디코더 구조로 구성된 시각 토크나이저가 핵심이며, 이는 픽셀을 이산 표현으로 매핑하고, 양자화된 특징으로부터 이미지를 재구성합니다.
- **Performance Highlights**: 이 논문에서는 새로운 Lookup-Free Quantizer의 재생 구현을 통해 MAGVIT-v2와 유사한 재구성 성능(1.18 vs. 1.15 rFID on ImageNet 128×128)을 달성했습니다. 또한, 비대칭 토큰 분할(asymmetric token factorization)을 통해 코드북의 잠재력을 향상시키고, 이를 활용한 '다음 서브 토큰 예측(next sub-token prediction)'을 도입하여 더 나은 생성 품질을 이루어냈습니다. 이 결과는 ImageNet 데이터셋에서 기존 방법들을 능가하는 성능을 보였습니다.

### [Qihoo-T2X: An Efficiency-Focused Diffusion Transformer via Proxy Tokens for Text-to-Any-Task](https://arxiv.org/abs/2409.04005)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.04005.png)

Vote: 11

Authors: Jiasong Feng, Dawei Leng, Xiaodan Liang, Yuhui Yin, Ao Ma, Jing Wang

- **What's New**: 최근 핵심 확산 모델(core diffusion models)의 발전, 특히 Sora(OpenAI, 2024), Stable Diffusion 3(Stability AI, 2024), PixArt-α/Σ/δ(Chen et al., 2023; 2024a; 2024b), Vidu(Shengshu AI, 2024), Lumina-T2X(Gao et al., 2024), Flux(BlackForestlabs AI, 2024), CogVideoX(Yang et al., 2024) 등이 사진과 비디오 생성에 큰 성과를 이뤘습니다. 하지만 기존 모델들은 높은 시퀀스 길이에 따른 계산 복잡도 문제를 가지고 있어 실용적인 한계를 가지고 있습니다. 이 문제를 해결하기 위해, 본 논문에서는 프록시 토큰 확산 변환기(Proxy Token Diffusion Transformer, PT-DiT)를 제안하고, 텍스트-이미지, 텍스트-비디오 및 텍스트-멀티뷰 생성 모델을 포함한 Qihoo-T2X 시리즈를 발표합니다.
- **Technical Details**: PT-DiT는 전역 주의 메커니즘(global attention mechanism) 대신 프록시 토큰 주의(proxy token attention)를 사용하여 시각 토큰 간의 상호작용의 계산 복잡도를 줄입니다. 이는 시각적 정보의 지역적 유사성을 활용하여 프록시 토큰을 통해 전역 정보를 전달함으로써 이루어집니다. 또한, 스윈 변환기(Swin Transformer)와 유사하게 윈도우 주의 및 시프트 윈도우 주의(window and shift-window attention) 기법을 적용하여 텍스처 모델링 능력을 향상시켰습니다.
- **Performance Highlights**: 이미지 생성 작업에서, PT-DiT는 동일한 파라미터 규모에서 Pixary-α에 비해 약 33%의 계산 복잡도를 줄였습니다. 비디오 생성 작업에서는 2D 공간 및 1D 시간 주의가 제한된 공간-시간 모델링을 제공하는 반면, PT-DiT는 3D 정보를 효율적으로 추출하여 전체 계산 복잡도를 줄입니다. 실험 결과, PT-DiT는 이미지 생성 작업에서 DiT의 52%, Pixart-α의 65% 계산 복잡도로 동등한 파라미터 크기에서 경쟁력 있는 성능을 보여주었습니다. 비디오 생성 작업에서, PT-DiT는 CogVideoX의 77.2%, EasyAnimate의 85% 계산 복잡도로 효율성을 크게 향상시켰습니다.

### [Spinning the Golden Thread: Benchmarking Long-Form Generation in Language Models](https://arxiv.org/abs/2409.02076)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.02076.png)

Vote: 6

Authors: Yuhao Wu, Ming Shan Hee, Zhiqing Hu, Roy Ka-Wei Lee

- **What's New**: Spinning the Golden Thread(SGT) 벤치마크 소개는 자연어 처리(NLP)에서 매우 긴 텍스트 생성 및 처리에 중점을 둡니다. 기존 벤치마크는 대부분 짧은 텍스트(<2K tokens)를 대상으로 하고 있어 긴 텍스트 생성(≥16K tokens) 작업을 평가하는데 적합하지 않았습니다.
- **Technical Details**: SGT는 네 개의 시나리오 - 다이어리 쓰기(Diary Writing), 메뉴 디자인(Menu Design), 고층 빌딩 디자인(Skyscraper Design), 도시 계획(Urban Planning) - 를 통해 모델의 초장문 생성 능력을 평가합니다. 각 시나리오에는 단일 인스턴스, 범위, 정기성 등 세 가지 하위 시나리오가 있어 상세한 평가를 가능하게 합니다.
- **Performance Highlights**: 실험 결과 대부분의 장문 처리 모델이 SGT에서의 과제를 해결하지 못했으며, 특히 텍스트 길이가 16K 이내일 때도 높은 성능을 보이지 못했음을 알 수 있었습니다. 이는 현재의 LLM들이 장문 텍스트 생성에 있어서 한계가 있다는 점을 시사합니다.

### [GST: Precise 3D Human Body from a Single Image with Gaussian Splatting Transformers](https://arxiv.org/abs/2409.04196)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.04196.png)

Vote: 6

Authors: Joao F. Henriques, Abdullah Hamdi, Lorenza Prospero, Christian Rupprecht

- **What's New**: 새로운 아키텍처인 Gaussian Splatting Transformer (GST)를 통해 단일 이미지로부터 실시간에 가까운 속도로 정밀한 3D 인간 모델을 재구성할 수 있는 방법을 제안합니다. 이 방법은 기존의 확산(prior) 모델에 의존하지 않으며, 향상된 3D 렌더링 속도와 유연한 편집 기능을 갖추고 있습니다.
- **Technical Details**: GST는 단일 이미지를 입력으로 하여 3D Gaussian Splats를 예측하는 트랜스포머(transformer) 아키텍처를 사용합니다. 모델은 다중 뷰 감독(multi-view supervision)을 활용하여 고가의 3D 포인트 클라우드 없이 정밀한 3D 관절과 신체 자세를 예측합니다. Gaussian Splats는 SMPL 모델을 기반으로 하여 배치되며, 이를 통해 초기화 및 자세와 외형의 공동 최적화가 가능합니다. 모델의 백본(backbone)은 HMR2를 따르며, ViT를 사용하여 이미지를 시각적 토큰으로 맵핑합니다.
- **Performance Highlights**: GST는 실제로 여러 뷰를 사용할 수 없는 경우에도 단일 이미지 입력으로부터 사용자 정의 뷰 신테시스를 수행할 수 있으며, 기존의 확산 모델을 사용하지 않고도 실시간에 가까운 예측을 달성할 수 있습니다. 이는 특히 VR 및 AR과 같은 3D 공간 컴퓨팅 응용 프로그램에 유리합니다. 기존의 방법들과 비교했을 때, GST는 3D 감독 없이도 시각적 품질과 3D 자세 추정 지표에서 동등하거나 더 나은 성능을 보입니다.

