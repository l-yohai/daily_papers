## Daily Papers (2024-10-12)

### [Accelerated Preference Optimization for Large Language Model Alignment](https://arxiv.org/abs/2410.06293)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06293.png)

Vote: 2

Authors: Huizhuo Yuan, Jiafan He, Quanquan Gu

- **What's New**: 이 논문에서는 인간 피드백 기반 강화 학습(RLHF)에서 더욱 빠른 최적화를 가능하게 하는 가속된 선호 최적화(Accelerated Preference Optimization, APO) 방법을 소개합니다. 전통적인 RLHF 방법에서는 보상 모델(reward model)과 정책 최적화(policy optimization)가 중요한 역할을 했지만, 이 연구에서는 Nesterov의 모멘텀 접근을 활용하여 직접적인 선호 최적화가 가능하도록 개선하였습니다.
- **Technical Details**: APO는 Nesterov’s acceleration technique을 기반으로 한 iterative preference optimization의 변형을 제안합니다. 이는 각 정책 업데이트 후 외삽(extrapolation) 단계를 포함하여 최적화 과정을 가속화합니다. 이 방법은 Bradley-Terry(BT) 모델 하에서 이론적으로 서브최적성 갭(sub-optimality gap)을 개선할 수 있음을 입증하였습니다. 또한, 일반적인 선호 모델에 대해서도 SPPO 손실 함수와 함께 g속할 수 있음을 보여주었습니다.
- **Performance Highlights**: 실험적으로, APO는 DPO 방법을 Mistral-7B-Instruct-v0.2 모델에 적용하여 AlpacaEval 2.0 평가 작업에서 31.73%의 승률을 기록하였으며, 이는 iterative DPO 대비 1.78% 개선된 수치입니다. 또한, 두 번의 반복만으로 37.53%의 승률을 달성하였으며, 이는 DPO 세 번 반복 시의 성능 수준을 매칭하며 응답 길이를 줄이는 데 성공했습니다. MT-Bench의 다섯 가지 일반 지시-따르기 작업에서도 평균 점수가 9.57/10으로 더욱 우수한 성능을 입증했습니다.

