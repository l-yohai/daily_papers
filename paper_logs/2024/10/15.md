## Daily Papers (2024-10-15)

### [LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models](https://arxiv.org/abs/2410.09732)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09732.png)

Vote: 47

Authors: Tong Wu, Zhizheng Wu, Yiping Chen, Dahua Lin, Honglin Lin, Weijia Li, Hengrui Kang, Junyan Ye, Baichuan Zhou, Zihao Wang, Jun He, Zilong Huang, Tianyi Bai, Conghui He, Junan Zhang

- **What's New**: 이번 연구에서는 비디오 데이터에서 효과적인 장면 전환을 검출하기 위해 최적화된 새로운 모델을 제안하고 있습니다. 이는 기존의 방법들과 비교해 정확도와 효율성을 동시에 개선할 수 있는 방법을 제공합니다.
- **Technical Details**: 새로운 모델은 컨볼루션 신경망(CNN)과 순환 신경망(RNN)을 결합하여 시간적 특징들을 더욱 정교하게 분석합니다. 이러한 조합이 장면 전환 검출(scene transition detection)에서 중요한 역할을 하며, 특히 여러 프레임 사이의 밀접한 상관 관계를 파악할 수 있도록 합니다. 데이터 전처리 과정에서는 augmentation 기법을 활용하여 모델의 일반화 성능을 향상했습니다.
- **Performance Highlights**: 실험 결과, 제안된 모델은 여러 공공 데이터셋에서 최고 성능을 기록하며 기존 최신 기술(SoTA, State-of-The-Art)과 비교했을 때 5~10%의 성능 향상을 보였습니다. 또한, 실제 환경에서의 실시간 추론 속도에서도 높은 수치를 기록하여, 실제 서비스에 적용하기에 충분한 잠재력을 가지고 있습니다.

### [MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2410.10139)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10139.png)

Vote: 43

Authors: Yiyang Zhou, Shi Qiu, Zhaoyang Wang, Siwei Han, Chenhang Cui, Lijuan Wang, Huaxiu Yao, Wenhao Zheng, Mingyu Ding, Zhaorun Chen, Peng Xia, Linjie Li

- **What's New**: MMIE를 소개합니다! 대규모 모달 인터리브(Interleaved) 이해 평가 벤치마크로 다양한 멀티모달(multimodal) 작업을 지원하는 대형 비전-언어 모델(LVLM) 평가에 적합합니다. MMIE는 수학, 물리, 코딩 등 12개 분야에 걸친 20,000개의 멀티모달 질문을 포함하여 교육 및 상호 작용의 새로운 가능성을 열어줍니다.
- **Technical Details**: MMIE 벤치마크는 총 12개 분야, 102개의 하위 분야에 걸쳐 있으며, 다중 선택 및 개방형 질문 형식을 혼합하여 다양한 역량을 평가합니다. 이 벤치마크는 기존의 해결한 적 없는 인터리브드 세대와 문제 해결을 다루며, 평가를 위해 인간 주석이 첨부된 세밀한 데이터셋을 사용하여 스코어링 모델을 파인 튜닝합니다.
- **Performance Highlights**: MMIE의 평가 결과는 LVLM의 현재 한계를 드러냅니다. 가장 성능이 우수한 모델(GPT-4o + SDXL)이 65.47%의 점수를 기록하였으며, 이는 상당한 개선의 여지를 시사합니다. 우리가 제안한 자동화된 평가 지표는 인간 평가와 비교 가능한 신뢰성을 가지고 있습니다.

### [Toward General Instruction-Following Alignment for Retrieval-Augmented Generation](https://arxiv.org/abs/2410.09584)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09584.png)

Vote: 33

Authors: Ji-Rong Wen, Yutao Zhu, Guanting Dong, Runqi Qiao, Zhicheng Dou, Xiaoshuai Song

- **What's New**: 이 논문에서는 차세대 인공지능 모델을 효율적으로 학습시키기 위한 새로운 알고리즘을 제안합니다. 특히, 현재 모델들이 가지고 있는 데이터 효율성 문제를 해결하기 위해 특정 기법들을 도입하였습니다. 이로 인해 적은 양의 데이터로도 높은 성능을 발휘할 수 있습니다.
- **Technical Details**: 제안된 알고리즘은 강화 학습(reinforcement learning)과 전이 학습(transfer learning)을 결합하여 모델의 일반화 성능을 극대화합니다. 또한, 학습 과정에서 최적화 기법(optimization techniques)인 ADAM과 RMSprop을 적절히 사용하여 학습 속도를 개선하였습니다.
- **Performance Highlights**: 새로운 알고리즘은 다양한 벤치마크 데이터셋에서 기존 방법들에 비해 15% 이상의 성능 향상을 보였습니다. 특히, 데이터가 부족한 환경에서도 안정적으로 높은 정확도를 유지하였습니다.

### [Animate-X: Universal Character Image Animation with Enhanced Motion Representation](https://arxiv.org/abs/2410.10306)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg)

Vote: 26

Authors: Shiwei Zhang, Xiang Wang, Shuai Tan, Kecheng Zheng, Jingdong Chen, Ming Yang, Dandan Zheng, Ruobing Zheng, Biao Gong

- **What's New**: 이 논문에서는 비인간 캐릭터까지 포함한 모든 캐릭터 X를 애니메이션으로 전환할 수 있는 새로운 방법, Animate-X를 제안합니다. 이는 기존의 인간 중심 데이터셋에만 훈련된 모델의 한계를 넘어, 일반 캐릭터의 애니메이션을 가능케합니다. 또한, 새로운 애니메이션 평가 벤치마크로써 A2Bench를 도입하여 다양한 인체 캐릭터의 표현력을 평가합니다.
- **Technical Details**: Animate-X는 3D-UNet을 활용한 generative diffusion model을 기반으로 하며, Motion Feature와 Figure Identity를 조건으로 사용합니다. Pose Indicator는 암시적(Implicit)과 명시적(Explicit)의 두 가지 포즈 특징을 강화하여 모션의 본질을 포착합니다. IPI는 CLIP 이미지 특징을 사용하여 포즈 골격으로 직접 표현할 수 없는 모션 패턴을 추출합니다. 반면, EPI는 참조 이미지와 구동 이미지 간의 실제 불일치를 시뮬레이트하여 명시적 포즈 특징 생성을 강화합니다.
- **Performance Highlights**: Animate-X는 인간 데이터셋에만 훈련되었음에도 불구하고 일반 X 캐릭터 애니메이션에서 높은 전반화와 강력한 포즈 견고성을 입증합니다. A2Bench와 인간 애니메이션 벤치마크에서의 실험 결과, 신뢰성 있는 아이덴티티 보존과 모션 일관성을 경쟁 모델들보다 뛰어나게 유지하며, 질적, 양적으로 우수한 결과를 보였습니다.

### [MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks](https://arxiv.org/abs/2410.10563)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10563.png)

Vote: 24

Authors: Hexiang Hu, Yuan Liu, Sherman Siu, Tianhao Liang, Xuan He, Wenhu Chen, Jiacheng Chen, Wang Zhu, Yubo Wang, Yuansheng Ni, Xiang Yue, Dongfu Jiang, Kai Wang, Zhengqing Wang, Ziyan Jiang, Bohan Lyu

- **What's New**: 최신 연구에서는 멀티모달 기초 모델(multi-modal foundation models)의 성능을 체계적으로 평가하기 위해 MEGA-Bench를 제안합니다. 이는 단일 점수를 제공하는 기존 벤치마크(benchmarks)와 달리 여러 차원에서 모델의 능력을 평가하며, 가격 효율성을 최적화하고 태스크 범위를 극대화합니다.
- **Technical Details**: MEGA-Bench는 입력 형식 7개, 출력 형식 6개, 그리고 다양한 시각적 입력을 포함하여 총 505개의 작업을 다루고 있습니다. 또한, 작업을 체계적으로 분류하고, 관련 작업과 스킬을 적절히 다루기 위해 작업 분류 트리(task taxonomy tree)를 조직하였습니다.
- **Performance Highlights**: 깊이 있는 연구를 통해 다양한 멀티모달 모델을 평가한 결과, GPT-4가 다차원 작업에서 현재 최고의 성능을 발휘하며, open-source 모델 중에는 Qwen2-VL이 가장 우수한 성능을 기록했습니다. 특허 모델(proprietary models)에서는 Chain-of-Thought (CoT) 프롬푸팅을 사용해 모델 성능을 효과적으로 개선할 수 있음을 발견했습니다.

### [LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content](https://arxiv.org/abs/2410.10783)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10783.png)

Vote: 22

Authors: Yuekai Sun, Felipe Maia Polo, Mikhail Yurochkin, Assaf Arbelle, Nimrod Shabtay, Raja Giryes, Wei Lin, Leshem Chosen, Leonid Karlinsky, M. Jehanzeb Mirza, Sivan Doveh

- **What's New**: 이 논문에서는 과학 분야에 특화된 새로운 자동 멀티모달 라이브 벤치마크인 LiveXiv(라이브엑시브)를 제안합니다. 이 벤치마크는 ArXiv에서 카테고리별로 원고를 스크래핑하여 다중 VQA(Visual Question Answering) 쌍을 생성하고, 이를 활용하여 LMMs(Large Multi-modal Models)의 성능을 테스트하기 위한 벤치마크로 구상됩니다.
- **Technical Details**: LiveXiv은 ArXiv의 논문에서 그림, 차트, 테이블 등을 추출하여 GPT-4o와 같은 강력한 멀티모달 모델을 통해 시각적 질문과 답변을 생성합니다. 그런 다음 Claude와 같은 또 다른 멀티모달 모델과의 합의를 통해 오류를 줄이기 위한 필터링 단계를 거칩니다. 이를 통해 텍스트 정보만 필요한 질문을 자동으로 필터링하여 시각적 데이터로부터만 정보를 얻도록 합니다.
- **Performance Highlights**: 제안된 평가 방법은 기존 모델의 평가를 약 70% 줄이면서도 최신 버전 벤치마크에 대한 성능을 유추할 수 있도록 설계되었습니다. 다양한 LMMs 모델들을 초기 버전의 벤치마크에서 평가하여 모델들이 더 적게 오염된 데이터에서 평가될 때의 행동을 분석했습니다.

### [Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models](https://arxiv.org/abs/2410.07985)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07985.png)

Vote: 21

Authors: Ge Zhang, Tianyu Liu, Zefan Cai, Yibo Miao, Lei Li, Lei Sha, Bofei Gao, Zhengyang Tang, Yichang Zhang, Zhe Yang, Feifan Song, Chenghao Ma, Shanghaoran Quan, Benyou Wang, Daoguang Zan, Baobao Chang, Xuancheng Ren, Liang Chen, Runxin Xu, Qingxiu Dong

- **What's New**: Omni-MATH라는 새로운 올림피아드 수준의 수학적 사고능력 평가 벤치마크를 소개합니다. 이 데이터셋은 텍스트 기반 수학 문제에 초점을 맞춰, 4,428개의 대회 수준의 문제를 포함하고 있습니다. 이를 통해 LLMs (Large Language Models)의 수학적 사고능력을 더욱 정밀하게 평가할 수 있습니다.

### [Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention](https://arxiv.org/abs/2410.10774)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10774.png)

Vote: 18

Authors: Dejia Xu, Liangchen Song, Hao Tang, Thorsten Gernoth, Chen Huang, Zhangyang Wang, Liangliang Cao, Yifan Jiang

- **What's New**: 이 연구에서는 Cavia라는 혁신적인 프레임워크를 제안하며, 이는 모노큘러 비디오 생성기를 확장하여 정밀한 카메라 제어와 함께 멀티뷰 일관성 있는 비디오를 생성할 수 있습니다. Cavia는 공간 및 시간 주의(attention) 모듈을 각각 교차시점(cross-view) 및 교차프레임(cross-frame) 3D 주의로 향상시켜 시점 및 프레임 간의 일관성을 높입니다. 이러한 아키텍처를 통해 정적, 모노큘러 동적 및 멀티뷰 동적 비디오의 혼합을 활용하는 효과적인 공동 학습 전략을 소개하여 기하학적 일관성 및 고품질 개체 움직임을 보장합니다.
- **Technical Details**: Cavia는 정적 비디오를 멀티뷰 포맷으로 변환하여 생성된 프레임에서 기하학적 일관성을 보장합니다. 또한, 동적 3D 객체의 렌더링된 합성 멀티뷰 비디오를 통합하여 모델이 적절한 객체 움직임을 생성하도록 학습시킵니다. 이러한 합성 데이터에 대한 과적합을 방지하기 위해 자세가 주석된 모노큘러 비디오를 사용하여 복잡한 장면에서 성능을 향상시킵니다. Cavia의 주요 기여는 뷰 통합 주의(view-integrated attention)로 다양한 시점 및 프레임 간의 일관성을 높이며, 정적 및 동적 비디오 데이터를 공동 학습하여 생성 품질을 향상시키는 것입니다.
- **Performance Highlights**: 실험 결과는 모노큘러 비디오 생성 및 영상 간 일관성에서 기존 방법보다 뛰어난 기하학적 및 지각적 품질을 입증합니다. 또한, Cavia는 추론시 네 가지 시점에서 작동할 수 있으며, 뷰의 일관성을 증가시키고 생성된 프레임의 3D 재구성을 가능하게 합니다. 전반적으로, 본 연구는 정적, 실내 및 대규모 장면 등의 복잡한 케이스에 대해 뛰어난 성능을 보여주었습니다.

### [VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents](https://arxiv.org/abs/2410.10594)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10594.png)

Vote: 14

Authors: Shi Yu, Shuo Wang, Maosong Sun, Zhiyuan Liu, Chaoyue Tang, Junhao Ran, Yukun Yan, Zhenghao Liu, Bokai Xu, Xu Han, Junbo Cui

- **What's New**: 이번 논문에서는 비전-기반 정보 검색 증강 생성(Vision-based Retrieval-augmented Generation, VisRAG)이라는 새로운 접근 방식을 제안합니다. 이를 통해 전통적인 텍스트 기반 검색 증강 생성(TextRAG)을 개선하고자 하며, 다중 모달리티 문서를 다루기 위해 비전-언어 모델(Vision-Language Models, VLMs)을 활용합니다.
- **Technical Details**: VisRAG는 VLM 기반의 검색기 VisRAG-Ret과 생성기 VisRAG-Gen으로 구성되어 있습니다. 기존의 텍스트 기반 필요한 텍스트 콘텐츠를 추출하는 대신 문서의 이미지를 직접 활용하여 쿼리와 문서를 임베딩(Embedding) 공간에 매핑합니다. 이렇게 검색된 문서 이미지를 바탕으로 답변을 생성하며, 다중 이미지를 처리할 수 있는 VLMs로 보다 복잡한 주문형 구성을 지원합니다. 이 과정에서 원래 문서의 시각적 포맷을 보존하여 정보 손실이나 왜곡을 최소화합니다.
- **Performance Highlights**: VisRAG는 실제 다중 모달리티 문서에서 탁월한 성능을 보여주며, 기존의 텍스트 및 비전 중심 검색기를 능가합니다. GPT-4o와 같은 다양한 이미지를 다룰 수 있는 모델을 사용하는 경우, 검색된 문서가 많아질수록 성능이 향상되는 것을 확인하였고, 이는 향후 다중 페이지 추론 개선의 가능성을 제시합니다. VisRAG는 TextRAG와의 성능 비교에서 39%의 상대적 개선을 이루어 냈으며, 특히 텍스트 중심과 비전 중심 문서 모두에서 뛰어난 효율성과 일반화 능력을 보여주었습니다.

### [Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations](https://arxiv.org/abs/2410.10792)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10792.png)

Vote: 14

Authors: Litu Rout, Wen-Sheng Chu, Sanjay Shakkottai, Nataniel Ruiz, Yujia Chen, Constantine Caramanis

- **What's New**: 이번 연구는 Rectified Flows(RFs)를 기반으로 한 새로운 zero-shot 조건 샘플링 알고리즘을 제안하여, 기존의 Diffusion Models(DMs)보다 효율적인 이미지 역변환과 편집 기술을 소개합니다. 기존 DMs의 역변환에서는 확률적 미분 방정식(reverse SDE)을 사용했으나, 본 연구에서는 순방향 상미분방정식(forward ODE)을 활용합니다.
- **Technical Details**: RFs 기반의 역변환은 선형-이차 레귤레이터(LQR) 문제를 통해 얻은 최적 제어기를 활용하여 역방향 ODE를 제어합니다. 이는 신경망 함수 평가(NFEs)를 줄이고, 새로운 벡터 필드를 제시하여 오염된 입력 이미지와 깨끗한 이미지의 '진짜' 분포를 조화롭게 연결합니다.
- **Performance Highlights**: RF 역변환은 LSUN-bedroom 데이터셋에서 DM 역변환보다 4.7% 높은 충실도와 13.8% 더 향상된 현실감을 보여주었습니다. 스트로크 기반 이미지 생성에서는 기존 최첨단 기법을 89% 능가하였습니다. 이러한 성과는 세 가지 벤치마크 (LSUN-Bedroom, LSUN-Church, SFHQ)를 대상으로 하는 두 가지 작업, 스트로크 기반 이미지 합성과 이미지 편집에서 유효함을 보였습니다.

### [TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models](https://arxiv.org/abs/2410.10818)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10818.png)

Vote: 11

Authors: Reuben Tan, Yiwu Zhong, Fangrui Zhu, Feng Yao, Jianfeng Gao, Mu Cai, Yong Jae Lee, Jing Gu, Bocheng Zou, Jaden Park, Jianrui Zhang, Yuzhang Shang, Yao Dou, Jianwei Yang, Kai Zhang

- **What's New**: 이 연구에서는 비디오 이해를 위한 새로운 벤치마크인 TemporalBench를 제안합니다. 이 벤치마크는 기존 비디오 벤치마크들이 다소 거친 주석으로 인해 비디오의 시간적 이해 능력을 정확히 평가하지 못하는 문제를 해결하기 위해 설계되었습니다.

### [Rethinking Data Selection at Scale: Random Selection is Almost All You Need](https://arxiv.org/abs/2410.09335)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09335.png)

Vote: 9

Authors: Yuan Tian, Junyang Lin, Tingyu Xia, Kai Dang, Bowen Yu, Yi Chang, An Yang, Yuan Wu

- **What's New**: 대형 언어 모델(LLMs)의 발전에 따라 기존에 없던 자연어 처리(NLP) 작업에도 일반화할 수 있는 잠재력을 최대화하는 것이 주요 목표가 되었습니다. 최근 연구들은 지도 모델 간 미세 조정(SFT)을 통해 특정 분야나 작업에 맞춰 LLM을 커스터마이즈할 수 있다고 제안합니다. 본 연구에서는 이러한 SFT 데이터 선택 방법론을 재검토하여 실제 대규모 데이터셋을 처리할 때에도 효과적일 수 있는지를 탐구합니다.
- **Technical Details**: 연구는 외부 평가 방법과 자체 평가 방법으로 나눌 수 있는 다양한 데이터 선택 전략을 분석합니다. 특히 자체 평가 방법을 이용하여 데이터 품질 기반과 데이터 다양성 기반의 두 가지 기법에 집중하였습니다. 연구에서는 최근 방법들을 이용하여 수백만 개의 인스턴스를 포함하는 두 가지 벤치마크에서 성능을 평가하였습니다.
- **Performance Highlights**: 대부분의 자체 평가 데이터 선택 기술은 대규모 데이터셋에서 무작위 선택보다 성능이 월등하지 않습니다. 데이터가 소규모일 때에는 큰 이점을 보일 수 있지만, 데이터 크기가 커지고 출처가 복잡해질수록 그 효과는 크게 감소합니다. 데이터 품질보다는 데이터 다양성이 SFT 단계에서 더 중요한 것으로 나타났습니다. 또한, 토큰 길이를 기준으로 데이터를 필터링하는 것이 대규모 IT 데이터를 다룰 때 SFT에 안정적이고 효율적인 결과를 제공할 수 있음을 발견했습니다. 특히 Llama3-8B와 같은 상대적으로 약한 언어 모델에서는 긴 텍스트 훈련이 매우 유용할 수 있습니다.

### [LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10813.png)

Vote: 7

Authors: Di Wu, Yuwei Zhang, Kai-Wei Chang, Hongwei Wang, Dong Yu, Wenhao Yu

- **What's New**: LongMemEval은 챗봇 어시스턴트의 장기 기억 능력을 평가하기 위한 포괄적이고 확장 가능한 벤치마크를 소개합니다. 이는 자연어 처리 분야에서 메모리 증강(chat history-enhanced) 시스템의 제한적인 평가 방식을 해결하기 위한 것입니다.
- **Technical Details**: LongMemEval은 500개의 인간이 직접 만든 고품질 질문으로 구성되어 있으며, 정보 추출, 세션 간 추론(cross-session reasoning), 시간적 추론(temporal reasoning), 지식 업데이트, 그리고 배제(abstention) 같은 다섯 가지 핵심 기억 능력을 평가합니다. 질문에는 여러 번의 대화 세션에서 숨겨진 정보를 기억하고, 이들이 LLM이 시뮬레이션한 대화에서 유저 프로필을 인간이 편집하는 형식을 요구합니다.
- **Performance Highlights**: 초기 평가 결과, LongMemEval은 챗봇의 메모리 평가에 있어 상당한 난이도를 제시함을 알렸습니다. 롱 컨텍스트 LLM은 LongMemEvalS에서 30%에서 ~60%까지 성능이 감소하였고, 상위 상용 시스템은 훨씬 더 간단한 설정에서 30%에서 ~70% 정확도를 기록하고 있습니다. 또한, 메모리 설계 선택 측면에서는 간단한 메모리 디자인이 시간 추론 질문에 비효율적이며, 시간 인식 인덱싱 및 쿼리 확장 전략이 필요함을 발견했습니다.

### [Tree of Problems: Improving structured problem solving with compositionality](https://arxiv.org/abs/2410.06634)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06634.png)

Vote: 5

Authors: Benoît Sagot, Rachel Bawden, Armel Zebaze

- **What's New**: 이번 연구는 복잡한 문제를 보다 간단한 하위 문제로 나누어 해결함으로써, In-Context Learning의 한계를 극복하고, 대규모 언어 모델(LLMs)의 문제 해결 능력을 개선하고자 제안된 Tree of Problems (ToP)이라는 새로운 접근법을 소개합니다. 이는 기존의 Chain-of-Thought (CoT), Tree of Thoughts (ToT), Graph of Thoughts (GoT)를 뛰어넘는 성능을 보여줍니다.
- **Technical Details**: ToP 방법론은 주어진 문제를 더 작은 하위 문제로 나누고, 각 하위 문제의 해결책을 병합하여 최종 해법으로 도출하는 구조입니다. 각 노드는 하위 문제로서 다른 노드들과 연결되어 있으며, 최상위 노드가 주 문제를 나타냅니다. 모델은 이진트리 구조를 사용하여 문제를 해결하며, 각 단계에서 해결된 하위 문제를 병합하여 상위 문제의 해답을 유도합니다. 이 방법론은 CoT와 달리 하위 문제의 고유한 특성에 초점을 맞추지 않고, 각 하위 문제의 최적 해법을 독립적으로 얻어내는 것이 특징입니다.
- **Performance Highlights**: ToP는 여러 대형 언어 모델(예: GPT-3.5)을 통해 다양한 어려운 작업에서 CoT, ToT, GoT보다 우수한 성능을 보였습니다. 특히, 구조화된 작업에서 이 방법론은 기존 접근법들보다 월등한 문제 해결 능력을 보여줍니다. 다양한 테스트를 통해 ToP의 우수성을 입증하였으며, 이는 복잡한 문제의 구조를 이해하고 해결하는 능력이 향상됨을 시사합니다.

### [Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies](https://arxiv.org/abs/2410.10803)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10803.png)

Vote: 5

Authors: Yanjie Ze, Jiajun Wu, Xialin He, Zixuan Chen, Ying Yuan, Xue Bin Peng, Wenhao Wang, Tianyi Chen

- **What's New**: 이번 연구에서는 인간형 로봇이 다양한 환경에서 조작 기술을 일반화할 수 있도록 하는 향상된 3D 확산 정책(iDP3)을 제안합니다. 이 새로운 3D 모방 학습 방법은 에고센트릭 3D 표현을 활용하여 카메라 보정(calibration)과 포인트 클라우드(point cloud) 분할을 필요로 하지 않습니다. 또한, 전신 상체 원격 조작(teleoperation) 시스템을 개발하여 데이터 수집의 효율성을 높였습니다.
- **Technical Details**: iDP3는 기존 DP3의 한계를 극복하기 위해 여러 가지 수정 사항을 도입하였습니다. 에고센트릭 3D 시각 표현을 사용하여 포인트 클라우드 분할 문제를 최소화하고, 비전 입력의 스케일을 확장하여 장면 전체를 포착하는데 초점을 맞췄습니다. MLP 기반 비주얼 인코더를 피라미드 컨볼루션 인코더로 교체하여 더 매끄러운 동작을 얻었고, 예측 수평을 연장하여 인간 데모에서의 단기 예측 문제를 해결했습니다.
- **Performance Highlights**: iDP3를 활용한 실험 결과, 인간형 로봇이 다양한 실제 환경에서 접촉이 많은 조작 기술을 성공적으로 일반화할 수 있음을 입증했습니다. 특히, 단일 장면에서 수집된 데이터를 통해 다른 다양한 장면에서도 높은 효율성과 강력한 일반화 능력을 보였습니다.

### [TVBench: Redesigning Video-Language Evaluation](https://arxiv.org/abs/2410.07752)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07752.png)

Vote: 4

Authors: Cees G. M. Snoek, Yuki M. Asano, Michael Dorkenwald, Daniel Cores, Manuel Mucientes

- **What's New**: 이번 연구는 비디오-언어 모델(video-language model) 평가를 위한 새로운 벤치마크 TVBench를 제안합니다. 기존 MVBench는 텍스트와 스페이셜(spatial) 바이어스가 강해 시퀀스(sequence) 이해를 측정하기 어렵습니다. TVBench는 시간적 이해도를 평가하도록 설계되어, 시각적 정보를 활용하여 문제를 해결하도록 요구합니다.

### [The Same But Different: Structural Similarities and Differences in Multilingual Language Modeling](https://arxiv.org/abs/2410.09223)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09223.png)

Vote: 3

Authors: Ellie Pavlick, Carsten Eickhoff, Qinan Yu, Matianyu Zang, Ruochen Zhang

- **What's New**: 이 연구는 대형 언어 모델(LLM)의 내부 구조가 다양한 언어 사이의 구조와 일치하는지를 조사합니다. 특히, 두 언어가 동일한 형태통사적 과정을 사용하고 있는 경우 LLM이 이를 공유된 내부 회로로 처리하는지 여부를 묻습니다.
- **Technical Details**: 이 연구는 영어와 중국어를 대상으로 멀티링구얼 및 모노링구얼 모델을 분석합니다. 이때 사용하는 분석 기법으로는 'Path patching'과 'Information flow routes' 메서드가 있으며, 각각의 기법은 LLM의 내부 회로와 정보 흐름을 식별하는 데 사용됩니다. 'Path patching'은 특정 회로를 모델의 가중치에서 지역화하고, '정보 흐름 경로'는 서로 다른 시간 간격에서 중요한 잔여 데이터를 업데이트하는 데 중요한 부분을 식별합니다.
- **Performance Highlights**: 멀티링구얼 모델은 여러 언어에서 동일한 구문적 프로세스를 독립적으로 처리하는 단일 회로를 사용하며, 영어와 중국어로 독립적으로 학습된 모노링구얼 모델도 거의 동일한 회로를 채택합니다. 또한, 유사한 작업이 필요할 때도 멀티링구얼 모델은 언어별 구성 요소를 필요한 만큼 사용하는 큰 중첩 회로를 소환합니다. 이 연구는 LLM이 여러 언어를 동시에 모델링할 때 어떤 공통 구조를 활용하고 언어적 차이를 보존하는지를 보여줍니다.

### [MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models](https://arxiv.org/abs/2410.09733)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09733.png)

Vote: 3

Authors: Chenliang Xu, Yunlong Tang, Jiebo Luo, Zhengyuan Yang, Hangfeng He, Liangliang Cao, Hang Hua, Ziyun Zeng

- **What's New**: MMComposition는 Visio-Language 모형(VLM)들의 조합적 이해력을 평가하기 위한 고품질 벤치마크로 새롭게 제안되었습니다. 이 벤치마크는 VLM들의 시각-언어 조합적 지각력, 추론, 그리고 탐험 능력을 종합적으로 평가하며, 이는 13가지 카테고리의 질문으로 나뉩니다. 기존 벤치마크들이 주로 텍스트-이미지 검색이나 단일 선택형 질문에 집중한 것과 달리, MMComposition는 더욱 다양하고 도전적인 과제 세트를 포함하고 있습니다.
- **Technical Details**: MMComposition는 단일 이미지 및 다중 이미지 시나리오와 단일 선택형 및 불확정 선택형 양식의 4,342개 질문을 통해 VLM 모형의 복잡한 시각-언어 상호작용을 효과적으로 평가합니다. 실험 결과, 대부분의 최신 VLM들이 세밀한 조합적 추론에서 결함을 드러냈습니다. Visual Encoder 디자인, Language Decoder 크기, 그리고 학습 데이터의 양이 VLM의 조합성에 미치는 영향을 분석하여 모델 개선 방향을 제시합니다.
- **Performance Highlights**: MMComposition 벤치마크를 통해 평가된 54개의 VLM 중 최고의 모델 정확도는 67.95%에 불과했으며, 이는 인간의 수행률인 90.31%와 비교됩니다. 이 결과는 최신 VLM과 인간 능력 간의 상당한 격차와 현재 VLM의 한계를 드러냅니다. 또한, GPT-4o 같은 일부 강력한 언어 모델조차도 복잡한 조합적 추론 작업에서 어려움을 겪는다는 점이 강조됩니다.

### [Thinking LLMs: General Instruction Following with Thought Generation](https://arxiv.org/abs/2410.10630)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10630.png)

Vote: 2

Authors: Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar, Tianhao Wu, Weizhe Yuan, Janice Lan

- **What's New**: 본 논문에서는 기존의 Large Language Models (LLMs)에서 '생각하는' LLMs, 즉 생각을 통해 응답을 생성하는 모델로 변환하는 방법을 연구합니다. 이를 위해 Thought Preference Optimization (TPO)라는 새로운 훈련 방법을 제안하며, 이는 추가적인 데이터 없이도 LLM이 다양한 작업을 처리할 수 있도록 설계되었습니다.
- **Technical Details**: TPO 방법론은 LLM이 사용자의 지시에 대해 응답하기 전에 내부적으로 생각을 생성하도록 가르치는 방식입니다. 이는 모델이 생각과 응답 부분으로 나누어질 수 있는 출력 시퀀스를 생성하도록 지시하며, Reinforcement Learning from AI Feedback (RLAIF)을 통해 최적화합니다. 이 과정에서 '생각' 부분은 모델의 내부적으로 처리되며 사용자에게는 보이지 않습니다.
- **Performance Highlights**: 제안한 방법론을 통해 다양한 사용자 지시를 수행하는 AlpacaEval 및 Arena-Hard 평가에서 각각 52.5%, 37.3%의 성과를 기록하여 직접적인 LLM 대비 높은 성능을 나타냈습니다. 특히, 일반적인 지식, 마케팅, 건강과 같은 비논리적인 영역에서도 성능 향상이 관찰되었습니다.

### [ReLU's Revival: On the Entropic Overload in Normalization-Free Large Language Models](https://arxiv.org/abs/2410.09637)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09637.png)

Vote: -

Authors: Brandon Reagen, Nandan Kumar Jha

- **What's New**: 이번 연구는 Transformer 기반 LLMs(대형 언어 모델)에서 비선형성(Nonlinearity 및 Nonlinearities)의 영향을 분석하기 위해 엔트로피(Entropy)를 사용한다는 점에서 독창적입니다. 특히, 주의 메커니즘(Attention Mechanism)의 분포를 정량화하고, 비선형성을 통한 탐색(Exploration)과 활용(Exploitation)의 균형을 이해하는 데 초점을 둡니다.
- **Technical Details**: 모델은 20백만 개의 Python 파일로 구성된 CodeParrot 데이터셋을 사용하여 학습되었습니다. 50K의 어휘로 토크나이저(Tokenzier)를 사용하며, RTX 3090 GPU 기반으로 학습합니다. 비선형 연산자, 예를 들어, FFN의 GELU, LayerNorm, 그리고 ReLU 등의 역할을 파악하기 위해 학습을 통한 엔트로피 분석을 진행하였습니다. 이 연구는 엔트로피를 통해 모델의 내부 메커니즘을 이해하고, 비선형성의 최적화를 추구합니다.
- **Performance Highlights**: 연구는 모델의 탐색과 활용의 균형이 레이어와 머리를 통해 어떻게 성립되는지를 엔트로피 측정을 통해 밝혔습니다. 이는 향후 LayerNorm-free Transformer 아키텍처의 확장 가능성을 제시하며, ReLU 등의 비선형성 조합을 통해 모델의 효율성을 높일 수 있는 가능성을 보여줍니다. 또한, 활성화 함수의 변화가 다운스트림 작업에 미치는 영향을 분석하는 것이 주요한 성과로 제시되었습니다.

