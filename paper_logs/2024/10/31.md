## Daily Papers (2024-10-31)

### [Task Vectors are Cross-Modal](https://arxiv.org/abs/2410.22330)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22330.png)

Vote: 6

Authors: Amir Bar, Trevor Darrell, Grace Luo

- ***What's New***: Vision-and-Language Models(VLMs)에 대한 새로운 연구가 제시되었으며, 이 연구는 서로 다른 모달리티와 방식으로 정의된 작업이 유사한 Task Vector로 변환될 수 있음을 밝혀냈습니다. 이로 인해 Text에서 Image로 작업을 전환하는 것이 가능해졌습니다.
- ***Technical Details***: VLMs는 문장이나 이미지 입력을 통해 제공된 지정된 작업 예제 또는 지시사항을 기반으로 internal representations을 연구하였습니다. 연구 결과, VLMs의 토큰은 입력, 작업, 응답의 세 가지 단계를 거쳐 변환되며, 이는 다양한 모달리티에도 일관된 방식으로 일어나는 것으로 나타났습니다. 또, 텍스트 모달리티를 통해 얻은 Task Vector를 이미지 모달리티로 전환함으로써 더 나은 Task 표현을 이끌어낼 수 있음을 발견하였습니다.
- ***Performance Highlights***: Cross-modal patching은 동일한 컨텍스트 윈도우에 텍스트 예를 제공하는 것보다 14-33% 더 나은 성능을 보이며, Text-to-Image 전환 설정에서 최고의 성능을 발휘했습니다. 또한, Visual Recognition 단계로 인해 이미지 ICL보다 텍스트 ICL이 작업을 더 명확히 표현할 수 있음을 발견했습니다.

### [Stealing User Prompts from Mixture of Experts](https://arxiv.org/abs/2410.22884)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22884.png)

Vote: 6

Authors: Jamie Hayes, Ilia Shumailov, Nicholas Carlini, Itay Yona

- ***What's New***: 이 연구는 Mixture-of-Experts (MoE) 모델의 구조적 취약점을 활용하여 사용자 프롬프트를 탈취할 수 있는 새로운 공격기법인 MoE Tiebreak Leakage 공격을 소개합니다. 이 연구는 특정 암시적인 종속성을 악용하여 사용자의 민감한 정보를 노출시키는 최초의 사례입니다.
- ***Technical Details***: MoE 모델에서 Expert-Choice-Routing(ECR) 전략을 사용하는 경우, 토큰 드롭 현상과 이의 배치 내 종속성을 통한 크로스 배치 사이드 채널을 활용하는 방법으로 비밀 메시지를 탈취할 수 있습니다. 공격자는 사전 지식 없이 O(VM2) 수의 쿼리를 통해 피해자의 프롬프트를 단계적으로 추출하며, 이는 모델의 구조적 결함을 악용해 프롬프트 유출을 가능케 합니다.
- ***Performance Highlights***: 실험 결과 ECR 전략을 사용하는 Mixtral 모델에서 1000개 비밀 메시지 중 996개, 4838개의 비밀 토큰 중 4833개를 성공적으로 추출하였습니다. 이 공격은 모델의 모자 용량, 차단 시퀀스 등 다양한 요소에 따라 성공률이 영향을 받으며, 전체 성공률은 99.9%에 달합니다.

### [Decoding Reading Goals from Eye Movements](https://arxiv.org/abs/2410.20779)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20779.png)

Vote: 11

Authors: Omer Shubi, Cfir Avraham Hadar, Yevgeni Berzak

- ***What's New***: 이 연구는 독자가 텍스트를 읽을 때의 다양한 목표를 독자의 눈 움직임 패턴으로부터 해독할 수 있는지를 최초로 평가한 것입니다. 정보 탐색(Information Seeking) 및 일반 독해(Ordinary Reading)라는 일상 생활에서 흔히 접할 수 있는 두 가지 독서 목표가 초점으로, 대규모의 시선 추적 데이터(Eye-Tracking Data)와 최신 모델들을 활용하여, 독서 목표를 해독할 수 있는 가능성을 탐색합니다.
- ***Technical Details***: 제안된 해독 작업은 하나의 텍스트 항목에 대해 한 명의 참여자의 눈 움직임을 기반으로 하여 그들이 일반 독해를 했는지, 아니면 특정 정보를 찾기를 했는지를 예측하는 것입니다. 총 10개의 최신 예측 모델이 적용되었으며, 개별 및 모델 앙상블(Ensemble)의 일반화 능력이 평가되었습니다. Ensemble 모델은 개별 모델보다 성능이 뛰어나며, 테스트 시 다양한 텍스트 항목 및 참여자에 대해 높은 확률로 이 작업을 수행할 수 있음을 보여줍니다.
- ***Performance Highlights***: 로버타 아이(Fixations) 모델은 새로운 텍스트 항목(Novel Texts), 신규 참여자(Novel Participants), 및 두 경우 모두에서 가장 높은 정확도를 보였으며, Logistic Ensemble 모델은 모든 사례에서 최고 성능을 기록했습니다. 특히, 새로운 텍스트 및 참여자에 대한 정확도는 특별히 인상적이며, 이러한 차이는 모델이 다양한 시선 데이터 및 작업 측면을 포착할 수 있음을 시사합니다.

### [ReferEverything: Towards Segmenting Everything We Can Speak of in Videos](https://arxiv.org/abs/2410.23287)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.23287.png)

Vote: 12

Authors: Pavel Tokmakov, Anurag Bagchi, Martial Hebert, Yu-Xiong Wang, Zhipeng Bao

- ***What's New***: ReferEverything는 자연어로 설명할 수 있는 다양한 개념의 비디오 세그멘테이션을 위해 REM 프레임워크를 소개합니다. 이 방법은 인터넷 규모 데이터셋에서 학습한 비디오 확산 모델(Video Diffusion Models)의 시각-언어 표현을 활용합니다. 또한, 지정 객체 세그멘테이션 데이터셋(Referral Object Segmentation Datasets)으로 미세 조정하여 드문 객체와 새로운 객체도 정확하게 세그먼트하고 추적할 수 있습니다. 새로운 기준인 Referral Video Process Segmentation(Ref-VPS) 벤치마크를 통해 동적 개념에 대한 일반화 능력을 입증합니다.
- ***Technical Details***: REM 프레임워크는 강력한 시각-언어 표현을 유지하면서, 비디오 확산 모델 학습을 활용하여 원래의 생성 모델 표현을 최대한 보존합니다. 기존의 참조 이미지 및 비디오 세그멘테이션 데이터셋을 사용하여 미세 조정하면서 객체 마스크(Object Masks)를 생성하도록 출력 형식을 조정합니다. 주어진 시각적 문자 표현과 정의된 파이썬 기능 서명을 바탕으로 코드를 작성해야 하며, 사람의 손으로 작성된 테스트 케이스를 포함해 모든 작업을 지원합니다.
- ***Performance Highlights***: REM은 Ref-DAVIS와 같은 도메인 내 데이터셋에서 최첨단 방법과 동등한 성능을 보이고, 도메인 외 데이터에서는 최대 12포인트 더 높은 영역 유사성을 기록했습니다. 이는 인터넷 규모의 사전 학습을 통해 얻은 시각-언어 표현의 강력한 일반화 능력을 활용한 결과입니다. REM은 기존 방법을 최대 32% 상대적 성능 향상과 함께 뛰어난 세그멘테이션을 보여줍니다.

### [Toxicity of the Commons: Curating Open-Source Pre-Training Data](https://arxiv.org/abs/2410.22587)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22587.png)

Vote: 4

Authors: Catherine Arnett, Pierre-Carl Langlais, Eliot Jones, Ivan P. Yamshchikov

- ***What's New***: 이 논문은 공개 도메인 데이터로 학습된 모델의 유해 출력 문제를 해결하기 위해 새로운 데이터 큐레이션 파이프라인을 제안합니다. 이 파이프라인은 독특한 챌린지를 가진 공개 데이터에 대해 최적의 유해성 필터링을 제공합니다.
- ***Technical Details***: 제안된 파이프라인은 다섯 가지 차원(인종/출신 기반, 성별/성별 기반, 종교, 능력 기반 차별, 폭력)으로 분류된 텍스트로 구성된 맞춤형 학습 데이터세트 ToxicCommons를 생성하며, 이를 통해 독자적인 분류기 Celadon을 훈련합니다. Celadon은 대량의 공개 데이터에서 유해한 콘텐츠를 효율적으로 감지할 수 있도록 설계되었습니다.
- ***Performance Highlights***: Celadon은 640K 샘플의 데이터로 훈련되었으며, 총 5개의 차원에서 정확도와 정밀도가 높은 성능을 보였습니다. 특히 폭력 차원의 경우 가중 정확도가 74%로, 모델이 대부분의 경우 올바른 클래스에 가깝게 예측하고 있음을 시사합니다.

### [CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation](https://arxiv.org/abs/2410.23090)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.23090.png)

Vote: 44

Authors: Ji-Rong Wen, Hongjin Qian, Zhicheng Dou, Tetsuya Sakai, Yongkang Wu, Yiruo Cheng, Ziliang Zhao, Guanting Dong, Kelong Mao

- ***What's New***: CORAL은 다중 턴 대화형 검색-증강 생성(Retrieval-Augmented Generation; RAG) 시스템을 평가하기 위해 설계된 대규모 벤치마크로, 실제 다중 턴 대화 상황에서 RAG 시스템의 능력을 평가할 수 있도록 설계되었습니다. 이 벤치마크는 위키백과에서 자동으로 생성된 다양한 정보 탐색 대화를 포함하며 개방형 도메인, 지식 집중도, 자유형 응답, 주제 전환 등의 주요 과제를 다룹니다.
- ***Technical Details***: CORAL 벤치마크는 8,000개의 정보 탐색 대화로 구성되며, 각 대화는 위키백과에서 추출한 제목을 기반으로 생성됩니다. 대화는 다양한 샘플링 전략에 따라 생성되며, 각 대화는 관련된 질문과 그에 대한 상세한 응답으로 이루어져 있습니다. 대화의 자연스러움을 높이기 위해 GPT-4를 사용해 질문을 대화체로 콘텍스트화합니다. 세 가지 기본적인 대화형 RAG 작업을 지원하며, 콘텍스트 기반의 검색, 응답 생성 및 인용 레이블링을 포함합니다.
- ***Performance Highlights***: 대화형 검색에서는 커머셜 클로즈드 소스 LLM보다 파인 튜닝된 오픈 소스 LLM이 더 나은 검색 성능을 보였으며, 입력 길이를 줄여 노이즈를 걸러내는 것이 응답 품질을 유지하면서 인용 레이블링의 정확성을 개선할 수 있음을 보였습니다. 모델 매개변수를 500M에서 7B로 확장한 결과, 생성 측면에서는 성능 향상이 더 이상 나타나지 않았지만, 인용 레이블링에서는 3B에서 7B로의 확장에 따라 더욱 정확해지는 경향을 보였습니다.

### [SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation](https://arxiv.org/abs/2410.23277)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.23277.png)

Vote: 4

Authors: Yuanhao Zhai, Kai-Wei Chang, Lingjie Li, Maxine Wu, Lijuan Wang, Jianfeng Wang, Yingnian Wu, Kevin Lin, Zhengyuan Yang, Beide Liu, Yining Hong, Chung-Ching Lin

- ***What's New***: 새로운 듀얼-속도 학습 시스템 SLOWFAST-VGEN이 도입되어 행동 유도 장면의 일관성을 유지하는 장시간 비디오 생성이 가능합니다. 이 시스템은 인간의 두뇌에서 작용하는 보완 학습 메커니즘을 모방하여 일반적 세계 동역학의 느린 학습과 새로운 경험으로부터의 에피소드 메모리 빠른 저장을 결합합니다. TEMP-LORA를 사용한 임시 학습과 대규모 데이터셋을 통해 다양한 시나리오에 대응할 수 있습니다.
- ***Technical Details***: SLOWFAST-VGEN은 마스크된 조건부 비디오 확산 모델을 사용하여 느린 학습을 수행하며, 시간 LoRA(Temporal LoRA)를 기반으로 에피소드 메모리를 저장하는 빠른 학습 기법을 도입했습니다. 이 과정은 모델의 파라미터에 에피소드 메모리를 저장함으로써 긴 문맥 수용 능력을 높입니다. TEMP-LORA는 빠른 학습 동안 여러 에피소드의 TEMP-LoRA 파라미터를 느린 학습 과정에 통합하는 '느린-빠른 학습 루프' 알고리즘에 사용됩니다.
- ***Performance Highlights***: SLOWFAST-VGEN은 여러 행동 유도 비디오 생성 지표에서 기존의 모델 성능을 능가했습니다. FVD 점수 측면에서 514를 달성하여 다른 모델보다 현저히 뛰어난 성능을 보였으며, 평균적으로 0.37의 장면 전환 수를 기록했습니다. TEMP-LORA 모듈은 생성된 장면의 일관성을 높이고 장시간 비디오의 품질을 향상시켰습니다.

### [A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks](https://arxiv.org/abs/2410.22391)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22391.png)

Vote: 12

Authors: Sepp Hochreiter, Korbinian Pöppel, Johannes Brandstetter, Razvan Pascanu, Maximilian Beck, Günter Klambauer, Thomas Schmied, Thomas Adler, Vihang Patil

- ***What's New***: 이 연구에서는 로봇틱스 작업을 위한 빠른 추론이 가능한 대규모 순환 액션 모델(Large Recurrent Action Model; LRAM)을 제안하였습니다. 이 모델은 xLSTM을 사용하여 시퀀스 길이 추론 능력이 뛰어나며 선형 시간 복잡도를 제공합니다.
- ***Technical Details***: 제안된 LRAM은 xLSTM을 중심으로 한 현대적인 순환 아키텍처로 구성되어 있으며, 6개 도메인의 432개 작업에 대해 실험을 통해 그 성능을 확인하였습니다. 이 모델은 구조적 상태 공간 모델(SSM)인 Mamba와 GPT-2 스타일의 Transformer 모델과 비교되었습니다. xLSTM의 비선형 게이팅과 안정화된 RNN을 활용하여 중요한 입력에 더 강한 집중을 두고 있으며, 두 가지 변종(mLSTM, sLSTM)을 포함하여 언어 모델링에서의 성능을 증진합니다.
- ***Performance Highlights***: 실험 결과, xLSTM 기반의 LRAM은 다양한 모델 크기에서 Transformer에 비해 더 나은 성능과 속도를 보여줍니다. 특히, 206M 파라미터 모델에서 xLSTM은 더 큰 스케일에서의 모델 확장이 용이하며 높은 데이터 효율성을 나타냈습니다. 이를 통해 인-컨텍스트 학습(ICL)과 상용 환경에서의 실시간 응용을 지원할 잠재력을 보였습니다.

### [TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters](https://arxiv.org/abs/2410.23168)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.23168.png)

Vote: 6

Authors: Yue Fan, Federico Tombari, Jan Eric Lenssen, Liwei Wang, Muhammad Ferjad Naeem, Haiyang Wang, Bernt Schiele, Yongqin Xian

- ***What's New***: TokenFormer는 Transformer의 확장 문제를 해결하기 위해 모델 파라미터를 토큰으로 취급하는 natively scalable 아키텍처를 제안했습니다. 이를 통해 모델 구조의 유연성을 높이고, 모델을 처음부터 다시 훈련할 필요 없이 효율적으로 확장할 수 있습니다.
- ***Technical Details***: TokenFormer는 Attention 메커니즘을 활용하여 토큰 간의 상호작용과 토큰-파라미터 상호작용을 통합합니다. 각 모델 파라미터를 학습 가능한 토큰으로 처리하며, 모든 Linear Projection을 Token-Parameter Attention (Pattention) 레이어로 대체합니다. 이는 모델 확장의 효율성을 높이고, 새로운 키-값 파라미터 쌍을 추가함으로써 가장 작은 모델에서 가장 큰 모델로 점진적 확장이 가능합니다.
- ***Performance Highlights***: TokenFormer는 124M에서 1.4B 파라미터 모델까지 점진적으로 확장하면서도 처음부터 훈련된 Transformer와 유사한 성능을 보이며, 훈련 비용을 절반 이상 감소시킵니다. 또한, 긴 텍스트를 처리할 때 TokenFormer는 Transformer에 비해 훨씬 더 낮은 컴퓨팅 비용을 나타내어 긴 컨텍스트 모델링의 효율성을 확보합니다.

### [AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels](https://arxiv.org/abs/2410.20050)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20050.png)

Vote: 6

Authors: Xiao Zhou, Xiangxu Zhang, Zheng Liu, Lei Li

- ***What's New***: AutoMIR는 관련성 레이블 없이 효과적인 제로샷 의료 정보 검색(Medical Information Retrieval)을 위한 새로운 방법을 제안합니다. 이 방법은 Self-Learning Hypothetical Document Embeddings (SL-HyDE)를 도입하여 대형 언어 모델(Large Language Models; LLMs)을 활용해 쿼리에 기반한 가상 문서를 생성하고, 이러한 문서가 실제 관련 문서를 찾도록 안내합니다. 새로운 벤치마크인 중국 의료 정보 검색 벤치마크(Chinese Medical Information Retrieval Benchmark; CMIRB)를 제안하여 다양한 실제 의료 시나리오를 기반으로 한 평가 기준을 제공합니다.
- ***Technical Details***: SL-HyDE는 대형 언어 모델을 생성기로 사용해 쿼리에 응답하는 가상의 문서를 생성하고, 검색 모형이 이 가상 문서를 기반으로 가장 관련성 높은 문서를 검색하게 합니다. 학습 과정에서, SL-HyDE는 라벨 없이도 검색 성능을 향상시키기 위해 자기 학습 메커니즘을 사용합니다. 생성기가 고품질의 가상 문서를 생성하여 검색 모형에 의사-라벨 데이터를 제공합니다. CMIRB는 다섯 가지 과업과 열 개의 데이터셋으로 구성되어 있으며, 이를 통해 다양한 검색 모형의 성능을 평가합니다.
- ***Performance Highlights***: SL-HyDE는 CMIRB 벤치마크에서 기존 방법들을 뛰어넘는 검색 정확도를 보여주며, 다양한 LLMs와 리트리버(Retrievers) 구성에서 강력한 일반화와 확장성을 입증합니다. 예를 들어, SL-HyDE는 NDCG@10 기준으로 HyDE(Qwen2 생성기 + BGE 리트리버) 조합을 평균 4.9% 향상시켰고, BGE를 단일로 사용할 경우보다 7.2% 더 나은 성능을 보여주었습니다.

### [On Memorization of Large Language Models in Logical Reasoning](https://arxiv.org/abs/2410.23123)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.23123.png)

Vote: 4

Authors: Xinyun Chen, Badih Ghazi, Bo Li, Chiyuan Zhang, Yangsibo Huang, Da Yu, Ravi Kumar, Chulin Xie, Bill Yuchen Lin

- ***What's New***: 이 논문은 대형 언어 모델(Large Language Models; LLMs)의 논리적 추론 과정에서 암기 현상을 체계적으로 조사하는 것을 목표로 합니다. 저자들은 Knights and Knaves(K&K) 퍼즐을 활용하여 LLMs가 암기에 의존하는지 여부와 그들이 진정한 추론 능력을 개발할 수 있는지를 평가했습니다.
- ***Technical Details***: 암기 정도를 측정하기 위해, 저자들은 K&K 퍼즐의 문제 수준에서의 국소적 변화(Local Perturbation)를 포함한 잘 정의된 암기 점수(Memorization Score)를 제안했습니다. 새로운 논리적 추론 벤치마크는 다양한 난이도의 퍼즐을 생성할 수 있으며, 수학적 구조 변화를 통해 문제를 자동으로 수정 및 해결할 수 있습니다.
- ***Performance Highlights***: 연구 결과, 퍼즐의 학습 집합에 대해 고도의 암기 성능을 보였으나, 이러한 퍼즐이 약간 변형될 경우 실패함을 발견했습니다. 그럼에도 불구하고, 모델은 암기와 더불어 추론 능력도 향상하여 테스트 세트에서 일관된 성과를 보여주었습니다.

