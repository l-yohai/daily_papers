## Daily Papers (2024-10-02)

### [Law of the Weakest Link: Cross Capabilities of Large Language Models](https://arxiv.org/abs/2409.19951)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.19951.png)

Vote: 25

Authors: Zhengxing Chen, Wenhan Xiong, Liang Tan, Mike Lewis, Dhruv Mahajan, Sharan Narang, Laurens van der Maaten, Ming Zhong, Aston Zhang, Chenguang Zhu, Chloe Bi, Sravya Popuri, Xuewei Wang, Rui Hou, Melanie Kambadur, Jiawei Han, Sergey Edunov

- **What's New**: 이 논문은 GAN(생성적 적대 신경망)을 이용해 이미지의 퀄리티를 향상시키는 새로운 접근 방식을 제안합니다. 연구진은 기존의 방법들보다 더 자연스러운 이미지 변환을 목표로 하고 있으며, 변환 과정에서 손실되는 디테일을 최소화하려고 합니다.
- **Technical Details**: 제안된 방법은 기존의 GAN 기반 모델을 개선하여 이미지 변환 중 발생할 수 있는 왜곡과 손실을 줄입니다. 이를 위해 새로운 손실 함수(loss function)를 도입하여 훈련과정에서 더 많은 디테일을 유지할 수 있도록 합니다. 또한, 여러 단계에서 진행되는 다중 스케일(multiscale) 접근 방식을 사용하여 이미지의 다양한 특성을 포착합니다.
- **Performance Highlights**: 실험 결과, 제안된 모델은 기존의 기술들에 비해 이미지 생성 및 변환 품질에서 뛰어난 성능을 보여줍니다. 특히, 자연스러운 색상 표현과 미세한 텍스처 유지에서 두드러진 개선을 확인할 수 있었습니다.

### [TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices](https://arxiv.org/abs/2410.00531)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.00531.png)

Vote: 20

Authors: Mohsen Guizani, Wenjiao Feng, Zonghang Li, Hongfang Yu

- **What's New**: 최근 대규모 언어 모델(LLM)이 클라우드 기반의 추론 서비스를 통해 배포되고 있습니다. 하지만 이는 사용자의 개인정보가 노출될 위험을 내포하고 있어 네트워크 엣지(edge) 환경으로의 전환이 필요합니다. 본 연구에서는 메모리와 연산 자원이 제한적인 엣지 디바이스에서 효율적인 LLM 서비스를 제공하기 위한 TPI-LLM이라는 프레임워크를 제안합니다.
- **Technical Details**: TPI-LLM은 텐서 병렬성(tensor parallelism)을 활용하여 엣지 디바이스에서의 추론 효율을 극대화합니다. 이는 모델 파라미터를 다수의 노드에 분산시켜 처리하며, 링크 지연을 줄이기 위해 스타 기반 올리듀스(allreduce) 알고리즘을 구현했습니다. 메모리 부족 문제를 해결하기 위해 슬라이딩 윈도우 메모리 스케줄러를 도입하여 메모리 사용을 최적화하였습니다.
- **Performance Highlights**: TPI-LLM은 Llama 3.1-8B/70B, Llama 2-3B/7B/13B/70B 및 Yi-34B 모델을 사용한 실험에서 Transformes, Accelerate, Galaxy 대비 메모리 사용량과 추론 속도를 크게 향상시켰습니다. 특히, Llama 2-70B 추론에서 8개의 저자원 디바이스에 분산하여 메모리 사용량을 3.1GB로 줄이고, 토큰 지연을 약 90% 이상 감소시켰습니다.

### [Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect](https://arxiv.org/abs/2409.17912)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.17912.png)

Vote: 13

Authors: Preslav Nakov, Yousef Khoubrane, Yassine Abbahaddou, Eric Moulines, Xuguang Ren, Guokan Shang, Sofiane Ennadir, Imane Momayiz, Michalis Vazirgiannis, Eric Xing, Amr Mohamed, Hadi Abdine

- **What's New**: 이 연구는 저자들이 모로코 아랍어(다리자)를 위한 대형 언어 모델(LLM)을 소개했다는 점에서 혁신적입니다. 이는 주로 언어적, 문화적 배경을 반영하여 지역방언을 강화하려는 첫 번째 시도입니다.
- **Technical Details**: 각기 다른 다리자 언어 자원을 통합하여 458K 개의 지시 샘플로 구성된 Darija-SFT-Mixture 데이터를 만듭니다. 또한, 다양한 파인 튜닝 방법과 기본 모델을 실험하여 최종 구성을 결정합니다. 평가를 위해 DarijaMMLU, DarijaHellaSwag, 그리고 DarijaBench와 같은 포괄적인 평가 모음을 개발했습니다.
- **Performance Highlights**: Atlas-Chat 모델은 Gemma 2 모델을 바탕으로 지침 튜닝 데이터를 통해 파인 튜닝되었으며, 이는 최첨단 및 아랍어 특화 LLM들보다 Darija에서 더 뛰어난 성능을 발휘합니다. 자동화된 메트릭과 시뮬레이션 승률에 따라 성과가 입증되었습니다.

### [One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos](https://arxiv.org/abs/2409.19603)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.19603.png)

Vote: 12

Authors: Ziteng Gao, Zechen Bai, Tong He, Mike Zheng Shou, Pichao Wang, Zheng Zhang, Lei Liu, Haiyang Mei, Joya Chen

- **What's New**: 비디오에서 언어 기반의 객체 세분화 해결을 위해 VideoLISA라는 새로운 비디오 중심의 MLLM을 도입하였습니다. 이 모델은 'Sparse Dense Sampling' 및 'One-Token-Seg-All' 기법을 활용하여 비디오의 시간적 이해와 일관된 객체 세분화를 달성합니다.

### [Flex3D: Feed-Forward 3D Generation With Flexible Reconstruction Model And Input View Curation](https://arxiv.org/abs/2410.00890)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.00890.png)

Vote: 10

Authors: Philip Torr, Filippos Kokkinos, Junlin Han, Andrea Vedaldi, Jianyuan Wang

- **What's New**: 이 논문에서는 Flex3D라는 새로운 프레임워크를 제안하여 3D 객체의 다양한 뷰를 생성하고, 이를 통해 최종 3D 재구성의 품질을 높여준다. 이 프레임워크는 새로운 multi-view 생성 전략과 유연한 feed-forward 재구성 모델인 FlexRM을 포함한다.
- **Technical Details**: Flex3D는 두 개의 diffusion 모델을 훈련시켜 다양한 방위각(azimuth)과 고도(elevation) 각도에서 새로운 뷰를 생성하도록 한다. 그런 다음, 생성 품질 분류기와 기능 매칭 네트워크를 사용하여 뷰의 일관성을 측정하고 최적의 뷰를 선택한다. FlexRM은 Instant3D 아키텍처에 기반하며, tri-plane 표현과 3D Gaussian Splatting을 결합하는 방식으로 다양한 입력 뷰와 시야각을 처리할 수 있다.
- **Performance Highlights**: 이 방법은 기존 최첨단 feed-forward 모델들과 비교하여 단일 뷰, 4개 뷰, 여러 뷰 환경에서 모두 최고 성능을 발휘하며, 세대(generation) 과제에서도 우수한 결과를 얻었다. 또한 설계 선택의 영향을 평가하기 위한 세부적인 ablation study도 수행되었다.

### [Illustrious: an Open Advanced Illustration Model](https://arxiv.org/abs/2409.19946)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.19946.png)

Vote: 9

Authors: Junha Lee, Min Song, Joy Song, Hyunju Lee, Dongha Kim, Sang Hyun Park, Jun Young Koh, Hoyeon Moon

- **What's New**: Illustrious는 최첨단 애니메이션 생성 모델로, 기존 많은 모델을 다양한 측면에서 능가합니다. 이 모델은 대규모 데이터셋과 세부적인 프롬프트 가이드를 활용하여 이전 모델들이 어려웠던 다양한 개념 조합을 정확하게 표현할 수 있습니다.

### [ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer](https://arxiv.org/abs/2410.00086)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.00086.png)

Vote: 8

Authors: Chaojie Mao, Chenwei Xie, Yulin Pan, Zeyinzi Jiang, Yu Liu, Jingren Zhou, Zhen Han, Jingfeng Zhang

- **What's New**: 이 논문은 최신 컴퓨터 비전(computer vision) 기술을 활용하여 개체 감지(object detection) 성능을 향상시키는 방법을 제안합니다. 특히 고해상도(high-resolution) 이미지에서의 정확도를 높이는 데 중점을 두고 있습니다.
- **Technical Details**: 제안된 방법은 기존의 CNN(convolutional neural networks) 구조를 개선하여 고해상도 데이터를 효과적으로 처리할 수 있도록 설계되었습니다. 여러 스케일에 걸쳐 정보를 통합하는 새로운 네트워크 아키텍처를 도입하며, 이는 ResNet이나 EfficientNet 같은 기존의 백본 네트워크(backbone network)와 쉽게 결합될 수 있습니다.
- **Performance Highlights**: 실험 결과, 제안된 모델은 COCO 데이터셋에서 기존 대비 평균 정밀도(average precision)를 8% 이상 향상시켰음을 보여줍니다. 특히 대형 객체에 대한 인식률이 월등하게 향상되었습니다. 이는 실시간 실용성을 위해 중요한 요소입니다.

### [SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs](https://arxiv.org/abs/2410.00337)

![](/avatars/2638af4626e8a4e3a95f845b94ad94f6.svg)

Vote: 7

Authors: Yingjie Cai, Leheng Li, Bingbing Liu, Ying-Cong Chen, Qing Lian, Weichao Qiu, Xu Yan

- **What's New**: 본 논문에서는 3D 점유 상태(occupancy state) 예측 관련 새로운 도전에 부응하기 위해 3D 다면 이미지(MPIs)를 활용한 혁신적인 이미지를 생성하는 프레임워크 'SytheOcc'를 제안합니다. 이는 정밀한 3D 기하학적 제어를 가능하게 하여 다양한 응용 프로그램을 지원합니다.
- **Technical Details**: 제안된 프레임워크는 3D 다면 이미지(Multi-Plane Images, MPIs)로 점유 상태를 표현하여 더 정밀한 이미지 생성 제어를 제공합니다. 각 평면은 특정 깊이에서의 의미론적 레이블을 나타내며, 이 정보들은 픽셀 수준에서 생성된 이미지와 정렬됩니다. 이를 위해 MPI 인코더를 도입하여 특징을 인코딩하며, 롱테일 데이터 케이스 학습을 용이하게 하는 리웨이팅(reweighting) 방법을 적용합니다.
- **Performance Highlights**: 실험 결과는 이 합성 데이터가 3D 점유 예측에서 모델 인식을 향상시키는 것에 있어 효과적인 데이터 증강 효과를 제공한다는 것을 보여줍니다. 특히, 제안된 방법은 이전 연구들에 비해 이미지 품질과 인식능력에서 상당한 발전을 이룹니다.

### [Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration](https://arxiv.org/abs/2410.00418)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.00418.png)

Vote: 6

Authors: Michael Elad, Guy Ohayon, Tomer Michaeli

- **What's New**: 이번 연구 논문은 Posterior-Mean Rectified Flow (PMRF)라는 새로운 프레임워크를 제안합니다. PMRF는 포토리얼리스틱 이미지 복원(Photo-realistic image restoration) 작업에 있어 왜곡(distortion)과 자연스러움(perceptual quality) 간의 균형을 유지하며 최적의 성능을 도모합니다. 기존의 GAN 기반 방법보다 실험적으로 더 우수한 성능을 보이는 것으로 나타났습니다.
- **Technical Details**: PMRF는 먼저 모델을 사용하여 복원된 출력과 실제 이미지를 비교하며 최소 제곱 오차(MSE)를 최소화합니다. 그 다음, rectified flow 모델을 학습하여 각 후과(mean)와 실제 이미지 간 경로를 예측합니다. 테스트 시, PMRF는 후과 예측을 초기 조건으로 설정하여 ODE를 해결합니다. 이는 왜곡을 최소화하면서도 완벽한 자연스러운 지표(perceptual index) 조건을 만족하는 희망하는 추정치 X^0를 추구하는 것입니다.
- **Performance Highlights**: PMRF는 다양한 얼굴 이미지 복원 작업에서 새로운 최첨단 성능을 기록했습니다. 특히, 어려운 블라인드(face blind) 이미지 복원 작업에서 뛰어난 성능을 발휘했으며, 나머지 작업에서도 기존 프레임워크와 대비해 동등하거나 뛰어난 성능을 나타냈습니다. 이 문서에서는 이러한 성과의 배경과 PMRF의 장점을 자세히 설명하고 있습니다.

### [Visual Context Window Extension: A New Perspective for Long Video Understanding](https://arxiv.org/abs/2409.20018)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.20018.png)

Vote: 5

Authors: Zhenzhong Chen, Hongchen Wei

- **What's New**: 이 논문에서는 LMMs(Large Multimodal Models)에서 시각적 및 언어적 토큰 간의 모듈리티 차이를 활용하여, 새로운 타입의 컨텍스트 윈도우(context window)를 정의하고 있습니다. 이는 LMM들이 비디오 데이터 길이가 길어짐에 따라 성능 저하 문제를 해결하기 위한 것으로, 시각 컨텍스트 윈도우와 언어 컨텍스트 윈도우로 설정해 각각의 최대 길이를 구분하여 다룹니다.
- **Technical Details**: 이 연구에서는 시각적 프레임 임베딩을 위한 점진적 풀링 전략(progressive pooling strategy)을 제안하여 메모리 소비를 줄이는 동시에 장시간 비디오 시퀀스를 처리할 수 있도록 합니다. 또한, 위치 임베딩의 기본 주파수를 컨텍스트 윈도우 비율로 재정의하여 시각적 컨텍스트 윈도우의 유효 범위를 확장합니다. 이로 인해 재훈련 없이도 LMM들이 긴 비디오 시퀀스를 처리할 수 있습니다.
- **Performance Highlights**: 제안된 방법은 긴 비디오 이해 벤치마크에서 일관되게 성능 향상을 보이며, 특히 MLVU 벤치마크에서 GPT-4o보다 우수한 성능을 기록했습니다. 이는 시각 정보의 손실을 최소화하면서도 비디오 프레임 수가 증가함에 따라 성능이 향상되는 점이 주목할 만합니다.

### [Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models](https://arxiv.org/abs/2410.00231)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.00231.png)

Vote: 5

Authors: Qi Wu, Chelsea Finn, Xuxin Cheng, Xiaolong Wang, Zipeng Fu

- **What's New**: 최근 몇 년간, 학습 기반 방법을 통해 발달한 4족 보행 로봇(Quadrupedal robots)이 다양한 지형에서 뛰어난 기동성과 견고함을 보여주었다. 그러나, 이러한 로봇이 일상적인 실내 환경에서 인간을 도울 수 있는 잠재력은 아직 충분히 발휘되지 않았다. 이에 대해 새롭게 제안된 Helpful DoggyBot 시스템은 이러한 한계를 극복하고, 인간의 명령을 이해하며 다양한 실내 환경에서 일반화할 수 있는 유용한 이동식 조작(manipulation) 능력을 갖추고자 한다.
- **Technical Details**: Helpful DoggyBot 시스템은 로봇의 아랫면 앞쪽에 간단하지만 효과적인 1-DoF 집게(gripper)를 장착하여 물체를 잡고 이동할 수 있도록 고안되었다. 강화 학습과 시뮬레이션을 이용하여 일반 목적의 저수준 제어기를 훈련하고, 이는 egocentric depth와 고유 감각(proprioception)을 이용한다. 사전 훈련된 비전-언어 모델(VLMs)과 실시간 피쉬아이(fish-eye) 카메라 스트리밍 데이터를 활용하여 목표 객체를 식별, 위치 추적 및 반응적 탐색 명령을 생성한다.
- **Performance Highlights**: Helpful DoggyBot는 한 번도 보지 못한 침실과 거실에서 물체 수집 태스크를 수행하며 60%의 성공률로 랜덤하게 놓여진 인형을 침대 위에서 가져오는 데 성공했다. 특히, 학습 과정에서 실제 데이터 수집 없이 이러한 일반화를 달성함으로써 다양한 가정 환경에 적응할 수 있는 잠재력을 보여주었다. 로봇의 주된 기여에는 1-DoF 집게 디자인, 시뮬레이션에서 훈련된 저수준 제어기의 현실 파쿠르 같은 이동성, VLMs를 활용한 반응적 명령 생성이 있다.

### [DressRecon: Freeform 4D Human Reconstruction from Monocular Video](https://arxiv.org/abs/2409.20563)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.20563.png)

Vote: 5

Authors: Deva Ramanan, Donglai Xiang, Jeff Tan, Gengshan Yang, Shubham Tulsiani

- **What's New**: 새로운 연구인 DressRecon은 약간 헐렁한 옷이나 소지하고 있는 물체와 상호작용하는 사람들의 아바타를 단일 시점의 비디오로부터 재구성하는 방법을 제안합니다. 전통적으로 이런 유형의 고품질 재구성은 비용이 많이 드는 여러 시점의 캘리브레이션(capturing)을 요구합니다. 그러나 DressRecon은 인공 신경망 기반의 암시적 모델을 학습하여 몸과 옷 변형을 분리된 운동 층으로 구분합니다. 이 과정에서 이미지 기반의 사전 정보(예: 마스크, 노멀 맵, 몸의 자세 등)를 활용하여 최적화합니다.
- **Technical Details**: DressRecon은 인공 신경망을 활용하여 시간 변화가 있는 3D 인간을 수초적 시점(모노클러) 비디오 기반으로 재구성합니다. 이 방법은 비디오별 최적화를 위해 차등 가능한 렌더링(differentiable rendering)을 사용합니다. 주요 구성 요소는 큰 움직임을 가진 사지와 옷 및 물체의 변형을 표현할 수 있는 계층적 운동 모델입니다. 이 과정에서 몸의 자세, 표면 노멀 및 광학 흐름과 같은 이미지 기반의 선험 정보를 활용하여 최적화를 보다 안정적이고 다루기 쉽게 만듭니다. 결과적인 신경 필드는 시간 일관성이 있는 메시(mesh)로 추출되거나 고품질 인터랙티브 렌더링을 위해 명시적 3D 가우시안으로 변환될 수 있습니다.
- **Performance Highlights**: DressRecon은 헐렁한 옷과 물체 변형이 많은 데이터셋에서 기존 기술보다 더 높은 정밀도의 3D 재구성을 제공합니다. 특히 영상에서 다양한 변형을 취급할 수 있는 유연성을 유지하면서도 높은 충실도를 유지하는 계층적 'bag-of-bones' 운동 모델 덕분에 사실적으로 4D 인간을 표현할 수 있습니다.

### [What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study](https://arxiv.org/abs/2410.00545)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.00545.png)

Vote: 2

Authors: Ana Guerberof, Luisa Bentivogli, Beatrice Savoldi, Sara Papi, Matteo Negri

- **What's New**: 본 연구는 기계 번역(Machine Translation, MT)에서 성별 편향(Gender Bias)이 남성과 여성에게 제공되는 서비스 품질의 차이를 야기하는지를 인적 중심적 관점에서 조사하고자 합니다. 기존 연구에서는 주로 자동 평가를 통해 성별 편향을 분석했지만, 본 연구는 인간 참여자를 통한 실험을 통해 더 실제적인 영향을 평가하고자 합니다.
- **Technical Details**: 연구에서는 다양한 데이터셋, 언어, 사용자에 걸쳐 실험을 진행했습니다. 88명의 참가자가 MT 출력을 포스트 에디팅(Post-editing)하여 여성적 또는 남성적 번역을 보장하도록 설정되었습니다. 이 과정에서 편집 시간과 편집 횟수 등의 행동 데이터를 기록하여 성별 간 노력을 비교했습니다.
- **Performance Highlights**: 실험 결과, 여성적 번역이 평균적으로 두 배의 시간이 소요되며, 네 배의 편집 작업이 필요한 것으로 나타났습니다. 자동 평가 방법은 발견된 인적 중심적 노력을 정확히 반영하지 못한다는 것을 보여주었습니다. 이런 차이는 번역 과정에서 다양한 이해 당사자에게 경제적 부담을 불공정하게 줄 수 있습니다. 이를 통해 성별 편향이 실제 사용자에게 미치는 영향을 정량화할 수 있으며, 이는 작업량과 경제적 비용을 기준으로 평가되었습니다. 본 연구는 인간 중심적 평가를 통해 성별 편향의 함의를 이해하는 데 중요한 발걸음을 내디뎠습니다.

### [Embodied-RAG: General non-parametric Embodied Memory for Retrieval and Generation](https://arxiv.org/abs/2409.18313)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.18313.png)

Vote: 1

Authors: Aarav Bajaj, Matthew Johnson-Roberson, So Yeon Min, Quanting Xie, Yonatan Bisk, Tianyi Zhang, Ruslan Salakhutdinov

- **What's New**: 이 논문에서는 로보틱스(Robotics) 분야에서 복잡한 움직임을 조정하기 위해 복합적인 정책 학습을 탐구합니다. 특히, 여러 에이전트(Agents) 간의 조정 문제를 해결하기 위한 새로운 알고리즘을 제안합니다. 이 알고리즘은 비선형(non-linear) 조건에서도 전환 가능한 정책(policy)을 학습하는 데 중점을 둡니다.
- **Technical Details**: 제안된 알고리즘은 강화 학습(Reinforcement Learning)과 다양체 학습(Manifold Learning)을 결합하여 에이전트들이 복잡한 행동을 효율적으로 학습할 수 있게 돕습니다. 이를 위해 다양한 환경에서 에이전트 간 상호작용을 고려한 시뮬레이션을 진행하며, 비선형 상태 전이 가능성을 고려해 설계되었습니다.
- **Performance Highlights**: 실험 결과, 제안된 알고리즘은 기존의 방법들보다 적은 학습 시간 내에 높은 정밀도로 여러 에이전트 간의 협력적인 행동을 학습할 수 있는 능력을 보여주었습니다. 또한 복잡한 로보틱 작업에서도 적절한 해결책을 제시하며, 이동 로봇 및 다중 에이전트 로봇 시스템에서 우수한 성능을 입증하였습니다.

