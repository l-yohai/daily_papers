## Daily Papers (2024-10-21)

### [Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key](https://arxiv.org/abs/2410.10210)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10210.png)

Vote: 2

Authors: ['Jintao Huang', 'Yuze Zhao', 'Yunlin Mao', 'Yingda Chen', 'Daoze Zhang', 'Xingjun Wang']

- ***What's New***: 이 연구는 고품질 데이터(High Quality Data)를 결합한 최소 튜닝(Minimum Tuning) 기법을 통해 LLM에서 긴 출력을 확보하는 방법을 제시합니다. 연구는 인간과의 정렬을 바탕으로 한 모델(Human-aligned model)을 출발점으로 사용하여 소수의 고품질 데이터 샘플로도 모델의 성능을 향상시킬 수 있음을 보여줍니다.
- ***Technical Details***: 연구진은 LongBench-Write 벤치마크를 사용하여 모델의 성능을 평가하고 Output Length Score 𝑆𝐿와 Output Quality Score 𝑆𝑄를 적용했습니다. LongWriter-6K 데이터셋의 666개 고품질 데이터 샘플을 활용하여 필터링하고, 인간과의 정렬 모델에서 시작해 작은 양의 고품질 데이터를 통해 효율적인 튜닝을 수행합니다.
- ***Performance Highlights***: 고품질 데이터를 사용하여 작은 양(약 3.74%)의 훈련 데이터로도 원래의 LongWriter 모델과 비슷한 성능을 달성했습니다. 튜닝된 모델은 요구된 길이의 텍스트 출력을 더 잘 따라가며, 이는 인간 정렬 모델에서 출발해 비용 효율적인 튜닝 접근법을 가능하게 합니다.

### [LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2410.13618)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13618.png)

Vote: 4

Authors: ['Yujia Wu', 'Ran Ran', 'Yiming Shi', 'Jiwei Wei', 'Chengwei Sun', 'Shiyuan He', 'Yang Yang']

- ***What's New***: LoLDU는 Lower-Diag-Upper(LDU) 분해를 활용한 새로운 파라미터 효율적인 미세 조정(Parameter-Efficient Fine-Tuning; PEFT) 방법입니다. 이 방법은 기존의 PEFT 방법들이 필요로 하는 것보다 훨씬 적은, 0.00025%에 불과한 파라미터 수로도 모델 성능을 유지할 수 있는 혁신적인 접근 방식을 제시합니다.
- ***Technical Details***: LoLDU는 LDU 분해를 통해 메트릭스를 초기화하여, 초점이 LDU 분해의 대각 행렬을 최적화하는 데 맞춰져 있습니다. 이 방법은 직교성을 유지해 사전 훈련된 지식을 보존하며 일반화를 강화하고, 각 변환의 확장 요소를 최적화하는 추론기법을 사용합니다.
- ***Performance Highlights***: LoLDU는 다양한 모델 아키텍처와 과제 유형에 대한 종합적인 실험을 통해 그 효과와 다목적성을 입증했습니다. 실험 결과, LoLDU는 RoBERTa, LLaMA2-7B 등 여러 모델과 데이터셋에서 매우 적은 수의 학습 가능한 파라미터로도 기존의 풀 파인튜닝 성능과 비슷한 결과를 보였습니다.

### [MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization](https://arxiv.org/abs/2410.12957)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12957.png)

Vote: 3

Authors: ['Zhou Zhao', 'Ziang Zhang', 'Xize Cheng', 'Ruiqi Li', 'Shengpeng Ji', 'Siqi Zheng']

- ***What's New***: MuVi는 비디오의 시각적 콘텐츠와 음악을 생성하는 혁신적인 프레임워크로서, 시각적 의미 정렬(semantic alignment)과 리듬 동기화(rhythmic synchronization) 문제를 효과적으로 해결합니다. 특히 콘트라스티브 뮤직-비주얼 사전학습(contrastive music-visual pre-training) 스킴을 도입하여 음악 구문의 주기성을 기반으로 시각적 동기화를 보장합니다.
- ***Technical Details***: MuVi는 비디오의 문맥적, 시간적 관련성을 추출하기 위해 시각 어댑터(visual adaptor)를 사용합니다. 이 특징들은 비디오와 잘 조화되는 음악을 생성하기 위해 사용되며, 무작위 음악-비디오 쌍을 사용하여 음 -비 동기화를 강조하고 패널티를 부여하는 콘트라스티브 사전학습을 적용하였습니다. 음악 생성기 자체는 흐름 매칭(flow-matching) 기반으로 설계되어 있으며, 비자연스런 방법(Non-autoregressive)을 사용하여 스타일과 장르를 제어할 수 있습니다.
- ***Performance Highlights***: 실험 결과 MuVi는 오디오 품질과 시간 동기화 면에서 뛰어난 성능을 보였으며, 특히 비트 히트 스코어(Beats Hit Score)와 시맨틱 동기화(SIM)에서 측정된 수치를 통해 음악의 리듬과 비디오의 동작이 잘 맞물려 있음을 확인했습니다. 또한 맞춤형 음악 스타일을 실시간으로 생성할 수 있는 인컨텍스트 학습(in-context learning) 능력도 입증되었습니다.

### [A Comparative Study on Reasoning Patterns of OpenAI's o1 Model](https://arxiv.org/abs/2410.13639)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13639.png)

Vote: 12

Authors: ['Wangchunshu Zhou', 'Tuney Zheng', 'Jialong Wu', 'Siwei Wu', 'Jian Yang', 'Xinrun Du', 'Minghao Liu', 'Zhaoxiang Zhang', 'Qunshu Lin', 'Junbo Zhao', 'Zhongyuan Peng', 'Chenghua Lin', 'Yizhi Li', 'Jiachen Ma', 'Ge Zhang', 'Wenhao Huang', 'J. H. Liu']

- ***What's New***: 이 논문은 OpenAI의 o1 모델이 보여주는 독특한 추론 패턴을 다양한 추론 벤치마크에서 비교하여 분석한 최초의 연구입니다. o1은 기존 추론 능력을 크게 향상시켰으며, 특히 수학 및 코딩 작업에서 월등한 성능을 보였습니다. 또한, o1 모델의 Test-time Compute 방법을 활용한 다양한 접근법을 기존 메소드들과 비교하여 연구하였습니다.
- ***Technical Details***: o1 모델은 Best-of-N, Step-wise BoN, Agent Workflow, Self-Refine과 같은 Test-time Compute 방법들과 비교되었으며, GPT-4o를 백본으로 사용하여 일반적인 추론 벤치마크에 대한 성능을 테스트했습니다. 벤치마크로는 HotpotQA, Collie, USACO, AIME가 사용되었으며, 이들은 LLMs가 잘 다루지 못하는 문제의 난이도를 반영하기 위해 데이터 필터링을 통해 강화되었습니다.
- ***Performance Highlights***: o1 모델은 대부분의 벤치마크에서 최고의 성능을 기록하였으며, 특히 수학 및 코드 작업에서 CoT(Chain of Thought) 기반 접근법을 활용하여 뛰어난 성과를 보였습니다. Agent Workflow 방법은 모든 작업에서 상당한 성능 향상을 보였으나, 특히 Collie와 같은 엄격한 형식 요구사항이 있는 작업에서는 BoN과 Step-wise BoN의 성능이 크게 제한되었습니다. o1의 중요한 추론 패턴으로는 시스템 분석(Systematic Analysis), 메소드 재사용(Method Reuse), 분할 정복(Divide and Conquer) 등이 있으며, 이는 o1의 성능 향상에 중요한 역할을 했습니다.

### [MedMobile: A mobile-sized language model with expert-level clinical capabilities](https://arxiv.org/abs/2410.09019)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09019.png)

Vote: 7

Authors: ['Eric Karl Oermann', 'Daniel Alexander Alber', 'Krithik Vishwanath', 'Jaden Stryker', 'Anton Alaykin']

- ***What's New***: MedMobile은 모바일 디바이스에서도 실행 가능한 3.8억 파라미터 크기의 모바일 크기 언어 모델로, 전문가 수준의 클리니컬 기능을 제공합니다. 이 모델은 MedQA(USMLE)에서 75.7%의 점수를 기록하며, 이는 의사 합격 기준(약 60%)을 초과하고, 자체 크기보다 100배 큰 모델들의 성능에 근접합니다.
- ***Technical Details***: MedMobile은 phi-3-mini라는 오픈 소스 3.8B 파라미터 모델을 의료 분야 데이터로 파인튜닝하여 개발되었습니다. 인간 전문가 및 GPT-4와 교과서에서 생성한 합성 데이터를 사용해 모델의 성능을 높였으며, Chain-of-Thought(CoT), 앙상블(ensembling), 및 지도 학습 파인튜닝(SFT)이 성능 향상에 크게 기여하였습니다. 반면, Retrieval Augmented Generation(RAG)은 큰 개선을 보이지 않았습니다.
- ***Performance Highlights***: MedMobile은 MedQA에서 75.7%의 정확도를 기록하며, 10B 미만의 파라미터 공간 내에서 현존 최고 기록을 가진 UltraMedical 8B(76.1%)에 거의 근접한 성능을 발휘합니다. 총 9개의 평가 과제 중 6개에서 UltraMedical 8B를 능가하거나 동일한 성능을 보여 주며, 5B 미만 파라미터 공간에서 최초로 USMLE 스타일의 문제를 통과한 모델입니다.

### [MoH: Multi-Head Attention as Mixture-of-Head Attention](https://arxiv.org/abs/2410.11842)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11842.png)

Vote: 18

Authors: ['Shuicheng Yan', 'Bo Zhu', 'Peng Jin', 'Li Yuan']

- ***What's New***: MoH는 Transformer 모델의 핵심인 다중 헤드 주의 메커니즘을 개선하여 효율성을 높이는 새로운 접근 방식입니다. MoH는 주의 헤드를 Mixture-of-Experts(MoE) 메커니즘 내의 '전문가'로 취급하여, 각 토큰이 적절한 주의 헤드를 선택함으로써 추론 효율성을 높입니다.
- ***Technical Details***: MoH는 다중 헤드 주의를 MoE 메커니즘과 통합합니다. MoH는 여러 주의 헤드와 각 토큰에 대해 최상위 K개의 헤드를 활성화하는 라우터로 구성됩니다. 표준 다중 헤드 주의의 합산을 가중 합산으로 대체하여 주의 메커니즘의 유연성을 증가시켰습니다.
- ***Performance Highlights***: MoH는 여러 모델 구조의 다양한 실험에서 다중 헤드 주의보다 50%~90%의 헤드만을 사용하여 더 나은 성능을 발휘했습니다. 예를 들어, MoH-ViT-B는 ImageNet-1K 분류 벤치마크에서 84.9%의 Top-1 정확도를 기록했으며, 이는 75%의 주의 헤드만 사용한 것입니다. 이는 MoH가 다중 헤드 주의의 유망한 대안임을 보여줍니다.

### [DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control](https://arxiv.org/abs/2410.13830)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13830.png)

Vote: 20

Authors: ['Haonan Qiu', 'Hangjie Yuan', 'Yingya Zhang', 'Rui Zhao', 'Zhizhong Huang', 'Feng Liu', 'Yutong Feng', 'Yujie Wei', 'Shiwei Zhang', 'Hongming Shan', 'Jiaxin Ye', 'Xiang Wang']

- ***What's New***: DreamVideo-2는 제로샷 기법을 활용하여 테스트 시간에 추가 미세 조정 없이 주제 및 모션 궤적을 사용자 지정하여 비디오를 생성할 수 있는 새로운 비디오 커스터마이제이션 프레임워크입니다. 이는 주제를 학습하기 위해 레퍼런스 어텐션(Reference Attention)을 도입하고, 경계 상자에서 파생된 박스 마스크를 사용하여 정확한 모션 제어를 달성하는 마스크 기반 모션 모듈을 설계했습니다.
- ***Technical Details***: DreamVideo-2는 주제 이미지와 경계 상자 시퀀스를 입력으로 하여 비디오를 생성합니다. 모션 제어를 위해 스파티오템포럴 인코더(Spatiotemporal Encoder)와 스페이셜 컨트롤넷(Spatial ControlNet)으로 구성된 마스크 기반 모션 모듈을 사용합니다. 주제 학습과 모션 제어의 균형을 맞추기 위해 혼합 마스크(Blended Masks)와 재조정된 확산 손실(Reweighted Diffusion Loss)을 도입하여 주제 학습과 모션 제어의 기여도를 차별화합니다.
- ***Performance Highlights***: DreamVideo-2는 텍스트 정렬, 주제 충실도 및 모션 제어 정밀도 측면에서 최신 방법보다 우수합니다. 주어진 시험 데이터 세트에서 mIoU 및 CD 메트릭에서 크게 우수한 성과를 보이며, 사용자 지정 비디오 생성의 실제 응용 가능성을 개선합니다.

### [PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment](https://arxiv.org/abs/2410.13785)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13785.png)

Vote: 17

Authors: ['Wangchunshu Zhou', 'Kang Zhu', 'Shawn Wang', 'Jiaheng Liu', 'Ke Xu', 'Jie Fu', 'Zekun Moore Wang', 'Wenhao Huang']

- ***What's New***: PopAlign은 대형 언어 모델(LLMs)의 정렬(Alignment)을 보다 포괄적으로 수행하기 위해 새로운 대조 패턴(Diversified Contrastive Patterns)을 도입합니다. 이는 모형의 반응을 인간의 선호도에 맞게 조절할 때, 기존의 한정된 대조 패턴에 비해 확장된 대조 패턴을 통해 보다 포괄적인 정렬을 제공합니다.
- ***Technical Details***: PopAlign 프레임워크는 프롬프트(Prompt), 모델(Model), 파이프라인(Pipeline) 수준에서 여섯 가지 대조 전략을 통합하여 별도의 피드백 레이블링 절차 없이 선호도 데이터를 증진시킵니다. 이 여섯 가지 전략은 프리픽스 대조(Prefix Contrast), 데몬 대조(Demon Contrast), 이리시브 대조(Elicitive Contrast), NParam 대조(Number-of-Parameter Contrast), 리더보드 대조(Leaderboard Contrast), 리파인 대조(Refine Contrast)를 포함합니다.
- ***Performance Highlights***: PopAlign은 기존 방법보다 다양한 작업에서 선호도 데이터의 포괄적인 정렬을 달성하며, 특히 이리시브 대조(Elicitive Contrast) 전략을 통한 성능 향상이 눈에 띕니다. 또한 다른 선호도 최적화 알고리즘이나 모델에 따라 서로 다른 효과를 나타내며, 안전성 또는 전반적인 정렬 효과 측면에서 다양한 강점을 보였습니다.

### [Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems](https://arxiv.org/abs/2410.13334)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13334.png)

Vote: 9

Authors: ['Haebin Seong', 'Isack Lee']

- ***What's New***: 이 논문은 대형 언어 모델(Large Language Models; LLMs)의 안전 정렬(safety alignment)이 의도하거나 의도하지 않은 편향을 어떻게 유발하고 이러한 편향이 '탈옥(jailbreak)' 공격을 통해 악용될 수 있는지를 조사합니다. 특히, GPT-4o 모델에서 비이진 키워드와 시스젠더 키워드 간의 20%, 백인과 흑인 키워드 간의 16% 차이를 보이는 탈옥 성공률을 발표합니다.
- ***Technical Details***: PCJailbreak는 LLMs의 안전 기준에 맞춰 의도적으로 주입된 편향이 어떻게 고의적으로 악용되어 해로운 콘텐츠를 생성할 수 있는지를 분석합니다. 이 연구는 다양한 인구 통계 및 사회 경제적 그룹의 키워드를 사용하여 탈옥 성공률을 평가하고 이러한 편향을 완화시키는 방어 기법인 PCDefense를 제안합니다.
- ***Performance Highlights***: GPT-4o와 같은 최신 모델에서, 비특권 그룹에 대한 키워드가 특권 그룹에 비해 상당한 탈옥 성공률을 보이며, 이를 통해 의도적인 편향이 생성되었음을 알 수 있습니다. PCDefense는 추가 추론 단계 없이 방어 프롬프트를 활용하여 성능 개선을 보여주며, LLAMA3 모델은 다른 모델에 비해 더 낮은 탈옥 성공률을 보여 안전성에 초점을 맞추고 있음을 나타냅니다.

### [MobA: A Two-Level Agent System for Efficient Mobile Task Automation](https://arxiv.org/abs/2410.13757)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13757.png)

Vote: 29

Authors: ['Yansi Li', 'Kai Yu', 'Yixiao Wang', 'Zichen Zhu', 'Liangtai Sun', 'Kunyao Lan', 'Yixuan Jiang', 'Lu Chen', 'Hao Tang', 'Hao Zhou', 'Situo Zhang']

- ***What's New***: MobA는 Mobile Task Automation을 위한 이중 수준 에이전트 시스템으로, 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)을 이용하여 사용자 명령의 이해와 계획 기능을 강화합니다. 이 시스템은 고급 글로벌 에이전트(Global Agent; GA)가 사용자 명령을 해석하며, 로컬 에이전트(Local Agent; LA)는 화면 제어를 담당합니다.
- ***Technical Details***: MobA의 구조는 Global Agent와 Local Agent로 나뉩니다. Global Agent는 사용자의 명령을 해석하고, 기억 모듈(Memory Module)에서 관련 경험을 검색하여 계획 모듈(Plan Module)을 통해 할 일을 하위 작업으로 나눕니다. Local Agent는 하위 작업과 기억을 받아 적절한 작업을 선택하고 실행합니다. 이중 반영 메커니즘(Double-reflection mechanism)을 사용해 하위 목표의 완료 여부를 사전에 확인하고, 작업 후에도 성공을 검증합니다.
- ***Performance Highlights***: MobA는 여러 애플리케이션과 다양한 난이도의 50가지 실제 작업에서 테스트된 MobBench에서 Milestone Score Rate에서 66.2%를 기록하며, 기존의 모바일 에이전트 시스템을 앞서는 성과를 보였습니다. 이는 MLLM 기반 모바일 에이전트가 복잡한 작업을 효율적으로 관리할 수 있음을 입증합니다.

### [TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration](https://arxiv.org/abs/2410.12183)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12183.png)

Vote: 2

Authors: ['Kunchang Li', 'Yiwei Guo', 'Yu Qiao', 'Shaobin Zhuang', 'Yali Wang']

- ***What's New***: TransAgent는 이종 에이전트 협업을 통해 비전-언어 기반 모델(Vision-Language Foundation Models)의 전이를 가능하게 하는 새로운 프레임워크입니다. 이 프레임워크는 고유한 방식으로 고립된 에이전트의 지식을 통합하여, 멀티소스 지식 증류(Multi-Source Knowledge Distillation)를 통해 CLIP와 같은 모델의 일반화 능력을 향상시킵니다. 이는 학습 과정 중 추가적인 추론 비용 없이 이질적 에이전트 11개와 유연하게 협업합니다.
- ***Technical Details***: TransAgent는 비전, 언어, 멀티모달 연구에서 이질적 에이전트 11개를 활용하여 CLIP와 같은 모델에 지식을 전수합니다. 비전 에이전트는 MAE, DINO, SAM, ViTDet와 같이 세밀한 이미지 모델링에 중점을 두고 있으며, 언어 에이전트로는 GPT-3, Vicuna가 사용됩니다. 멀티모달 에이전트는 BLIP2, Shikra와 같은 텍스트-이미지, 이미지-텍스트 변환을 수행하여 비전-언어 정렬을 촉진합니다.
- ***Performance Highlights***: TransAgent는 11개의 시각 인식 데이터셋에서 SOTA(State-of-the-Art) 성능을 달성하며, CoOp를 평균 약 10%, 유로SAT에서 20% 이상 능가합니다. 이는 대규모 도메인 변화를 포함하는 데이터셋에서도 효과적으로 일반화 성능을 보입니다.

### [WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines](https://arxiv.org/abs/2410.12705)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12705.png)

Vote: 21

Authors: ['Haryo Akbarianto Wibowo', 'Muhammad Farid Adilazuarda', 'En-Shiun Annie Lee', 'Garry Kuwanto', 'Jan Christian Blaise Cruz', 'Yutong Wang', 'Genta Indra Winata', 'Natasha Santosa', 'Adam Nohejl', 'Yinxuan Gui', 'Maria Angelica Riera Machin', 'Marina Zhukova', 'Ashmari Pramodya', 'Aulia Adila', 'David Ifeoluwa Adelani', 'Stephanie Yulia Salim', 'Patrick Amadeus Irawan', 'Taro Watanabe', 'Jan Wira Gotama Putra', 'Ubaidillah Ariq Prathama', 'Michael Anugraha', 'Holy Lovenia', 'Shi-Xiong Zhang', 'Anirban Das', 'Ching Lam Cheng', 'Derry Tanti Wijaya', 'Daud Abolade', 'Yi Zhou', 'Frederikus Hudi', 'Ayu Purwarianti', 'Afifa Amriani', 'Anar Rzayev', 'Rio Alexander Audino', 'Fariz Ikhwantri', 'Junho Myung', 'Nedjma Ousidhoum', 'Rifki Afina Putri', 'Candy Olivia Mawalim', 'Emmanuele Chersoni', 'Peerat Limkonchotiwat', 'Lucky Susanto', 'Bryan Wilie', 'Alice Oh', 'Raj Dabre', 'Samuel Cahyawijaya', 'Alham Fikri Aji', 'Chong-Wah Ngo', 'Shogo Okada', 'Enrico Santus', 'David Anugraha', 'Hanyang Zhao']

- ***What's New***: WORLDCUISINES는 다국어 및 다문화 시각 기반 언어 이해를 위한 대규모 벤치마크로, 30개의 언어와 방언에 걸쳐 1백만 개 이상의 텍스트-이미지 쌍을 포함하는 시각적 질문 답변(VQA) 데이터셋을 제공합니다. 이 벤치마크는 음식 이름과 그 기원을 예측하는 과제를 포함하며, 이는 현재까지 가장 큰 다문화 VQA 벤치마크입니다.
- ***Technical Details***: WORLDCUISINES는 VQA 데이터셋인 WC-VQA와 세계 요리에 대한 가용 지식 베이스(WC-KB)로 이루어져 있습니다. WC-KB는 전 세계의 2,414가지 요리를 다루고, 6,045장의 이미지와 메타데이터를 포함합니다. VQA 데이터 생성은 WC-KB를 기반으로 하고, 요리 이미지와 관련된 질문 및 맥락을 통해 이루어집니다. 질문은 30개의 언어로 번역되며, 트레이닝 세트와 두 가지 크기의 평가 세트가 준비되어 있습니다.
- ***Performance Highlights***: WC-VQA 데이터셋에 대한 성능 테스트 결과, 저작권 모델 GPT-4o는 여러 설정에서 최고의 성능을 보였으며, 반복적인 맥락이 포함된 경우 성능이 향상되었습니다. 그러나 관객을 혼란스럽게 하는 역방향 맥락에서 모델은 성능 저하를 겪었습니다. 실험 결과에 따르면, VLM은 정확한 맥락에서는 잘 작동하지만, 지역 특산 요리나 저자원 언어에는 어려움을 겪고 있다는 점을 보여주었습니다.

### [Roadmap towards Superhuman Speech Understanding using Large Language Models](https://arxiv.org/abs/2410.13268)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13268.png)

Vote: 32

Authors: ['Haizhou Li', 'Yuhao Zhang', 'Benyou Wang', 'Fan Bu', 'Qun Liu', 'Xidong Wang']

- ***What's New***: 이 논문은 대형 언어 모델(LLMs)를 활용하여 인간의 수준을 넘는 음성 이해(Speech Understanding)를 목표로 하는 로드맵(Roadmap)을 제시합니다. 이는 자동 음성 인식(ASR)부터 음향 지식과 비어문 정보의 통합을 통해 고급 과제를 수행할 수 있는 초인간 모델(Superhuman Models)까지 5단계로 나뉘어 있으며, 이를 평가하기 위한 SAGI 벤치마크도 소개합니다.
- ***Technical Details***: 제안된 로드맵은 음성 LLMs를 통해 비어문적 정보(Non-Semantic Information)와 세계 지식(World Knowledge)을 유지하는 것을 강조합니다. 음성 LLMs는 기본적으로 ASR 기능을 가지고 있으면서 점진적으로 비어문적 특징을 이해하고, 추상 음향 지식(Abstract Acoustic Knowledge)을 활용함으로써 다양한 복잡한 작업을 수행할 수 있도록 발전해야 합니다. 또한, 음성 이해 능력을 평가하기 위한 다양한 작업이 포함된 SAGI 벤치마크가 설계되었습니다.
- ***Performance Highlights***: 현재의 음성 LLMs는 일부 영역에서는 인간 성능을 초과할 수 있지만, 비어문적 정보의 인식과 이해에서 여전히 제한점을 보입니다. 특히, 추상 음향 지식의 결핍은 더 높은 수준의 작업 수행에서의 공통 병목 현상으로 확인되었습니다. GPT-4o는 음성 지침을 따르는 데 이점이 있지만, 여전히 개선의 여지가 많이 남아 있습니다.

### [Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens](https://arxiv.org/abs/2410.13863)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13863.png)

Vote: 27

Authors: ['Tianhong Li', 'Kaiming He', 'Yonglong Tian', 'Chen Sun', 'Lijie Fan', 'Siyang Qin', 'Michael Rubinstein', 'Deqing Sun', 'Yuanzhen Li']

- ***What's New***: Fluid는 새로운 랜덤 순서(autoregressive model) 텍스트-이미지 생성 모델로, 연속 토큰(continuous tokens)을 사용하여 이미지 품질을 크게 향상시킨다는 것을 보여주었습니다. Fluid 10.5B 모델은 MS-COCO 30K에서 새로운 제로샷 FID(Frechet Inception Distance) 6.16을 달성하였으며, GenEval 벤치마크에서 0.69의 전체 점수를 기록하며 최신 성능을 자랑합니다.
- ***Technical Details***: Fluid는 텍스트-이미지 생성에서 모델의 성능을 지속적으로 검토하여 두 가지 주요 요인을 탐구합니다: 모델이 연속 토큰을 사용하는지 여부와 토큰이 랜덤 순서로 생성되는지 여부입니다. Fluid는 연속 토큰을 사용하여 정보 손실을 최소화하고, BERT와 유사한 양방향(attention mechanism)을 통해 랜덤 순서로 토큰을 생성하여 글로벌 구조를 리얼타임으로 조정할 수 있는 이점을 가집니다.
- ***Performance Highlights***: Validation 손실이 모델 크기와 비례하여 로그스케일로 감소하며, Fluid 모델의 성능은 연속 토큰을 사용한 랜덤 순서 모델에서 가장 뛰어남을 보여주었습니다. GenEval에서 Fluid 모델은 3.1B 파라미터까지 보여지는 모든 평가 지표에서 지속적인 개선을 보여주며, 최신 모델들과 비교해 뛰어난 이미지-텍스트 정렬 및 생성 품질을 제공합니다.

### [γ-MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models](https://arxiv.org/abs/2410.13859)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13859.png)

Vote: 5

Authors: ['Jiayi Ji', 'Zhiqiang Shen', 'Yiyi Zhou', 'Xiaoshuai Sun', 'Rongrong Ji', 'Yaxin Luo', 'Gen Luo']

- ***What's New***: γ-MoD는 다중 모드 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 계산 효율성을 높이기 위한 새로운 혼합 깊이(Mixture-of-Depths; MoDs) 적응 전략을 제안합니다. 이 전략은 많은 중복 레이어를 활성화 토큰(Activated Tokens) 관점에서 식별하여 대체하는 모습을 보여줍니다.
- ***Technical Details***: γ-MoD는 주의 지도 계층의 순위(ARank)를 도입하여 토큰의 중복성을 측정하고, 이를 통해 중복되는 레이어를 MoD 레이어로 대체합니다. 공유된 시각-언어 라우터(Shared Vision-Language Router)와 마스크된 경로 학습(Masked Routing Learning)을 통해 MoD의 효과를 극대화합니다. 이러한 설계를 통해 90% 이상의 밀집 레이어를 MoD 레이어로 변환할 수 있습니다.
- ***Performance Highlights***: γ-MoD는 기존의 MLLMs에서 평균 성능 저하가 1.5%에 불과한 상태로 트레이닝 시간은 31%, 추론 시간은 53.2%까지 감소시켰습니다. 이 접근 방식은 다양한 MLLM 구조와 파라미터 규모에 대한 일반화 능력을 보여줍니다.

### [JudgeBench: A Benchmark for Evaluating LLM-based Judges](https://arxiv.org/abs/2410.12784)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12784.png)

Vote: 27

Authors: ['Kyle Montgomery', 'Chenguang Wang', 'Ion Stoica', 'Sijun Tan', 'Siyuan Zhuang', 'Raluca Ada Popa', 'William Y. Tang', 'Alejandro Cuadron']

- ***What's New***: JudgeBench는 LLM 기반 심사위원(LLM-based Judges)을 평가하기 위한 새로운 벤치마크입니다. 이 벤치마크는 지식, 추론, 수학, 코딩과 같은 다양한 어려운 반응 쌍에 대해 LLM 기반 심사위원의 능력을 평가하도록 설계되었습니다.
- ***Technical Details***: JudgeBench는 기존 데이터셋을 변환하여 객관적인 정확성을 반영하는 선호 레이블을 갖춘 어려운 반응 쌍(challenging response pairs)으로 만드는 새로운 파이프라인을 활용합니다. 이 데이터셋은 GPT-4o를 비롯한 강력한 모델을 사용하여 생성된 350개의 반응 쌍으로 구성되며, 각 쌍은 하나의 객관적으로 정확한 응답과 미묘한 오류를 포함한 하나의 잘못된 응답을 포함합니다. GPT-4o를 통해 생성된 반응에 대한 객관적인 평가를 지원합니다.
- ***Performance Highlights***: JudgeBench는 기존 벤치마크보다 훨씬 더 어려운 도전 과제를 제시하며, GPT-4o와 같은 강력한 모델도 무작위 추측보다 약간 우수하게 성능을 보였습니다. Skywork의 Fine-tuned 심사위원은 57.43%의 정확도를 기록한 반면, 최신 OpenAI 모형인 o1-preview는 75.43%의 정확도를 달성했습니다. 이러한 결과는 현재 LLM 기반 심사위원이 더욱 발전된 AI 모델을 평가할 때의 한계를 보여주며, 향후 연구 방향성을 제시합니다.

### [Can MLLMs Understand the Deep Implication Behind Chinese Images?](https://arxiv.org/abs/2410.13854)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13854.png)

Vote: 6

Authors: ['Feiteng Fang', 'Yuelin Bai', 'Shiwen Ni', 'Yifei Zhang', 'Guangzeng Han', 'Xi Feng', 'Yiming Liang', 'Bingli Wang', 'Jinchang Hou', 'Qinrui Li', 'Chenghua Lin', 'Wenhao Huang', 'Min Yang', 'Ge Zhang', 'Xingwei Qu', 'Ziqiang Liu', 'Chenhao Zhang', 'Jiaheng Liu', 'Xinrun Du', 'Qixuan Zhao', 'Kaixin Deng']

- ***What's New***: 이 연구에서는 다중 모드 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 중국 이미지 해석 능력을 평가하기 위해 중국 이미지 암시 이해 벤치마크(CII-Bench)를 소개합니다. CII-Bench는 중국 전통 예술 및 인터넷에서 수집된 이미지를 포함하여 중국 문화에 대해 심도 있는 이해를 요구합니다.
- ***Technical Details***: CII-Bench는 698개의 이미지와 800개의 다중 선택 질문으로 구성되어 있으며, 삶, 예술, 사회, 정치, 환경, 중국 전통 문화 등 6가지 도메인에서 다양한 유형의 이미지를 포함합니다. 이 데이터셋은 30명의 학부생에 의해 수집 및 주석이 달렸으며, GPT-4o 모델을 사용한 평가도 포함되었습니다.
- ***Performance Highlights***: 시험 결과 인간의 평균 정확도는 78.2%에 달했지만, 가장 높은 성능의 MLLM은 64.4%만을 기록했습니다. 특히, 중국 전통 문화 이미지에서 MLLMs의 성능은 부족한 이해를 보여주었습니다. 모델들은 감정 힌트를 포함할 때 더 나은 성능을 보였으나, 여전히 감정 이해에 어려움을 겪고 있습니다.

### [Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment](https://arxiv.org/abs/2410.09347)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09347.png)

Vote: 2

Authors: ['Jun Zhu', 'Huayu Chen', 'Hang Su', 'Peize Sun']

- ***What's New***: 이 연구에서는 새로운 기법인 Condition Contrastive Alignment (CCA)을 소개하여 AutoRegressive (AR) 모델의 시각적 생성 품질을 개선하고, 기존의 Classifier-Free Guidance (CFG)의 필요성을 제거합니다. CCA는 사전 학습된 모델을 미세 조정함으로서 동일한 표본 분포를 맞추고, CFG와 비교해 샘플링 비용을 절반으로 줄입니다.
- ***Technical Details***: Condition Contrastive Alignment (CCA)는 이미지를 위한 양성 조건과 음성 조건을 활용하여 대조 학습으로 조건 간 차이를 학습합니다. 이는 사전 훈련된 모델을 미세 조정하는 단순한 손실 함수를 사용하여 수행되며, 새로운 데이터셋이 필요하지 않습니다. 실험적으로 확인된 결과, CCA는 사전 학습된 데이터셋으로 단 1 에포크의 훈련만으로 CFG와 동등한 성능을 제공합니다.
- ***Performance Highlights***: CCA는 모든 검증된 모델에서 '샘플링 없는 상태(Without guidance)'에서도 높은 성능을 보였습니다. 예를 들어, LlamaGen-L 모델은 FID 점수가 19.07에서 3.41로 개선되었고, IS 점수는 64.3에서 288.2까지 상승했습니다. 이러한 성과는 샘플링 비용이 기존 방식의 절반으로 줄어든 상태에서 일어납니다.

### [A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models](https://arxiv.org/abs/2410.13841)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13841.png)

Vote: 14

Authors: ['Yaojie Lu', 'Xianpei Han', 'Qiaoyu Tang', 'Keming Lu', 'Hongyu Lin', 'Bowen Yu', 'Le Sun', 'Le Yu']

- ***What's New***: 이 논문은 대규모 사전 학습 모델의 후속 학습 단계에서 델타 매개변수(Delta Parameters)를 편집하는 방법에 대한 통합 관점을 제시합니다. 저자들은 리만 합(Riemann Sum) 근사를 기반으로 손실 함수의 변화를 분석하여 다양한 후 수정 작업이 모델 성능에 미치는 영향을 설명합니다.
- ***Technical Details***: 리만 합 근사(Riemann Sum Approximation)를 사용하여 델타 매개변수 편집의 손실 변화를 체계적으로 분석했습니다. DARE와 DELLA-Merging 같은 방법은 무작위 삭제와 리스케일을 통해 손실을 제로로 유지하여 경쟁력 있는 성능을 유지합니다. BitDelta, Twin-Merging 등은 양자화(Quantization) 및 저순위 근사(Low-rank Approximation)로 성능이 감소할 수 있습니다. EXPO는 델타 매개변수를 적절히 확장하여 성능을 향상시킵니다.
- ***Performance Highlights***: 실험 결과, VeriT와 LLaMA 3, Mistral 등의 시각 및 언어 모델에서 이론적 분석이 뒷받침되었습니다. EXPO와 같은 방법은 모델의 조정 데이터에서 손실을 줄여 더 잘 정렬된 모델을 생성합니다. DARE의 경우 무손실의 성능을 유지하면서 델타 매개변수를 99%까지 삭제할 수 있습니다.

### [SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation](https://arxiv.org/abs/2410.13293)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13293.png)

Vote: 3

Authors: ['Prakhar Dixit', 'Tim Oates']

- ***What's New***: SBI-RAG는 수학 문장형 문제(Math Word Problem; MWP)를 보다 효율적으로 해결하기 위해 스키마 기반 지시(Schema-Based Instruction; SBI)와 검색을 통한 생성(Retrieval-Augmented Generation)을 통합한 혁신적인 프레임워크입니다. 이 접근 방식은 큰 언어 모델(Large Language Model; LLM)을 활용하여 단계별 추론을 강조하고, 문제 해결 과정에서 스키마를 통해 해결 방식을 유도함으로써 학생들의 논리적 사고를 향상시킵니다.
- ***Technical Details***: SBI-RAG는 DistilBERT를 이용한 스키마 분류기를 통해 주어진 문제에 대한 적절한 스키마를 예측하며, Ollama Llama 3.1 LLM을 활용해 문맥과 스키마별 프롬프트를 생성합니다. 검색-강화 생성(RAG) 프레임워크를 활용하여 관련 문서를 검색, 문맥 내 정보를 바탕으로 구조화된 답변을 제공합니다. 이 시스템은 GSM8K 데이터셋을 통해 평가되며, 문제 해결의 논리성을 평가하기 위해 '추론 점수(reasoning score)'라는 새로운 지표를 도입합니다.
- ***Performance Highlights***: SBI-RAG 프레임워크는 GPT-4와 GPT-3.5 Turbo와 비교하여 응답의 추론 품질에서 더 높은 점수를 기록하였습니다. SBI-RAG의 추론 점수가 0.588인 반면, GPT-4는 0.491, GPT-3.5 Turbo는 0.290으로 나타났습니다. 통계적 검정 결과, SBI-RAG는 두 모델에 비해 유의미하게 높은 성능을 보였습니다. 이는 교육적 맥락에서 스키마 기반 추론이 LLM 단독으로 생성한 응답보다 더 나은 품질의 추론을 제공할 수 있음을 시사합니다.

### [BenTo: Benchmark Task Reduction with In-Context Transferability](https://arxiv.org/abs/2410.13804)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13804.png)

Vote: 19

Authors: ['Tianyi Zhou', 'Hongyu Zhao', 'Ming Li', 'Lichao Sun']

- ***What's New***: BenTo는 대형 언어 모델(LLMs)의 평가 비용을 줄이기 위해 벤치마크 작업을 효율적으로 축소하는 방법을 제안합니다. 특히, 문맥 학습(in-context learning; ICL)을 사용하여 두 작업 간의 전이 가능성을 추정하는 비용 효율적이고 훈련이 필요 없는 방식인 맥락 내 전이 가능성(In-Context Transferability; ICT)을 도입하여 벤치마크 축소를 가능하게 합니다.
- ***Technical Details***: BenTo는 벤치마크 작업의 대표적인 하위 집합을 선택하기 위해 시설 위치 함수(facility location function)를 최적화하는 방식으로 작업의 전이 가능성과 관련성을 평가합니다. ICT를 사용하여 두 작업 사이의 전이 가능성을 추정하고, 이를 통해 현대 LLM 벤치마크(e.g., MMLU, FLAN)를 5%로 축소하면서도 원래 벤치마크와 비교해 평가 오차를 4% 이하로 유지할 수 있습니다.
- ***Performance Highlights***: BENTO 방법은 57개의 MMLU 작업 중 단 3개의 작업 선택 시에도 평균 97%의 평가 정확도를 달성하며, GPT-4와 같은 기존 방법보다 낮은 비용에서 더 정확한 평가 결과를 제공합니다. 이는 LLM의 성능 평가에 있어 전이 가능성에서 얻은 정보가 어떻게 효율적으로 활용될 수 있는지를 보여줍니다.

### [MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models](https://arxiv.org/abs/2410.13085)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13085.png)

Vote: 19

Authors: ['Kangyu Zhu', 'Peng Xia', 'Huaxiu Yao', 'Linjun Zhang', 'Tianze Wang', 'James Zou', 'Sheng Wang', 'Weijia Shi', 'Haoran Li']

- ***What's New***: MMed-RAG는 의료 대형 시각-언어 모델(Med-LVLMs)의 사실성을 개선하기 위해 설계된 다목적 멀티모달 RAG 시스템입니다. 의료 영상의 다양한 도메인을 보다 효과적으로 처리하기 위해 도메인 인식 정보 검색 메커니즘과 적응형 검색 문맥 선택 방법을 소개하고, 입증된 RAG 기반의 선호 조정 전략을 통합하여 RAG 과정이 충분히 일반적이고 신뢰할 수 있도록 만들며, 검색 문맥을 도입할 때 정렬을 크게 개선합니다.
- ***Technical Details***: MMed-RAG는 의료 영상의 도메인에 따른 최적의 검색기를 선택하는 도메인 인식 검색 메커니즘을 도입합니다. 각 입력 의료 이미지를 적응형으로 분석하여 해당하는 검색 모델을 선택하고, 검색된 문맥의 수를 선택하는 적응형 보정 접근 방식을 포함합니다. 끝으로, 교차 모달 정렬 및 모델과 참사이의 정렬을 개선하기 위해 RAG 기반의 선호 조정 방법을 도입합니다. 이러한 선호 쌍은 모델이 입력된 의료 이미지를 활용하지 않고 응답을 생성하지 않도록 촉구하여 교차 모달 정렬을 개선하고, 검색된 문맥을 이해하도록 장려하여 전반적인 정렬을 개선하는 두 가지 목표를 달성하기 위해 설계되었습니다.
- ***Performance Highlights***: MMed-RAG는 다섯 가지 의료 데이터셋(영상의학, 안과, 병리학)에 걸친 의료 VQA 및 보고서 생성 작업에서 Med-LVLMs의 사실 정확성을 평균 43.8％ 향상시킬 수 있음을 실험적으로 보여줍니다. 이러한 실험 결과는 제안된 구성 요소의 효과를 더욱 입증하고, 미세 조정 문제를 해결하기 위한 이론적 분석을 지원합니다. 또한, MMed-RAG는 Med-LVLMs의 크로스 모달 정렬과 전반적인 사실 정렬을 크게 개선하여 의료 멀티모달 작업에서 확장성과 효과성을 보여줍니다.

### [Harnessing Webpage UIs for Text-Rich Visual Understanding](https://arxiv.org/abs/2410.13824)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13824.png)

Vote: 23

Authors: ['Yifan Song', 'Xiang Yue', 'Graham Neubig', 'Yuxiao Qu', 'Wenhu Chen', 'Wai Lam', 'Tianyue Ou', 'Junpeng Liu', 'Chenyan Xiong']

- ***What's New***: 웹페이지 UI를 활용한 텍스트가 풍부한 시각 이해(Harnessing Webpage UIs for Text-Rich Visual Understanding) 연구에서는 다양한 멀티모달 태스크와 UI 레이아웃을 아우르는 MultiUI 데이터셋을 소개합니다. 이 데이터셋은 1백만 개의 웹사이트에서 추출된 730만 개의 샘플로 구성되어 있으며, 웹 UI 데이터의 광범위한 적용 가능성을 강조합니다.
- ***Technical Details***: MultiUI 데이터셋은 웹페이지의 접근성 트리(Accessibility Tree)를 활용하여 일반 멀티모달 명령어를 생성하고, UI 스크린샷과 결합하여 멀티모달 모델을 학습시킵니다. 데이터 수집 파이프라인은 웹 사이트 스크래핑, 사이트 선별, 태스크 추출 및 명령어 생성의 4단계로 나뉩니다. 다양한 디바이스 타입과 윈도우 크기 변형을 소개하여 모델의 강건성을 높였습니다.
- ***Performance Highlights***: MultiUI로 학습된 모델이 VisualWebBench에서 최대 48% 성능 향상을 보였으며, Mind2Web 데이터셋에서 엘리먼트 정확도를 19.1% 향상시켰습니다. 이는 웹 UI 데이터가 시각적 이해를 향상시키고 비UI 도메인에서도 뛰어난 성능을 발휘할 수 있음을 보여줍니다.

### [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13848.png)

Vote: 24

Authors: ['Xingchao Liu', 'Zhiyu Wu', 'Wen Liu', 'Chong Ruan', 'Ping Luo', 'Xiaokang Chen', 'Yiyang Ma', 'Zhenda Xie', 'Chengyue Wu', 'Zizheng Pan', 'Xingkai Yu']

- ***What's New***: Janus는 멀티모달 이해 및 생성을 위한 통합적인 프레임워크를 제공합니다. 기존의 단일 비주얼 인코더에 의존하던 방법을 개선하여 이해와 생성을 위해 시각적 인코딩을 분리했습니다. 이로 인해 시각적 인코더의 다양한 역할 간의 충돌을 완화하고 프레임워크의 유연성을 높였습니다.
- ***Technical Details***: Janus는 멀티모달 이해와 생성을 위한 독립적인 두 개의 시각적 인코딩 경로를 도입했습니다. 동일한 트랜스포머 아키텍처에 의해 통합됩니다. 멀티모달 이해에서는 SigLIP( 시그니파이어 이미지-텍스트 학습) 인코더를 사용하여 고차원적 의미적 특징을 추출하고, 생성에서는 VQ 토크나이저를 사용하여 이미지를 이산 아이디로 변환합니다. 세 가지 단계로 훈련이 진행되며, 각 단계는 시각과 언어 요소의 개념적 연결을 강화, 통합 프리트레이닝, 및 지도학습 튜닝으로 구성됩니다.
- ***Performance Highlights***: Janus는 다양한 멀티모달 이해 및 생성 벤치마크에서 기존 모델보다 우수한 성능을 발휘했습니다. MMBench, SEED-Bench, POPE와 같은 이해 벤치마크에서 탑 성과를 기록했으며, MSCOCO-30K 및 GenEval에서 FID 8.53과 61%의 정확도를 달성하여 시각 생성 능력에서도 뛰어난 결과를 보여줍니다.

### [Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats](https://arxiv.org/abs/2410.12781)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12781.png)

Vote: 4

Authors: ['Kai Zhang', 'Zexiang Xu', 'Fujun Luan', 'Sai Bi', 'Chen Ziwen', 'Yicong Hong', 'Li Fuxin', 'Hao Tan']

- ***What's New***: Long-LRM은 긴 사진 시퀀스에서 대형 장면을 재구성할 수 있는 일반화 가능한 3D 가우시안 복원 모델(Long-sequence Large Reconstruction Model; Long-LRM)을 제안합니다. 이는 기존의 3D 가우시안 스플래팅(3D Gaussian Splatting; 3D GS)보다 두 배 이상의 효율성을 가지며 32개의 입력 이미지를 1.3초 만에 처리합니다.
- ***Technical Details***: Long-LRM의 아키텍처는 최신 마마바2 블록(Mamba2 blocks)과 클래식 트랜스포머 블록(Transformer blocks)에 기반하여 설계되어 있습니다. 이는 보다 많은 토큰을 처리할 수 있게 해주며, 토큰 병합(token merging)과 가우시안 가지치기(Gaussian pruning) 단계로 효율성과 품질의 균형을 맞춥니다. 이 모델은 960×540 해상도의 32개의 소스 이미지로부터 데이터 손실을 방지하기 위해 패치화된 형식으로 작동하며, 변환을 통해 픽셀 정렬된 가우시안 프리미티브(Gaussian primitives)를 회귀합니다.
- ***Performance Highlights***: Long-LRM은 대규모 장면 데이터셋인 DL3DV-140과 Tanks and Temples에서 성능을 평가하였으며, 3D GS의 최적화 기반 접근법에 비해 두 배 이상의 효율성을 보여주었습니다. 960×540 해상도에서 높은 수준의 렌더링 품질을 유지하면서도 2차 복잡도보다 빠른 예측을 가능케 하여 1.3초 내에 전체 장면을 피드포워드 방식으로 재구성합니다.

### [Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant](https://arxiv.org/abs/2410.13360)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13360.png)

Vote: 7

Authors: ['Changsheng Li', 'Jiaming Han', 'Haoran Hao', 'Yu-Feng Li', 'Xiangyu Yue']

- ***What's New***: 이 논문에서는 사용자의 개인 맞춤형 비주얼 개념을 이해하고 생성할 수 있는 다중 모달 LLM(Multimodal Large Language Model)의 새로운 개인화 프레임워크인 RAP(Retrieval Augmented Personalization)을 소개합니다. 일반 MLLM을 시작점으로 사용자의 데이터베이스를 통해 실시간 개념 편집이 가능하며 무한한 시각적 개념을 확장할 수 있는 개인 맞춤형 비서로 전환합니다.
- ***Technical Details***: RAP 프레임워크는 크게 세 가지 단계로 구성됩니다. (a) 기억하기(Remember): 사용자의 이름, 아바타 등 사용자와 관련된 정보를 저장하기 위한 키-값 데이터베이스를 설계합니다. (b) 검색하기(Retrieve): 대화가 시작되면 멀티모달 검색기를 통해 데이터베이스에서 관련 정보를 검색합니다. (c) 생성하기(Generate): 입력 질의와 검색된 개념의 정보를 MLLM에 삽입하여 개인 맞춤형 지식 증강 응답을 생성합니다. 추가로 대규모 데이터 수집 파이프라인과 개인화 훈련용 데이터를 제작하여 MLLM을 훈련했습니다.
- ***Performance Highlights***: RAP-MLLM은 개인 맞춤형 이미지 캡션 생성, 질문 응답 및 시각적 인식 등의 다양한 작업에서 뛰어난 유연성과 생성 품질을 발휘합니다. 본 모델들은 LLaVA(2023), Phi3-V(2024)와 같은 대규모 데이터셋을 미리 학습함으로써 추가 미세 조정 없이 무한한 시각적 개념으로 일반화할 수 있습니다.

### [MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures](https://arxiv.org/abs/2410.13754)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13754.png)

Vote: 67

Authors: ['Bo Li', 'Yifan Song', 'Kaichen Zhang', 'Michael Shieh', 'David Junhao Zhang', 'Kabir Jain', 'Deepanway Ghosal', 'Fuzhao Xue', 'Xiang Yue', 'Mahir Shah', 'Yang You', 'Jinjie Ni', 'Zian Zheng']

- **What's New**: MixEval-X는 실세계 데이터 혼합을 통해 다양한 입력과 출력 모달리티에 대한 평가를 표준화하고 최적화하기 위해 설계된 최초의 벤치마크입니다. 이 연구는 다중 모달 평가가 일관되지 않은 표준과 편향성을 극복하고, 실제 세계의 과제 분포에 맞추어 효과적으로 일반화할 수 있도록 하는 방법론을 제안합니다.
- **Technical Details**: MixEval-X는 여덟 가지의 입력-출력 모달리티 조합을 포함하며, 각 모달리티는 다중 모달 이해(MMU), 다중 모달 생성(MMG), 에이전트 작업으로 분류됩니다. 웹 사용자 질의 탐지 파이프라인을 통해 다양한 입력-출력 모달리티에 걸쳐 잘 분포된 실제 세계의 질의를 수집하고, 다중 모달 벤치마크 풀을 활용하여 이러한 분포에 맞추어 새로운 벤치마크 분포를 재구축합니다. 또한, 적응-수정 파이프라인을 통해 MMG와 에이전트 작업을 자동으로 생성합니다.
- **Performance Highlights**: MixEval-X의 메타평가 결과, 벤치마크 샘플이 실제 세계 과제 분포와 잘 일치하고, 크라우드 소싱 기반의 실제 세계 평가와 강한 상관관계를 나타내며 효율적임을 보여주었습니다. 특히, Image2Text 평가 결과는 사용자 주도 평가와 최대 0.98의 상관관계를 보여줍니다.

### [Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models](https://arxiv.org/abs/2410.12771)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12771.png)

Vote: 4

Authors: ['Misko Dzamba', 'Muhammed Shuaibi', 'Meng Gao', 'Luis Barroso-Luque', 'Brandon M. Wood', 'Xiang Fu', 'Ammar Rizvi', 'C. Lawrence Zitnick', 'Zachary W. Ulissi']

- ***What's New***: Open Materials 2024 (OMat24)는 Meta의 FAIR팀이 개발한 대규모 무기 재료 데이터셋으로, 1억 1,000만 건 이상의 밀도 함수 이론(DFT; Density Functional Theory) 연산을 포함하고 있습니다. 새로운 EquiformerV2 모델은 최첨단 성능을 보여주며, 이 데이터셋은 AI를 활용한 재료 과학 혁신을 가속화하는 데 중요한 자원이 될 것입니다.
- ***Technical Details***: OMat24 데이터셋은 다양한 무기 벌크 재료를 대상으로 하는 DFT 단일점 계산, 구조 최적화, 분자 동역학 궤적을 포함합니다. EquiformerV2 모델은 OMat24 데이터셋과 MPtraj, Alexandria 데이터셋을 사용하여 사전 훈련했으며, DeNS 프로토콜을 사용해 데이터 증강을 통해 모델의 예측 정확도를 높였습니다.
- ***Performance Highlights***: EquiformerV2 모델은 Matbench Discovery 벤치마크에서 F1 스코어 0.916 및 에너지 오차 20 meV/atom을 기록하며 가장 높은 성능을 보였습니다. 이는 기존의 독점 모델과 비교해도 경쟁력 있는 수준입니다. OMat24와 MPtrj 데이터셋의 사전 학습은 모델 성능을 크게 향상시켰습니다.

### [MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models](https://arxiv.org/abs/2410.13370)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13370.png)

Vote: 2

Authors: ['Guangyong Chen', 'Donghao Zhou', 'Jiaze Wang', 'Jiancheng Huang', 'Xiaowei Hu', 'Jinbin Bai', 'Hao Chen', 'Pheng-Ann Heng']

- ***What's New***: MagicTailor는 텍스트에서 이미지로 변환하는 확산 모델(Text-to-Image Diffusion Models)을 사용하여 개념의 특정 구성 요소를 재구성할 수 있는 새로운 작업, 구성 요소 제어 기반 개인화를 소개합니다. 이는 시맨틱 오염(Semantic Pollution)과 시맨틱 불균형(Semantic Imbalance)을 극복하기 위해 설계되었습니다.
- ***Technical Details***: MagicTailor는 Dynamic Masked Degradation(DM-Deg)과 Dual-Stream Balancing(DS-Bal)이라는 두 가지 핵심 기술을 사용합니다. DM-Deg는 원하지 않는 시각적 시맨틱스를 동적으로 혼란스럽게 만들어 시맨틱 오염을 방지하고, DS-Bal은 목표 개념 및 구성 요소의 시각적 시맨틱스를 균형 있게 학습할 수 있도록 보장합니다.
- ***Performance Highlights***: MagicTailor는 구성 요소 제어 기반 개인화 작업에서 최첨단(SOTA) 성능을 달성하며, 이는 정량적 및 정성적 실험을 통해 입증되었습니다. 사용자 연구에서도 MagicTailor가 텍스트 정렬, 아이덴티티 충실도, 생성 품질에서 높은 점수를 받은 것으로 나타났습니다.

### [VidPanos: Generative Panoramic Videos from Casual Panning Videos](https://arxiv.org/abs/2410.13832)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13832.png)

Vote: 9

Authors: ['Tali Dekel', 'Forrester Cole', 'Erika Lu', 'Shiran Zada', 'Jingwei Ma', 'Aleksander Holynski', 'Roni Paiss', 'Michael Rubinstein', 'Brian Curless']

- ***What's New***: VidPanos는 일상적으로 촬영된 패닝 비디오로부터 파노라마 비디오를 생성하는 혁신적인 시스템입니다. 이 방법은 일반적인 동적 장면의 비디오 파노라마를 만들어 냅니다.
- ***Technical Details***: VidPanos는 입력 비디오를 공간-시간 아웃페인팅(space-time outpainting) 문제로 변환하여 전체 파노라마 비디오를 완성합니다. 이를 위해 일반적으로 공간-시간적 맥락 창에 제한되는 생성 비디오 모델(generative video model)을 조정합니다. 이 과정에서 Lumiere와 Phenaki의 생성 비디오 모델을 사용하여 공간적 집합기법(spatial aggregation techniques)과 거친-to-세밀한(coarse-to-fine) 합성 접근 방식을 채택합니다.
- ***Performance Highlights***: Synthesized panoramas는 다양한 동적 장면에서 정지 장면 요소를 충실히 복원하며, 이동 물체를 상당히 그럴듯한 위치에 렌더링합니다. 예를 들어, 합성 여행 비디오 실험에서 이동하는 사람 또는 스케이트보더가 현실적인 동작을 보여주었으며, 이는 기존의 선형 보간이나 optick flow 기반 방법보다 월등한 성능을 입증합니다.

### [Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion](https://arxiv.org/abs/2410.13674)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13674.png)

Vote: 1

Authors: ['Tianyi Zhou', 'Shweta Bhardwaj', 'Yijun Liang']

- ***What's New***: Diffusion Curriculum(DisCL)는 이미지 교차 확산(image-guided diffusion)을 통해 합성 데이터와 실제 데이터 간의 스펙트럼을 생성하여, 적응형 교육(Adaptive Learning)을 가능하게 하는 새로운 생성형 커리큘럼 학습 접근법입니다. DisCL은 합성 이미지의 이미지 가이던스 수준을 조정하여 학습 단계마다 적절한 난이도와 다양성의 데이터를 제공합니다.
- ***Technical Details***: DisCL은 이미지 가이던스의 강도를 조절하여 합성 데이터와 실제 데이터를 결합한 광범위한 스펙트럼을 생성합니다. 코드베이스에서는 Stable Diffusion 모델이 사용되며, 합성 데이터 생성 시마다 이미지 가이던스의 레벨을 다양하게 조절해 실제 데이터와의 시각적 유사성을 맞춥니다. 데이터 생성 단계에서는 고난이도 샘플을 식별하고, 이러한 샘플을 바탕으로 가장 적합한 합성 데이터 가이던스 수준을 설정하여 모든 학습 단계에서 모델 성능을 최적화합니다.
- ***Performance Highlights***: DisCL은 iWildCam 데이터셋에 대해 ID 및 OOD 매크로 정확도를 각각 2.7%와 2.1% 향상시켰습니다. ImageNet-LT의 경우, 기본 모델의 테일 클래스 정확도를 4.4%에서 23.64%로 개선시켜 전체 클래스 정확도는 4.02% 증가했습니다. 이는 DisCL이 기존까지의 어려운 데이터 학습 문제를 효과적으로 해결함을 보여줍니다.

### [Movie Gen: A Cast of Media Foundation Models](https://arxiv.org/abs/2410.13720)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13720.png)

Vote: 70

Authors: ['Xiaoliang Dai', 'Ali Thabet', 'Mannat Singh', 'Samyak Datta', 'Simone Parmeggiani', 'Haoyu Ma', 'Tara Fowler', 'Zecheng He', 'Siddharth Bhattacharya', 'Xi Yin', 'Jonas Kohler', 'Matthew Yu', 'Yue Zhao', 'Simran Motwani', 'Andros Tjandra', 'Fraylie Nord', 'Edgar Schonfeld', 'Matt Le', 'Zijian He', 'Sara K. Sampson', 'Kunpeng Li', 'Peizhao Zhang', 'Dhruv Choudhary', 'Breena Kerr', 'Bowen Shi', 'Luya Gao', 'Dimitry Vengertsev', 'Yaniv Taigman', 'Licheng Yu', 'Rashel Moritz', 'Carleigh Wood', 'Rohit Girdhar', 'Tao Xu', 'Yuval Kirstain', 'Baishan Guo', 'Arun Mallya', 'Luxin Zhang', 'Shikai Li', 'Tingbo Hou', 'Karthik Sivakumar', 'Ji Hou', 'Samaneh Azadi', 'Elliot Blanchard', 'Vladan Petrovic', 'Ann Lee', 'Ching-Yao Chuang', 'Quentin Duval', 'Yen-Cheng Liu', 'John Hoffman', 'Jeff Liang', 'Dingkang Wang', 'Chih-Yao Ma', 'Guan Pang', 'Animesh Sinha', 'Albert Pumarola', 'Amit Zohar', 'Wei-Ning Hsu', 'Jialiang Wang', 'Kaolin Fire', 'Tianhe Li', 'Yi-Chiao Wu', 'Peter Vajda', 'Sharadh Ramaswamy', 'Boris Araya', 'Mary Williamson', 'David Yan', 'Felix Juefei-Xu', 'Kiran Jagadeesh', 'Cen Peng', 'Markos Georgopoulos', 'Steve Fine', 'Sai Saketh Rambhatla', 'Shelly Sheynin', 'Adam Polyak', 'Roshan Sumbaly', 'Yaqiao Luo', 'Apoorv Vyas', 'Andrew Brown', 'Sam Tsai', 'Ishan Misra', 'Artsiom Sanakoyeu', 'Mitesh Kumar Singh', 'Sanyuan Chen', 'Lawrence Chen', 'Ce Liu', 'Geet Sethi', 'Sean Bell', 'Yuming Du']

- ***What's New***: Movie Gen은 고품질 1080p HD 비디오와 동기화된 오디오를 생성하는 미디어 기초 모델 세트를 선보입니다. 사용자 이미지에 기반한 개인화된 비디오 생성 및 정밀 지침 기반 비디오 편집을 포함한 추가 기능도 제공합니다. 이 모델은 텍스트 기반 비디오 합성, 비디오 개인화, 비디오 편집, 비디오-오디오 생성 및 텍스트-오디오 생성에서 최신 상태를 설정합니다.
- ***Technical Details***: Movie Gen Video 및 Audio 모델은 각각 30B 및 13B 매개변수 변환기로, 73K 비디오 토큰의 최대 컨텍스트 길이로 훈련됩니다. 모델은 아키텍처, 잠재 공간, 훈련 목표 및 레시피, 데이터 큐레이션, 평가 프로토콜, 병렬화 기술, 추론 최적화 등 여러 기술 혁신을 통해 대규모 미디어 생성 모델의 트레이닝에서 사전 훈련 데이터, 모델 크기 및 훈련 계산의 이점을 얻습니다.
- ***Performance Highlights***: Movie Gen은 Runway Gen3, LumaLabs, OpenAI Sora 등 기존 상업 시스템을 능가하며, 비디오 개인화 및 정밀 비디오 편집에서도 새롭게 능력을 제공하여 이전 작업들을 초과합니다. Movie Gen Audio는 PikaLabs 및 ElevenLabs와 같은 상업 시스템들을 사운드 효과 생성, 음악 생성 및 오디오 확장에서 능가합니다.

## Daily Papers (2024-10-21)

### [How Do Training Methods Influence the Utilization of Vision Models?](https://arxiv.org/abs/2410.14470)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14470.png)

Vote: 4

Authors: ['Janis Keuper', 'Margret Keuper', 'Paul Gavrikov', 'Shashank Agnihotri']

- ***What's New***: 이 연구는 시각 모델(Vision Models)의 사용 방식이 교육 방법(Training Methods)에 어떻게 영향을 받는지를 조사하였습니다. 특히, 이미지넷-1k(ImageNet-1k) 분류 모델을 대상으로 아키텍처와 교육 데이터를 고정한 상태에서 다양한 교육 파이프라인을 적용하여 실험적 평가를 수행하였습니다.
- ***Technical Details***: 연구는 각 레이어의 가중치(Weights)를 임의의 값으로 대체한 후 모델의 결정 기능에 얼마나 영향을 미치는지를 평가하는 방법론을 사용하였습니다. 구체적으로, 로지츠(Logits)에 소프트맥스(Softmax)를 적용하여 나온 확률 벡터의 코사인 거리(Cosine Distance)를 측정하였고, 이 거리 변화가 각 레이어의 중요성(크리티컬리티; Criticality)을 나타냅니다.
- ***Performance Highlights***: 연구 결과에 따르면, 교육 방법에 따라 초기 레이어의 중요성이 증가하거나 감소하고, 자가 감독 학습(Self-Supervised Learning) 또는 적대적 학습(Adversarial Training) 등의 다른 방법에 따라 상충되는 효과가 나타났습니다. 특정 교육 방법의 결과로, 모델의 결정 기능이 초기에 집중되는 경향이 있는 것으로 관찰되었습니다.

### [FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model](https://arxiv.org/abs/2410.13925)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13925.png)

Vote: 13

Authors: ['Cai Zhou', 'Wanli Ouyang', 'Di Huang', 'ZiDong Wang', 'Zeyu Lu', 'and Lei Bai']

- ***What's New***: FiTv2는 이미지 해상도가 다양한 경우에도 효과적으로 학습할 수 있는 Flexible Vision Transformer(FiT)의 업그레이드 버전으로, 향상된 학습 전략과 새로운 아키텍처 디자인을 도입하여 Diffusion Model에서 사용 가능합니다. 이미지 크기와 가로세로비에 제한이 없도록 설계되었습니다.
- ***Technical Details***: FiTv2는 쿼리-키 벡터 정규화(Query-Key Vector Normalization), 적응형 계층 정규화 적응 모듈(AdaLN-LoRA), 교정된 흐름 스케줄러(rectified flow scheduler), 로그-노멀 샘플러(Logit-Normal sampler)를 포함한 여러 혁신적인 설계를 통합합니다. 또한, 효과적인 혼합 데이터 전처리 전략을 적용하여 다양한 해상도의 이미지 합성 효율성을 높였습니다.
- ***Performance Highlights***: FiTv2는 이전 모델들에 비해 2배의 수렴 속도를 보입니다. 다양한 해상도의 이미지 생성에서 뛰어난 성능을 보였으며, 320×320, 224×448, 160×480 해상도에서 FID 점수 기준으로 최상의 결과를 달성했습니다. 또한, high-resolution 이미지 생성 및 텍스트-이미지 생성 작업에서도 우수한 성능을 입증했습니다.

### [A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement](https://arxiv.org/abs/2410.13828)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13828.png)

Vote: 3

Authors: ['Huazheng Wang', 'Yifan Zeng', 'Mengdi Wang', 'Liu Leqi', 'Hui Yuan', 'Yue Wu']

- ***What's New***: 이 논문은 인간 피드백을 통한 강화 학습(Reinforcement Learning from Human Feedback; RLHF) 방법에서 마진 기반 정렬(Margin-based Alignment) 방식의 언어 모델(Language Model; LM) 정렬에서 발생하는 일반적인 함정, 즉 'Gradient Entanglement'을 식별합니다. 이 문제는 선호되는(Preferred) 답변과 비선호되는(Dispreferred) 답변의 확률이 동기화되어 증가하거나 감소하는 문제를 초래합니다.
- ***Technical Details***: 마진 기반 목표의 본질적인 효과를 'Gradient Entanglement'으로 정의하고, 선호되는 로그 확률의 기울기(inner product)와 비선호되는 기울기 사이의 내적이 클 때 이 효과가 문제를 일으킨다는 조건을 도출했습니다. 다양한 선호 최적화 알고리즘의 학습 동역학을 설명하고 개선 가능한 방법을 제안합니다.
- ***Performance Highlights***: 이 논문은 마진 기반 목표가 LLM, 특히 DPO(Direct Preference Optimization)를 사용하는 경우 특정 상황에서 선호와 비선호 확률을 개별적으로 변경하는 데 어려움이 있음을 실험적으로 입증합니다. 선호 데이터를 동일한 길이로 정규화했을 때 성능이 개선되었음을 확인했습니다.

### [SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments](https://arxiv.org/abs/2410.11331)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11331.png)

Vote: 2

Authors: ['Kruthika KR', 'Rakshit Aralimatti', 'Syed Abdul Gaffar Shakhadri']

- ***What's New***: SHAKTI는 25억 개의 파라미터를 가진 소형 언어 모델(Small Language Model)로, 엣지 AI(Edge AI) 및 낮은 자원 환경에 최적화되어 있습니다. 이는 실시간 AI 애플리케이션을 위해 높은 성능의 NLP와 효율적이고 정밀한 최적화를 결합하여 성능과 효율성을 모두 제공하는 솔루션입니다.
- ***Technical Details***: SHAKTI는 Variable Grouped Query Attention(VGQA), Pre-Normalization, SwiGLU 활성화 함수, 그리고 Rotary Positional Embeddings(RoPE)을 통해 엣지 장치에서 효율적으로 작동합니다. VGQA는 주어진 키에 대해 여러 쿼리를 그룹화되어 메모리 사용을 줄이고 추론 시간을 가속화합니다. 또한, RoPE는 긴 텍스트 시퀀스를 적은 메모리 사용으로 처리할 수 있게 하여 긴 문서 요약이나 복잡한 쿼리에 적합합니다.
- ***Performance Highlights***: SHAKTI는 2.5억 파라미터를 가진 모델이면서도 MMLU에서 71.7%의 점수를 기록하여 더 큰 모델인 Phi-3 Mini-4k와 Gemma 7B보다 우수한 성능을 보여주었습니다. PIQA에서는 86.2%의 성과를 나타내며 Shakti-LLM은 자원 제약 환경에서 효율적인 성능을 제공합니다.

### [Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation](https://arxiv.org/abs/2410.13232)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13232.png)

Vote: 31

Authors: ['Kai Tzu-iunn Ong', 'Dongha Lee', 'Jihoon Kim', 'Namyoung Kim', 'Sunghwan Kim', 'Gwanwoo Song', 'Hyungjoo Chae', 'Jinyoung Yeo', 'Minju Gwak']

- **What's New**: 이 연구는 LLM 기반의 웹 에이전트에 세계 모델 (World Model)을 처음으로 통합하여 현재 LLM의 환경 동적 이해 능력 부족 문제를 해결합니다. 시뮬레이션을 통해 에이전트의 행동 결과를 예측함으로써 결정 과정을 향상시키며, 트리 탐색 기반 에이전트보다 비용 및 시간 효율성을 강조합니다.
- **Technical Details**: 본 연구에서는 전환 중심 관찰 추상화 (Transition-focused Observation Abstraction)를 제안하여 세계 모델이 시뮬레이션할 다음 관찰을 자유형 자연어로 예측하도록 훈련합니다. 각 행동 후보의 결과를 예측하고 가치 함수 (Value Function)를 통해 보상을 추정하여 최적의 행동을 선택합니다.
- **Performance Highlights**: WebArena와 Mind2Web 벤치마크 실험에서 WMA 웹 에이전트는 정책 모델 훈련 없이도 성능을 개선하고, 최근 트리 탐색 에이전트에 비해 6.8배 높은 비용 효율성과 5.3배 빠른 시간 효율성을 입증했습니다. Mind2Web에서 이전 SOTA 성과를 능가했습니다.

### [Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media](https://arxiv.org/abs/2410.12791)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12791.png)

Vote: 3

Authors: ['Rebecca M. M. Hicke', 'Ross Deans Kristensen-McLachlan', 'Mette Thunø', 'Márton Kardos']

- ***What's New***: 이 논문은 중국 디아스포라 미디어의 정보 역학을 효율적으로 분석하기 위해 설계된 새로운 방법론인 KeyNMF를 소개합니다. 특히, 2024년 유럽 의회 선거 시기가 다가오는 기간 동안 이 미디어의 주제적 변화를 모델링하여 중국 정부가 디아스포라를 대상으로 하는 정보 제어를 어떻게 수행하는지를 조사합니다.
- ***Technical Details***: KeyNMF는 Transformer 기반의 문맥적 임베딩 모델을 활용하여 고정 및 동적 주제 모델링을 구현하는 새로운 접근 방식입니다. 이 방법론은 NMF(Non-negative Matrix Factorization)의 신뢰성, 안정성, 확장성 및 해석가능성을 바탕으로 문서에서 키워드 중요도를 계산하고 이를 비음수 행렬 분해로 분해하여 주제를 추출합니다.
- ***Performance Highlights***: KeyNMF는 여러 중국어 데이터셋과 메트릭에서 높은 성능을 보였습니다. 특히 외부 일관성 측면에서 우수한 결과를 기록했으며, 기존의 고전적인 주제 모델에 비해 성능이 크게 개선되었습니다.

### [Teaching Models to Balance Resisting and Accepting Persuasion](https://arxiv.org/abs/2410.14596)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14596.png)

Vote: 1

Authors: ['Mohit Bansal', 'Elias Stengel-Eskin', 'Peter Hase']

- ***What's New***: 이 연구에서는 LLMs(대형 언어 모델)가 설득에 취약하며, 이것이 모델이 적대적인 대화 상대와 마주할 때 위험을 초래할 수 있음을 지적합니다. 새로운 훈련 방법인 PBT(Persuasion-Balanced Training)를 소개하여 부정적 설득에 저항하면서도 긍정적 설득을 수용할 수 있도록 훈련 방법을 제안하였습니다.
- ***Technical Details***: PBT는 다중 에이전트 재귀적 대화 트리를 활용하여 데이터 생성하고, 선호 최적화를 통해 설득을 적절히 수용하도록 모델을 훈련시킵니다. 이 과정에서 QA(Question-Answering) 설정을 통해 두 LLM이 논쟁하며 대화 트리를 생성합니다. 각 턴의 다양한 답변을 비교해 긍정적 및 부정적 설득 데이터를 도출합니다.
- ***Performance Highlights***: PBT로 훈련된 모델은 Misinformation에 대한 저항성이 향상되고, 'Are you sure?' 프롬프트에도 높은 정확성을 유지하여 Flipflopping을 방지합니다. 이는 Mistral-7B, Llama-3.1-8B, Llama-3.1-70B 모델을 대상으로 평균 정확도 63.88%를 기록하며, 원래 모델의 48.87%보다 성능이 월등함을 보여줍니다. 팀 환경에서도 설득할 수 있는 모델은 협업 속에서 성능을 전반적으로 향상시킬 수 있음을 확인했습니다.

### [Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning](https://arxiv.org/abs/2410.14208)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14208.png)

Vote: 1

Authors: ['Chenyan Xiong', 'Xiaochuan Li', 'Zichun Yu']

- ***What's New***: Montessori-Instruct는 학생(학생)의 학습 선호도에 맞춘 고유한 데이터 합성을 수행하는 혁신적인 데이터 합성 프레임워크입니다. 이 접근 방식은 Synthetic Data의 효과를 향상시키고, 학생의 학습 향상에 기여하는 더욱 영향력 있는 합성 데이터를 생성합니다.
- ***Technical Details***: Montessori-Instruct는 영향 함수(influence functions)를 활용해 학생 모델의 데이터 선호도를 정확히 측정하고 이를 기반으로 교사 모델(teacher model)을 최적화합니다. Direct Preference Optimization (DPO)을 통해 학생의 학습에 직접적으로 연결된 데이터를 생성하도록 교사를 최적화합니다. 이 과정은 영향력 있는 데이터를 식별하고 교사 모델을 점진적으로 최적화하여 반복됩니다.
- ***Performance Highlights***: Montessori-Instruct는 Alpaca Eval과 MT-Bench에서 각각 18.35%와 46.24%의 성능 향상을 달성하며, 기존 합성 방법보다 우수한 성능을 보입니다. 다양한 NLP 작업에서도 일반화 능력이 입증되었습니다. 이는 학생의 데이터 선호도를 반영한 교사의 최적화가 학습 성능을 향상시킴을 나타냅니다.

### [Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities](https://arxiv.org/abs/2410.11190)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11190.png)

Vote: 13

Authors: ['Changqiao Wu', 'Zhifei Xie']

- ***What's New***: Mini-Omni2는 GPT-4o와 유사한 기능성을 가진 최초의 오픈 소스 다중 모드 언어 모델(multi-modal language model)로, 시각, 청각, 텍스트 및 청각 중단 메커니즘을 제공합니다. 이 모델은 실시간 스트리밍 음성 출력을 지원하며, 명령 기반의 사용자와의 상호작용을 통해 더욱 유연한 사용자 상호작용을 가능케 합니다.
- ***Technical Details***: Mini-Omni2는 CLIP의 시각 컴포넌트와 Whisper의 음성 인코더를 활용하여 시각 및 청각 모달리티에 대한 고효율 데이터 사용을 최적화합니다. 모델은 세 단계의 훈련 과정을 통해 모달리티 확장을 위한 효율적인 훈련 방식을 채택하며, 명령 기반의 중단 메커니즘을 통해 스트리밍 토큰을 사용하여 모델이 자신의 오디오 출력 스트림을 외부 의미에 따라 제어할 수 있도록 하였습니다.
- ***Performance Highlights***: Mini-Omni2의 음성 인식 정확도는 librispeech-other 데이터셋에서 Whisper 모델을 능가하며, 이는 모델의 견고한 성능을 보여줍니다. 시각 모달리티 추가 후 Mini-Omni 대비 약간의 정확도 저하는 발생하지만, 이는 데이터 비중 감소에 기인할 수 있습니다.

### [SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs](https://arxiv.org/abs/2410.13276)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13276.png)

Vote: 15

Authors: ['Zhichen Zeng', 'Ting Cao', 'Mao Yang', 'Dayou Du', 'Yizhao Gao', 'Shijie Cao', 'Hayden Kwok-Hay So', 'Fan Yang']

- ***What's New***: SeerAttention은 대형 언어 모델(LLMs)에서 내재된 스파스 주의(attention sparsity)를 학습하여 효율성을 개선하는 새로운 주의 메커니즘입니다. 기존의 스파스 주의가 미리 정의된 패턴이나 휴리스틱에 의존했던 것과 달리, SeerAttention은 학습 가능한 게이트를 도입하여 동적으로 중요한 블록을 선택할 수 있습니다. 이는 정확도와 속도 향상을 효과적으로 균형 잡습니다.
- ***Technical Details***: SeerAttention은 블록-스파스 주의를 사용하여 계산과 메모리 오버헤드를 줄입니다. 이를 위해 맞춤형 FlashAttention 구현을 개발하여 블록 레벨의 주의 맵을 최소 오버헤드로 추출합니다. 학습 가능한 'Attention Gate'는 Q와 K 입력 매트릭스를 풀링하여 중요한 블록을 식별하며, 이러한 블록 정보는 이후 주의 계산에 사용됩니다. 훈련 과정에서 AttnGate는 표준 주의에서 생성된 주의 맵을 자신만의 방식으로 학습합니다.
- ***Performance Highlights***: SeerAttention은 post-training 단계에서 최신 방법을 능가하며, 긴 문맥의 미세 조정에서도 뛰어난 성능을 보입니다. 특히 YaRN과 결합하여 긴 문맥의 모델에서 스파시티 비율 90%를 달성하면서도 최소한의 손실을 보입니다. FlashAttention-2에 비해 최대 5.67 배의 속도 향상을 이루었습니다.

### [Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts](https://arxiv.org/abs/2410.14677)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14677.png)

Vote: 7

Authors: ['Anastasia Voznyuk', 'Yury Chekhovich', 'Andrey Grabovoy', 'German Gritsai']

- ***What's New***: 이 연구는 기계 생성 텍스트(AI-generated Texts)의 탐지 품질을 평가하기 위해 사용되는 데이터셋의 질에 대해 체계적인 리뷰를 제공하고 있습니다. 특히 데이터셋이 낮은 평가 품질을 제공하는지와 같은 문제를 해결하고 AI 탐지기를 더 잘 지원하기 위한 고품질 데이터셋 개발 방법을 제안하고 있습니다.
- ***Technical Details***: 연구진은 AI 탐지기(AI Detectors)의 신뢰성을 개선하기 위해 여러 데이터셋을 분석하고 경쟁 대회 및 연구 논문에서 사용된 데이터셋을 체계화했습니다. 탐지의 질을 평가하는 방법으로 PHD(Topological Time Series), KL-Divergence, Attention Maps 등을 사용하여 데이터셋의 전반적인 품질을 측정했습니다. 또한, 문장 순서를 임의로 바꾸거나 단어를 대체하는 등 텍스트의 소규모 수정이 AI 모델에 미치는 영향을 평가하여 데이터셋의 로버스트성을 검토했습니다.
- ***Performance Highlights***: 복잡한 데이터셋에서 모델들의 탐지 성능은 데이터셋의 질에 크게 영향을 받는 것으로 나타났습니다. RuATD와 같은 일부 데이터셋에서는 AI 탐지기의 성능이 0.82의 정확도에 그쳤으며, 반면 다른 데이터셋에서는 탐지기가 거의 완벽에 가까운 성능을 보였습니다. 이는 데이터셋의 품질이 탐지기의 성능에 큰 영향을 미친다는 점을 시사하며, 보다 정교한 탐지기에 대한 필요성을 강조합니다.

### [BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities](https://arxiv.org/abs/2410.14672)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14672.png)

Vote: 2

Authors: ['Shaozhe Hao', 'Kwan-Yee K. Wong', 'Xuantong Liu', 'Bojia Zi', 'Shihao Zhao', 'Xianbiao Qi', 'Rong Xiao', 'Kai Han']

- ***What's New***: BiGR는 바이너리 잠재 코드를 활용한 새로운 조건부 이미지 생성 모델로, 생성과 표현 능력을 동시에 향상시킵니다. 이 모델은 생성과 분별 작업을 동일한 프레임워크 내에서 통합하는 최초의 모델입니다.
- ***Technical Details***: BiGR는 바이너리 토크나이저, 마스킹 모델링 메커니즘, 그리고 바이너리 트랜스코더를 사용하여 바이너리 코드 예측을 수행합니다. 새로운 샘플링 방법인 엔트로피 정렬 샘플링을 도입해 효율적인 이미지 생성을 가능합니다. 전체 프레임워크는 언어 모델 아키텍처로 구성되며, 이미지 토큰의 바이너리 코드를 생성하고 예측하는 과정을 통해 작동합니다.
- ***Performance Highlights***: BiGR는 FID-50k 및 리니어-프로브 정확도에서 기존의 모델에 비해 더 우수한 성능을 보이며, 다양한 시각 작업에서 제로-샷 범용성을 보여줍니다. 특히, 고해상도 이미지 생성, 인페인팅, 아웃페인팅, 편집, 보간 및 추가 기능에서 뛰어난 성능을 기록했습니다. 실험 결과는 BiGR의 생성 및 분별 작업에서의 뛰어난 성능을 보여주며, 이 모델의 확장 가능성을 제시합니다.

### [DPLM-2: A Multimodal Diffusion Protein Language Model](https://arxiv.org/abs/2410.13782)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13782.png)

Vote: 6

Authors: ['Zaixiang Zheng', 'Shujian Huang', 'Fei Ye', 'Quanquan Gu', 'Dongyu Xue', 'Xinyou Wang']

- ***What's New***: DPLM-2는 단백질 구조와 서열의 동시 생성을 목표로 하는 멀티모달 단백질 기초 모델로, 기존의 단일 모달 모델의 한계를 넘어 구조와 서열을 함께 이해하고 생성할 수 있습니다. DPLM-2는 기존의 DPLM을 확장하여 실험적 구조와 고품질 합성 구조를 학습하여 구조 및 서열의 결합 분포를 학습합니다.
- ***Technical Details***: DPLM-2는 루프프리 양자화(lookup-free quantization) 기반의 토크나이저를 사용하여 3D 좌표를 이산 토큰으로 변환하여 구조 학습을 가능하게 합니다. 효율적인 웜업 전략을 통해 대규모 진화 데이터와 사전 학습된 서열 기반 모델의 구조적 귀납 편향을 활용합니다. 서열과 구조를 공동으로 처리하여 고도의 상관성을 가진 단백질 구조와 서열을 동시에 생성합니다.
- ***Performance Highlights***: DPLM-2는 다양한 조건부 생성 작업에서 경쟁력 있는 성능을 보여줍니다. 무조건적인 단백질 생성에서 높은 디자인 가능성과 접히는 능력을 가지며, 특정 문제들에서는 기타 강력한 모델과 견줄 수 있는 성능을 나타내며, 대장형 및 모티프 유지 생성 과제에서도 뛰어난 성공률을 보입니다.

### [UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models](https://arxiv.org/abs/2410.14059)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14059.png)

Vote: 40

Authors: ['Honghai Yu', 'Yuzhe Yang', 'Yilin Guo', 'Jimin Huang', 'Benyou Wang', 'Qianqian Xie', 'Mingcong Lei', 'Yueru He', 'Yifei Zhang', 'Haining Wang', 'Xiao Zhang', 'Yan Hu', 'Ruoli Gan']

- ***What's New***: UCFE 벤치마크(UCFE Benchmark)는 대형 언어 모델(Large Language Models; LLMs)의 금융 분야에서 복잡한 실제 작업처리 능력을 평가하기 위한 새로운 프레임워크입니다. 이 벤치마크는 인간 전문가의 평가와 동적이고 작업에 따른 상호작용을 결합하여 금융 시나리오의 복잡성을 시뮬레이션합니다.
- ***Technical Details***: UCFE 벤치마크는 사용자 중심의 설계(User-Centric Design)를 채택하여 분석가, 금융 전문가, 규제 전문가, 일반 대중 등 4가지 사용자 그룹대표 역할을 시뮬레이션하도록 LLMs를 평가합니다. 이 벤치마크는 17가지 사용자 프로필에 맞춘 작업 유형을 개발하여, 제로샷(Zero-shot) 및 퓨샷(Few-shot) 설정에서 다중 회환 대화를 포함하는 330개의 데이터 포인트를 제공합니다.
- ***Performance Highlights***: UCFE 벤치마크에서 LLMs의 성능은 사용자 선호도와 높은 상관성을 보였으며, 피어슨 상관 계수 0.78을 기록했습니다. 특히, 금융 텍스트에 대해 훈련된 LLMs는 전문적인 금융 개념 이해 및 사용자 의도 해석에서 유의한 향상을 보였습니다. 중형 모델(7B~14B 파라미터)은 컴퓨팅 효율성과 도메인 전문성 간의 균형을 유지하면서 특히 우수한 성과를 보였습니다.

### [Looking Inward: Language Models Can Learn About Themselves by Introspection](https://arxiv.org/abs/2410.13787)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13787.png)

Vote: 3

Authors: ['Tomek Korbak', 'Miles Turpin', 'John Hughes', 'Henry Sleight', 'Owain Evans', 'Ethan Perez', 'Felix J Binder', 'James Chua', 'Robert Long']

- ***What's New***: 이 논문에서는 '내성(Interospection)'을 통해 대형 언어 모델(LLMs)이 자신에 대한 정보를 획득할 수 있는 가능성을 조사합니다. 이는 모델이 훈련 데이터로부터 직접 파생되지 않은 내부 상태에서 기인한 지식을 습득할 수 있음을 의미하며, 이는 모델 해석 가능성을 향상시킬 수 있습니다.
- ***Technical Details***: 연구진은 LLM을 미세 조정하여 가설 시나리오에서 자신의 행동의 속성을 예측하도록 학습시켰습니다. 예를 들어 '주어진 입력 P에 대해 당신의 출력은 단기 혹은 장기 옵션에 더 유리합니까?'와 같은 질문을 통해 내성을 시험하였습니다. 실험에서는 GPT-4, GPT-4o, Llama-3 모델이 사용되었으며, 각 모델이 자체 행동을 예측하는 성능을 비교하였습니다.
- ***Performance Highlights***: 실험 결과, '자기예측(self-prediction)'이 '교차예측(cross-prediction)'보다 우수하였으며, 이는 모델이 내성을 통해 자기 행동의 경향에 대한 특권적 접근을 하고 있음을 시사합니다. 예를 들어, Llama-3-70B 모델이 GPT-4o 모델보다 17% 높은 정확성을 보였습니다.

### [NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](https://arxiv.org/abs/2410.14669)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14669.png)

Vote: 23

Authors: ['Ranjay Krishna', 'Wenxuan Peng', 'Jean de Dieu Nyandwi', 'Zixian Ma', 'Deva Ramanan', 'Zhiqiu Lin', 'Graham Neubig', 'Daniel Jiang', 'Baiqi Li', 'Simran Khanuja']

- ***What's New***: NaturalBench는 시각-언어 모델(Vision-Language Models, VLMs)을 평가하는 새로운 척도로, 인간이 쉽게 해결할 수 있는 자연적인 이미지와 질문에 VLM이 어려움을 겪는지 확인하기 위한 자료를 제공합니다. 이 벤치마크는 두 개의 이미지와 질문을 쌍으로 구성하여 '눈먼' 솔루션이 시각적 입력 없이 답을 제시하지 못하도록 구성되어 있어 더 도전적입니다.
- ***Technical Details***: NaturalBench는 자연 이미지-텍스트 코퍼스에서 기존 모델(CLIP, ChatGPT 등)을 활용하여 자동 생성된 10,000개의 인간 검증 VQA 샘플로 구성됩니다. 각 질문에는 서로 다른 답을 생성하는 두 개의 이미지가 연관되어 있으며, 모델이 정밀한 평가를 받을 수 있도록 27개의 세부 기술 태그를 포함합니다. 또한, 비언어적 소스를 포함하여 다양한 데이터 출처를 이용해 지속적으로 업데이트할 수 있는 동적 평가 시스템을 제공합니다.
- ***Performance Highlights***: 53개의 최신 VLM을 평가한 결과, 모델의 성능은 인간의 50-70% 수준에 그쳐 여전히 높은 도전 과제가 남아 있음을 확인했습니다. 예를 들어, 모델들은 논리적 추론 및 속성 바인딩과 같은 복합적 기술에 어려움을 겪으며, 자연 반대 샘플에서 심각한 편향을 드러냈습니다. 디바이어싱을 하면 성능이 크게 개선됨을 보여주어 향후 연구에 중요한 인사이트를 제공합니다.

### [HART: Efficient Visual Generation with Hybrid Autoregressive Transformer](https://arxiv.org/abs/2410.10812)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10812.png)

Vote: 5

Authors: ['Enze Xie', 'Haotian Tang', 'Zhuoyang Zhang', 'Han Cai', 'Song Han', 'Junsong Chen', 'Yecheng Wu', 'Junyu Chen', 'Shang Yang', 'Yao Lu']

- ***What's New***: HART(Hybrid Autoregressive Transformer)는 1024x1024 이미지 생성을 직접적으로 수행하면서, 디퓨전 모델과 비교 가능한 품질을 제공하는 효율적인 저작권 모형입니다. 이는 하이브리드 토큰화(hybrid tokenization) 및 잔여 확산(residual diffusion)을 통해, 이미지의 전체 구조를 포착하는 이산 토큰과 세부 사항을 다듬는 잔여 토큰을 생성함으로써 달성되었습니다.
- ***Technical Details***: HART의 하이브리드 토크나이저는 연속적인 특성을 디코딩할 수 있도록 하여 전통적인 이산 토크나이저의 한계를 극복합니다. 이는 VAR에서 확장된 확장 가능한 해상도의 자회귀 변환기를 사용하여 이산 토큰을 모델링하고, 37M 파라미터만으로 잔여 환산을 하는 경량 잔여 확산 모듈을 통해 잔여 토큰을 학습합니다. 이 과정에서 VAR의 절대 위치임베딩을 상대 임베딩으로 전환하여 높은 해상도에서 훈련 비용을 효과적으로 줄입니다.
- ***Performance Highlights***: HART는 1024x1024 해상도에서 디퓨전 모델에 비해 4.5-7.7배 높은 처리량과 3.1-5.9배 낮은 지연 시간을 달성했습니다. 또한, MJHQ-30K와 ImageNet 데이터셋에서의 평가에서도 이를 뒷받침하는 품질을 자랑하며, MACs에서 10.7배 적은 계산량으로 MAR을 능가하는 성능을 보여주었습니다.

### [DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation](https://arxiv.org/abs/2410.13726)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13726.png)

Vote: 7

Authors: ['Chenyu Liu', 'Jia Pan', 'Jun Du', 'Hanbo Cheng', 'Limin Lin', 'Jiefeng Ma', 'Pengcheng Xia', 'Pengfei Hu']

- ***What's New***: DAWN은 비자동회귀(diffusion) 프레임워크를 통해 동적 프레임 아바타를 생성하여 포트레이트와 음성이 주어졌을 때 생동감 있는 토킹 헤드 비디오를 제작하는 획기적인 접근 방법을 소개합니다. 이는 일반적인 토킹 헤드 생성에 맞춰 설계된 최초의 비자동회귀 솔루션으로, 더 빠른 추론 속도와 높은 품질을 보장합니다. 또한, DAWN은 비자동회귀 접근 방식의 가능성을 제공하며, 방대한 실험 결과로부터 그 효과와 잠재적인 영향을 입증합니다.
- ***Technical Details***: DAWN은 세 가지 주요 컴포넌트로 구성되어 있습니다: (1) 잠재 흐름 생성기(Latent Flow Generator; LFG) (2) 조건부 오디오-비디오 흐름 확산 모델(Audio-to-Video Flow Diffusion Model; A2V-FDM), 그리고 (3) 포즈 및 깜박임 생성 네트워크(Pose and Blink generation Network; PBNet)입니다. LFG는 비디오의 잠재 모션 표현을 모델링하며, A2V-FDM은 오디오에서 모션 표현을 생성하고, PBNet은 오디오에서 자연스러운 포즈와 깜박임 시퀀스를 생성합니다. 이러한 구조는 오디오와 포즈/깜박임 신호를 기반으로 생동감 있는 토킹 헤드 비디오를 생성하도록 설계되었습니다.
- ***Performance Highlights***: 우리 방법은 실험에서 FID, FVD32, FVD16, CSIM, BAS, Blink/s 지표에서 가장 우수한 성능을 보였으며, 이는 시각적 품질, 리듬 있는 포즈, 자연스러운 깜박임을 포함한 다양한 평가에서 기존 최첨단 방법을 능가합니다. 또한 비자동회귀 전략을 사용하여 더 빠른 생성 속도를 달성하였으며, 잠재적인 오류 축적 문제를 해결하여 긴 비디오에서도 안정적으로 높은 품질의 결과를 유지했습니다.

