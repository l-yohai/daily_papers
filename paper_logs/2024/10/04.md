## Daily Papers (2024-10-04)

### [Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models](https://arxiv.org/abs/2410.02740)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02740.png)

Vote: 38

Authors: Haotian Zhang, Vasileios Saveris, Chen Chen, Meng Cao, Zhe Gan, Hong-You Chen, Bowen Zhang, Zhengfeng Lai, Yinfei Yang, Peter Grasch, Juan Lao Tebar, Wenze Hu

- **What's New**: 이 논문에서는 멀티모달(Multimodal) 기초 모델을 위한 다양한 형식의 캡션을 생성할 수 있는 새로운, 제어 가능하고 확장 가능한 캡션 생성 파이프라인을 소개합니다. 이 파이프라인은 다양한 기초 모델에 특화된 대규모 이미지-텍스트 데이터를 구축하도록 설계되었습니다.
- **Technical Details**: 기존의 웹 크롤링 데이터, 특히 AltText는 종종 시각적 세부 정보가 부족하고 잡음이 많은 콘텐츠를 포함하고 있습니다. 연구진들은 LLaVA에 의해 생성된 합성 캡션을 사용하여 CLIP을 훈련하여 이러한 문제를 탐구했습니다. 합성 캡션만 사용할 경우 성능 저하가 발생했지만, 원래의 AltText와 합성 캡션을 결합하면 최고의 결과가 나타났습니다. 또한 짧은 합성 캡션(SSC)과 기술적인 합성 캡션(DSC)을 사용하여 CLIP에 미치는 영향을 조사하였습니다.
- **Performance Highlights**: 실험 결과, CLIP의 성능은 더 설명적인 캡션보다 짧은 캡션을 사용할 때 더 나은 결과를 보여주었습니다. 또한 잡음이 있는 AltText와 합성 캡션을 혼합하여 사용하면 데이터 다양성과 이미지-텍스트 정렬을 모두 향상시킬 수 있음을 발견했습니다. 이는 다양한 형식의 캡션을 생성하는 파이프라인의 필요성과 중요성을 강조합니다.

### [Video Instruction Tuning With Synthetic Data](https://arxiv.org/abs/2410.02713)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02713.png)

Vote: 28

Authors: Ziwei Liu, Chunyuan Li, Yuanhan Zhang, Zejun Ma, Bo Li, Jinming Wu, Wei Li

- **What's New**: 대규모 컴퓨팅과 데이터가 중요한 시대에서 비주얼 인스트럭션 튜닝(Visual Instruction Tuning)은 일반적인 비주얼 어시스턴트의 기초를 마련했습니다. 새로운 연구는 비디오-언어 인스트럭션 데이터의 품질을 높이기 위한 데이터셋 LLaVA-Video-178K를 소개합니다. 이는 GPT-4o와 인간의 노력으로 개발되었고, 비디오 설명, 자유형식 및 다중 선택 질문 응답을 포함하여 높은 품질의 데이터셋을 제공합니다.
- **Technical Details**: 1. 다양한 비디오 소스에서 동적 비디오를 선택하여 선택된 비디오는 초기 상태 그대로 유지하였습니다. 2. 재발적 상세 캡션 생성 파이프라인(Recurrent Detailed Caption Generation Pipeline)으로 프레임당 풍부한 샘플링을 통해 상세한 비디오 캡션을 생성합니다. 3. 16가지 질문 유형을 정의하여 GPT-4o를 prompt하여 다양한 시나리오의 질문 응답 페어를 생성합니다.
- **Performance Highlights**: LLaVA-Video는 LLaVA-Video-178K의 상세한 특징 덕분에 기존 기법보다 세부적인 내용을 더 잘 이해할 수 있는 성능을 보여주며, 제한된 GPU 메모리 내에서 최대 프레임 샘플링을 수행합니다. 새로운 비디오 표현 기법 LLaVA-VideoSlowFast는 일반적인 방법보다 최대 3배 더 많은 프레임을 효율적으로 활용할 수 있게 하여 성능을 극대화합니다.

### [Loong: Generating Minute-level Long Videos with Autoregressive Language Models](https://arxiv.org/abs/2410.02757)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02757.png)

Vote: 27

Authors: Tianwei Xiong, Yuqing Wang, Yang Zhao, Bingyi Kang, Jiashi Feng, Daquan Zhou, Zhijie Lin, Xihui Liu

- **What's New**: Loong이라는 새로운 autoregressive (자가회귀) LLM 기반 비디오 생성 모델을 제안합니다. 이 모델은 분 단위의 긴 비디오를 생성할 수 있도록 설계되었습니다. 이를 통해 기존의 2-10초 길이의 동영상 생성에서 벗어나 더 긴 동영상 생성이 가능해졌습니다.
- **Technical Details**: Loong 모델은 두 가지 주요 구성 요소로 이루어져 있습니다: 비디오 토크나이저(tokenizer)와 autoregressive LLM입니다. 비디오 토크나이저는 3D CNN 아키텍처 기반으로, 영상을 공간-시간적으로 압축한 후, Clustering Vector Quantization(CVQ)를 사용해 이산 토큰으로 변환합니다. Autoregressive LLM은 이러한 토큰을 바탕으로 텍스트와 비디오 토큰의 통합된 시퀀스를 모델링하여 다음 비디오 토큰을 예측합니다. 또한, 훈련 시 이득 손실을 보완하기 위해 progressive short-to-long 훈련 전략과 손실 가중치 조정을 사용합니다.
- **Performance Highlights**: Loong은 높은 시공간 해상도의 비디오를 생성하며, 기존보다 긴 시퀀스를 처리하도록 최적화되어 있습니다. 다양한 실험 결과를 통해 이 모델이 분 단위의 긴 비디오를 효과적으로 생성할 수 있음을 입증했습니다. 이는 자연어 처리의 긴 시퀀스 처리 능력을 비디오 생성 영역으로 확장한 것으로, LLM의 새로운 가능성을 열었습니다.

### [LLaVA-Critic: Learning to Evaluate Multimodal Models](https://arxiv.org/abs/2410.02712)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02712.png)

Vote: 22

Authors: Dong Guo, Heng Huang, Chunyuan Li, Haoqi Fan, Qinghao Ye, Xiyao Wang, Tianyi Xiong, Quanquan Gu

- **What's New**: 이 연구는 처음으로 평가를 위한 명령 수행 데이터를 구축하고, 이를 기반으로 LMM, LLaVA-Critic을 개발한 것입니다. LLaVA-Critic은 열린 LMMs가 평가 기준에 따라 신뢰성 있는 점수를 제공하며, AI 피드백 생성 시에 효율적인 보상 신호를 생성할 수 있도록 합니다.
- **Technical Details**: 이 연구에서는 광범위한 이미지와 평가 명령 샘플을 포괄하는 46,000개 이미지와 113,000개의 평가 명령 샘플로 구성된 데이터셋을 소개합니다. LLaVA-Critic은 개방형 모델들의 평가 능력을 확장하여 효과적인 평가와 피드백을 제공합니다. 평가 프로세스는 점수, 관련 추론을 투명하게 제시하는 점수가 1에서 10까지 매겨집니다.
- **Performance Highlights**: LLaVA-Critic은 상업적 GPT 모델들과 높은 상관 관계를 보이며, 자원이 제한된 환경에서 모델 개발자들에게 비용 효율적인 대안으로 작용할 수 있습니다. 또한, Direct Preference Optimization에서 LLaVA-Critic은 사람 피드백에 의존하는 LLaVA-RLHF의 보상 모델을 능가하는 피드백을 제공합니다. 아울러, 오픈 소스 비전 보조의 발전을 지원하기 위해 비평 교육 데이터를 공개하고 있습니다.

### [Contrastive Localized Language-Image Pre-Training](https://arxiv.org/abs/2410.02746)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02746.png)

Vote: 20

Authors: Haotian Zhang, Xinze Wang, Meng Cao, Zhe Gan, Hong-You Chen, Bowen Zhang, Zhengfeng Lai, Yinfei Yang, Keen You, Marcin Eichner

- **What's New**: 이 논문에서는 기존의 CLIP(Contrastive Language-Image Pre-training) 모델을 개선하여 MLLMs(Multimodal Large Language Models)에서 더 우수한 지역적(localization) 기능을 제공하는 새로운 프레임워크인 Contrastive Localized Language-Image Pre-Training (CLOC)을 제안합니다. 이 모델은 지역 수준에서의 세밀한 표현을 가능하게 하는 'Promptable Embeddings'라는 학습 목표를 도입하며, 이미지 인코더가 더 정밀하게 비전-언어 정렬을 수행할 수 있도록 지원합니다.
- **Technical Details**: CLOC는 CLIP의 원래 손실에 지역-텍스트 대조 손실을 추가하여 이미지 임베딩에서 공간 힌트를 기반으로 한 경량 추출 모듈을 통해 지역 임베딩을 추출합니다. 이를 위해 시각적으로 풍부한 이미지 캡셔너와 오픈 보캐블러리 디텍터를 결합해 2억 개의 이미지-텍스트 데이터셋을 구성하여 CLIP의 한계를 극복하고자 합니다.
- **Performance Highlights**: 31개의 평가 작업(이미지-텍스트 작업, 새로 구성된 지역-텍스트 작업 및 MLLMs와의 다운스트림 평가)를 통해 CLOC가 기존 CLIP 모델을 일관되게 뛰어넘는 성능을 발휘함을 입증했습니다. 이러한 실험 결과는 특히 MLLMs에서의 지역적 이해가 필요한 상황에서 CLOC의 우수성을 보여줍니다.

### [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://arxiv.org/abs/2410.02073)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02073.png)

Vote: 19

Authors: Stephan R. Richter, Yichao Zhou, Aleksei Bochkovskii, Marcel Santos, Hugo Germain, Vladlen Koltun, Amaël Delaunoy

- **What's New**: 이 논문에서는 Depth Pro라는 새로운 기초 모델을 소개하여 zero-shot 방식으로 메트릭 단일 카메라 기반 깊이 추정을 수행합니다. 이 모델은 이미지 메타데이터 없이 절대 스케일로 메트릭 깊이 맵을 생성하며, 고해상도 이미지를 0.3초 안에 처리할 수 있습니다.
- **Technical Details**: Depth Pro는 글로벌 이미지 문맥을 포착하기 위한 효율적인 다중 스케일 ViT 기반 아키텍처를 설계하고, 경계 추적 정밀도를 평가하기 위한 새로운 메트릭 세트를 도입합니다. 실시간 인터랙티브 응용 시나리오를 위해 낮은 지연 시간으로 구동됩니다.
- **Performance Highlights**: Depth Pro는 기존의 모든 작업을 능가하는 경계 추적 능력을 보여주며, 특히 Object 경계를 따라가는 능력에서 큰 개선을 보여줍니다. 높은 경계 정확성을 유지하면서 1에서 2 오더의 크기로 빠르고 정확한 경계와 메트릭 깊이 맵을 제공하며, 이는 기존의 최신 연구보다 훨씬 빠릅니다.

### [VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment](https://arxiv.org/abs/2410.01679)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01679.png)

Vote: 16

Authors: Milad Aghajohari, Siva Reddy, Eva Portelance, Alessandro Sordoni, Amirhossein Kazemnejad, Aaron Courville, Nicolas Le Roux

- **What's New**: 현재 연구는 대형 언어 모델(LLM)을 강화 학습 방법으로 미세 조정하는 과정에서 신용 할당 문제(credit assignment problem)에 대한 새로운 접근법을 제시합니다. VinePPO라는 새로운 기법은 기존 강화 학습 알고리즘인 Proximal Policy Optimization(PPO)의 한계를 극복하고, 언어 환경의 고유한 특성을 활용하여 중간 단계를 재설정할 수 있는 점을 활용합니다.
- **Technical Details**: 기존 PPO에서는 value network(가치 네트워크)를 사용하여 모델의 중간 행동이 미래 보상에 얼마나 기여하는지 예측했습니다. 하지만 VinePPO는 몬테카를로 표본(Monte Carlo samples)을 활용하여 가치 추정을 보다 정확하게 수행하며, 이렇게 함으로써 값 네트워크에 대한 의존도를 줄입니다. 이 방법은 특정 언어 상황에서 중간 상태를 쉽게 재설정할 수 있는 속성을 활용합니다.
- **Performance Highlights**: VinePPO는 도전적인 데이터셋에서 기존 PPO 및 RL-free 방법을 consistently 능가하며, GPU 자원을 덜 소모하고도 높은 성능을 냅니다. 예를 들어, VinePPO는 적은 수의 gradient updates로도 PPO의 성능을 초과하며, wall-clock time 측면에서도 최대 3배의 효율을 보여줍니다.

### [Large Language Models as Markov Chains](https://arxiv.org/abs/2410.02724)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02724.png)

Vote: 15

Authors: Nicolas Boullé, Ievgen Redko, Linus Bleistein, Abdelhakim Benechehab, Oussama Zekri, Ambroise Odonnat

- **What's New**: 최근 인공지능(AI)과 머신러닝(ML) 분야에서는 Transformer 아키텍처를 기반으로 한 대형 언어 모델(LLMs)이 도입되면서 상당한 발전이 이루어졌습니다. 이러한 모델들은 방대한 데이터로 학습되었으며, 기계 번역, 텍스트 생성, 질문 응답, 감성 분석 등의 자연어 처리 작업에 널리 적용되고 있습니다. 다만, 현재까지 LLMs의 성능이 어떻게 뛰어난 추론 능력을 발휘하는지에 대한 명확한 과학적 설명이 부족합니다. 본 연구는 이러한 지식 격차를 해소하기 위해 LLMs의 추론 능력을 명시적으로 설명하는 접근 방식을 제안합니다. 특히, LLMs를 유한 상태 공간에서의 마코프 체인으로 해석하는 방법을 도입하여 이론적인 분석이 가능합니다. 이는 언어 모델의 입력 및 출력 시퀀스가 대응 관계를 가질 수 있음을 함의하며, LLMs의 일반화 오류에 대한 수학적 경계를 제시합니다.

### [Distilling an End-to-End Voice Assistant Without Instruction Training Data](https://arxiv.org/abs/2410.02678)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02678.png)

Vote: 14

Authors: Diyi Yang, Weiyan Shi, Michael Ryan, Yanzhe Zhang, Ella Li, William Held

- **What's New**: 이 논문에서는 대규모 언어 모델(LLMs)의 음성 인터페이스 가능성을 탐구하고, 기존 데이터의 인위적인 '한계'를 극복할 수 있는 새로운 접근법, Distilled Voice Assistant (DiVA)를 제안합니다. 이러한 접근법은 LLM의 망각(forging) 문제를 해결하며, ASR(Automatic Speech Recognition) 데이터만 사용하여도 좋은 일반화를 보여줍니다.
- **Technical Details**: DiVA는 Whisper-Large-v3 오디오 인코더에서 시작하여, 텍스트-오디오 교차 주의 메커니즘을 초기화하는 Q-Former를 사용합니다. DiVA는 컨텍스트 증류(context distillation) 기법을 통해 훈련되며, 이를 통해 자가 감독 학습(self-supervised learning)을 활용해 다양한 음성 작업에 대해 일반화할 수 있습니다.
- **Performance Highlights**: DiVA는 Spoken Question Answering, Classification, Translation의 작업에서 좋은 성능을 보여주며, 유저에 의해 72%의 시도에서 가장 경쟁력 있는 기준 모델인 Qwen 2 Audio보다 선호됩니다. 이는 DiVA가 100배 이상 적은 훈련 연산을 사용하면서도 이루어진 결과입니다.

### [Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models](https://arxiv.org/abs/2410.02416)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02416.png)

Vote: 12

Authors: Romann M. Weber, Otmar Hilliges, Seyedmorteza Sadat

- **What's New**: 이 논문에서는 새로운 신경망 최적화 알고리즘(optimization algorithm)을 제안합니다. 이 알고리즘은 기존의 기법보다 빠르게 수렴하며, 더 나은 일반화 성능을 보인다고 주장합니다.

### [CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling](https://arxiv.org/abs/2409.19291)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.19291.png)

Vote: 11

Authors: Xiaoye Qu, Tong Zhu, Yu Cheng, Jihai Zhang

- **What's New**: 새로운 제안인 Diversified Multiplet Upcycling (DMU)은 CLIP 모델을 개선하기 위한 간단하면서도 효과적인 모델-무작위 전략입니다. 이는 희소 활성화 Mixture of Experts (MoE) 프레임워크를 활용해 모델 용량을 확장하면서 기존의 사전 훈련된 촘촘한 체크포인트를 최대한 활용하여 처음부터 훈련할 필요를 피합니다.
- **Technical Details**: DMU는 Multistage Contrastive Learning (MCL)을 사용하여 기본적인 촘촘한 CLIP 모델을 미세 조정하고, 피드포워드 네트워크(FFN) 레이어를 통해 서로 다른 CLIP 모델들을 생성합니다. 이 모델들은 다양한 정보를 인코딩하며, 여러 FFN 전문가들을 생성하여 CLIP-MoE 모델을 초기화합니다. 최종적으로 CLIP-MoE의 라우터를 미세 조정하여 모든 전문가를 가득 활용하게끔 합니다.
- **Performance Highlights**: 고품질의 소규모 이미지-캡션 데이터셋을 사용함으로써 MCL-초기화된 CLIP-MoE는 CLIP 성능을 크게 향상시켰습니다. 검색 작업에서 CLIP-MoE는 기존의 OpenAI CLIP 모델보다 약 20% 뛰어난 성능을 보였으며, 이와 동시에 훈련 비용을 기본 CLIP 모델의 2% 미만으로 유지했습니다. CLIP-MoE는 MLLM의 비전 인코더로도 대체 원래의 비전 인코더를 사용하여 상당한 개선을 나타냈습니다. CLIP-MoE는 다른 파인 튜닝 기반 및 인기 있는 MoE 구조 방법을 능가합니다.

### [Training Language Models on Synthetic Edit Sequences Improves Code Synthesis](https://arxiv.org/abs/2410.02749)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02749.png)

Vote: 10

Authors: Ulyana Piterbarg, Lerrel Pinto, Rob Fergus

- **What's New**: 이 논문은 대형 언어 모델(LLMs)이 코드 생성에서 맞닥뜨리는 zero-shot 생성의 한계를 개선하기 위한 새로운 접근 방안을 제안합니다. 'LintSeq'라는 알고리즘을 통해 코드 생성을 순차적 편집 문제로 재정의하여 문제 해결을 시도합니다. 이는 현재의 autoregressive(자동 회귀적) 방식 대신, 코드 편집 시퀀스를 예측하는 모델을 훈련하는 것을 목표로 하며, 코드 편집 시퀀스를 학습 데이터를 생성하기 위해 활용됩니다.
- **Technical Details**: LintSeq는 프로그램의 상태를 반복적으로 체크하며 오류 없는 프로그램 상태들을 샘플링하는 알고리즘으로, 프로그램을 작성하는 데 사용될 삽입 시퀀스를 샘플링합니다. 이 과정에서 'linter'라는 정적 코드 분석 도구가 사용됩니다. LintSeq는 두 가지 주요 단계로 이루어져 있습니다: 역방향 샘플링 단계와 순방향 편집 계산 단계입니다. 역방향 샘플링에서는 소스 파일부터 시작하여 빈 프로그램으로 끝나는 오류 없는 프로그램 상태 시퀀스를 샘플링합니다. 다음, Unix diff 연산자를 사용하여 소스 파일의 연속적인 버전 사이의 변화를 계산하고, 이를 편집 시퀀스로 출력합니다. 이러한 편집 시퀀스 데이터로 LM을 미세 조정하면 개선된 코드 다양성과 품질을 얻을 수 있습니다.
- **Performance Highlights**: 편집 시퀀스로 조정된 모델은 150M에서 14B 파라미터 사이의 다양한 모델 스케일에서 코드의 품질과 다양성을 향상시켰습니다. 이로 인해 inference-time(추론 시간) 동안의 계산량과 발생 가능한 샘플 수 사이에서 보다 나은 trade-off(절충점)를 제공합니다. 소형 LLM의 경우, 편집 시퀀스 모델은 GPT-4 모델과 경쟁력 있는 성능을 보이며, Llama 3.1 405B와 같은 오픈 소스 LLM로 문제당 한 번만 샘플링하는 것과 비슷한 누적 비용을 가집니다. 또한, 데이터 생성 동안 linter를 사용하지 않으면 편집 시퀀스 모델로 생성된 프로그램의 품질이 저하됩니다.

### [SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2410.02367)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02367.png)

Vote: 9

Authors: Pengle Zhang, Jintao Zhang, Jianfei Chen, Jun Zhu, Jia wei

- **What's New**: SageAttention은 주의(attention)의 양자화를 통해 정확성을 유지하면서도 주의(attention)를 가속화하는 방법을 제안합니다. 이는 기존 주의(attention) 알고리즘들이 높은 정밀도를 유지하면서 발생하는 성능 저하 문제를 해결하고 있습니다.
- **Technical Details**: SageAttention은 INT8을 사용하여 주의(attention)의 행렬(Q, K)을 양자화하며, P와 V는 FP16 상태를 유지하는 전략을 사용합니다. 또한, RTX4090과 3090 GPU에서의 효율적인 구현을 통해 속도를 두 배 이상 향상시키고자 합니다.
- **Performance Highlights**: SageAttention은 FlashAttention2와 xformers에 비해 2.1배와 2.7배 빠른 성능을 보여줍니다. 또한, RTX4090에서는 340 TOPS의 성능을 달성하며 FlashAttention3에 버금가는 성능을 나타냅니다. 특히, 이미지/비디오 생성 및 언어 모델링 작업에서 성능 손실 없이 기존보다 2배 이상의 속도 향상을 제공합니다.

### [MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis](https://arxiv.org/abs/2410.02103)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02103.png)

Vote: 7

Authors: Xiaobiao Du, Yida Wang, Xin Yu

- **What's New**: 본 연구에서는 다양한 Gaussian 기반의 명시적 방법에 대한 일반적인 최적화 방법으로써 MVGS를 제안합니다. MVGS는 관점 통일 학습을 활용하여 훈련 중 다중 뷰(view)를 통합하여 더 나은 새로운 뷰 합성(NVS) 정밀성을 제공합니다. 이는 단일 뷰 최적화가 특히 특정 뷰에 과적합하여 철저한 세부 사항을 제시하는 데 부족함을 보완합니다.
- **Technical Details**: MVGS는 다중 뷰를 통한 콘텍스트를 통합하기 위해 다중 뷰 통제 학습 전략을 제안합니다. 또한 저해상도에서 고해상도로의 교차 내재적 가이드라인을 도입하여 커다란 3D Gaussian을 더 컴팩트하게 만들어 고해상도 세부 사항을 조각합니다. 겹치는 3D 영역의 3D Gaussian은 레이 마칭(ray marching) 기술과 2D 손실 맵 안내로 밀도가 높아지는 전략을 통해 재구성 성능을 개선합니다.
- **Performance Highlights**: 다양한 기존 Gaussian 기반 방법들의 NVS 성능을 약 1 dB PSNR을 향상시켰음을 광범위한 실험에서 입증합니다. 추가적으로, 다양한 작업에 대해 고정 및 반사 객체 재구성, 4D 재구성, 대규모 장면 재구성에서 성능 향상을 보여줍니다.

### [L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?](https://arxiv.org/abs/2410.02115)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02115.png)

Vote: 6

Authors: Keyan Zhou, Zecheng Tang, Juntao Li, Jianye Hou, Min Zhang, Baibei Ji

- **What's New**: L-CiteEval라는 새로운 멀티태스크 기준(Test Benchmark)이 소개되었습니다. 이 기준은 롱컨텍스트(Long-context) 이해를 목표로 하고, 모델이 생성한 답변의 질과 인용의 품질을 평가합니다.
- **Technical Details**: L-CiteEval은 롱컨텍스트(8K에서 48K 토큰) 환경에서 5개의 주요 태스크 카테고리와 11가지 태스크를 포함하고 있습니다. 자동화 평가 스위트를 제공하며, 길이와 태스크 난이도에 따라 두 가지 변형(L-CiteEval-Length 및 L-CiteEval-Hardness)을 통해 모델의 성능을 평가합니다.
- **Performance Highlights**: 11개의 최신 롱컨텍스트 모델을 테스트했으며, 오픈소스와 클로즈드 소스 모델 간에 생성 품질에서는 큰 차이가 없었습니다. 그러나 인용 품질에서는 오픈소스 모델이 뒤처졌습니다. RAG(Retrieval-Augmented Generation) 기술을 활용하여 오픈소스 모델의 충실성을 크게 향상시켰습니다. 또한, 모델의 인용 생성 과정과 주의 메커니즘(예: retrieval head) 간의 상관관계를 밝혀, 롱컨텍스트 모델의 미래 평가와 발전에 대한 통찰을 제공합니다.

### [Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos](https://arxiv.org/abs/2410.02763)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02763.png)

Vote: 5

Authors: Jianrui Zhang, Mu Cai, Yong Jae Lee

- **What's New**: Vinoground라는 새로운 벤치마크가 도입되었습니다. 이는 길이가 짧고 자연스러운 비디오-캡션 쌍으로 구성되어 있으며, 대규모 멀티모달 모델(LMM)의 시간적 추론 능력을 평가하기 위한 것입니다. 특히, 짧은 비디오 내에서의 시간적 차이와 객체 변형을 이해하는 데 중점을 둡니다.
- **Technical Details**: Vinoground 벤치마크는 1000개의 짧고 자연스러운 비디오-캡션 쌍으로 구성되어 있습니다. 각 캡션 쌍은 단어 조합은 동일하지만 순서가 다른 긍정적(positive)과 부정적(negative) 표현으로 이루어져 있습니다. 이 벤치마크는 영상의 텍스트 점수(text score), 비디오 점수(video score), 그룹 점수(group score)를 통해 모델의 언어적, 시각적 및 시간적 추론 능력을 균형있게 평가합니다.
- **Performance Highlights**: 현재 최첨단 대규모 멀티모달 모델들은 짧은 비디오 이해 과제에서 시간적 추론에 서툴며, 대부분의 경우 비디오 점수에서 무작위 수준의 성능을 보였습니다. 또한, 세부적인 디테일을 놓치고 대략적인 수준에서만 비디오 프레임을 분석하는 경향이 있습니다. 이 연구는 기존의 시간적 반론 벤치마크들이 LMMs의 시간적 추론 부족을 충분히 드러내지 못하고 있음을 발견했습니다.

### [MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation](https://arxiv.org/abs/2410.02458)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02458.png)

Vote: 5

Authors: Amir Shmuel, Janine Mendola, Aman Chadha, Gurucharan Marthi Krishna Kumar

- **What's New**: 이 연구는 사전 학습된 대형 언어 모델(LLM)을 기반으로한 transformer block을 ViTs 기반의 의료 영상 분할 모델에 도입합니다. 이는 LLM의 frozen transformer block을 visual encoder로 사용하여 전통적인 비전-언어 모델(VLM)과 차별화합니다. 이를 통해 LLM의 언어 입력 없이도 영상 처리 성능을 크게 향상시킬 수 있으며, 큰 데이터셋이나 과도한 계산 자원을 요구하지 않습니다.

### [Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations](https://arxiv.org/abs/2410.02762)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02762.png)

Vote: 3

Authors: Yossi Gandelsman, Nick Jiang, Anish Kachinthaya, Suzie Petryk

- **What's New**: 이번 논문에서는 Vision-Language Models (VLMs)에서 발생하는 허상(hallucinations) 문제를 해결하기 위한 새로운 접근법을 도입합니다. 이 방법은 모델 성능에 영향을 주지 않으면서 이미지의 잠재 표현(latent representations)을 미세하게 수정하는 것입니다.
- **Technical Details**: VLMs의 이미지 잠재 표현을 텍스트로 해석하는 데 logit lens 기법을 사용하여 이미지를 공간적으로 이해하고 허상 객체를 내부 표현에서 제거합니다. 'ProjectAway'라는 알고리즘을 통해 목표 객체의 이미지 특징을 텍스트 특징과 직교화하여 제거합니다. 이 접근법은 LLaVA 1.5와 InstructBLIP 두 가지 VLM에 적용되었습니다.
- **Performance Highlights**: 이 방법은 두 VLM에서 mAP를 각각 22.45%와 47.17% 향상시켰으며, 표준 벤치마크에서 허상을 최대 25.7%까지 감소시켰습니다. 또한, 객체의 공간적 위치 파악에서 최첨단의 zero-shot segmentation 방법과 필적하는 성능을 보였습니다.

### [Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning](https://arxiv.org/abs/2410.02052)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02052.png)

Vote: 3

Authors: Xiao Yu, Baolin Peng, Vineeth Vajipey, Jianfeng Gao, Hao Cheng, Zhou Yu, Michel Galley

- **What's New**: 이번 연구에서는 Visial-Language Models(VLM)을 강화하기 위해 'Reflective Monte Carlo Tree Search (R-MCTS)' 에이전트를 소개합니다. 이는 기존의 MCTS(Monte Carlo Tree Search) 알고리즘을 확장하여, 높은 질의 반성을 통한 실시간 탐색 품질 개선과 다중 에이전트 디베이트 기술을 활용한 안정적인 상태 평가를 포함합니다.
- **Technical Details**: 기술적으로, R-MCTS 에이전트는 'Best-in-Tree SFT'와 'Tree-Traversal SFT' 두 가지 지도 학습 방식으로 새롭게 획득한 지식을 기본 VLM에 전달합니다. 이를 통해 모델은 어떤 행동이 최적인지 이해하고, 환경을 탐색하고 상태를 평가하며 적합한 상태로 되돌아가는 법을 학습합니다.
- **Performance Highlights**: 실험 결과, VisualWebArena 환경에서 GPT-4o 기반 R-MCTS 에이전트는 6%에서 30%까지 상대적인 성능 향상을 보여주었습니다. 특히, 'Best-in-Tree SFT'로 학습된 경우에는 탐색 성능의 97%를 불러오면서도 추론 비용을 4배 줄이는 효과를 확인했습니다. 'Tree-Traversal SFT'로 학습된 GPT-4o는 검색 보강 없이도 환경 탐색, 평가 및 백트래킹을 수행할 수 있는 능력을 나타냈습니다.

### [Intelligence at the Edge of Chaos](https://arxiv.org/abs/2410.02536)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02536.png)

Vote: 3

Authors: Syed A Rizvi, David van Dijk, Shiyang Zhang, Emanuele Zappala, Amin Karbasi, Aakash Patel, Sizhuang He, Nianchen Liu

- **What's New**: 이 논문에서는 인공지능(AI)와 이론적 계산 과학 분야에서 오랫동안 연구되어 온 지능의 출현과 특성에 대한 새로운 접근 방식을 제안합니다. 기존의 AI 방법론은 주로 인간의 인텔리전스가 내재된 고품질 데이터셋을 학습하는 데 중점을 두었으나, 이 연구는 단순한 시스템이더라도 복잡한 행동을 보여야 지능이 출현할 수 있다는 가설을 탐구합니다.

### [Learning the Latent Rules of a Game from Data: A Chess Story](https://arxiv.org/abs/2410.02426)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02426.png)

Vote: 3

Authors: Ben Fauber

- **What's New**: 이 논문은 소규모 사전학습 된 생성적 언어 모델(SLM)을 특별한 지침으로 미세조정하여 표준 대수적 체스 표기법(SAN)만을 사용하여 체스 규칙을 배우고, 숙련된 체스 게임을 수행할 수 있는지를 탐색합니다. 이는 스테판 츠바이크(Stefan Zweig)의 소설 '체스 이야기'의 가상 인물 Dr. B의 경험에서 영감을 받았습니다.
- **Technical Details**: 이 연구는 python-chess 라이브러리를 사용하여 SAN 형식의 체스 게임 데이터를 분석하고, 보드 상태를 평가하며, 가능한 이동의 합법성을 확인합니다. SAN은 각 체스판의 64개 칸에 고유한 두 글자 식별자를 할당하며, 파일(세로)과 랭크(가로)에 따라 구성됩니다.
- **Performance Highlights**: 이 접근 방식에서는 SAN 데이터만으로 Dr. B가 체스 규칙을 습득한 것과 같은 설정을 재현하려 했습니다. 이 방법론은 체스 엔진 및 강화 학습(RL)과는 달리 수백만 개의 체스 게임 데이터를 활용하여 강화 학습이 없이 체스 규칙을 학습하는 것을 목표로 합니다.

### [Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data](https://arxiv.org/abs/2410.02056)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02056.png)

Vote: 3

Authors: Sreyan Ghosh, Zhifeng Kong, Sonal Kumar, Bryan Catanzaro, Dinesh Manocha, Rafael Valle

- **What's New**: 이 연구에서는 Text-to-Audio (T2A) 모델을 활용한 새로운 데이터 증강 방법인 'Synthio'를 제안합니다. 이 방법은 소규모 오디오 분류 데이터셋을 합성 데이터로 확장함으로써 기존의 데이터 증강 한계를 넘어서고, 일관성과 다양성을 동시에 달성하도록 설계되었습니다.
- **Technical Details**: Synthio는 두 가지 주요 단계로 구성됩니다: 1) 'Aligning the Text-to-Audio Models with Preference Optimization' 단계를 통해 소규모 데이터셋의 음향적 특성에 일치하는 합성 음원을 생성합니다. 여기에는 T2A 모델을 선호 최적화 방법으로 정렬하여 원하는 음향적 특성을 가진 '텍스트-투-오디오' 생성을 유도합니다. 2) 'Generating Diverse Synthetic Augmentations' 단계에서는 다양한 오디오 캡션을 생성하여 언어 지침을 통한 오디오 상상(concept of language-guided audio imagination)을 실현, 다양한 구성을 가진 오디오를 생성합니다. 여기에는 MixCap이라는 방법을 통해 LLM을 반복적으로 활용, 존재하는 오디오와 새로운 음향 요소를 결합하여 캡션을 생성합니다.
- **Performance Highlights**: Synthio는 10개의 데이터셋과 4개의 저자원 환경에서 테스트되었으며, 캡션이 약한 AudioSet에서 훈련된 T2A 모델을 활용하더라도 베이스라인 대비 0.1%-39% 더 뛰어난 성능을 보였습니다. 이러한 테스트는 Synthio의 확장 가능성과 복잡한 작업에서의 강력한 성능을 강조합니다.

### [Contextual Document Embeddings](https://arxiv.org/abs/2410.02525)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02525.png)

Vote: 2

Authors: Alexander M. Rush, John X. Morris

- **What's New**: 이 연구에서는 문서 임베딩(doc embedding)의 컨텍스트화를 통해 특정 상황에서의 검색 성능을 향상시키고자 하는 새로운 접근법을 제안합니다. 이 방법은 문서 임베딩을 생성하기 위한 조밀한 인코더(dense encoder)의 학습 절차와 아키텍처를 변경합니다. 구체적으로, 문서를 보다 잘 구별할 수 있도록 대비 학습(contrastive learning) 및 클러스터링을 활용한 이웃 문서를 고려하여 학습하는 절차를 제안합니다. 또한, 새로운 인코더 아키텍처는 이웃 문서에 대한 정보를 임베딩 과정에 주입하여 임베딩을 강화합니다.
- **Technical Details**: 컨텍스트 트레이닝(contextual training)은 빠른 쿼리-문서 클러스터링을 사용하여 각 학습 배치에 대한 이웃 그룹을 생성합니다. 아키텍처 측면에서는 문서 주변의 문서 정보를 포괄하는 추가 조건화(module)를 갖춘 표준 BERT 스타일 인코더를 제안하여 임베딩 시 문서 간의 관계를 반영합니다. '컨텍스트 문서 임베딩(CDE)'이라고 불리는 이 방법은 말뭉치 수준의 통계와 유사한 방식으로 문서 임베딩을 만들며, 최종 임베딩 크기는 변하지 않아 별도의 저장 공간이 필요하지 않습니다.
- **Performance Highlights**: 실험 결과, 제안한 컨텍스트 대비 학습은 표준 텍스트 임베딩 모델 훈련을 향상시키며, 추가적인 하드 네거티브(hard negatives)와 같은 다른 접근 없이도 실행할 수 있음을 보여줍니다. 컨텍스트 인코더 아키텍처는 모든 테스트 설정에서 기본 모델 대비 추가적인 성능 향상을 보여주며, 특히 금융 및 의료 문서와 같은 특정 도메인에서 더 큰 향상을 보였습니다. 산업 규모의 학습을 통해, 당사의 모델은 작은(<250M 파라미터 이하) 모델 중에서 MTEB 벤치마크에서 최첨단 성능을 달성했습니다.

### [Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models](https://arxiv.org/abs/2410.01335)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01335.png)

Vote: 2

Authors: Bing Liu, Pritish Yuvraj, Benjamin Muller, Nayan Singhal, Hongjiang Lv, Rui Hou, Lucas Bandarkar

- **What's New**: 이 논문은 LLMs(Large Language Models)가 사전 학습(pre-trained)된 영어 중심 데이터를 벗어나 다국어 환경에서 활용될 수 있도록 새로운 솔루션을 제안합니다. 특히, 수학적 추론(math reasoning) 능력을 저자원 언어(lower-resource languages)로 이전하기 위한 방법으로 두 가지 LLMs를 결합해 Layer Swapping을 구현합니다.
- **Technical Details**: 이 방법은 두 가지 사전 학습 모델 변형체(variants)—영어 수학 샘플로 미세 조정(fine-tuning)된 모델과 목표 언어의 일반 지침 데이터를 받은 모델—을 만들어 '전문가(experts)'라 칭하고, 이 둘의 학습된 언어 및 작업 능력을 각 모델의 매개변수(parameter)를 조합하여 구성합니다. 특히, Transformer 모델의 상위 및 하위 레이어는 언어 전문가 (language expert)에서, 중간 레이어는 수학 전문가(math expert)에서 가져옵니다. 또한, 두 모델의 파라미터 값을 가중 평균으로 만든 1-layer 또는 2-layer '전환 구역(transition zones)'을 구성하여 부정적 간섭을 피합니다.
- **Performance Highlights**: 제안된 layer-swapped 모델은 다수의 저자원 언어(스와힐리, 텔루구, 벵골어, 일본어)에서 수학 성능이 상당히 향상되었습니다. MGSM 벤치마크에서의 테스트 결과, 이 모델은 개별 전문가 모델이나 기존 Base 수준보다 평균적으로 10% 더 우수한 성능을 보였습니다. 특히 스와힐리어에서는 혼합 스와힐리어와 수학 SFT 데이터로 미세 조정 된 모델보다 더 나은 성능을 보였습니다.

### [SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics](https://arxiv.org/abs/2410.01946)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01946.png)

Vote: 2

Authors: Bertram Ludäscher, Zhiwen You, Haotian Zhu, Kanyao Han, Jana Diesner

- **What's New**: 이 논문은 제한된 데이터 상황에서 성과를 내기 위해 prompt-based fine-tuning 방법을 개선했습니다. 특수한 과학 용어를 자동으로 검색하고 이들을 활용하여 모델의 예측 성능을 향상시키는 방법을 소개합니다. 이를 통해 다양한 과학 분야의 논문을 효과적으로 분류할 수 있는 SciPrompt라는 새로운 프레임워크를 제안합니다.
- **Technical Details**: SciPrompt는 세 가지 주요 단계로 구성됩니다: 1) 과학 용어 검색, 2) 레이블 용어 필터링, 3) 과학 주제 예측. 우선, 외부 지식 베이스(KBs)로부터 관련 도메인 문구를 검색합니다. 그 후, SciNLI 데이터셋을 기반으로 bi-encoder와 cross-encoder 모델을 미세 조정하여 관련성이 높은 도메인 문구를 선택합니다. 마지막으로, 선택한 과학 용어를 활용하여 enhanced verbalizer를 통해 특정 클래스로의 예측을 강화합니다.
- **Performance Highlights**: 제안된 SciPrompt 프레임워크는 기존의 최신 방법들과 비교했을 때, 특히 few-shot과 zero-shot 설정에서 뛰어난 성능을 보여줍니다. 이는 제한된 라벨 데이터 상황에서 대체로 우수한 성과를 나타냈습니다.

### [Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning](https://arxiv.org/abs/2410.00255)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.00255.png)

Vote: 2

Authors: Yan Yan, Yuzhang Shang, Haifeng Huang, Mubarak Shah, Weitai Kang

- **What's New**: 새로운 연구 Robin3D는 3D Large Language Model(3DLLM)로, Robust Instruction Generation(RIG) 엔진을 사용하여 생성된 대규모 지시 추종 데이터로 튜닝되었습니다. 두 가지 유형의 데이터를 생성하여 모델의 성능을 높였으며, 다양한 허브마크에서도 상태-최고(state-of-the-art) 성능을 달성했습니다.

### [Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models](https://arxiv.org/abs/2410.01782)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01782.png)

Vote: -

Authors: Md Asib Rahman, Md Rizwan Parvez, K S M Tozammel Hossain, Shafiq Joty, Enamul Hoque, Shayekh Bin Islam

- **What's New**: Open-RAG는 오픈 소스 대형 언어 모델(LLM)을 사용하여 Retrieval-Augmented Generation(RAG)의 Reasoning 능력을 향상시키기 위한 새로운 프레임워크를 제시합니다. 이 프레임워크는 효율적인 파라미터(PEFT) 희소 전문가 혼합 모델(MoE)로 LLM을 변환되고, 복잡한 추론 작업 및 다중 홉 쿼리를 처리하는 데 중점을 둡니다.
- **Technical Details**: Open-RAG는 자기반성 기반 생성 방식을 사용하여 모델의 동작을 제어하고 더 많은 컨텍스트 기반 응답 생성을 도모합니다. 'Retrieval', 'Relevance', 'Grounding', 'Utility'와 같은 네 가지 특수 반성 토큰을 사용하여 출력 어휘를 확장합니다. 또한, 하이브리드 적응형 검색 스키마(adaptive hybrid retrieval schema)를 개발하여 검색 빈도와 속도 간의 균형을 맞추는 방식을 채택했습니다. 이 스키마는 모델의 자신감을 기반으로 동적으로 검색 필요성을 결정합니다.
- **Performance Highlights**: Open-RAG는 이전 오픈 소스 기반 RAG 모델들보다 사실상의 정확성과 추론 능력을 크게 향상시켰으며, 종종 최첨단(SoTA) 독점 LLM과 그 RAG 모델들와 필적하거나 이를 능가했습니다. PopQA, TriviaQA 등 다양한 벤치마크에서 Open-RAG는 자주 ChatGPT-RAG, Self-RAG, RAG 2.0 등을 초과하는 성능을 보여줍니다.

