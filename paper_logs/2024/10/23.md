## Daily Papers (2024-10-23)

### [MiniPLM: Knowledge Distillation for Pre-Training Language Models](https://arxiv.org/abs/2410.17215)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.17215.png)

Vote: 4

Authors: ['Minlie Huang', 'Fandong Meng', 'Hao Zhou', 'Jie Zhou', 'Yuxian Gu']

- ***What's New***: 이 연구에서는 기존의 Knowledge Distillation(KD)이 가진 문제점을 해결하기 위한 새로운 프레임워크 MINIPLM을 소개했습니다. MINIPLM은 대형 언어 모델로부터 작은 학생 모델에 지식을 전이할 때 효율성, 유연성, 효과성을 모두 고려한 방법을 제공합니다.
- ***Technical Details***: MINIPLM은 Difference Sampling을 사용하여 훈련 데이터를 조정함으로써 대형 교사 모델의 지식을 소형 학생 모델에 효과적으로 전수합니다. 이는 온라인 교사 모델 추론 대신 오프라인을 사용하며, 여러 학생 모델에 대한 지식 전수 가능성을 높입니다. 또한, 큰 모델과 작은 모델 간의 예측 차이를 활용하여 데이터의 난이도와 다양성을 높입니다.
- ***Performance Highlights***: MINIPLM을 활용한 학생 모델들은 9가지의 다운스트림 테스크에서 고성능을 기록하며, Pre-Train without KD 대비 프리트레이닝 연산 비용을 2.2배 감소시켰습니다. 또한, MINIPLM은 제약된 데이터 상황에서도 데이터를 더 효율적으로 사용하여 2.4배 더 적은 데이터로 동일한 성능을 달성할 수 있었습니다.

### [3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors](https://arxiv.org/abs/2410.16266)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16266.png)

Vote: 0

Authors: ['Xi Liu', 'Siyu Huang', 'Chaoyi Zhou']

- ***What's New***: 3DGS-Enhancer는 시야 일관성을 유지한 2D 확산 프라이어(2D Diffusion Priors)을 활용하여 무제한 3D 가우시안 분할(3D Gaussian Splatting)을 개선하는 새로운 파이프라인을 소개합니다. 이는 입력 시야가 드문 경우에도 고품질의 새로운 시점을 생성하도록 돕습니다.
- ***Technical Details***: 3DGS-Enhancer는 2D 비디오 확산 모델(Video Latent Diffusion Models; LDM)을 활용하여 3D 시점 일관성 문제를 해결합니다. 이 모델은 렌더링된 새로운 시점의 잠재적 특징을 복원하고 공간-시간 디코더(Spatial-Temporal Decoder)를 통해 입력 시점과 통합합니다. 향상된 시점을 사용하여 초기 3DGS 모델을 미세 조정함으로써 렌더링 성능을 크게 향상시킵니다.
- ***Performance Highlights***: 3DGS-Enhancer는 대규모 무제한 장면 데이터셋 실험에서 최첨단 방법들과 비교할 때 우수한 재구성 성능과 높은 충실도의 렌더링 결과를 보여줍니다. 이는 PSNR, SSIM, LPIPS 등 다양한 지표에서 기존 방법들보다 높은 점수를 기록하며, 특히 드문 시야에서의 고주파 디테일 복원에 탁월한 성능을 발휘합니다.

### [EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search](https://arxiv.org/abs/2410.14649)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14649.png)

Vote: 4

Authors: ['Eldar Kurtic', 'Denis Kuznedelev', 'Oliver Sieberling', 'Dan Alistarh']

- ***What's New***: EvoPress는 대형 언어 모델(LLM; Large Language Models)의 최적 동적 모델 압축을 위한 진화적 탐색 프레임워크를 제안합니다. 기존의 계층 단위의 압축 기법이 의존하던 단순한 오류 척도에 기반하지 않고, 에러 모노토닉성(오류 단일성)이 항상 성립하지 않음을 관측하여, 최적화된 동적 압축을 제공합니다.
- ***Technical Details***: EvoPress는 계층별로 다양한 압축 수준을 할당하는 동적 압축 기법으로, 진화적 알고리즘을 사용하여 탐색을 수행하며, 수렴성을 증명하고 샘플과 평가의 복잡도가 낮습니다. 압축 성능을 평가할 때에는 Kullback-Leibler(KL) 발산을 사용하는 회귀 신호로 데이터의 제한된 부분을 사용하며, 이러한 방식으로 Llama, Mistral, Phi 모델의 압축 효율성을 입증합니다.
- ***Performance Highlights***: EvoPress는 구조적 가지치기, 비구조적 희소화, 동적 비트 수를 가진 양자화를 포함한 모든 압축 접근 방식에서 매우 경쟁력 있는 성능을 보여주었습니다. 해당 알고리즘은 5-6시간 내 RTX 3090 GPU에서 높은 압축 레벨에서도 수렴하며, 적은 샘플을 활용하여 약 1시간 만에 수렴하는 경량 버전도 제공합니다. 특히, 이전 최고 성능을 자랑하던 OWL과 동적 프로그래밍을 능가하는 결과를 보였습니다.

### [Aligning Large Language Models via Self-Steering Optimization](https://arxiv.org/abs/2410.17131)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.17131.png)

Vote: 11

Authors: ['Jingren Zhou', 'Hao Xiang', 'Xianpei Han', 'Hongyu Lin', 'Keming Lu', 'Bowen Yu', 'Le Sun', 'Yaojie Lu', 'Junyang Lin']

- ***What's New***: Self-Steering Optimization(SSO)은 사람이 주석을 달지 않고 자동 조정 시스템을 개발하려는 시도로 도입된 알고리듬입니다. SSO는 사전 정의된 원칙에 따라 학습 가능한 고품질의 선호 신호를 자동으로 생성하여, 인간의 개입 없이 대규모 언어 모델(LLM)의 정렬을 개선합니다.
- ***Technical Details***: SSO는 정책 모델의 학습 능력에 맞춰 선택된 응답과 거부된 응답 사이에 일관된 갭을 유지하면서, 정책 모델로부터 직접 신호를 추출하여 정확한 '다음-정책(on-policy)' 선호 신호를 생성합니다. 이를 위해 정책 모델은 주어진 쿼리와 대조 원칙을 기반으로 응답을 수집하고, 세 가지 주요 목표에 따라 모델을 최적화합니다.
- ***Performance Highlights***: SSO는 추가적인 인간 주석 없이도 6개의 주관적, 객관적 벤치마크 전반에 걸쳐 Qwen2 및 Llama3.1 모델의 성능을 유의미하게 향상시켰습니다. 또한 보상 모델의 성능을 대폭 향상시켰으며, 사후 처리 데이터 세트를 통해 제공될 예정입니다.

### [PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction](https://arxiv.org/abs/2410.17247)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.17247.png)

Vote: 30

Authors: ['Yuhang Zang', 'Feng Wu', 'Jiajie Lu', 'Dahua Lin', 'Yuhang Cao', 'Xiaoyi Dong', 'Jiaqi Wang', 'Conghui He', 'Long Xing', 'Pan Zhang', 'Qidong Huang']

- ***What's New***: PyramidDrop는 대형 시각-언어 모델(Large Vision-Language Models; LVLMs)의 시각적 중복성을 줄여, 훈련 및 추론의 효율성을 향상시키는 새로운 전략을 제안합니다. 이 방법은 모델의 각 단계 끝에서 미리 정의된 비율에 따라 일부 이미지 토큰을 제거하여 피라미드 형태의 토큰 구조를 형성합니다.
- ***Technical Details***: PyramidDrop은 LVLM을 여러 단계로 나누어 각 단계의 끝에서 이미지 토큰을 경량 유사성 계산을 통해 순위를 매겨 일부를 떨어뜨립니다. 초기 얕은 층에서는 모든 이미지 토큰을 유지하지만 점차적으로 층이 깊어질수록 불필요한 중복성을 제거해, 훈련과 추론 효율성을 최대화합니다.
- ***Performance Highlights***: PyramidDrop을 적용한 LLaVA-NeXT 모델은 40%의 훈련 시간 절감과 55%의 추론 FLOPs 가속을 이루었습니다. 성능 면에서도 기존 방법대비 유사한 수준을 유지하며, 높은 해상도의 벤치마크에서도 보다 나은 결과를 보여줍니다. 또한, PyramidDrop은 훈련 없이도 추론 가속화를 위한 플러그 앤 플레이 전략으로 사용될 수 있으며, 상대적으로 낮은 추론 비용으로 우수한 성능을 제공합니다.

### [SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes](https://arxiv.org/abs/2410.17249)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.17249.png)

Vote: 24

Authors: ['Yi-Ruei Liu', 'Cheng-De Fan', 'Yu-Lun Liu', 'Jie-Ying Lee', 'Chen-Wei Chang', 'Jiun-Long Huang', 'Yu-Chee Tseng']

- ***What's New***: SpectroMotion은 3D Gaussian Splatting (3DGS)과 물리 기반 렌더링 (Physically-Based Rendering; PBR) 및 변형 필드 (Deformation Fields)를 결합하여 동적 섬광 장면 (Dynamic Specular Scenes)을 재구성하는 새로운 접근 방식을 제안합니다. 이는 사물을 변형하는 동안의 표면 법선을 정확하게 계산하기 위한 잔여 보정 기법 및 시간 변화 조명 조건에 적응하는 변형 가능한 환경 지도를 통합하여 기존 방법들의 한계를 극복합니다.
- ***Technical Details***: SpectroMotion은 3DGS와 PBR을 결합하여 동적 대부분 장면을 조합합니다. 정확한 표면 법선 계산을 위한 잔여 보정 기법과 시각 변화를 반영하는 변형 가능한 환경 지도를 제공하며, 계층적-세부 (Coarse-to-Fine) 훈련 전략을 사용하여 장면 기하학 및 섬광 색상 예측을 향상시킵니다. 이를 통해 복잡한 동적 반사 장면의 고해상도 뷰 합성을 가능하게 합니다.
- ***Performance Highlights***: SpectroMotion은 NeRF-DS 데이터 세트에서 기존 방법에 비해 향상된 PSNR, SSIM 및 LPIPS 점수를 달성하며, 특히 동적 섬광 물체의 합성에서는 가장 높은 정밀도를 기록합니다. 이는 다이나믹하고 섬광적인 장면의 렌더링에서 최고 수준의 방법들을 능가하는 성능을 보여 주며, 복잡한 시나리오에서의 견고한 재구성을 제공함을 입증합니다.

### [JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation](https://arxiv.org/abs/2410.17250)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.17250.png)

Vote: 7

Authors: ['Kazuki Egashira', 'Kiyoharu Aizawa', 'Xiang Yue', 'Graham Neubig', 'Yuki Imajuku', 'Shota Onohara', 'Atsuyuki Miyai', 'Jeonghun Baek']

- ***What's New***: JMMMU는 일본어 문화 맥락에서 LMMs(Large Multimodal Models)를 평가하기 위해 설계된 최초의 대규모 일본어 벤치마크입니다. 이는 문화 무관 주제(culture-agnostic subjects)와 일본 문화에 특화된 주제(culture-specific subjects) 두 개의 보완적 서브셋으로 구성되어 있습니다.
- ***Technical Details***: JMMMU는 28개의 다양한 과목을 다루며, 총 1,320개 질문과 1,118개의 이미지를 포함합니다. 문화 무관 서브셋은 24개의 과목에서 720개의 질문으로 구성되며, 이는 영어 벤치마크인 MMMU와 직접 비교할 수 있는 방식으로 일본어로 번역되었습니다. 문화 특화 서브셋은 일본 전통 예술, 유산, 역사 등을 반영하는 질문들로 구성되어 있습니다.
- ***Performance Highlights***: JMMMU에서 평가한 결과, 오픈 소스 모델의 전체 성능은 최고 40.5%에 그쳤으며, 독점 모델은 최대 58.6%에 도달하여 개선의 여지가 큽니다. 특히, 문화 무관 서브셋에서 영어에 비해 일본어로 질문할 때 성능이 떨어지는 경향이 관찰되었습니다. 반면, 일본어 데이터로 훈련된 일부 모델은 그 격차를 줄였습니다. GPT-4o와 Claude 3.5 Sonnet 간의 문화 특화 질문에서 상당한 성능 차이가 드러나, 초기의 번역 기반 평가만으로는 모델의 진정한 다중언어 및 문화 이해를 파악하기 어려움을 지적하였습니다.

### [Mitigating Object Hallucination via Concentric Causal Attention](https://arxiv.org/abs/2410.15926)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.15926.png)

Vote: 3

Authors: ['Yun Xing', 'Ivan Laptev', 'Yiheng Li', 'Shijian Lu']

- ***What's New***: Concentric Causal Attention (CCA)라는 새로운 위치 정렬 전략을 제안하여 대형 비전-언어 모델(Large Vision-Language Models; LVLMs)에서 발생하는 객체 환각(Object Hallucination)을 완화합니다. 기존의 회전 위치 인코딩(Rotary Position Encoding; RoPE)로 인한 장기 감쇠 문제를 해결하며, 시각적 토큰과 명령어 토큰 간의 상대적 거리를 줄여 모델의 인식 능력을 향상시키고, 환각 현상을 줄입니다.
- ***Technical Details***: CCA는 시각적 토큰을 조직화하는 위치 재조정 모듈과 2D 연속 위치 의존성을 모델링하는 인과 마스크 수정 모듈로 구성되어 있습니다. 기존 LVLMs의 행렬 스캔 순서를 따르지 않고, 2D 이미지의 주변부에서 중앙으로 집중하는 순서로 시작하여, 상대 거리를 줄이고 2D 공간적 지역성을 유지합니다. 모델은 Vicuna-7B를 기반으로 사전 훈련되었으며, 기본적으로 LLaMA 모델에서 사용되는 로터리 행렬이 적용된 시각-명령어 상호 작용을 개선합니다.
- ***Performance Highlights***: 다수의 객체 환각 벤치마크에서 CCA-LLaVA 모델은 기존의 환각 경감 방법보다 여러 데이터셋에서 최대 7.86%까지의 정확도 개선을 보여주었습니다. CHAIR 평가에서는 긴 텍스트 응답 생성에서 3.2% 개선을 기록하였고, MME의 4가지 지각 하위 과제에서도 총 76.33의 향상을 보였습니다. 이는 CCA 전략이 전반적인 인식 능력을 개선할 수 있음을 시사합니다.

### [xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs](https://arxiv.org/abs/2410.16267)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16267.png)

Vote: 5

Authors: ['Can Qin', 'Le Xue', 'Juan Carlos Niebles', 'Ran Xu', 'Caiming Xiong', 'Silvio Savarese', 'Manli Shu', 'Shrikant Kendre', 'Michael S. Ryoo', 'Honglu Zhou']

- ***What's New***: xGen-MM-Vid(BLIP-3-Video)는 비디오 데이터를 처리하기 위한 새로운 비전-언어 모델입니다. 이 모델의 새로운 점은 'temporal encoder'를 도입하여 시간적 정보를 훨씬 적은 수의 비주얼 토큰(visual tokens)으로 표현할 수 있게 했다는 점입니다. 이전의 모델들이 수천 개의 토큰을 사용한 데 비해, xGen-MM-Vid는 32개의 비주얼 토큰 만으로도 비디오를 효과적으로 표현하는 데 성공했습니다.
- ***Technical Details***: BLIP-3-Video 모델은 기존의 이미지 기반 비전-언어 모델인 BLIP-3에 기반하여 개발되었습니다. 이 모델은 (1) 각 프레임을 입력으로 받는 비전 인코더(SigLIP), (2) 프레임별 토큰 수를 줄이기 위한 토크나이저, (3) 비디오 수준 토큰 표현을 만드는 'temporal encoder', (4) 비디오 토큰과 텍스트 프롬프트 토큰을 기반으로 출력 텍스트 캡션을 생성하는 autoregressive LLM으로 구성됩니다. 특히, 이 모델은 TokenLearner와 Token Turing Machines(TTM)과 같은 학습 가능한 공간-시간 어텐션 풀링 방법을 temporal encoder로 활용했습니다.
- ***Performance Highlights***: BLIP-3-Video는 4B의 파라미터로 34B 수준의 최신 모델들과 동등한 비디오 질문-응답 정확도를 달성했습니다. 특히 MSVD-QA와 같은 공개 벤치마크에서, 128개의 비주얼 토큰을 사용해도 안정적인 성능을 보여주었으며 이는 다른 모델들이 사용하는 수천 개의 토큰보다 훨씬 효율적입니다. 이로 인해 텍스트+비디오 입력에 대한 텍스트 생성 작업에서 높은 성능과 효율성을 동시에 얻을 수 있습니다.

### [Improve Vision Language Model Chain-of-thought Reasoning](https://arxiv.org/abs/2410.16198)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16198.png)

Vote: 5

Authors: ['Zhe Gan', 'Ruohong Zhang', 'Bowen Zhang', 'Yiming Yang', 'Ruoming Pang', 'Zhiqing Sun', 'Haotian Zhang', 'Yanghao Li', 'Yinfei Yang']

- ***What's New***: 이 논문은 Vision Language Model(VLM)의 Chain-of-thought(CoT) reasoning을 개선하기 위한 새로운 접근 방법을 제안합니다. VLM이 복잡한 문제를 해결하기 위한 추론 능력을 향상시키기 위해 GPT-4o 모델에서 논리를 증류하여 학습 데이터를 풍부하게 하며, 강화학습을 적용하여 추가적인 추론 품질을 조정합니다.
- ***Technical Details***: 이 접근법은 두 가지 주요 단계를 포함합니다. 첫째, GPT-4o 모델을 활용하여 논리를 풍부하게 만들어 VLM을 미세 조정합니다(Supervised Fine-tuning; SFT). 둘째, 모델이 생성한 추론 사슬의 긍정(올바른) 및 부정(잘못된) 쌍을 구성하여 Direct Preference Optimization(DPO) 알고리즘을 적용함으로써 모델의 추론 능력을 세밀화합니다.
- ***Performance Highlights***: 실험 결과, 이 방법론은 여러 벤치마크 데이터셋에서 CoT reasoning의 성능을 크게 향상시켰으며, 직접적인 응답 예측에 대한 더 나은 일반화 성능을 보여주었습니다. 특히, 193,000개의 CoT 예제를 사용하여 LLAVA-REASONER-SFT 모델이 VLM CoT reasoning 성능을 의미 있게 개선한다는 점을 입증했습니다.

### [Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes](https://arxiv.org/abs/2410.16930)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16930.png)

Vote: 1

Authors: ['Zack Gottesman', 'Jonathan Kropko', 'Bryan R. Christ', 'Thomas Hartvigsen']

- ***What's New***: MathNeurosurgery (MathNeuro)는 LLMs의 수학적 추론 능력을 전진 패스만 사용하여 수학에 특화된 매개변수를 격리하는 새로운 방법입니다. 이는 일반 언어 작업에 중요한 매개변수를 제거하여 수학 관련 매개변수만을 격리하며, 매개변수의 선택적 조정을 통해 수학적 성능을 개선하면서도 다른 작업의 성능은 변함 없이 유지합니다.
- ***Technical Details***: MathNeuro는 Wanda 방법에 기반하여 가중치와 활성화를 통해 매개변수 중요도를 계산합니다. 수학 작업에 중요한 매개변수를 식별한 후, 다른 일반 언어 이해 작업에 중요한 매개변수는 제거합니다. 이렇게 식별된 매개변수를 절단한 경우 모델의 수학적 추론 능력은 효과적으로 저하되며, 이 식별 매개변수를 작은 일정한 상수로 확장하면 수학적 성능이 4-17% 향상됩니다.
- ***Performance Highlights***: MathNeuro는 Phi 1.5, Llama 3.2 1B IT, Gemma 2 2B IT, Llama 3.2 3B IT, Llama 3.1 8B IT 등 최신 LLMs를 통해 수학에 특화된 매개변수를 효과적으로 식별하여 모델의 수학적 성능을 강화하는 결과를 보여주었습니다. 또한, 대부분의 수학적인 성능 향상은 단일 샘플을 사용하여 매개변수를 식별할 때에도 유지되어 데이터 효율성이 뛰어납니다.

### [Frontiers in Intelligent Colonoscopy](https://arxiv.org/abs/2410.17241)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.17241.png)

Vote: 1

Authors: ['Fahad Shahbaz Khan', 'Peng Xu', 'Ge-Peng Ji', 'Jingyi Liu', 'Deng-Ping Fan', 'Salman Khan', 'Nick Barnes']

- ***What's New***: 이 논문은 대장내시경 기술의 최신 지능화된 발전과 다중모달 의료 응용에 관한 새로운 연구를 조사합니다. 저자들은 대장내시경 장면 인식을 위한 네 가지 작업, 즉 분류(Classification), 탐지(Detection), 분할(Segmentation), 시각-언어 이해(Vision-Language Understanding)을 통해 현재의 데이터 중심 및 모델 중심 환경을 평가하여 도메인 특유의 도전과 기회를 식별했습니다.
- ***Technical Details***: 이 연구에서는 대규모 다중모달 인스트럭션 튜닝 데이터셋(ColonINST), 대장내시경에 특화된 다중모달 언어 모델인 ColonGPT, 다중모달 벤치마크를 포함하여 세 가지 기본 이니셔티브를 설립합니다. 이 데이터셋은 19개의 공개 소스에서 303,001개의 대장내시경 이미지를 수집하고, 대장내시경 절차에서 다양한 시나리오를 반영합니다. 또한, 연구팀은 설명형 의료 캡션을 생성하기 위해 GPT-4V를 활용하여 450,724개의 인간-기계 대화 쌍을 구성하였습니다.
- ***Performance Highlights***: ColonGPT 모델은 LLaVA-Med 및 LLaVA와 같은 기존의 AI 모델을 능가하여, CLS 작업에서는 85.81%, REG 작업에서는 83.42%의 정확도를 기록했습니다. 또한, ColonGPT는 최소한의 데이터로 신속하게 학습할 수 있어 프로토타입 개발 시간을 5시간으로 단축시킵니다. 이 모델은 대장내시경에서 시각적 해석 및 일상적 작업 수행에 유리한 능력을 보여줍니다.

