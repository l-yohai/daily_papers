## Daily Papers (2024-10-29)

### [Fast Best-of-N Decoding via Speculative Rejection](https://arxiv.org/abs/2410.20290)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20290.png)

Vote: 5

Authors: Peter Bartlett, Ming Yin, Momin Haider, Andrea Zanette, Jiahao Qiu, Hanshi Sun, Mengdi Wang, Ruiqi Zhang, Huitao Yang

- ***What's New***: 본 논문에서는 새로운 추측거절(Speculative Rejection) 알고리즘을 제안하여 대형언어모델(LLM)의 추론 시간 정렬(inference-time alignment) 성능을 크게 향상시켰습니다. 이는 기존 Best-of-N 방법과 비교하여 16에서 32배 빠른 계산 효율성을 제공합니다.
- ***Technical Details***: 추측거절(Speculative Rejection) 알고리즘은 초기적으로 큰 배치 사이즈를 사용하여 Best-of-N의 초기 단계를 시뮬레이션합니다. 이후, 보상 모델에 따라 낮은 점수를 받을 가능성이 있는 문장 생성을 조기에 중단하여 메모리 사용을 최적화하며, 고품질 응답 생성을 보장합니다.
- ***Performance Highlights***: 실험 결과, 추측거절 알고리즘은 단일 GPU에서 실행할 수 있으며, 비슷한 지연 시간 내에 Best-of-N 방법이 16~32개의 GPU를 필요로 하는 수만큼의 보상을 달성했습니다. AlpacaFarm 데이터를 활용한 실험에서는 다양한 생성 및 보상 모델을 사용하여 이 알고리즘의 효율성과 효과성을 입증했습니다.

### [Language Models And A Second Opinion Use Case: The Pocket Professional](https://arxiv.org/abs/2410.20636)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20636.png)

Vote: 1

Authors: David Noever

- ***What's New***: 본 연구는 전문적 의사결정에서 대형 언어 모델(LLMs)의 역할을 두 번째 의견 도구로 테스트하여 복잡한 의료 사례에서의 적용 가능성을 평가했습니다. 특히, Medscape에서 183개의 복잡한 의료 사례를 분석하고 LLMs의 성능을 의사들의 군중 소싱 응답과 비교했습니다. 최신 모델의 정확도가 80% 이상을 기록하여 대부분의 인간 메트릭을 초과했습니다. 연구는 LLMs가 2차 의견 생성기 역할을 하여 임상 결정에서 인지 편향을 완화할 가능성을 보여줍니다.
- ***Technical Details***: 이 연구는 Medscape의 20개월 기간 동안의 사례를 분석하여 183개의 도전적 사례를 도출했습니다. 각 사례는 임상의에 의해 제출된 진단 상담을 위한 상세한 임상 표현으로 구성되며, 450페이지의 환자 프로파일, 검사 결과 및 치료 권고 사항들로 구성됩니다. 연구는 닫힌 소스의 기초 모델과 개방형 모델, 그리고 시각-기반 다중 모드 모델을 사용하여 평가했습니다. 표준화를 위해 초기 평가에서 텍스트 전용 사례 이력을 사용하여 모든 모델 간의 비교 가능성을 보장했습니다.
- ***Performance Highlights***: 기초 대형 모델은 명확한 진단 기준이 있는 사례에서는 81% 이상의 높은 정확도를 보였으나, 비전형적 사례에서는 43%로 성능이 급격히 감소했습니다. 인문적-어려운 사례들에서는 모델의 합의 일치가 가장 낮았습니다. LLM은 주로 기존 임상 의사결정 트리와 연관된 설명을 제공하여 다른 진단 가능성을 포괄적으로 탐색하는 능력을 보였습니다.

### [Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction](https://arxiv.org/abs/2410.18481)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18481.png)

Vote: 3

Authors: Srikanth Madikeri, Sergio Burdisso, Petr Motlicek

- ***What's New***: Dialog2Flow(D2F)는 대화 흐름을 자동으로 추출하기 위한 새로운 문장 임베딩 방법론으로, 문장을 의사소통 및 정보 제공 기능에 따라 그룹화하여 라텐트 공간에 매핑합니다. 이는 대화를 연속적인 궤적으로 모델링하여 하부 워크플로우를 확인하는 데 중요한 역할을 합니다.
- ***Technical Details***: D2F는 20개의 작업 지향 대화 데이터셋을 통합하여 표준화된 액션 주석으로 구성된 종합적인 데이터셋을 통해 사전 훈련되었습니다. 소프트 콘트라스티브 손실(Soft Contrastive Loss)을 도입하여 액션의 의미론적 정보를 활용, 표현 학습에 대한 지도가 됩니다. 이는 기존의 감독 대조 손실법보다 우수한 성능을 보입니다.
- ***Performance Highlights***: D2F는 일반적 문장 임베딩 및 대화별 문장 임베딩을 포함한 여러 임베딩과의 평가에서 다양한 도메인에서 뛰어난 정성적 및 정량적 결과를 보여주었습니다. 이는 대화 흐름을 더 정확하게 추출하여 대화 시스템 디자인을 향상시키는 잠재력을 갖고 있음을 시사합니다.

### [A Survey of Small Language Models](https://arxiv.org/abs/2410.20011)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20011.png)

Vote: 13

Authors: Joe Barrow, Yu Wang, Tong Yu, Ryan A. Rossi, Hanieh Deilamsalehy, Ryan Aponte, Junda Wu, Huanrui Yang, Zhengmian Hu, Mihir Parmar, Franck Dernoncourt, Sungchul Kim, Xuan Shen, Sasidhar Kunapuli, Ashish Singh, Nedim Lipka, Thien Huu Nguyen, Chien Van Nguyen, Mike Rimer, Jiuxiang Gu, Nesreen K. Ahmed, Samyadeep Basu, Ruiyi Zhang, Yu Xia, Zhehao Zhang, Namyong Park, Xiang Chen, Jian Chen

- ***What's New***: 이 논문은 작은 언어 모델(Small Language Models; SLMs)에 대한 종합적인 조사 보고서를 제공하며, 특히 SLM의 효율성과 성능이 커지는 이유를 설명합니다. 대형 언어 모델의 자원 소모 문제를 해결하기 위해 작은 용량의 모델이 뜨고 있는 배경도 설명합니다.
- ***Technical Details***: 이 논문에서는 모델 압축, 가지치기(pruning), 양자화(quantization) 등의 방법론을 포함하여 SLM을 최적화하기 위한 기술들을 새로운 분류체계로 제시하고, 각 기술의 사전처리(모델 아키텍처), 훈련, 후처리(모델 압축) 과정에서의 사용에 대해 설명합니다. 아울러, 각 기술이 최적화하는 제약 조건에 따라 분류합니다. 또한, SLM의 효율적인 훈련 방법 및 파인튜닝과 관련된 최신 기법들을 다룹니다.
- ***Performance Highlights***: MobileLLM과 같은 최신의 소형 LLM들은 낮은 메모리 사용과 높은 처리량을 제공하며, 효율적인 온-디바이스(on-device) AI 애플리케이션에 활용될 수 있습니다. Apple 등의 기업은 다양한 작업에 슬림한 모델을 적용하여 훈련 및 실행 비용을 줄이고 있습니다.

### [LongReward: Improving Long-context Large Language Models with AI Feedback](https://arxiv.org/abs/2410.21252)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21252.png)

Vote: 13

Authors: Zhenyu Hou, Xin Lv, Shulin Cao, Yilin Niu, Lei Hou, Jiajie Zhang, Yuxiao Dong, Ling Feng, Juanzi Li, Zhongni Hou

- ***What's New***: LongReward는 기존 LLM을 이용하여 장문 문맥 상황에서 AI 모델 반응에 대한 보상을 제공하는 새로운 방법입니다. 이는 추가적인 강화 학습(RL)을 통해 모델의 성능을 향상시키며, 사람들이 가치 있게 여기는 네 가지 차원인 도움성(helpfulness), 논리성(logicality), 신뢰성(faithfulness), 그리고 완전성(completeness)에 따라 모델 반응을 평가합니다.
- ***Technical Details***: LongReward는 오프라인 RL 알고리즘인 DPO와 결합되어 장문 문맥 모델의 결함을 효과적으로 줄이고 성능을 향상시킵니다. 모델 반응은 주어진 문맥과 함께 학습된 LLM을 사용하여 각 차원별로 0에서 10까지 점수로 평가되고 이 평균이 최종 보상으로 사용됩니다. 특히, 도움성과 논리성은 주어진 쿼리와 반응의 심리적 학습을 통해 평가합니다.
- ***Performance Highlights***: Llama-3.1-8B와 GLM-4-9B 모델에서 LongReward를 사용한 DPO 모델들은 각각 4.9%와 5.5%의 성능 향상을 보였으며, 이는 기존의 SFT 모델을 능가하는 결과입니다. 또한 LongReward는 모델의 짧은 명령 추종 능력도 향상시켰고, 장문 문맥 DPO와 표준 단문 문맥 DPO를 결합하여 양쪽의 성능 저하 없이 공동으로 성능을 개선할 수 있음을 보여주었습니다.

### [Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation](https://arxiv.org/abs/2410.18565)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18565.png)

Vote: 34

Authors: Adrian Gwoździej, Krzysztof Ociepa, Remigiusz Kinas, Łukasz Flis, Krzysztof Wróbel

- ***What's New***: Bielik 7B v0.1은 폴란드어 자연어 처리(Natural Language Processing; NLP)를 위한 7억 파라미터의 텍스트 생성 모델로서, 폴란드어 텍스트에 대한 이해와 생성 능력을 대폭 향상시켰습니다. 이 모델은 가중 지시 교차 엔트로피 손실(Weighted Instruction Cross-Entropy Loss) 및 적응형 학습률(Adaptive Learning Rate) 등의 혁신적인 기술을 통해 언어 모델 개발의 핵심 과제를 해결했습니다.
- ***Technical Details***: Bielik 7B v0.1 모델은 Transformer 아키텍처를 기반으로 하며, 자기 주의 메커니즘(Self-attention with causal masks), 그룹화된 쿼리 주의 메커니즘(Grouped-query attention), 슬라이딩 윈도우 주의 메커니즘(Sliding Window Attention) 등을 사용하여 성능을 향상시켰습니다. 기본적으로 Mistral 7B v0.1 모델에 기반을 두어 폴란드어를 이해하고 생성할 수 있도록 개선되었습니다.
- ***Performance Highlights***: Bielik 7B v0.1 모델은 다양한 NLP 작업에서 평균 점수가 Mistral-7B-v0.1와 비교하여 9 퍼센트포인트 증가하는 성과를 보여주었습니다. 폴란드 MT-Bench에서는 Reasoning에서 6.15/10, Role-playing에서 7.83/10을 기록하며 우수한 결과를 나타냈습니다. 이 모델은 특히 RAG Reader 작업에서 뛰어난 성능을 발휘했습니다.

### [COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training](https://arxiv.org/abs/2410.19313)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.19313.png)

Vote: 10

Authors: Kurt Keutzer, Ligeng Zhu, Yao Lu, Song Han, Haocheng Xi, Jianfei Chen, Han Cai

- ***What's New***: COAT는 대형 모델을 메모리 효율적으로 FP8 정밀도로 훈련하기 위해 옵티마이저 상태와 활성화(Activation)를 압축한 혁신적인 훈련 프레임워크를 제안합니다. 이는 기존의 BF16과 비교하여 메모리 사용량을 1.54배 줄이고, 훈련 속도를 1.43배 향상시키며, 수천억 매개변수 트랜스포머 모델의 훈련을 더 적은 GPU로 가능하게 만듭니다.
- ***Technical Details***: COAT는 FP8 훈련의 단점을 해결하기 위해 동적 범위 확장(Dynamic Range Expansion)과 혼합 세분화 활성화 양자화(Mixed-Granularity Activation Quantization)라는 두 가지 주요 기술을 도입합니다. 동적 범위 확장은 FP8 표현 범위와 맞추어 옵티마이저 상태의 분포를 조정하여 양자화 오류를 최소화합니다. 또한, 비선형 층의 세분화된 양자화를 통해 정확도를 유지하는 동시에 효율성을 높입니다.
- ***Performance Highlights***: COAT는 다양한 작업에서 거의 손실 없는 성능을 달성하면서 BF16과 비교하여 메모리 사용량을 최대 1.55배 줄이고 훈련 속도를 1.44배 향상시켰습니다. 이는 대형 언어 모델(LLM)과 비전 언어 모델(VLM)의 훈련 및 미세 조정에서도 일관된 성능을 유지하며, 대규모 모델 훈련을 위한 실용적인 솔루션을 제공합니다.

### [Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation](https://arxiv.org/abs/2406.10615)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.10615.png)

Vote: 2

Authors: Jiacheng You, Yingdong Hu, Tong Zhang, Yang Gao

- ***What's New***: SGRv2는 로봇 조작에서 샘플 효율성을 높이기 위해 시각적 및 행동 표현을 개선한 모방 학습(imitation learning) 프레임워크입니다. SGRv2의 핵심 디자인은 로봇의 행동이 주로 목표 객체와 해당하는 지역 환경과의 상호작용에 의해 영향을 받는다는 '행동 지역성(action locality)'이라는 중요한 편향을 포함합니다.
- ***Technical Details***: SGRv2는 SGR의 기초 위에 구축된 시각운동 정책 프레임워크로, 포인트별 특징을 추출하는 인코더-디코더 아키텍처, 그리고 목표 위치를 예측하기 위한 전략, 중요한 지역을 강조하기 위한 포인트별 가중치 적용, 학습 효율성을 높이기 위한 밀집 감독 방법을 통합합니다. RLBench, ManiSkill2, MimicGen과 같은 벤치마크에서 실험한 결과, SGRv2가 SGR을 포함한 여러 베이스라인을 능가하며 높은 성능을 보였습니다.
- ***Performance Highlights***: RLBench에서 5개의 시연으로 SGRv2는 평균 성능의 53.2%를 달성하며 가장 경쟁력 있는 베이스라인인 RVT를 상당히 능가합니다. ManiSkill2와 MimicGen에서는 단밀제어를 사용할 때 성공률이 기존 SGR의 2.54배에 달하며, 실제 환경에서도 8회의 시연으로 기본 모델에 비해 현저히 높은 성공률을 기록했습니다.

### [GPT-4o System Card](https://arxiv.org/abs/2410.21276)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21276.png)

Vote: 31

Authors: Sam Schoenholz, Jay Chen, Sean Grove, Tomer Kaftan, Veit Moeller, Alex Beutel, Miles Brundage, Reah Miyara, Henrique Ponde de Oliveira Pinto, Brad Lightcap, Will Sheu, Filippo Raso, Ishaan Gulrajani, Ji Lin, Rocky Smith, Thomas Cunninghman, Adam P. Goucher, Joyce Lee, Kevin Liu, Daniel Levy, Natalia Gimelshein, Mo Bavarian, Daniel Kappler, Casey Chu, Noah Deutsch, Ananya Kumar, Mark Chen, Lukasz Kaiser, Romain Huet, Alexi Christakis, Andrea Vallone, Cary Bassin, Michael Wu, Chan Jun Shern, Jacob Coxon, Rachel Dias, Antonia Woodford, Avi Nayak, Jane Park, Oleg Boiko, Aleksander Mądry, Thomas Raoux, Chak Li, Nik Tezak, Maddie Simens, Tyce Walters, Evan Mays, Ian O'Connell, Ian Kivlichan, Nick Stathas, Chelsea Voss, Randall Lin, Pavel Belov, Ian Silber, Rajan Troll, Kavin Karthik, Ibrahim Okuyucu, Olivia Watkins, Mayank Gupta, Toki Sherbakov, Joaquin Quinonero Candela, Tyna Eloundou, Ben Rossen, Wojciech Zaremba, Wesam Manassra, Duncan Findlay, Steve Lee, Philippe Tillet, Stewart Hall, Tom Rubin, Prafulla Dhariwal, Kenny Hsu, Aaron Hurst, Tejal Patwardhan, Kiel Howe, Peter Bakkum, Joshua Achiam, Dev Valladares, Lindsay McCallum, Vlad Fomenko, Allison Tam, Ben Wang, Miles Wang, Jiayi Weng, Huiwen Chang, Hongyu Ren, Wayne Chang, Kevin Button, Mianna Chen, Jacob Menick, Heather Whitney, Andrey Mishchenko, Michael Lampe, Reimar Leike, Minal Khan, Madelaine Boyd, Roy Chen, Anuj Gosalia, Christopher Hesse, Heewoo Jun, Elizabeth Yang, Scott Ethersmith, Fred von Lohmann, Luke Metz, Kenny Nguyen, Joanne Jang, Jessica Gan Lee, Lia Guy, Thomas Shadwell, Josh Gross, Shino Jomoto, Jiahui Yu, Andrew Kondrich, Niko Felix, Yunxing Dai, Lukas Kondraciuk, Meghan Shah, Johannes Heidecke, Avital Oliver, Ikai Lan, Katy Shi, Nitish Keskar, Yilei Qian, Antoine Pelisse, Jonathan Lachman, Jason Wei, Neil Chowdhury, Raul Puri, Molly Lin, Tristan Heywood, Chen Ding, Spencer Papay, Doug Li, Sam Altman, Jan Leike, Tao Xu, Luke Hewitt, Lu Zhang, Todd Underwood, Denny Jin, Beth Hoover, James Aung, Owen Campbell-Moore, Josh Snyder, Li Jing, Lindsey Held, Barret Zoph, Natalie Cone, Jie Tang, Rachel Lim, Tianhao Zheng, Andrew Cann, Geoff Salmon, Leher Pathak, Christian Gibson, Suchir Balaji, Adam Lerer, Srinivas Narayanan, Michelle Pokrass, Rob Donnelly, Rapha Gontijo Lopes, Jonathan Uesato, Valerie Qi, Keren Gu-Lemberg, Gabriel Goh, Youlong Cheng, Giulio Starace, Ruby Chen, Mark Hudnall, Constantin Koumouzelis, Lauren Itow, Jonathan Ward, Larry Kai, Michael Petrov, Phil Tillet, Farzad Khorasani, Tal Stramer, Christine McLeavey, Weiyi Zheng, Adam Perelman, Peter Hoeschele, Ola Okelola, Kai Hayashi, James Crooks, Wenda Zhou, Brydon Eastman, Haitang Hu, Yuchen He, Rahul Arora, Michele Wang, Christine Choi, Rowan Zellers, Mia Glaese, Dimitris Tsipras, AJ Ostrow, Scott Gray, Jason Phang, Nora Puckett, Hyung Won Chung, Andrew Braunstein, Miguel Castro, Bowen Cheng, Louis Feuvrier, Chad Nelson, Thomas Degry, Greg Brockman, Brandon Walkin, Peter Deng, Shengjia Zhao, Alex Borzunov, Claudia Fischer, Benjamin Zweig, Mikhail Pavlov, OpenAI, Olivier Godement, Rob Honsby, Charlotte Barette, Edede Oiwoh, Sonia Phene, Tarun Gogineni, Shantanu Jain, Yury Malkov, Alec Radford, Bogo Giertler, Natan LaFontaine, Chris Koch, Yuchen Zhang, Marwan Aljubeh, Jonathan McKay, Jason Teplitz, Sherwin Wu, Alex Baker-Whitcomb, Troy Peterson, Arka Dhar, Jamie Kiros, Coley Czarnecki, Jordan Sitkin, Haiming Bao, Joe Landers, Ruslan Nigmatullin, Carroll Wainwright, Marat Dukhan, Xia, Akila Welihinda, Alexis Conneau, Ehsan Asdar, Steve Coffey, Hannah Wong, Ilya Sutskever, Yash Patil, Leo Chen, Mateusz Litwin, Meng Jia Yang, Heather Schmidt, Felipe Petroski Such, Francis Zhang, Shamez Hermani, Miguel Oom Temudo de Castro, Alex Nichol, Angela Jiang, Ilya Kostrikov, Andrew Galu, Ali Kamali, Natalie Summers, Christina Kim, Taya Christianson, Ben Sokolowsky, Kayla Wood, Andrej Karpathy, Sean Metzger, Oleg Murk, Allan Jabri, Joy Jiao, Jieqi Yu, Ben Leimberger, Philip Pronin, Hendrik Kirchner, Hadi Salman, Jakub Pachocki, Oliver Jaffe, Joe Beutler, Chris Beaumont, Bob McGrew, Lyric Doshi, Shuaiqi, Yongjik Kim, Allison Moyer, Nick Ryder, Brendan Quinn, Jeff Harris, Brian Guarraci, Channing Conger, Peter Welinder, Yu Zhang, Lauren Workman, Chong Zhang, Rory Carmichael, Alex Carney, Bobby Spero, Elizabeth Proehl, Mehmet Yatbaz, Yujia Jin, Dane Sherburn, Alex Kirillov, Ricky Wang, Amin Tootoochian, Brian Hsu, Sandhini Agarwal, Mark Gray, Patrick Chao, Rohit Ramchandani, Andrew Tulloch, Natalie Staudacher, Clemens Winter, David Farhi, Behrooz Ghorbani, Blake Samic, Samuel Miserendino, Maya Shetty, Mira Murati, Eugene Brevdo, Todor Markov, Kendra Rimbach, Liam Fedus, Ashley Pantuliano, Camillo Lugaresi, Justyn Harriman, Michelle Fradin, Thomas Dimson, Tom Stasi, Kai Fricke, Jenia Varavva, Cary Hudson, Aditya Ramesh, Paul McMillan, Eric Peterson, Josh Kaplan, Krithika Muthukumar, Matthew Zeng, Reza Zamani, Ted Sanders, Eric Wallace, Karan Singhal, Chris Hallacy, Colin Wei, Ian Osband, Liang Zhou, Alexander Kirillov, Nick Turley, Nacho Soto, Alex Renzin, Daniel Levin, Peng Su, Vishal Kuo, John Schulman, Colin Jarvis, Alex Tachard Passos, Vinnie Monaco, Jong Wook Kim, David Robinson, Angela Baek, Juntang Zhuang, Jason Kwon, Madeleine Thompson, Lien Mamitsuka, Max Johnson, Freddie Sulit, Alex Paino, Tal Broda, Andrew Codispoti, Ryan Cheu, Mada Aflak, Sara Culver, David Carr, Cheng Lu, Saachi Jain, Eric Kramer, Noel Bundick, Renaud Gaubert, Eric Antonow, Gene Oden, Marvin Zhang, Rohan Sahai, Ingmar Kanitscheider, Jessica Shieh, Sam Toizer, Lilian Weng, Aidan Clark, Duc Phong Nguyen, Peter Bak, Michael Janner, Eric Sigler, Joel Parish, Alex Chow, Amin Tootoonchian, Long Ouyang, Shirong Wu, Edmund Wong, Jos Kraaijeveld, Ofir Nachum, Jason Wolfe, James Lennon, Haoyu Wang, Amadou Crookes, Murat Yesildal, Qiming Yuan, Bright Kellogg, David Sasaki, Lama Ahmad, David Mely, Ian Sohl, Alan Hayes, Mengchao Zhong, Joost Huizinga, James Betker, Nithanth Kudige, Peter Dolan, Kyle Luther

- ***What's New***: OpenAI의 새로운 모델 GPT-4o는 자동회귀형 옴니 모델로, 텍스트, 오디오, 이미지, 비디오를 입력으로 받아서 텍스트, 오디오, 이미지 출력을 생성할 수 있습니다. 비영어권 언어에서 텍스트 성능이 향상되었으며, API 사용 시 비용이 50% 절감되었습니다. 또한, vision 및 audio 이해 능력이 기존 모델에 비해 크게 개선되었습니다.
- ***Technical Details***: GPT-4o 모델은 GPT-4 Turbo 성능과 맞먹는 텍스트 생성 성능을 가지고 있으며, 오디오 입력에 대해서도 인간 수준의 응답 시간을 구현할 수 있습니다. 데이터는 주로 웹 크롤링을 통해 수집된 공개 데이터와 비공개 데이터 파트너십을 통해 얻어진 데이터로, 이미지, 오디오, 비디오와 같은 멀티모달 데이터가 포함되어 있습니다.
- ***Performance Highlights***: GPT-4o는 비영어권 언어에서 텍스트 성능이 크게 향상되었으며, 비용 효율적인 측면에서 API 사용 비용이 기존 대비 50% 절감됩니다. 또한, 모델은 고급 음성 모드에서 다양한 방언을 처리할 수 있으며, 안전성 평가에서는 크게 개선된 거부 응답율과 민감한 특성 추론 억제 능력을 보여주었습니다.

### [GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation](https://arxiv.org/abs/2410.20474)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20474.png)

Vote: 11

Authors: Phillip Y. Lee, Minhyuk Sung, Taehoon Yoon

- ***What's New***: GrounDiT는 훈련 없이 텍스트 기반 이미지 생성에서 공간적 배치를 향상시키는 새로운 접근법을 제안합니다. 기존의 방법들이 번거롭게 훈련을 요할 때, GrounDiT는 Transformer의 유연성을 활용하여 각 바운딩 박스에 대응하는 노이즈 패치를 생성하여 더욱 정밀한 공간적 제어를 가능케 합니다.
- ***Technical Details***: GrounDiT는 Diffusion Transformers(DiT)의 독특한 성질인 semantic sharing을 활용하여, 작은 패치와 생성 가능한 크기의 이미지가 동시에 디노이징될 때 이들이 '의미적 복제체'가 되는 현상을 적용합니다. 이를 통해 각 바운딩 박스에 맞는 노이즈 패치를 별도의 브랜치에서 디노이징한 후, 원본 이미지에 이식함으로써 공간적 제어를 최적화합니다. 이 방법은 Stage 1에서는 크로스-어텐션 맵을 사용한 전체적 이미지 업데이트, Stage 2에서는 지역 패치 재배치로 세밀한 제어를 구현합니다.
- ***Performance Highlights***: 실험 결과, GrounDiT는 HRS와 DrawBench 벤치마크에서 최첨단 성능을 기록했습니다. 특히 공간적 정확성에서 PixArt-R&B와 다른 기존 방법들을 능가하며, 이는 새로운 패치 재배치 기법이 매끄럽게 공간적 제약을 준수하게 하는 데 효과적임을 입증합니다.

### [LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior](https://arxiv.org/abs/2410.21264)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21264.png)

Vote: 5

Authors: Yixuan Ren, Abhinav Shrivastava, Hanyu Wang, Hao Chen, Saksham Suri

- ***What's New***: LARP는 최근 비디오 토크나이저(Tokenizing), 특히 자주회귀(Autoregressive) 생성 모델에서의 한계를 극복하기 위한 새로운 비디오 토크나이저입니다. 기존의 패치 단위 토크나이저가 지역적 정보를 담는 데 한정되었다면, LARP는 학습된 전체 정보 퀘리(learned holistic queries)를 사용해 더 전역적이고 의미론적인 표현을 캡처합니다. 또한, 다양하게 조정 가능한 수의 불연속 토큰을 지원하여 작업의 요구에 맞게 적응적이고 효율적인 토크나이징을 가능하게 합니다.
- ***Technical Details***: LARP는 비디오 입력을 자주회귀적 과정에 맞춰 학습하기 위해 경량 자회귀 변환기(AR transformer)를 이용한 훈련 시간 전 단계 모델로 통합합니다. 이로 인해 비디오 복원과 더불어 자주회귀 생성에 유리한 구조의 잠재 공간을 학습할 수 있게 합니다. LARP는 학습된 쿼리를 비디오 패치와 결합하여 패치 단위의 제약을 벗어나 고차원적이고 전역적인 비디오 표현을 만들어냅니다.
- ***Performance Highlights***: LARP는 UCF-101 클래스 조건부 비디오 생성 벤치마크에서 상태의 최첨단 FVD(Frechét Video Distance) 57을 달성하며, MAGVIT-v2와 같은 독점적인 AR 방법보다 우수한 성능을 보였습니다. 또한, K600 프레임 예측 벤치마크에서도 우수성을 증명하며 LARP의 표현력의 확장 가능성을 강조했습니다.

### [VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks](https://arxiv.org/abs/2410.19100)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.19100.png)

Vote: 3

Authors: Dan Zhao, Paul Pu Liang, Lawrence Jang, Justin Lin, Rogerio Bonatti, Yinheng Li, Charles Ding, Kazuhito Koishida

- ***What's New***: VideoWebArena는 긴 문맥 비디오 이해를 통해 멀티모달 에이전트(Long Context Multimodal Agents)의 능력을 평가하는 새로운 오픈 소스 벤치마크입니다. 사용자가 AI 에이전트를 통해 다양한 작업을 수행할 수 있도록 하며, 주로 기술 유지를 평가하는 기술 유지(Skill Retention) 작업과 비디오에서 정보 검색을 평가하는 사실 유지(Factual Retention) 작업에 중점을 둡니다.
- ***Technical Details***: VideoWebArena는 총 약 4시간의 콘텐츠로 구성된 2,021개의 웹 에이전트 작업으로 이루어져 있으며, 이는 수작업으로 제작된 비디오 튜토리얼을 기반으로 합니다. 기술 유지와 사실 유지 두 가지 주요 업무로 구성되어 있으며, 에이전트는 각 작업에서 인간 시연을 사용하여 작업을 수행하거나, 비디오에서 작업 완료에 필요한 정보 검색 능력을 평가받습니다. 다양한 상태-최신 모델(Large Language Models; LLMs)을 평가하여 모델의 긴 문맥 비디오 이해 역량을 제시합니다.
- ***Performance Highlights***: 비디오 이해에서 Gemeni 1.5 Pro와 GPT-4o 모델은 제한된 성능을 보였으며, 사실 유지 작업의 경우 최고 모델이 13.3%의 성공률을 나타냈고, Q/A 쌍에서는 45.8%를 기록했습니다. 그러나 인간 수행 수준과는 거리가 멀다 판단됩니다. 기술 유지 작업에서는 튜토리얼 없이 긴 문맥 모델의 성능이 더 좋았으며, 이는 튜토리얼이 부정적 잡음을 일으켜 성과에 부정적 영향을 미쳤음을 시사합니다.

### [AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant](https://arxiv.org/abs/2410.18603)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18603.png)

Vote: 25

Authors: Chengyou Jia, Junlin Hu, Qiushi Sun, Zhuohang Dang, Tianbao Xie, Fangzhi Xu, Zhiyong Wu, Minnan Luo

- ***What's New***: AgentStore는 이기종 에이전트(Heterogeneous Agents)를 동적으로 통합하여 운영체제(OS) 작업을 자동화할 수 있는 확장 가능한 플랫폼을 제안합니다. 다양한 서드 파티 에이전트를 쉽게 통합할 수 있으며, 새로운 운영체제의 변화에도 빠르게 적응할 수 있습니다. 또한 핵심 메타에이전트(MetaAgent)와 에이전트토큰(AgentToken) 전략을 제안하여, 전문화된 제너럴리스트 컴퓨터 보조사의 구축을 지원합니다.
- ***Technical Details***: AgentStore는 20개 이상의 다양한 기능을 갖춘 에이전트를 통합하여 OS 작업을 자동화합니다. 에이전트통합 프로토콜을 통해 새 에이전트를 추가할 수 있으며, MetaAgent는 이기종 에이전트의 증가를 효율적으로 관리하고, 이를 통해 여러 에이전트의 협업을 지원합니다. 에이전트토큰은 각 통합된 에이전트를 학습 가능한 토큰으로 표현하여, MetaAgent가 적절한 에이전트를 활성화하여 작업을 수행할 때 사용됩니다.
- ***Performance Highlights***: AgentStore는 OSWorld 벤치마크에서 11.21%였던 기존 시스템의 성능을 23.85%로 두 배 이상 혁신적으로 향상시켰습니다. 이와 같은 결과는 AgentStore가 일반화 및 전문화 두 측면 모두에서 에이전트 시스템의 능력을 확장하는 중요성을 보여줍니다. 모바일 환경에서도 탁월한 적응성을 입증하였습니다. 특히 AgentToken 전략은 탁월한 효율성 및 효과성을 통해 다수의 에이전트를 동적으로 관리하는 데 있어 높은 수행률을 기록했습니다.

### [Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines](https://arxiv.org/abs/2410.21220)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21220.png)

Vote: 5

Authors: Zhixin Zhang, Yiyuan Zhang, Xiaohan Ding, Xiangyu Yue

- ***What's New***: Vision Search Assistant는 VLMs(대형 시각-언어 모델)과 웹 에이전트를 협력하여 시각적 콘텐츠와 관련된 질문에 실시간으로 답할 수 있도록 하는 새로운 프레임워크입니다. 이를 통해 모델이 처음 보는 이미지에 대해서도 정보에 입각한 응답을 제공할 수 있습니다.
- ***Technical Details***: Vision Search Assistant는 세 가지 주요 단계로 구성됩니다. 첫째, '시각적 콘텐츠 공식화(Visual Content Formulation)' 단계에서 VLM은 중요한 시각적 객체와 그 상관관계를 텍스트로 설명합니다. 둘째, '웹 지식 검색(Web Knowledge Search)'은 사용자의 요구사항과 상관된 공식화에 대해 웹 에이전트를 통해 검색 쿼리를 생성하고, 검색 과정을 통해 얻은 웹 지식을 요약합니다. 마지막으로 '협력적 생성(Collaborative Generation)' 단계에서는 VLM이 모든 시각적 객체와 웹 지식을 통합하여 최종 답변을 생성합니다.
- ***Performance Highlights***: Vision Search Assistant는 열린 세트(오픈 세트) QA와 닫힌 세트(클로즈드 세트) QA 과제에서 뛰어난 성능을 보여줍니다. 열린 세트 평가에서는 사실성, 관련성, 지원성 측면에서 경쟁 모델을 초월하는 성과를 나타냈습니다. 닫힌 세트 평가에서는 구체적인 디테일과 추론 능력에서 LLaVA-1.6-7B 모델을 포함한 여러 모델보다 뛰어난 성과를 기록했습니다.

### [MarDini: Masked Autoregressive Diffusion for Video Generation at Scale](https://arxiv.org/abs/2410.20280)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20280.png)

Vote: 13

Authors: Jürgen Schmidhuber, Haozhe Liu, Juan-Manuel Pérez-Rúa, Zijian Zhou, Menglin Jia, Shikun Liu, Tao Xiang, Xiao Han, Sen He, Kumara Kahatapitiya, Jui-Chieh Wu, Ding Liu, Mengmeng Xu, Juan C. Pérez, Yanping Xie

- ***What's New***: MarDini는 비디오 생성을 위한 새로운 비디오 확산 모델로, 마스크된 자동회귀(Masked Auto-regression; MAR) 방식을 통합하여 시간 계획은 MAR이, 공간 생성은 확산 모델(Diffusion Model; DM)이 담당하는 비대칭 네트워크 구조를 제안합니다. 이를 통해 비디오 생성, 비디오 확장, 이미지-비디오 생성 등 다양한 작업을 단일 모델에서 수행할 수 있습니다.
- ***Technical Details***: MarDini는 저해상도 입력으로 각 마스크된 프레임의 계획 신호를 생성하는 MAR 기반 계획 모델과 경량의 생성 모델로 구성되어 있습니다. 계획 모델은 대부분의 매개변수를 포함하고 있으며, 생성 모델은 이러한 계획 신호를 사용하여 디노이징(diffusion de-noising)을 통해 고해상도 프레임을 생성합니다. 비디오 생성은 공간 및 시간적 모델링으로 분해되며, 저해상도 계획 모델은 많은 매개변수를 사용해 시간 모델링을 처리하고, 생성 모델은 적은 매개변수로 고해상도 공간 모델링을 수행합니다.
- ***Performance Highlights***: MarDini는 최신의 비디오 인터폴레이션 성능을 기록하며, 엔트로피 조정과 확률 기반의 시도를 통해 제공된 시간적 위치에서 마스크된 프레임을 재구성하여 효율적으로 비디오를 생성합니다. 특히, FVD(Frechet Video Distance) 등 성능 지표에서 높은 점수를 기록하며, 경쟁 모델들보다 적은 연산 필요성을 보입니다.

### [Bi-Level Motion Imitation for Humanoid Robots](https://arxiv.org/abs/2410.01968)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01968.png)

Vote: 0

Authors: Wenshuai Zhao, Michael Muehlebach, Yi Zhao, Joni Pajarinen

- ***What's New***: Bi-Level Motion Imitation (BMI)은 인간의 모션 캡처 데이터에서 학습한 인간형 로봇의 동작 모방을 위한 새로운 프레임워크로, 인간과 로봇 간의 신체적 차이 때문에 발생하는 물리적으로 불가능한 참조 동작 문제를 해결합니다. BMI는 모션 생성 모델과 로봇 정책을 교대로 최적화하여, 참조 동작이 로봇의 물리적 제약에 맞게 조정될 수 있도록 합니다.
- ***Technical Details***: 제안된 프레임워크는 자가 일관성을 보장하는 생성적 오토인코더(SCAE)를 활용하여 인간 모션의 희소하고 구조화된 표현을 학습합니다. 그런 다음, SCAE를 기반으로 잠재 공간 샘플링을 통해 로봇 정책을 사전 학습하고, 이를 기반으로 이중 최적화를 통해 참조 동작을 미세하게 조정합니다. 이 과정은 로봇의 물리적 제약을 존중하면서도 인간의 모션 패턴에 근접한 동작을 생성합니다.
- ***Performance Highlights***: MIT 인간형 로봇을 시뮬레이션하여 동작을 모방한 결과, 제안된 방법은 기존의 라벨된 동작 데이터셋을 사용하는 것보다 향상된 정책 학습을 보여주었습니다. 특히, BMI는 킥 동작에서 가장 긴 시간 동안의 안정성을 유지하였고, 점프 동작에서도 개선된 성능을 확인할 수 있었습니다. 추가 실험을 통해 로봇에 질량 변화가 있을 때도 학습된 정책이 잘 작동한다는 점을 확인하였습니다.

### [Neural Fields in Robotics: A Survey](https://arxiv.org/abs/2410.20220)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20220.png)

Vote: 3

Authors: Zsolt Kira, Jonathan Tremblay, Yen-Chen Lin, Mauro Comi, Abhinav Valada, Muhammad Zubair Irshad, Rares Ambrus, Nick Heppert

- ***What's New***: 이 논문은 Neural Fields(NFs)를 로봇 공학에 적용한 최신 기술과 연구 동향을 종합적으로 정리합니다. NFs는 2D 데이터를 활용하여 3D 장면을 지속적으로 표현하고 그 안의 다양한 물리적 및 기하학적 성질을 정확히 추론할 수 있도록 합니다. NFs가 가진 잠재력을 포즈 추정, 조작, 내비게이션, 물리 및 자율주행 등 다양한 로봇 공학 분야에 적용하는 아이디어와 그 가능성을 탐색합니다.
- ***Technical Details***: NFs는 포즈 추정, 조작, 내비게이션 등에서 그 강점을 발휘하며, 이 연구는 Occupancy Networks, Signed Distance Fields, Neural Radiance Fields, Gaussian Splatting과 같은 네 가지 주요 프레임워크를 제시합니다. 다양한 센서 데이터와 통합되고 NFs가 생성하는 연속적인 표현은 고품질 3D 재구성을 가능하게 합니다. 이는 로봇의 실시간 인식 능력을 향상시키는 데 중요한 역할을 합니다.
- ***Performance Highlights***: NFs는 200편 이상의 논문을 분석하여 그들이 갖는 강점과 한계를 평가하였습니다. 예를 들어, NFs는 복잡한 3D 기하학적 구조를 단순화된 데이터로 효율적으로 표현할 수 있어, 기존 방법보다 연산 자원을 절약하면서도 고해상도의 재구성을 가능하게 합니다. 또한 멀티 센서 통합을 통해 더욱 적응력 있고 강력한 환경 인식을 지원합니다.

### [Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction](https://arxiv.org/abs/2410.21169)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21169.png)

Vote: 16

Authors: Junyuan Zhang, Bin Wang, Victor Shea-Jay Huang, Conghui He, Qintong Zhang, Wentao Zhang, Matthieu Lin, Hao Liang, Zhengren Wang, Shawn Wang

- ***What's New***: 이 연구는 문서 파싱(DP)에 대한 포괄적인 검토를 제공하며, 대량 언어 모델(LLM)을 활용한 최첨단 기술을 심층적으로 다룹니다. 특히 복잡한 레이아웃 처리, 다중 모듈 통합, 그리고 고밀도 텍스트 인식과 같은 챌린지를 해결하기 위한 방향성을 제시합니다.
- ***Technical Details***: 이 연구는 모듈러 파이프라인 시스템에서 대형 비전-언어 모델로 발전하는 문서 파싱의 핵심 방법론을 다룹니다. 레이아웃 분석(Layout Analysis), 컨텐츠 추출(content extraction) 및 다중 모달 데이터 통합과 같은 핵심 요소를 상세히 조사합니다. 또한, 최근의 딥 러닝 기반 기술이 문서 파싱 도구와 유망한 문서 파서를 나타내며, 객체 탐지 알고리즘(adapted object detection algorithms)과 인스턴스 세분화(instance segmentation) 기법에 대한 활용을 강조합니다.
- ***Performance Highlights***: 문서 파싱 연구에서 고성능을 자랑하는 대형 모델, 특히 GPT-4, LLaVA, Nougat 등이 다양한 도전 과제를 극복하는 데 어떻게 기여했는지를 설명합니다. 예를 들어, Nougat 모델은 아카데믹 문서를 Markdown 형식으로 직접 변환할 수 있으며, Vary는 차트와 문서 OCR에서 개선된 성능을 보여주었습니다. 또한, 다양한 문서 유형과 구조를 인식하는 혁신적인 접근이 연구되고 있습니다.

### [Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA](https://arxiv.org/abs/2410.20672)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20672.png)

Vote: 1

Authors: Hrayr Harutyunyan, Sangmin Bae, Tal Schuster, Ziwei Ji, Adam Fisch, Seungyeon Kim

- ***What's New***: 이 연구는 기존의 대형 언어 모델(LLMs)을 작은 크기로 변환하면서도 성능 손실을 최소화한 '재귀적 트랜스포머(Recursive Transformers)'를 제안합니다. 특히, 계층 간 파라미터의 공유를 통해 모델의 크기를 줄이고, 레이어별 저랭크 어댑터(LoRA) 모듈을 통합하여 성능을 개선한 '완화된 재귀적 트랜스포머(Relaxed Recursive Transformers)'를 소개합니다.
- ***Technical Details***: 이 논문은 트랜스포머 아키텍처의 계층 공유(파라미터 공유) 방법을 재검토하고, 기존의 예훈련된 트랜스포머로부터 효율적인 초기화를 통해 재귀적 트랜스포머를 구현하는 방법을 설명합니다. 또한, T-SVD(Truncated SVD)를 사용하여 LoRA 모듈을 초기화하고, 반복 계층 구조에서의 오버헤드를 줄이면서 성능을 극대화하는 전략을 제안합니다.
- ***Performance Highlights***: 실험 결과, 재귀적 모델인 Gemma 1B는 유사 크기의 미리 훈련된 모델들과 대비하여 더 나은 성능을 보여주었습니다. 특히, 해당 모델은 다단계 학습과 지식 증류를 통해 성능을 유지하면서도 보다 적은 양의 파라미터를 사용하여 고속 처리량(thoroughput)을 실현할 수 있음을 보여줍니다.

### [DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation](https://arxiv.org/abs/2410.18666)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18666.png)

Vote: 14

Authors: Yuang Ai, Hongxia Yang, Xiaotian Han, Zhengyu Chen, Quanzeng You, Xiaoqiang Zhou, Huaibo Huang

- ***What's New***: DreamClear는 새로운 Diffusion Transformer(DiT) 기반의 이미지 복원 모델로, GenIR 파이프라인과 결합하여 실제 이미지 복원을 위한 대규모 고품질 데이터셋을 생성합니다. 이는 기존 모델들이 다루지 못하던 다양한 열화 유형에 더 적합하게 설계되었습니다.
- ***Technical Details***: GenIR는 데이터를 이미지-텍스트 쌍 생성, 듀얼 프롬프트 기반의 미세 조정, 그리고 데이터 생성 & 필터링의 세 가지 단계로 처리합니다. 이를 통해 저작권 문제나 개인 정보 유출을 방지합니다. DreamClear는 Mixture of Adaptive Modulator(MoAM) 구조를 통해 토큰 기반 열화 사전 정보를 이용하여 다양한 복원 전문 지식을 통합합니다.
- ***Performance Highlights***: DreamClear는 다양한 벤치마크에서 최첨단 성능을 달성하며, 특히 LPIPS, DISTS, FID와 같은 지각적 품질 지표에서 우수한 성능을 보였습니다. 또한, 실제 환경에서 고품질 복원을 가능하게 하여, 낮은 참조 기반 왜곡 지표 점수에도 불구하고 높은 시각적 품질을 유지합니다.

