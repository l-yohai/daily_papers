## Daily Papers (2024-10-30)

### [Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning](https://arxiv.org/abs/2410.22304)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22304.png)

Vote: 11

Authors: Yihe Deng, Paul Mineiro

- ***What's New***: Flow-DPO는 LLM(대형언어모델)의 수학적 추론 능력을 향상시키기 위해 온라인 멀티 에이전트 학습(Multi-Agent Learning)을 사용하는 새로운 접근 방식을 제안합니다. 'Flow'라는 협업 구조를 통해 여러 LLM이 점진적으로 해답을 구축하도록 하여 효율성을 증가시킵니다.
- ***Technical Details***: Flow-DPO는 단계별 출력 생산(Incremental Output Production) 기법을 사용하여 두 개의 LLM, 즉 Answer LLM과 Stop LLM이 협업해 답변을 생성합니다. 온라인 직접 선호 최적화(Direct Preference Optimization; DPO) 학습을 롤아웃 방식으로 적용하고, LoRA 어댑터를 통해 두 LLM의 전문성을 강화합니다. 각 질문에 대해 랜덤 롤아웃을 수행하여 부분 응답을 완성하고, 이를 통해 학습을 진행합니다.
- ***Performance Highlights***: 모델 Llama-3-8B-Instruct에 적용한 결과, 온라인 DPO 학습으로 79%의 초기 추론 성능에서 83%까지의 정확도 개선을 이뤘습니다. Phi-3-medium-128k-instruct 모델의 경우도 79%의 초기 정확도를 83%로 향상시켰습니다. 이런 결과는 Flow-DPO가 기존 모델보다 우수한 이유 지문을 생성할 수 있음을 보여줍니다.

### [Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback](https://arxiv.org/abs/2410.21242)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21242.png)

Vote: 3

Authors: Yung-Sung Chuang, Nour Jedidi, James Glass, Leslie Shing

- ***What's New***: 이 논문은 ReDE-RF(Real Document Embeddings from Relevance Feedback)라는 새로운 방법을 소개합니다. 이는 기존의 가설적 문서 생성 대신 실제 문서의 연관성을 측정하여, 대규모 언어 모델(LLM)을 사용하여 사용 가능한 문서를 선택하고 근접 탐색을 수행합니다.
- ***Technical Details***: ReDE-RF는 LLM을 통해 초기 검색된 문서 세트를 '관련' 또는 '관련 없음'으로 표시하고, 관련 문서의 임베딩을 집합에서 불러와 쿼리 벡터를 업데이트하는 방식을 사용합니다. 이는 LLM이 구체적인 도메인 지식 없이도 연관성 판단만으로 검색 속도를 향상시키는 접근법입니다.
- ***Performance Highlights***: 실험 결과, ReDE-RF는 저자원이 필요한 검색 데이터 셋에서 최첨단 zero-shot dense retrieval 방법보다 최대 6% 더 나은 성능을 보였으며, 쿼리당 검색 지연 시간도 최대 11.2배까지 줄였습니다.

### [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](https://arxiv.org/abs/2410.21465)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21465.png)

Vote: 6

Authors: Harry Dong, Yuejie Chi, Beidi Chen, Size Zheng, Wenlei Bao, Li-Wen Chang, Xin Liu, Ningxin Zheng, Hanshi Sun

- ***What's New***: ShadowKV는 메모리 사용량 감소와 고속 추론을 위해 설계된 대용량 문맥 LLM 추론 시스템입니다. 이 시스템은 낮은 랭크의 키 캐시를 GPU에 저장하고 밸류 캐시는 CPU로 오프로딩하여 메모리 발자국을 줄입니다.
- ***Technical Details***: ShadowKV는 정확한 KV 선택 전략을 통해 추론 지연을 최소화하며, 실시간으로 희소 KV 쌍을 재구성합니다. 다양한 벤치마크에서 성능을 평가하여, A100 GPU 환경에서 최대 3.04배의 가속을 가능케 합니다.
- ***Performance Highlights***: ShadowKV는 Llama-3.1-8B 모델을 사용하여 기존보다 6배 큰 배치 사이즈를 지원하며, 무한 GPU 메모리 가정 하에 무상한 배치 사이즈로 달성할 수 있는 성능을 초과한 것으로 입증되었습니다.

### [Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse](https://arxiv.org/abs/2410.21333)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21333.png)

Vote: 4

Authors: Ilia Sucholutsky, Addison J. Wu, Ryan Liu, Jiayi Geng, Thomas L. Griffiths, Tania Lombrozo

- ***What's New***: 이번 연구에서는 많이 사용되는 Chain-of-Thought(CoT) 프롬프트가 특정 작업에서 모델 성능을 감소시킬 수 있다는 새로운 접근을 제시합니다. 특히, 인간의 성과를 저하시키는 사고가 모델에서도 비슷한 영향을 미칠 수 있음을 규명하고자 하였습니다.
- ***Technical Details***: CoT가 성능을 감소시키는 작업의 특성을 식별하기 위해 인지심리학에서 영감을 받아, 인간의 성능이 언어적 사고나 숙고에 의해 저하되는 경우를 모델에 적용했습니다. 암묵적 통계 학습, 시각적 인식, 패턴 예외 처리와 같은 세 가지 작업을 중심으로 CoT의 영향을 실험하였습니다.
- ***Performance Highlights***: CoT를 사용한 경우 OpenAI o1-preview 모델은 암묵적 통계 학습에서 GPT-4o zero-shot 모델 대비 36.3% 성능 저하를 보였습니다. 시각적 인식 작업에서도 6개의 비전-언어 모델 모두 성능 감소를 보였으며, 예외가 있는 패턴 작업에서는 레이블 학습 시간이 최대 331% 증가했습니다.

### [Measuring memorization through probabilistic discoverable extraction](https://arxiv.org/abs/2410.19482)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.19482.png)

Vote: 2

Authors: Itay Yona, Ilia Shumailov, Harsh Chaudhari, Jamie Hayes, Marika Swanberg

- ***What's New***: 본 연구는 확률적 발견 가능 추출(probabilistic discoverable extraction)이라는 새로운 방법을 도입하여 대형 언어 모델(LLMs)의 데이터 암기 문제를 보다 정확하게 측정하고자 합니다. 이 접근법은 다양한 샘플링(sampling) 방식과 여러 시도를 고려함으로써 기존의 추출 방법이 가진 제한점을 극복하고자 설계되었습니다.
- ***Technical Details***: 확률적 발견 가능 추출에서는 주어진 샘플링 방식 하에서 목표 시퀀스를 추출할 수 있는 확률을 정량화합니다. (n, p)-발견 가능 추출로 명명된 이 정의는 목표 시퀀스가 n번의 시도 후에도 일정 확률 p 이상으로 추출 가능한지를 평가합니다. 각 샘플링 프로세스는 독립적으로 실행되어 시퀀스를 생성하게 됩니다.
- ***Performance Highlights***: 실험 결과, (n, p)-발견 가능 추출 방법은 기존의 탐욕적 샘플링(greedy sampling) 기반의 추출 방법에 비해 더 높은 추출율을 보여주었습니다. 특히, 큰 모델이나 반복 학습된 데이터에서는 확연한 차이가 발생하여 현재의 발견 가능 추출 방법이 암기 위험을 과소평가할 가능성을 시사합니다.

### [CLEAR: Character Unlearning in Textual and Visual Modalities](https://arxiv.org/abs/2410.18057)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18057.png)

Vote: 173

Authors: Boris Mikheev, Alexey Dontsov, Ivan Oseledets, Dmitrii Korzh, Alexey Zhavoronkin, Oleg Y. Rogov, Aibek Alanov, Elena Tutubalina, Denis Bobkov

- ***What's New***: 이 논문은 새로운 벤치마크인 CLEAR를 소개합니다. CLEAR는 텍스트와 시각적 모달리티에서 기계 언러닝(Machine Unlearning)의 방법을 평가하기 위한 최초의 오픈 소스 멀티모달 언러닝(MMU) 벤치마크입니다. 이 벤치마크는 200명의 가상 인물과 3,770개의 이미지, 각기 대응되는 질문-답변 쌍을 포함하여 다양한 모달리티에서의 철저한 평가를 가능케 합니다.
- ***Technical Details***: CLEAR 벤치마크는 TOFU 데이터셋을 확장하여 개인 얼굴 이미지와 관련된 200명의 작가를 대상으로, 광범위한 이미지 생성을 통해 데이터를 제어하고 누수를 방지합니다. 모델은 주어진 전체 데이터셋에 미세 조정을 수행한 후 선택된 '포겟 세트(forget set)'에 대해 언러닝 절차를 거칩니다. 이 과정에서는 모델의 성능이 실제 세계의 작업에서 유지되는지를 확인하기 위해 평가됩니다.
- ***Performance Highlights***: 텍스트 도메인의 실험에서 IDK, SCRUB, DPO 방법이 얻어지는 포겟 메트릭(Forget Metric)이 가장 낮고, 유지 성능에는 큰 영향이 없어 최상의 성능을 보였습니다. 둘째, 시각적 도메인에서 SCRUBbio와 Twins 방법이 모든 고려된 메트릭에서 가장 뛰어난 성능을 나타냈습니다. 마지막으로, LoRA ℓ1 정규화를 적용하여 성능을 강화하고 유지 성능의 손상을 줄일 수 있음을 확인했습니다.

### [EoRA: Training-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation](https://arxiv.org/abs/2410.21271)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21271.png)

Vote: 4

Authors: Saurav Muralidharan, Min-Hung Chen, Yu-Chiang Frank Wang, Pavlo Molchanov, Charbel Sakr, Jan Kautz, Nai Chit Fung, Huck Yang, Kwang-Ting Cheng, Hongxu Yin, Shih-Yang Liu, Chein-Yi Wang

- ***What's New***: 본 논문에서는 큰 언어 모델(LLM)의 압축과 관련된 오류를 보완하기 위해 Training-free Eigenspace Low-Rank Approximation (EoRA)을 제안했습니다. EoRA는 사용자 요구사항에 맞게 압축 오류를 보상하는 잔여 저계수 경로를 추가함으로써 기존의 압축 형식에 제약받지 않는 유연성을 제공합니다.
- ***Technical Details***: EoRA는 입력 활성화의 고유 공간(eigenspace)으로 압축 오류를 투사(projection)하여 고유값(eigenvalue)을 중요도 지표로 사용하여 고유 공간 투사를 통해 중요한 오류 요소의 재구성을 우선시합니다. SVD와는 달리 EoRA는 잔여 저계수 경로를 활용하여 기존의 압축 형식에 구애받지 않고 짧은 시간 안에 옵티마이제이션을 수행할 수 있습니다.
- ***Performance Highlights***: EoRA는 LLaMA2/3 모델의 다양한 작업에서 SVD를 능가하며, ARC-Easy/ARC-Challenge와 MathQA에서 각각 31.31%/12.88% 및 9.69%의 오류 보상 성능 향상을 이끌어냅니다. 또한 EoRA는 3비트와 4비트 양자화에 있어서도 견고하며, 최소한의 정확도 저하만을 경험합니다.

### [Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'](https://arxiv.org/abs/2410.21647)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21647.png)

Vote: 2

Authors: Lin Tan, Shanchao Liang, Nan Jiang, Yiran Hu

- ***What's New***: REPOCOD는 LLMs(대형 언어 모델)가 실제 소프트웨어 개발 환경에서 코드를 생성할 수 있는지 평가하기 위한 새로운 코드 생성 벤치마크입니다. 이 데이터셋은 11개의 인기 있는 프로젝트에서 수집된 980개의 문제를 포함하고 있으며, 이 중 58%는 파일 수준 또는 저장소 수준의 맥락 정보를 요구합니다.
- ***Technical Details***: REPOCOD는 GitHub에서 파이썬이 주요 언어로 사용되며 최소 2k 스타가 있는 오픈 소스 저장소를 선택하여 기능을 수집합니다. 그런 다음, 해당 저장소에서 타겟 함수와 관련된 테스트 케이스를 수집하여, LLM이 생성한 함수의 정확성을 검증합니다. 평균적으로 구문 해석의 Cyclomatic 복잡도가 9.00이며, 257개의 인스턴스는 저장소 수준의 맥락 처리를 요구합니다.
- ***Performance Highlights***: SOTA LLM의 REPOCOD 벤치마크 평가 결과, 모든 모델이 pass@1 기준 30을 넘지 못하는 성능을 보여, 실제 소프트웨어 개발에서 LLM이 완전한 대체 수단이 되기에 충분하지 않음을 나타냈습니다. 특히, GPT-4o는 가장 좋은 성과를 기록했으나 pass@1 27.35에 그쳤습니다.

### [OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization](https://arxiv.org/abs/2410.19609)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.19609.png)

Vote: 13

Authors: Hongming Zhang, Zhenzhong Lan, Tianqing Fang, Dong Yu, Kaixin Ma, Wenlin Yao, Wenhao Yu, Hongliang He

- ***What's New***: OpenWebVoyager는 오픈 소스 환경에서 멀티모달 웹 에이전트를 구축하기 위한 새로운 프레임워크를 제안합니다. 이 시스템은 모방 학습(Imitation Learning)을 통해 기본 웹 네비게이션 능력을 학습한 후, 실제 웹 환경에서의 탐색과 피드백 및 최적화 사이클을 반복하여 스스로를 개선합니다.
- ***Technical Details***: OpenWebVoyager는 초기 모방 학습 단계를 거쳐 여러 탐색 피드백 최적화 사이클을 통해 실제 웹 네비게이션 작업을 학습합니다. 이는 웹 작업 쿼리 집합을 컴파일하고, 다중 모달 에이전트인 WebVoyager로부터 수집된 탐색 결과를 바탕으로 구현됩니다. 에이전트는 주어진 웹 페이지의 스크린샷과 접근성 트리를 관찰하여 행동을 결정하며, 각 단계의 탐색 경로 정확성은 GPT-4o에 의해 평가됩니다.
- ***Performance Highlights***: WebVoyager 테스트 세트에서 과제 성공률(Task Success Rate)이 초기 19.9%에서 25.8%로 증가하였고, Mind2Web 크로스 과제 세트에서는 6.3%에서 19.6%로 향상되었습니다. 이는 반복적인 최적화가 멀티모달 웹 에이전트의 성능을 효과적으로 높일 수 있음을 보여줍니다. 또한, 보지 못한 웹사이트에서도 소폭의 성능 향상이 관찰되어 보편성을 어느 정도 확보했음을 시사합니다.

### [Accelerating Direct Preference Optimization with Prefix Sharing](https://arxiv.org/abs/2410.20305)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20305.png)

Vote: 3

Authors: Sumanth Hegde, Franklin Wang

- ***What's New***: 본 논문에서는 Prefix Sharing이라는 새로운 기법을 도입하여, 대형 언어 모델(LLMs)의 오프라인 페어드 선호 최적화(Direct Preference Optimization; DPO)를 가속화합니다. 이 기법은 선택된 응답과 거부된 응답을 공통 프리픽스(prefix)를 공유하는 하나의 시퀀스로 처리하며, 이를 통해 중복된 계산을 제거합니다. 특히, 긴 프리픽스를 가진 데이터셋에서, 이 방법은 최대 1.5배의 훈련 속도 향상을 가져오며 수렴에 영향을 미치지 않습니다.
- ***Technical Details***: Prefix Sharing 기법은 사용자 정의 블록 희소(attention mask)를 사용하여 선택된 응답(ychosen)과 거부된 응답(yrejected)을 동일한 시퀀스로 결합합니다. 이 기법은 연산 효율성을 높이기 위해 PyTorch의 FlexAttention을 활용하여 계산 시간과 메모리 사용량을 줄입니다. 또한, Sequence Packing을 통해 작은 시퀀스 길이를 가진 데이터셋에서도 1.3-1.6배의 일관된 속도 향상을 제공합니다.
- ***Performance Highlights***: Prefix Sharing을 통해 여러 인기 있는 DPO 데이터셋에서 훈련 처리량이 1.1-1.5배 향상되었습니다. Sequence Packing과 결합할 경우, 특정 데이터셋에서 훈련 속도가 1.3-1.6배 증가했으며, 메모리 소비 또한 줄어듭니다. 이는 특히, 긴 시퀀스 및 높은 프리픽스 대 완료 길이 비율을 가진 시나리오에서 더욱 효과적입니다. 모든 실험은 NVIDIA H100 GPU에서 진행되었습니다.

### [AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions](https://arxiv.org/abs/2410.20424)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20424.png)

Vote: 20

Authors: Minghao Liu, Ge Zhang, David Ma, Wanjun Zhong, Yue Wang, Wenhao Huang, Xinyao Niu, Ziming Li, Wangchunshu Zhou, Qianbo Zang, Jian Yang, Jiawei Guo, Tuney Zheng, Jiaheng Liu

- ***What's New***: AutoKaggle는 테이블 데이터와 관련된 복잡한 데이터 과학 작업을 처리하는 다중 에이전트 시스템 기반의 프레임워크입니다. 이 프레임워크는 각 단계마다 사용자 개입을 허용하며 에이전트 간 협업을 통해 자동화된 데이터 파이프라인을 완성합니다. 이러한 접근은 데이터 과학 작업의 효율성을 크게 향상시킵니다.
- ***Technical Details***: AutoKaggle은 배경 이해, 데이터 탐색, 데이터 정리, 피처 엔지니어링, 모델 구축 및 예측 등의 6단계로 구성된 워크플로우를 제공합니다. 각 단계는 Reader, Planner, Developer, Reviewer, Summarizer라는 전문화된 에이전트들이 협력하여 수행합니다. 또한, 반복적인 디버깅과 단위 테스트를 통해 코드의 논리적 일관성과 정확성을 보장합니다. 데이터 정리(Data Cleaning), 피처 엔지니어링(Feature Engineering), 모델 구축 및 예측(Model Building, Validation, and Prediction)을 위한 광범위한 기계 학습 도구 라이브러리가 통합되어 있습니다.
- ***Performance Highlights***: AutoKaggle은 8개의 Kaggle 데이터 과학 대회에서 평균 85%의 유효 제출율과 82%의 종합 점수를 기록하며 강력한 성능을 입증했습니다. 이러한 결과는 Kaggle 플랫폼상의 복잡한 데이터를 자동화된 도구를 활용해 효과적으로 처리할 수 있음을 보여줍니다. 특히 GPT-4o 모델을 사용했을 때 경쟁 모델 대비 28% 향상된 성능을 보였습니다.

### [SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization](https://arxiv.org/abs/2410.21411)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21411.png)

Vote: 12

Authors: Chuang Gan, Donglai Wei, Zibin Meng, Wanhua Li, Hanspeter Pfister, Jiawei Zhou

- ***What's New***: SocialGPT는 시각 관계 추론을 위해 기초 모델(Foundation Models)을 사용하는 모듈식 프레임워크를 최초로 도입하였습니다. 이 프레임워크는 Vision Foundation Models(VFMs)의 시각적 인식 능력과 Large Language Models(LLMs)의 추론 능력을 결합하여, 추가적인 모델 훈련 없이 직관적인 언어 기반 설명을 제공함으로써 설명 가능한 인터프리터블한 답변을 제공합니다.
- ***Technical Details***: SocialGPT는 이미지 내용을 텍스트 사회 이야기로 변환하기 위해 VFMs를 사용하고, 그런 다음 LLMs를 사용하여 텍스트 기반 추론을 수행합니다. Greedy Segment Prompt Optimization (GSPO) 방법을 제안하여 자동 프롬프트 최적화를 수행하며, 이는 세그먼트 레벨에서 기울기 정보를 활용하여 탐욕적인 탐색을 수행합니다. 이러한 방법으로 인해 각 세그먼트는 시스템, 기대, 컨텍스트, 가이던스 부분으로 구성된 구조화된 소셜 관계 추론 프롬프트, SocialPrompt를 통해 LLMs를 지시합니다.
- ***Performance Highlights***: SocialGPT는 PIPA 및 PISC 데이터셋에서 제로샷(Zero-shot) 결과를 통해 높은 경쟁력을 입증하였으며, 추가 모델 훈련 없이 이전 기술보다 향상된 성능을 보였습니다. Vicuna-13B 모델을 사용한 SocialGPT는 기존의 최첨단 모델들과 비교해 PIPA 데이터셋에서 1.4%의 향상을 보이며 최고의 정확도를 기록했습니다. GSPO를 사용한 프롬프트 튜닝은 성능을 평균 약 2.38% 증가시켰습니다.

### [RARe: Retrieval Augmented Retrieval with In-Context Examples](https://arxiv.org/abs/2410.20088)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.20088.png)

Vote: 2

Authors: Eunsol Choi, Sujay Sanghavi, Atula Tejaswi, Yoonsang Lee

- ***What's New***: 이 연구에서는 LLMs(large language models)에서 자주 사용하는 In-context Examples를 검색 작업에서 임베딩 모델의 성능을 향상시키기 위해 어떻게 사용할 수 있는지를 탐구했습니다. 새로운 방법인 RARe(Retrieval Augmented Retrieval with In-Context Examples)를 소개하여, 사전 학습된 모델을 In-context Examples로 파인튜닝하여 검색 작업에서 일관되게 성능 향상을 달성합니다.
- ***Technical Details***: RARe 방법은 검색 시스템의 쿼리 형식을 수정하여 대상 쿼리와 의미적으로 유사한 In-context Examples를 제공합니다. 그런 다음 대조 손실(contrastive loss)으로 지속적인 파인튜닝을 적용하여 성능을 향상시킵니다. 이 방법론은 디코더 전용 언어 모델 및 기존 검색모델 등 다양한 기본 아키텍처에 유연하게 적용할 수 있습니다.
- ***Performance Highlights***: RARe는 다양한 오픈 도메인 검색 데이터셋(BeIR, RAR-b)에서 최대 +2.72% nDCG 성능 개선을 달성하였으며, 특히 기존 쿼리 예를 사용하는 모델과 비교했을 때 강력한 도메인 외 일반화 성능을 보였습니다. 이 방법은 여러 과제에서 기존의 베이스라인 모델들을 능가하며, 검색 작업에서 In-context Examples의 효과를 입증했습니다.

### [Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Dataset](https://arxiv.org/abs/2410.22325)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22325.png)

Vote: 6

Authors: Guangqi Jiang, Tao Huang, Huazhe Xu, Huanyu Li, Yifei Sun, Yongyuan Liang

- ***What's New***: 이 논문에서는 대규모 로봇 데이터셋(DRoid)을 활용하여 로봇 조작 중심의 표현(Manipulation-Centric Representation; MCR)을 학습하는 새로운 프레임워크를 제안합니다. 이 방식은 기존 인간 데이터셋보다 로봇 데이터셋이 조작 중심성을 학습하는 데 적합하다는 주장을 뒷받침합니다.
- ***Technical Details***: MCR 프레임워크는 동적 정렬 손실(Dynamics Alignment Loss)과 행동 예측 손실(Action Prediction Loss)을 도입하여 로봇의 상태와 행동을 이미지 관찰과 연결합니다. 또, 시간 대조 학습(Time Contrastive Learning)을 도입하여 시간 정보를 인코딩합니다. 교육 과정은 대규모 로봇 데이터셋에서 RGB 이미지와 로봇의 고유 수용 상태, 행동 정보를 활용하여 진행됩니다.
- ***Performance Highlights***: MCR은 4개 시뮬레이션 도메인 내 20개의 로봇 조작 과제에서 가장 강력한 기본 모델보다 14.8% 더 나은 성능을 보였으며, 3개의 실제 조작 과제에서는 성공률을 76.9% 향상시켰습니다. 이러한 결과는 대규모 로봇 데이터셋을 활용한 MCR 방법이 조작 중심성을 효과적으로 학습하여 조작 성능을 크게 개선할 수 있음을 보여줍니다.

### [Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2410.21845)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21845.png)

Vote: 5

Authors: Jianlan Luo, Jeffrey Wu, Charles Xu, Sergey Levine

- ***What's New***: 이 논문은 인간 피드백을 활용하여 강화학습(Reinforcement Learning; RL)으로 로봇의 정밀하고 섬세한 조작 기술을 향상시키는 방법을 제시합니다. 인간의 교정과 시연을 결합하여 실시간 환경에서 정책을 신속하게 학습할 수 있는 비전 기반 RL 시스템, HIL-SERL을 소개합니다. 이 시스템은 다양한 복잡한 조작 작업에서 뛰어난 성능을 보이며, 짧은 훈련 시간 내에 거의 완벽한 성공률을 달성합니다.
- ***Technical Details***: HIL-SERL 시스템은 사전 훈련된 시각 백본과 효율적인 오프-정책 RL 알고리즘(RLPD)을 사용하여 정책 학습의 안정성을 높였습니다. 시스템은 인간의 시연과 교정을 수용하여 학습 과정에서 정책이 실수를 학습하고 성능을 개선하도록 합니다. 정책은 작업의 성공 여부를 결정하는 이진 클래스 분류 관련 보상을 사용하여 학습되고, 훈련은 10Hz 환경 단계 속도로 수행됩니다.
- ***Performance Highlights***: HIL-SERL은 거의 모든 작업에서 1~2.5시간의 짧은 훈련 시간으로 100%에 가까운 높은 성공률을 달성했으며, 이전의 RL 및 모방 학습 방법보다 두 배 이상의 높은 성공률과 1.8배 빠른 작업 속도를 기록했습니다. 이는 RL이 실시간 훈련을 통해 다양한 비전 기반 조작 정책을 효과적으로 학습할 수 있음을 입증합니다. RL의 정책 개선으로 인해 개입률은 시간이 지나면서 감소하고, 훈련이 완료되면 거의 0에 도달합니다.

