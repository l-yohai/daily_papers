## Daily Papers (2024-10-25)

### [Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4](https://arxiv.org/abs/2410.16429)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16429.png)

Vote: 0

Authors: ['Brando Miranda', 'Leni Aniva', 'Sanmi Koyejo', 'Chuyue Sun', 'Clark Barrett']

- ***What's New***: Pantograph는 Lean 4 증명 보조기와 강력한 탐색 알고리즘을 결합하여 고급 정리 증명(Advanced Theorem Proving)과 데이터 추출(Data Extraction)을 가능하게 하는 새로운 인터페이스를 제공합니다. 다양한 사용 사례를 지원하며, 기계 학습 모델을 통해 Lean 4 정리를 증명하는 혁신적인 기능을 제안합니다.
- ***Technical Details***: Pantograph는 Lean 4 언어 서버 프로토콜(LSP)의 제한을 극복하고자 설계되어 있습니다. 사용자에게 메타변수 결합을 처리하고, 고급 추론 단계를 지원하며, 전체 증명 스크립트를 추출하는 등 다양한 기능을 제공합니다. 또, Pantograph는 sorry 키워드가 포함된 불완전한 증명을 재개할 수 있는 기능을 제공해, 기계 학습 모델의 증명 초안 작성 지원을 강화합니다.
- ***Performance Highlights***: Pantograph는 MiniF2F 벤치마크를 사용하여 DSP 접근 방식을 평가했으며, GPT-4o 모델을 통해 28%의 성공률을 기록했습니다. 이는 Lean 4에서 DSP가 처음으로 구현된 사례이며, 향후 연구를 위한 기본 성과를 제공합니다.

### [Value Residual Learning For Alleviating Attention Concentration In Transformers](https://arxiv.org/abs/2410.17897)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.17897.png)

Vote: 1

Authors: ['Tianyi Wu', 'Zhiyun Jiang', 'Zhanchao Zhou', 'Zhenzhong Lan']

- ***What's New***: 이 논문에서는 Transformer 모델의 주목 집중 문제를 해결하기 위한 새로운 접근법으로 ResFormer와 SVFormer를 제안합니다. ResFormer는 주목 기제의 가치(Value)를 첫 번째 레이어에서 모든 레이어로 잔차 연결(Residual Connection)을 통해 전달하여, 다층 주목에서 발생하는 주목 집중 문제를 완화합니다. SVFormer는 모든 레이어가 첫 번째 레이어에서의 동일한 가치 임베딩을 공유하도록 하여 KV 캐시를 50% 가까이 줄입니다.
- ***Technical Details***: ResFormer는 기존의 교차 레이어 주목(Cross-Layer Attention)을 대체하여, 현재 레이어와 첫 번째 레이어의 가치 벡터를 결합하는 잔차 연결을 사용합니다. 이는 확산되는 주목 패턴을 유지하면서도 더 깊은 레이어에서도 정보 전달을 개선합니다. SVFormer는 모든 레이어에 대해 동일한 첫 번째 레이어의 가치를 공유하여 연산과 캐시 사용을 효율화하며, 이를 통해 모델의 학습 속도를 크게 향상시킵니다.
- ***Performance Highlights***: 실험 결과 ResFormer는 vanilla Transformer, DenseFormer, NeuTRENO를 모든 설정에서 능가하는 성능을 보이며, SVFormer는 특히 시퀀스 길이가 길수록 vanilla Transformer보다 더 나은 성능을 보입니다. 또한, GQA와 같은 기존의 KV 효율적인 방법과 병용할 수 있어 더욱 높은 성능을 발휘할 수 있습니다.

### [ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment](https://arxiv.org/abs/2410.18194)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18194.png)

Vote: 0

Authors: ['Elyas Obbad', 'Brando Miranda', 'Suhana Bedi', 'Sanmi Koyejo', 'Kamal Obbad', 'Iddah Mlauzi', 'Rylan Schaeffer']

- ***What's New***: ZIP-FIT은 데이터 선택을 위한 임베딩 프리(Embedding-Free) 프레임워크로, gzip 압축을 통해 잠재적 훈련 데이터와 목표 작업 분포(Task Distribution) 간의 정렬을 직접 측정합니다. 이는 기존의 해싱 기법이나 단순한 통계적 특징에 의존하지 않고, 구문적 및 구조적 패턴을 포착하여 진정으로 작업에 적합한 데이터를 보다 정확하게 선택할 수 있게 합니다.
- ***Technical Details***: ZIP-FIT은 gzip 압축 기반으로 소스 데이터와 타겟 데이터 셋 간의 정렬을 측정하며, 선택된 데이터는 더 빠르고 효율적인 모델 학습을 가능하게 합니다. 압축을 활용하여 데이터 정렬을 측정하는 것은 전통적인 임베딩 기반 방법에 비해 경량화되고 계산 효율적입니다. NCD(Normalized Compression Distance)를 계산하여 각 샘플의 정렬 점수를 매기고, 가장 높은 점수를 가진 데이터를 선택하여 훈련에 사용합니다.
- ***Performance Highlights***: ZIP-FIT을 통해 선택된 데이터로 훈련된 모델은 DSIR 및 D4와 같은 기존 방법들보다 85.1% 더 빠르게 수렴하며, 테스트 손실이 최소화됩니다. 또한 데이터 선택 속도는 DSIR보다 최대 65.8% 더 빠르게 처리됩니다. 이러한 결과는 ZIP-FIT의 데이터 선택이 AutoFormalization과 파이썬 코드 생성과 같은 작업에서 뛰어난 성능을 발휘함을 보여주며, 더 적은 양의 고품질 데이터가 크기는 크지만 덜 표적화된 데이터보다 뛰어날 수 있음을 입증합니다.

### [Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance](https://arxiv.org/abs/2410.13816)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13816.png)

Vote: 1

Authors: ['Sergey Levine', 'Aviral Kumar', 'Mitsuhiko Nakamoto', 'Oier Mees']

- ***What's New***: 이 논문에서는 대형 로봇 모델(Generalist Robotic Policies)의 성능을 시험 배치 시 오프라인 RL(Value Functions)을 통해 서로 다른 정책의 액션을 결정하는 값 함수(Value-Guided Policy Steering; V-GPS)를 제안합니다. 상당히 규격화된 로봇 정책 또는 모델 가중치에 접근하지 않고도 다양한 일반 로봇 정책과 호환됩니다.
- ***Technical Details***: V-GPS는 오프라인 강화학습(Offline RL)을 통하여 학습된 값 함수(Q-function)를 통해 여럿의 액션 제안 중 성과 가능성이 높은 액션을 재순위화함으로써 최종적으로 선택하여 실행합니다. 이 Q-function은 로봇 데이터셋으로 사전 학습되며, 텍스트 지시를 기반으로 상태와 행동을 값 기능으로 변환하는 과정을 포함합니다.
- ***Performance Highlights***: 본 연구에서는 V-GPS가 다양한 형태의 오픈 소스 일반 로봇 정책에서 평균 성공률을 82%까지 개선할 수 있음을 시뮬레이션과 실제 환경 실험을 통해 입증했습니다. 특히 현실 환경에서의 임무 수행에서 자동적이고 일정한 성능 향상을 달성하였습니다.

### [CAMEL-Bench: A Comprehensive Arabic LMM Benchmark](https://arxiv.org/abs/2410.18976)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18976.png)

Vote: 4

Authors: ['Jorma Laaksonen', 'Ines Riahi', 'Ali Alharthi', 'Abduljalil Saif', 'Rao M. Anwer', 'Ahmed Heakl', 'Sara Ghaboura', 'Salman Khan', 'Omkar Thawakar', 'Fahad S. Khan']

- ***What's New***: CAMEL-Bench는 8개의 다양한 도메인과 38개의 세부 도메인을 아우르는 최초의 포괄적인 아랍어 대형 멀티모달 모델(Arabic LMM) 평가 벤치마크입니다. 이 벤치마크는 41,000개의 샘플 중 품질이 검증된 29,036개의 질문을 기반으로 합니다. 주로 영어 중심이던 기존 LMM 평가 벤치마크와 달리 아랍어를 사용하는 4억 명 이상의 인구를 대표하고자 합니다.
- ***Technical Details***: CAMEL-Bench는 다양한 실제 시나리오에 적합하도록 설계된 8개의 도메인으로 구성됩니다. 각 도메인은 세부적인 하위 도메인으로 나뉘며, 예를 들어 조합형 시각적 인식, 수학 및 논리 추론, 비디오 이해 등 아랍어 환경에서 필수적인 기술을 평가하도록 최적화되어 있습니다. 데이터는 원래 아랍어 문맥에서 수집되었거나, 영어 샘플을 GPT-4o를 통해 아랍어로 번역하고 엄밀하게 검증하였으며, 모든 과정은 원어민의 철저한 검토를 거쳤습니다.
- ***Performance Highlights***: 실험 결과 GPT-4o가 여러 도메인에서 탁월한 성능을 보였으나(MM 추론 57.90, 차트/다이어그램 이해 73.57), 모든 모델이 원격 감지(remote sensing), 아랍어 OCR 및 문서 이해와 같은 특정 영역에서는 어려움을 겪었습니다. 개방형 소스 모델 중에서는 Pangea-7B가 다중 모드 이해(40.09) 및 OCR(26.47) 영역에서 상대적으로 뛰어난 성과를 보였으나, 여전히 특화된 작업에서는 개선이 필요합니다.

### [Can Knowledge Editing Really Correct Hallucinations?](https://arxiv.org/abs/2410.16251)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16251.png)

Vote: 34

Authors: ['Xiongxiao Xu', 'Ali Payani', 'Kai Shu', 'Baixiang Huang', 'Canyu Chen']

- ***What's New***: 이 연구에서는 대형 언어 모델(LLM; Large Language Models)의 환각(Hallucinations)을 교정하기 위해 지식 편집(Knowledge Editing)을 실제로 얼마나 효과적으로 사용할 수 있는지 평가하는 새로운 벤치마크, HalluEditBench를 제안했습니다. 이는 9개의 도메인과 26개의 주제에 걸쳐 6,000개 이상의 환각 데이터를 포함하고 있습니다.
- ***Technical Details***: HalluEditBench는 위키피디아 기반의 방대한 환각 데이터셋을 구축하고, LLM의 성능을 5가지 측면(효율성, 일반화성, 이식성, 지역성, 견고성)에서 평가합니다. 주요 지식 편집 기술로는 FT-L, FT-M, MEMIT, ROME, LoRA, ICE, GRACE 등이 포함되어 있으며, 이 기술들은 각기 다른 측면에서 성능을 테스트받고 있습니다.
- ***Performance Highlights***: 기존의 평가 데이터셋에서 거의 100%의 정확도를 보였던 기술들도 실제 환각 수정에서는 더 낮은 성과를 보임을 발견했습니다. 특히 ICE와 GRACE가 높은 효과성을 보였으나 견고성에서는 낮은 점수를 기록했습니다. 각 모델과 도메인에 따라 지식 편집의 효과성이 다르게 나타나며, 모든 기술이 모든 평가 측면에서 고르게 높은 성과를 보이지는 않았습니다.

### [Stable Consistency Tuning: Understanding and Improving Consistency Models](https://arxiv.org/abs/2410.18958)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18958.png)

Vote: 4

Authors: ['Fu-Yun Wang', 'Hongsheng Li', 'Zhengyang Geng']

- ***What's New***: Stable Consistency Tuning(SCT)은 Consistency Models의 성능 향상을 위해 Variance Reduction 등의 기술을 도입한 새로운 접근방식입니다. 특히 SCT는 CIFAR-10과 ImageNet-64와 같은 벤치마크에서 기존의 최고 성능을 갱신하며, Consistency Models의 잠재력에 대한 새로운 시각을 제공합니다.
- ***Technical Details***: SCT는 마코프 결정 과정(Markov Decision Process; MDP)으로 Consistency Model의 학습 과정을 이해하고 분석합니다. 기존의 Easy Consistency Tuning(ECT) 기반으로 Variance Reduction을 위해 Score Identity를 적용하고, 훈련 안정성을 높이기 위해 점진적인 훈련 스케줄과 다단계 샘플링을 도입했습니다. 또한, Consistency Distillation과 Consistency Training/Tuning의 차이를 Temporal Difference Learning(TD Learning) 관점에서 상세히 분석하여 각 방법의 한계와 강점을 설명합니다.
- ***Performance Highlights***: SCT는 ImageNet-64에서 1-step FID 2.42, 2-step FID 1.55를 기록하며 Consistency Models의 새로운 State of the Art를 달성했습니다. 이는 이전의 ECT 대비 더욱 빠른 수렴 속도와 높은 성능 상한선을 보여줍니다. 또한, Variance Reduced Target 사용으로 1-step FID에서의 비약적인 감소를 보여줍니다.

### [Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering](https://arxiv.org/abs/2410.15999)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.15999.png)

Vote: 3

Authors: ['Giwon Hong', 'Yu Zhao', 'Aryo Pradipta Gema', 'Kam-Fai Wong', 'Pasquale Minervini', 'Alessio Devoto', 'Hongru Wang', 'Xiaotang Du']

- ***What's New***: 이 연구는 SAE(Spare Auto-Encoder)를 기반으로 한 표현 공학(Representation Engineering)을 사용하여 대형 언어 모델(LLM)의 지식 선택 행동을 조정하는 새로운 방법을 제시합니다. SPARE는 사전 훈련된 희소 오토인코더(Sparse Auto-Encoders; SAEs)를 활용하여, 추론 시간에 지식 충돌을 해결하는 데 사용됩니다.
- ***Technical Details***: SPARE는 LLM의 내부 활성화를 편집하여 지식 선택 행동을 제어합니다. SAE는 다량의 단일 의미론적 기능 사전을 생성하여 복잡한 다의적 활성화를 해석합니다. SPARE는 특정 지식 선택 행동과 관련된 SAE 활성화를 식별한 후, 이를 사용하여 모델의 내부 활성화를 편집할 수 있습니다. 이를 통해 추론 시간에 모델이 사용할 지식 원천을 조정할 수 있습니다.
- ***Performance Highlights***: SPARE는 열린 도메인의 질문-응답 작업에서 기존의 표현 공학과 대비 디코딩 방법보다 더 나은 성능을 보였습니다. SAE 기능의 0.05% 미만을 사용하여, SPARE는 LLM의 지식 선택 행동을 효과적으로 제어하며, 최신 표현 공학 방법들보다 10%, 대비 디코딩 방법들보다 15% 더 높은 정확성을 달성했습니다.

### [Language Models are Symbolic Learners in Arithmetic](https://arxiv.org/abs/2410.15580)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.15580.png)

Vote: 3

Authors: ['Ruidi Chang', 'Hanjie Chen', 'Chunyuan Deng', 'Zhiqi Li', 'Roy Xie']

- ***What's New***: 이 논문은 대형 언어 모델(Large Language Models; LLMs)가 산술 학습에서 부분적 결과를 사용하는지와 순전히 기호적(symbolic)으로 만 다루는지를 확인하기 위한 실험을 수행하였습니다. 이 연구는 LLMs가 기호 수준의 패턴 찾기에 더 가까운 학습을 하고 있음을 발견하였습니다.
- ***Technical Details***: 연구에서는 부분적 결과(partial products)를 LLM가 산술 학습에서 어떻게 사용하는지를 검토하였습니다. 산술 작업을 하위 그룹으로 분해하여 그룹의 복잡성과 선택이 학습의 어려움에 어떻게 영향을 미치는지를 확인하였습니다. 다양한 산술 계산 방법 중 표준 곱셈, 반복 덧셈, 격자 방법(lattice method), 그리고 이집트 곱셈(Egyptian multiplication)을 사용하여 실험하였습니다.
- ***Performance Highlights***: LLM은 학습 후 거의 모든 부분적 결과를 식별할 수 있었지만, 반복 덧셈에 대해서는 그 정확도가 매우 낮았습니다. 실험 결과는 특정 산술 학습의 어려움이 라벨 공간 엔트로피와 부분 집단 선택에 크게 좌우됨을 보여주었습니다.

### [MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms](https://arxiv.org/abs/2410.18977)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18977.png)

Vote: 6

Authors: ['Lei Zhang', 'Xuan Ju', 'Shunlin Lu', 'Ling-Hao Chen', 'Wenxun Dai']

- ***What's New***: MotionCLR는 인간 모션 생성 분야에서 인터랙티브 편집 문제를 해결하고자 하는 새로운 방식의 얼텐션 기반 모션 확산 모델로, 상세한 크로스 모달 대응을 명확히 모델링하는데 중점을 두었습니다. 이 모델은 불필요한 재훈련 없이 모션 편집 능력을 강화하며 인간 모션의 생성 및 편집을 지원합니다.
- ***Technical Details***: MotionCLR는 자체 및 크로스-어텐션 메커니즘을 활용하여 각 단어와 모션 프레임 간의 관계를 명확히 표현합니다. 이 모델의 주요 구성 요소는 Convolution Layer, Self-Attention Layer, Cross-Attention Layer, 그리고 FFN Layer로 구성된 CLR 블록입니다. 특히 크로스-어텐션 메커니즘은 문맥의 의미를 모션 시퀀스의 시간 단계에 명확히 활성화시키며, 자신-어텐션 메커니즘은 모션 프레임 간의 상관성을 측정합니다.
- ***Performance Highlights***: 실험 결과, MotionCLR는 최신 방법과 견줄 수 있는 생성 성능을 보이며, 텍스트와 모션의 정렬에서 더 높은 정밀도를 보입니다. 특히 TMR-유사성을 통해 텍스트-모션 정렬이 우수하다는 것을 확인할 수 있었으며, 모션의 다양성에서도 높은 스코어를 기록했습니다. 이러한 결과는 MotionCLR의 명확한 교차 모달 모델링의 이점을 보여줍니다.

### [WAFFLE: Multi-Modal Model for Automated Front-End Development](https://arxiv.org/abs/2410.18362)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18362.png)

Vote: 6

Authors: ['Nan Jiang', 'Shanchao Liang', 'Lin Tan', 'Shangshu Qian']

- ***What's New***: WAFFLE는 새로운 구조 인식 주의 메커니즘과 대조적 미세 조정 기법을 사용하여, UI 이미지와 HTML 코드 간의 이해를 향상시킵니다. 이를 통해 UI-to-HTML 코드 생성을 위한 다중 모달 모델의 효율성을 획기적으로 향상시킵니다.
- ***Technical Details***: WAFFLE는 구조 인식 주의 메커니즘(Structure-Aware Attention)을 통해 HTML의 계층적 구조 이해를 증진시키며, 대조적 학습(Contrastive Learning)을 활용하여 UI 이미지와 HTML 코드 간의 세부 차이를 학습합니다. 또한 새로운 데이터셋을 생성하여 23만 쌍 이상의 웹페이지와 HTML 코드로 모델을 훈련합니다.
- ***Performance Highlights***: WAFFLE로 미세 조정된 모델은 HTML 매칭(HTML Match)에서 최대 9%포인트 향상, CW-SSIM에서 0.0982 향상, CLIP에서 32.99 향상, LLEM에서 27.12%포인트 향상으로 기존 방법을 능가했습니다. 이는 두 가지 백본 모델에서도 기존 대조 실험보다 월등한 성능을 보여줍니다.

### [Why Does the Effective Context Length of LLMs Fall Short?](https://arxiv.org/abs/2410.18745)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18745.png)

Vote: 9

Authors: ['Shansan Gong', 'Lei Li', 'Yao Luo', 'Lingpeng Kong', 'Jun Zhang', 'Ming Zhong', 'Chenxin An', 'Jingjing Xu']

- ***What's New***: 이 논문에서는 대형 언어 모델(LLMs)의 효과적인 컨텍스트 길이가 훈련된 길이보다 짧아지는 원인을 밝히고 이를 개선하기 위한 새로운 방법인 STRING(ShifTed Rotray position embeddING)을 소개합니다. 이 방법은 훈련된 위치를 이동시켜 비효율적인 위치를 대체함으로써 모델이 기존의 훈련된 길이 내에서 더 나은 성능을 발휘하도록 합니다.
- ***Technical Details***: STRING은 좌측으로 치우친 위치 빈도 분포가 전체 훈련과정에 미치는 영향을 분석한 후 제안되었습니다. STRING은 자주 나타나는 위치 인덱스를 좌하단 삼각형으로 이동시켜 정보 수집을 최적화합니다. Flash Attention을 사용하여 효율적으로 구현할 수 있으며, 추가적인 훈련 없이도 적용 가능합니다.
- ***Performance Highlights***: 실험 결과, STRING은 Llama3.1 70B 및 Qwen2 72B와 같은 최신 대형 모델의 성능을 10점 이상 개선하였으며, RULER 및 InfiniteBench와 같은 인기 있는 장문 컨텍스트 벤치마크에서 새로운 최고 성능을 기록했습니다. STRING을 통합한 Llama 3.1 모델은 상용 모델인 GPT-4-128K를 능가하는 성과를 보였습니다.

### [Taipan: Efficient and Expressive State Space Language Models with Selective Attention](https://arxiv.org/abs/2410.18572)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18572.png)

Vote: 2

Authors: ['Huy Huu Nguyen', 'Ryan A. Rossi', 'Trung Bui', 'Thien Huu Nguyen', 'Viet Dac Lai', 'Franck Dernoncourt', 'Chien Van Nguyen', 'Hanieh Deilamsalehy', 'Puneet Mathur', 'Thang M. Pham', 'Ruiyi Zhang']

- ***What's New***: Taipan은 런타임과 메모리 사용의 효율성을 살리면서도 긴 문맥의 언어 모델링에서 뛰어난 성능을 제공하는 새로운 하이브리드 아키텍처를 소개합니다. 이는 Mamba-2 모델과 Selective Attention Layers(SALs)라는 특징적인 레이어를 결합하여 긴 거리의 문맥 상호작용이 필요한 토큰을 선택적으로 보강합니다.
- ***Technical Details***: Taipan은 Mamba-2 블록과 SALs를 주기적으로 삽입해 하이브리드 구조를 형성하며, 각 선택된 토큰은 SALs에서 특징 정제(feature refinement)와 주의 메커니즘 강화 과정을 거칩니다. 이로 인해 중요도가 낮은 토큰들은 주의 메커니즘을 생략하고, Mamba의 Markovian 특성만으로 처리됩니다. 이를 통해 Taipan은 효율적인 메모리 사용과 고성능을 동시에 구현합니다. 또한, Sliding Window Attention(SWA)을 활용하여 이론적으로 무한한 문맥 길이를 처리할 수 있습니다.
- ***Performance Highlights***: 다양한 규모와 작업에 대해 Taipan은 일관되게 상위 성능을 보여주며, 1백만 토큰에 이르는 긴 문맥에서도 뛰어난 예측 정확성을 유지합니다. 이는 Transformer, Mamba, Jamba 모델들과 비교했을 때 특히 긴 문맥에서 낮은 perplexity 및 latency를 자랑하며, 메모리 집약적인 작업에서도 가장 낮은 지연 시간을 기록합니다.

### [Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models](https://arxiv.org/abs/2410.18252)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18252.png)

Vote: 1

Authors: ['Sophie Xhonneux', 'Michael Noukhovitch', 'Aaron Courville', 'Rishabh Agarwal', 'Shengyi Huang', 'Arian Hosseini']

- ***What's New***: Asynchronous RLHF는 기존의 대형 언어 모델(LLM)의 강화 학습을 위한 RLHF의 새로운 패러다임을 제시합니다. 이 접근 방식은 비동기적으로 샘플을 생성하고 학습함으로써 훈련 속도를 40% 향상시키면서도 동기적 방법과 동일한 최종 성능을 유지합니다. 또한, 비정책적(off-policy) 데이터 학습을 통해 계산 자원을 최적화하려는 연구의 첫걸음을 내딛었습니다.
- ***Technical Details***: 기존의 RLHF는 온라인, 정책 기반(on-policy) RL로서, LLM 신호를 통해 동기적으로 반응을 생성하고 보상 모델로 라벨링한 후에 업데이트를 수행합니다. 이에 비해 Asynchronous RLHF는 생성과 학습을 분리하여 비동기적으로 진행하며, 이전 모델의 피드백을 통한 비정책적 학습을 활용합니다. 이러한 비동기적 접근 방식을 통해 더 나은 계산 효율성을 제시합니다.
- ***Performance Highlights***: Asynchronous RLHF는 2.8B Pythia 모델을 기준으로 약 25% 더 빠르게 훈련을 진행하면서, 동기적 방법과 동일한 최종 성과를 달성했습니다. LLaMA 3.1 8B 모델의 경우, 비동기적 접근이 동기적 접근에 비해 훈련 속도를 40%가량 향상시켰습니다. 이러한 성능 향상은 주로 vllm과 같은 효율적인 생성 라이브러리의 활용 덕분입니다.

### [Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances](https://arxiv.org/abs/2410.18775)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18775.png)

Vote: 7

Authors: ['Shilin Lu', 'Adams Wai-Kin Kong', 'Yuanzhi Zhu', 'Zihan Zhou', 'Jiayou Lu']

- ***What's New***: 이 연구는 W-Bench라는 종합 벤치마크를 소개하여 대규모 생성 모델 기반의 다양한 이미지 편집 기술에 대한 워터마킹 방법의 견고성을 평가합니다. VINE라는 새로운 워터마킹 방법을 제안하여 이미지 편집에 대한 견고성을 크게 향상시키면서 높은 이미지 품질을 유지합니다.
- ***Technical Details***: VINE는 고주파 패턴이 제거되고 저주파 대역은 상대적으로 안전하다는 이미지 편집의 주파수 특성을 분석하여, 이를 학습에 사용할 공격 대체물로 활용합니다. 여기서 주파수 특성이 유사한 블러 왜곡을 통해 워터마킹의 견고성을 개선했습니다. 또한, SDXL-Turbo라는 대규모 사전 학습된 확산 모델을 활용하여 더 미세하고 견고한 워터마크 임베딩을 구현했습니다.
- ***Performance Highlights***: VINE는 다양한 이미지 편집 기술 하에서도 뛰어난 워터마킹 성능을 보여주며 기존 방법들을 품질과 견고성 면에서 뛰어넘습니다. 특히, 다양한 편집 방법에서도 높은 품질의 이미지와 워터마크 추출 정확성을 유지하며, 최신 기법을 능가하는 성능을 나타냅니다.

### [LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias](https://arxiv.org/abs/2410.17242)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.17242.png)

Vote: 1

Authors: ['Sai Bi', 'Noah Snavely', 'Tianyuan Zhang', 'Hanwen Jiang', 'Haian Jin', 'Kai Zhang', 'Hao Tan', 'Zexiang Xu', 'Fujun Luan']

- ***What's New***: 이 연구는 공간-보기 입력으로부터 대규모 쉽게 확장 가능한 새로운 보기 합성을 위해 최소한의 3D 귀납적 편견(3D Inductive Bias)만 있는 새로운 변환기 기반 접근법인 Large View Synthesis Model (LVSM)을 제안합니다. 이를 통해 전통적인 3D 표현이 아닌 완전한 데이터 기반의 접근법으로 새로운 보기 합성을 수행할 수 있게 되었습니다.
- ***Technical Details***: LVSM은 인코더-디코더(Encoder-Decoder) 그리고 디코더 전용(Decoder-Only) 두 가지 변형을 포함합니다. 인코더-디코더 모델은 입력 이미지를 고정된 1D 잠재 토큰(latent tokens)으로 인코딩하고, 이를 통해 새로운 보기를 디코딩하여 장면의 완전히 학습된 표현을 제공합니다. 반면, 디코더 전용 모델은 중간 장면 표현 없이 직접적으로 입력 이미지를 새로운 시각으로 변환합니다. 두 모델 모두 3D 구조가 아닌 데이터 기반의 접근법으로 포토리얼리스틱한 품질을 달성했습니다.
- ***Performance Highlights***: 두 변형 모두 기존의 최고 성능 방법들을 1.5에서 3.5 dB PSNR의 상당한 마진으로 능가했습니다. 특히, 디코더 전용 LVSM은 뛰어난 확장성과 Zero-shot 일반화 성능을 보여주었으며, 인코더-디코더 모델은 빠른 추론 속도로 눈에 띄었습니다. 이러한 결과는 여러 벤치마크 데이터셋에서의 우수한 성능을 입증하였습니다.

### [Should We Really Edit Language Models? On the Evaluation of Edited Language Models](https://arxiv.org/abs/2410.18785)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18785.png)

Vote: 3

Authors: ['Xiaowen Chu', 'Zhenheng Tang', 'Peijie Dong', 'Zeyu Li', 'Xinglin Pan', 'Xiang Liu', 'Qi Li']

- ***What's New***: 이 연구에서는 언어 모델(Language Model; LM)의 지식을 효율적으로 업데이트하는 모델 편집(Model Editing) 방법에 대한 종합적인 평가를 수행했습니다. 기존의 편집 방법은 일반적인 성능 저하를 피하기 어려우며, 소규모 지식 업데이트에만 적합하다는 결론에 이르렀습니다.
- ***Technical Details***: 이번 연구는 다양한 모델 편집 방법과 언어 모델에 대해 평가했습니다. Meta-Learning 기반 MEND, Locate-then-Edit 방법으로 KN, ROME, MEMIT, PMET, Retrieval 방법으로 SERAC, Extra-parameter 방식의 GRACE를 비교하였습니다. Llama2-7B, Mistral-7B, GPT2-XL, Pythia 모델들에서 실험을 진행했습니다.
- ***Performance Highlights***: 편집 방법들은 적은 수의 편집에서는 모델의 능력을 잘 유지했으나, 편집 횟수가 50을 넘어갈 경우 모델의 성능 저하가 관찰되었습니다. 특히 ROME, MEND는 성능이 급격히 저하되는 반면, PMET, MEMIT는 수백 번의 편집 이후에도 비교적 안정적인 성능을 보였습니다. 그러나, 수천 번의 편집이 진행되면 모델의 내재된 지식 구조가 완전히 손상되는 '뮤팅 효과(Muting Effect)'가 나타났습니다.

### [Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch](https://arxiv.org/abs/2410.18693)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18693.png)

Vote: 30

Authors: ['Xiaobo Liang', 'Min Zhang', 'Xinyu Shi', 'Yuyang Ding', 'Qiaoming Zhu', 'Juntao Li']

- ***What's New***: 이 논문은 ScaleQuest라는 새로운 데이터 합성 방법을 제안하여 대형 언어 모델(LLM)의 추론 능력을 향상시킵니다. 기존의 복잡한 시드 데이터 없이도 '소형 모델'(7B)의 오픈소스 모델을 사용하여 초대형 데이터를 스크래치(scratch)로 생성할 수 있는 방법을 제공합니다.
- ***Technical Details***: ScaleQuest는 문제 해결 모델(problem-solving model)을 이용하여 질문을 생성하는 두 단계의 질문 조율 프로세스인 질문 미세조정(Question Fine-Tuning; QFT)과 질문 선호 최적화(Question Preference Optimization; QPO)를 사용합니다. 이어서 생성된 질문들은 언어 명확성, 해결 가능성, 난이도에 따른 필터링 과정을 거쳐 고품질 응답을 선택하기 위한 보상 기반의 필터링 전략을 도입합니다.
- ***Performance Highlights***: ScaleQuest를 통해 구성된 합성 데이터셋은 MATH 벤치마크에서 주류 오픈소스 모델의 성능을 29.2%에서 46.4%까지 향상시킵니다. Qwen2-Math-7B-Base 모델을 단순히 이 데이터셋으로 미세 조정했을 때, 폐쇄형 모델인 GPT-4-Turbo와 Claude-3.5 Sonnet을 능가하였습니다. 이는 추론 데이터 합성의 가능성과 효과를 보여줍니다.

### [Unbounded: A Generative Infinite Game of Character Life Simulation](https://arxiv.org/abs/2410.18975)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18975.png)

Vote: 16

Authors: ['Yael Pritch', 'Nataniel Ruiz', 'Mohit Bansal', 'Michael Rubinstein', 'Neal Wadhwa', 'Yuanzhen Li', 'David E. Jacobs', 'Jialu Li']

- ***What's New***: UNBOUNDED는 최신 생성 모델(Generative Models)의 발전을 활용하여 기존의 한정된 시스템을 초월한 최초의 생성형 무한 게임(Generative Infinite Game)을 소개합니다. 이 게임은 사용자가 자연어를 통해 자율적 가상 캐릭터와 상호 작용하여 끝없이 계속되는 플레이 경험을 제공합니다.
- ***Technical Details***: UNBOUNDED는 실시간으로 게임 메커니즘, 내러티브, 캐릭터 상호작용을 생성할 수 있는 특화된 디스틸 대형 언어 모델(LLM)과 다양한 환경에서 캐릭터의 일관된 비주얼 생성을 보장하는 새로운 IP- 어댑터(Image Prompt Adapter)를 통합하여 개발되었습니다. 이를 통해 사용자는 고유의 캐릭터를 정의하고, 인터랙티브한 속도로 다양한 환경을 탐험할 수 있습니다.
- ***Performance Highlights***: 정성적 및 정량적 분석을 통해 기존의 방법들에 비해 캐릭터와 환경의 시뮬레이션, 사용자 지시 따름, 내러티브 일관성, 그리고 비주얼 일관성에서 상당한 개선을 보여주었습니다.

### [Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits](https://arxiv.org/abs/2410.18234)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18234.png)

Vote: 1

Authors: ['Arash Behboodi', 'Hassan Dbouk', 'Christos Louizos', 'Roland Memisevic', 'M. Reza Ebrahimi', 'Ashish Khisti']

- ***What's New***: 이 논문에서는 멀티-드래프트 추측 샘플링(Multi-Draft Speculative Sampling)을 소개합니다. 이는 최적 수용 확률을 달성하기 위한 두 단계 솔루션을 제안하며, 이는 중요도 샘플링(Importance Sampling)과 단일 드래프트 추측 샘플링(Single-Draft Speculative Sampling)으로 구성됩니다.
- ***Technical Details***: 이 연구는 토큰 레벨의 최적 드래프트 선택 규칙을 이항 프로그램 문제로 정립합니다. 두 개의 동일한 드래프트 모델의 경우, 최적 수용 확률의 명시적 표현을 제공하며, 최적 확률을 1로 만드는 기초 조건을 제시합니다. 또한, 가중 중요도 샘플링(Weighted Importance Sampling)을 기반으로 한 새로운 토큰 선택 방법을 제안합니다.
- ***Performance Highlights***: 제안된 방법은 다양한 실험에서 블록 효율(Block Efficiency)과 토큰 속도(Token Rate)에서 기존의 방법들보다 일관된 성능 향상을 보였습니다. 특히, 동일하지 않은 분포를 갖는 드래프트 모델에서도 성능이 우수하였습니다.

### [Data Scaling Laws in Imitation Learning for Robotic Manipulation](https://arxiv.org/abs/2410.18647)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18647.png)

Vote: 1

Authors: ['Fanqi Lin', 'Jiacheng You', 'Chuan Wen', 'Pingyue Sheng', 'Yingdong Hu', 'Yang Gao']

- ***What's New***: 이 논문은 로보틱스, 특히 로봇 조작에서 데이터 스케일링이 유사한 법칙이 존재하는지 여부를 조사합니다. 데이터 스케일링을 통해 단일 작업 로봇 정책을 어떠한 환경에서도 같은 카테고리의 임의의 객체에 대해 제로-샷으로 배포 가능한지에 대해 실증적인 연구를 수행하여 새로운 데이터 수집 전략을 제안합니다.
- ***Technical Details***: 연구는 다양한 환경과 객체에서 4만 회 이상의 데모를 수집하여, 데이터 스케일링이 로봇 조작의 일반화 성능에 어떻게 영향을 미치는지를 조사합니다. 연구는 행동 복제를 이용하여 환경과 객체의 다양성이 데모의 양보다 더 중요함을 밝혀내고, 효율적인 데이터 수집 전략이 있는지를 확인합니다. 이 전략으로 두 가지 새로운 작업에 대해 한낮 동안 데이터 수집을 통해 90% 성공률을 달성했습니다.
- ***Performance Highlights***: 폴 워터(Pour Water)와 마우스 정렬(Mouse Arrangement) 작업에서 새로운 환경 및 객체에서 각각 90% 이상의 성공률을 보였습니다. 이번 실험 결과는 로봇 조작의 제로-샷 일반화 성능을 획기적으로 개선할 수 있는 가능성을 보여주며, 본 연구를 통해 제안된 데이터 수집 전략이 적은 시간과 자원을 들여 유의미한 성과를 낼 수 있음을 시사합니다.

### [Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs](https://arxiv.org/abs/2410.18451)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18451.png)

Vote: 7

Authors: ['Chaojie Wang', 'Liang Zeng', 'Jujie He', 'Rui Yan', 'Shuicheng Yan', 'Yahui Zhou', 'Yang Liu', 'Jiacai Liu', 'Chris Yuhao Liu']

- ***What's New***: 이번 연구에서는 대형 언어 모델(LLMs)에서 리워드 모델링(Reward Modeling)을 개선하기 위한 다양한 기술을 소개합니다. 특히, Skywork-Reward 데이터셋을 통해 기존 데이터보다 훨씬 적은 80K 선호 페어를 활용하여 모델 성능을 향상시켰습니다. 이 방법론은 LLMs의 성능 향상을 직접적으로 촉진하며, RewardBench 리더보드에서 강력한 성능을 발휘하여 실제 선호 학습 응용에 실질적인 영향을 미쳤습니다.
- ***Technical Details***: Skywork-Reward 모델 시리즈는 Skywork-Reward-Gemma-27B와 Skywork-Reward-Llama-3.1-8B로 구성되어 있으며, Discriminative 모델 카테고리에 속합니다. 주로 공용 데이터를 기반으로 한 경량의 고품질 데이터 수집을 통해 투명성과 재현성을 보장하였습니다. 데이터 선택 및 필터링 과정에서 선택된 데이터를 통해 모델의 성능을 최적화하였으며, 다양한 손실 함수에 대한 실험을 통해 Bradley-Terry 손실이 가장 일관되게 뛰어난 성능을 발휘했습니다.
- ***Performance Highlights***: Skywork-Reward-Gemma-2-27B는 RewardBench에서의 성능 테스트에서 평균 93.8점을 기록하며 최상위 성능을 달성했고, Skywork-Reward-Llama-3.1-8B는 대부분의 경쟁 모델을 능가하는 성능을 보였습니다. 특히 Chat Hard 카테고리에서는 90 이상의 점수를 유일하게 기록하여 뛰어난 성능을 보여주었습니다. 데이터의 품질이 양보다 중요하다는 점을 강조하였으며, 세부적인 데이터 필터링과 선택으로 보다 효과적인 리워드 모델을 구현할 수 있음을 입증했습니다.

### [CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models](https://arxiv.org/abs/2410.18505)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18505.png)

Vote: 3

Authors: ['Shuhao Gu', 'Liangdong Wang', 'Jijie Li', 'Chengwei Wu', 'Quanyue Ma', 'Hanyu Zhao', 'Bo-Wen Zhang', 'Guang Liu', 'TengFei Pan', 'Xiaofeng Shi']

- ***What's New***: CCI3.0-HQ는 고품질 대규모 중국어 데이터를 사전 학습용으로 설계한 새로운 데이터셋으로, 혁신적인 2단계 하이브리드 필터링을 통해 데이터 품질을 크게 향상시켰습니다. 이를 통해 높은 수준의 데이터 정제 및 분류를 도입하여 기존의 CCI3.0, SkyPile, WanjuanV1 등의 데이터셋을 능가하는 성능을 보여줍니다. 또한, 이 데이터셋은 고품질 중국어 웹 데이터 분류를 위한 F1 점수 최적화를 실현하였습니다.
- ***Technical Details***: CCI3.0-HQ 데이터셋은 '기본 처리(Fundamental Processing)'와 '고품질 처리(High-Quality Processing)'의 두 단계로 구성된 데이터 처리 파이프라인을 채택했습니다. 기본 처리에는 안전 필터링, 텍스트 추출 및 정리, 문서 수준 중복 제거, 품질 분류 기반 필터링이 포함됩니다. 고품질 처리에서는 Qwen2-72B-Instruct 모델을 활용해 고품질 샘플을 식별하고, 0.5B의 품질 분류기를 학습시키는 과정을 통해 규모 있는 데이터셋으로 정제합니다.
- ***Performance Highlights***: CCI3.0-HQ의 성능 실험 결과, 다양한 기준에서 SkyPile 및 WanjuanV1과 같은 기존 데이터셋을 능가했으며, 특히 중국어 평가 메트릭인 CEval과 CMMLU에서 뛰어난 점수를 기록했습니다. 고품질 필터링 능력을 바탕으로 한 classifierCCI3.0-HQ 분류기는 다른 기존 분류기보다 우수한 성능을 발휘하여, 종합적인 언어 모델 학습에서 더욱 강력한 결과를 보였습니다.

### [Framer: Interactive Frame Interpolation](https://arxiv.org/abs/2410.18978)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18978.png)

Vote: 22

Authors: ['Hao Ouyang', 'Hao Chen', 'Kecheng Zheng', 'Wen Wang', 'Yujun Shen', 'Chunhua Shen', 'Qiuyu Wang', 'Biao Gong', 'Zhekai Chen']

- ***What's New***: Framer는 사용자가 설정한 선행 이미지와 후행 이미지 사이에 매끄러운 전환을 생성할 수 있는 상호작용 프레임 보간(interactive frame interpolation) 시스템을 제안합니다. 사용자는 특정 지점의 이동 경로를 직접 지정함으로써 지역적인 움직임을 세밀하게 조정할 수 있습니다. 뿐만 아니라, '자동 운전(autopilot)' 모드도 제공하여 사용자가 키포인트(keypoints) 경로를 자동으로 추정하고 수정할 수 있습니다.
- ***Technical Details***: Framer는 미리 훈련된 대규모 비디오 확산 모델(video diffusion model)을 활용하여 비디오 프레임 보간을 수행합니다. 사용자는 특정 지점을 따라 경로를 설정하기 위해 제어 분기를 도입할 수 있으며, 추론 단계에서 점 궤적 입력을 기반으로 비디오 보간을 안내합니다. 또한, '자동 운전' 모드에서는 양방향 점 추적(bi-directional point-tracking) 방법을 제안하여 비디오 시퀀스 전체에서 일치하는 점의 경로를 추정하고 업데이트합니다. 이러한 설계를 통해 사용자는 다양한 형태의 이미지 및 타임랩스 비디오 생성 등을 포함한 다양한 응용 프로그램에서 유용한 성능을 발휘합니다.
- ***Performance Highlights***: Framer는 복잡한 움직임과 유의미한 외형 변화가 포함된 경우 기존 방법들에 비해 보다 부드럽고 시각적으로 매력적인 전환을 생성하여 우수한 성능을 입증합니다. 특히 인간 사용자의 선호도 조사에서, Framer가 생성한 결과물이 높은 평가를 받았습니다. 또한, Framer는 PSNR, SSIM, LPIPS 등의 재구축 메트릭과 FID, FVD와 같은 생성 메트릭에서 경쟁 모델들을 능가하는 성능을 보였습니다.

### [Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss](https://arxiv.org/abs/2410.17243)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.17243.png)

Vote: 45

Authors: ['Kehan Li', 'Fei Wu', 'Zhiqiang Hu', 'Deli Zhao', 'Lidong Bing', 'Xin Li', 'Zesen Cheng', 'Sicong Leng', 'Hang Zhang']

- ***What's New***: 이 논문은 대조 손실(Contrastive Loss)으로 인한 GPU 메모리 소비 문제를 해결하고자 새로운 블록 기반 타일(Tile) 계산 전략을 제안합니다. 이 접근법은 유사성 행렬(Similarity Matrix)을 전체적으로 구성하지 않고도 대조 손실을 거의 무한대 배치 크기로 스케일링 할 수 있어 메모리 비용을 크게 줄입니다.
- ***Technical Details***: Inf-CL은 대조 손실 계산을 작고 순차적으로 계산 가능한 타일로 분할하여 메모리 사용을 타일 크기와 평행 타일의 수에 제한합니다. 타일 계산은 코어스-그레인(Cross-GPU)와 파인-그레인(In-GPU)으로 나누어져 있으며, CUDA 핵심 수준에서 행 별 계산이 병렬화되어 I/O 오버헤드를 줄입니다. 또한, 고리 기반 통신(Ring-Based Communication)을 활용하여 GPU 간 동기화를 최적화합니다.
- ***Performance Highlights***: Inf-CL은 기존 메모리 효율적 솔루션들에 비해 메모리 사용을 두 자리 수 감소시키면서도 유사한 속도를 유지하며, 정확성을 떨어뜨리지 않고 32개의 A800 80GB GPU를 사용하여 ViT-L/14 모델의 배치 크기를 10M 이상으로 확장할 수 있습니다.

### [DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations](https://arxiv.org/abs/2410.18860)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18860.png)

Vote: 1

Authors: ['Ahmed Abdulaal', 'Tom Diethe', 'Philip Teare', 'Beatrice Alex', 'Aryo Pradipta Gema', 'Pasquale Minervini', 'Amrutha Saseendran', 'Chen Jin']

- ***What's New***: DeCoRe는 대형 언어 모델(LLMs)의 환각(hallucination) 문제를 완화하기 위한 새로운 디코딩 전략입니다. 모델에서 특정 'retrieval heads'를 마스킹하여 발생하는 환각을 기반 모델과 대비시키는 방식으로, 이 방식을 통해 더 명확하고 사실에 기반한 응답을 생성하도록 합니다.
- ***Technical Details***: DeCoRe는 Transformer 아키텍처에서 'retrieval heads'를 마스킹하여 모델의 정보 회수 능력을 저하시켜 환각을 유도합니다. 그런 다음, 이를 기반 LLM의 출력과 대비시켜 보다 정확한 응답을 생성합니다. 이 과정에서 조건부 엔트로피를 사용하여 다음 토큰 예측의 불확실성을 동적으로 조정합니다.
- ***Performance Highlights***: DeCoRe는 높은 문맥적 충실도를 요구하는 작업에서 성능을 크게 개선했습니다. 요약 작업(XSum)에서 18.6%, 지시사항 충족(MemoTrap)에서 10.9%, 개방형 질문 응답(NQ-Open)에서 2.4%를 개선했으며, MuSiQue를 사용한 장기 추론 작업에서도 성능 향상을 보였습니다.

### [LOGO -- Long cOntext aliGnment via efficient preference Optimization](https://arxiv.org/abs/2410.18533)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18533.png)

Vote: 34

Authors: ['Min Zhang', 'Qiaoming Zhu', 'Zechen Sun', 'Zecheng Tang', 'Juntao Li']

- ***What's New***: LOGO(Long cOntext aliGnment via efficient preference Optimization)는 롱 컨텍스트(Large Context) 모델(LCMs)의 생성 성능 향상을 위한 새로운 학습 전략입니다. 처음으로 효율적인 선호 최적화(Preference Optimization) 기법을 도입하여 긴 컨텍스트 정렬을 가능하게 했습니다.
- ***Technical Details***: LOGO는 GPU 메모리 경계 문제를 극복하기 위해 참조 없는 참조 없는 선호 최적화 전략을 사용하며, 훈련 데이터 구축에 포지션 합성(Position Synthesis) 방법을 채택합니다. Llama-3-8B-Instruct-80K 모델이 실제 장문 컨텍스트 작업에서 GPT-4와 비교할 만한 성과를 달성하도록 0.3B 데이터만으로 훈련할 수 있으며, 기타 작업에서 모델의 기본 기능을 보존합니다.
- ***Performance Highlights***: Llama-3-8B-LOGO 모델은 실제 장문 컨텍스트 작업에서 GPT3.5-Turbo를 크게 앞서며 일부 독점 모델, 예를 들어 GPT-4의 성능에 근접합니다. 또한, LOGO는 짧은 컨텍스트 모델의 컨텍스트 윈도우 크기를 최대 8배까지 확장할 수 있어 성능을 크게 향상시킵니다.

### [ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning](https://arxiv.org/abs/2410.17779)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.17779.png)

Vote: 3

Authors: ['Yong Luo', 'Han Hu', 'Jianyuan Guo', 'Zhiwei Hao', 'Li Shen', 'Yonggang Wen']

- ***What's New***: ADEM-VL은 시각-언어(Vision-Language; VL) 모델 튜닝의 효율성을 높이기 위해 개발된 프레임워크로, 대규모 언어 모델(LLMs)을 활용하여 추가 학습 가능한 매개변수 없이 교차 주의 메커니즘(Cross-Attention)의 유사도 측정 방식을 사용합니다. 이를 통해 학습 및 추론 속도를 크게 개선하고 메모리 복잡성을 줄였습니다.
- ***Technical Details***: ADEM-VL은 다중 스케일 특징 생성(multiscale feature generation)을 비전(vision) 인코더의 단일 순방향 패스에서 수행하여 표현 학습을 강화합니다. 또한, 적응형 융합(adaptive fusion) 메커니즘을 도입하여 텍스트 토큰 별로 주의 점수(attention score)에 따라 비관련 시각 정보를 버림으로써 가장 관련성 있는 시각적 특징에 중점을 둡니다. 이러한 방식은 복잡한 크로스-어텐션 모듈을 단순화하여 매개변수의 수를 현저히 줄이는 데 기여하며, 비전 특징을 언어 공간에 내장시킵니다.
- ***Performance Highlights***: ScienceQA 데이터셋에서 ADEM-VL은 평균 정확도가 0.77% 개선된 94.55%를 기록하며, 기존 방법들을 능가하는 결과를 보여주었습니다. 또한, 학습 및 추론 단계에서는 각각 15%와 3% 빠르게 작동하여 프레임워크의 우월성을 입증했습니다.

### [SMITE: Segment Me In TimE](https://arxiv.org/abs/2410.18538)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18538.png)

Vote: 6

Authors: ['Ghassan Hamarneh', 'Andrea Tagliasacchi', 'Saeid Asgari Taghanaki', 'Amirhossein Alimohammadi', 'Sauradip Nag', 'Ali Mahdavi Amiri']

- ***What's New***: SMITE는 새로운 방법으로 텍스트-이미지 확산 모델을 사용하여 비디오에서 임의의 세분화(Granularity)를 제어하는 비디오 객체 분할을 가능하게 했습니다. 이를 통해 원본 비디오 프레임에서 파생되지 않은 한두 장의 참조 이미지 만으로 비디오의 객체를 세분하게 분할할 수 있습니다.
- ***Technical Details***: SMITE는 사전 훈련된 텍스트-이미지 확산 모델(Text-to-Image Diffusion Model)에 추가적인 시간적 주의를 활용하여 시각적 일관성을 유지합니다. 또한, 주어진 비디오에서 객체를 정확히 추적하고 시간적 투표 메커니즘을 사용하여 각 픽셀의 레이블 일관성을 유지합니다. 자세히는 Inflated UNet과 주목 모듈을 통해 세분화 정확도를 높이고 참조 이미지와의 정렬을 위한 크로스-어텐션을 미세 조정합니다.
- ***Performance Highlights***: 새로 제안된 SMITE-50 벤치마크에서 실험 결과, SMITE는 다른 최신 방법론 대비 더 높은 mIOU와 F-측정치 성능을 기록하였습니다. 예를 들어, 'Faces' 카테고리에서 mIOU가 77.28%, 'Horses'에서 75.09%를 달성하여 다른 방법론보다 뛰어난 성능을 보여주었습니다.

### [Distill Visual Chart Reasoning Ability from LLMs to MLLMs](https://arxiv.org/abs/2410.18798)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18798.png)

Vote: 9

Authors: ['Xuanjing Huang', 'Zhiheng Xi', 'Xiaoran Fan', 'Qi Zhang', 'Wei He', 'Zifei Shan', 'Yiwen Ding', 'Tao Gui', 'Wanxu Zhao']

- ***What's New***: 이 논문에서는 대형 언어 모델(LLMs)에서 다중모달 대형 언어 모델(MMLMs)로 시각적 차트 추론 능력을 증류하는 새로운 방법인 코드-매개 번역(Code-as-Intermediary Translation; CIT)을 제안합니다. 이 방법은 텍스트 기반의 차트 생성 코드와 대규모의 이유 집중 차트, Q&A 쌍을 포함하는 REACHQA 데이터셋을 생성하여 모델의 시각적 인식 및 추론 능력을 강화합니다.
- ***Technical Details***: CIT는 시각적 차트를 텍스트 표현으로 번역하는 매개체로 코드(Code)를 활용합니다. 시작 코드(seed code)로부터 자가 지시(Self-Instruct) 및 진화 지시(Evol-Instruct) 방법을 통해 다양한 차트 생성 코드를 합성하고, 생성된 코드로부터 생성된 차트와 관련 지시(instruction)를 양방향으로 생성합니다. 최종적으로 생성한 데이터셋인 REACHQA는 3,249개의 차트와 19,963개의 Q&A 쌍을 포함합니다.
- ***Performance Highlights***: REACHQA로 미세조정한 모델은 기존의 차트 관련 벤치마크와 복잡한 시각적 추론 작업에서 성능이 크게 향상되었습니다. 특히, LLaVA-Next-Llama3-8B 모델은 평균 성능이 34.8% 향상되었습니다. 또한, 수학적 추론을 평가하는 MathVista와 MATH-Vision에서도 긍정적인 성능을 보여주며, 시각적 추론 능력의 전이 가능성을 입증했습니다.

### [The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI](https://arxiv.org/abs/2410.18441)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.18441.png)

Vote: 1

Authors: ['Fulu Li']

- ***What's New***: 이 논문은 생성 AI의 핵심 기술에 대한 수학적 문제 제시와 확률적 최적화 탐색에 대한 심층 분석을 제공하며, 특히 Transformer 모델의 주요 구성 요소들에 대해 다루고 있습니다. 새로운 접근 방식으로는 Byte-Pair Encoding (BPE)와 WordPiece 접근 방식을 기반으로 한 서브워드 인코딩(SWE)에 대한 최적 솔루션을 제안하고, Word2vec 모델의 하이퍼파라미터를 최적화하기 위한 교차 엔트로피 최적화 방법을 제시합니다.
- ***Technical Details***: 논문에서는 개선된 byte-pair encoding (eBPE) 알고리즘을 통해 훈련 데이터의 우도(likelihood)를 최대화하고, 교차 엔트로피(CE) 기법을 사용한 word2vec 하이퍼파라미터 최적화를 통해 모델 성능을 향상시키는 방법을 제시합니다. 또, rotary positional encoding (RoPE)와 Attention with Linear Biases (ALiBi)를 조합하여 추론 능력을 향상시키도록 제안합니다. PrFlashAttention은 블록 거리에 대한 확률분포를 통해 주어진 Attention 계산에서 어떤 블록이 참여할지를 결정하는 방법으로, autoregressive language models의 텐서 모양을 유지합니다. 또한, Multi-query attention (MQA)에서의 key-value (KV) 캐시의 단계를 조정하여 점진적인 양자화 저하를 통해 모델의 품질을 합리적으로 유지합니다.
- ***Performance Highlights***: 논문에서 제안한 최적화 방법들과 구조의 성능에 대한 실험 결과는 현재의 강화된 알고리즘들이 생성 AI 모델에서 효율적으로 작동함을 보여줍니다. 특히, PrFlashAttention과 SAQ (Staircase Adaptive Quantization) 방법은 효율적이며 비용 절감에도 긍정적인 영향을 미칠 것으로 기대됩니다.

