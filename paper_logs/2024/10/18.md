## Daily Papers (2024-10-18)

### [LLMtimesMapReduce: Simplified Long-Sequence Processing using Large Language Models](https://arxiv.org/abs/2410.09342)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09342.png)

Vote: 34

Authors: ['Zihan Zhou', 'Zhixing Tan', 'Xinyi Chen', 'Rongqiao An', 'Yu Chao', 'Maosong Sun', 'Haoyu Wang', 'Xu Han', 'Zhiyuan Liu', 'Shuo Wang', 'Xiaodong Shi', 'Chong Li', 'Zhili Li', 'Qi Shi']

- ***What's New***: 이 연구에서는 긴 텍스트를 효과적으로 처리하기 위한 새로운 훈련 없는 프레임워크인 LLM×MapReduce를 제안합니다. 이 프레임워크는 문서를 여러 청크(chunks)로 나누고 각 청크의 중간 답변을 집계하는 방식의 '분할 정복(divide-and-conquer)' 접근 방식을 통해 긴 문서의 포괄적인 이해를 가능하게 합니다.
- ***Technical Details***: LLM×MapReduce는 장거리 정보 손실을 최소화하기 위해 맵 단계(map stage)에서 청크 간 의존성 및 충돌 문제를 해결할 수 있는 구조화된 정보 프로토콜(Structured Information Protocol)과 컨텍스트 내 신뢰도 조정 메커니즘(In-Context Confidence Calibration Mechanism)을 도입합니다. 결합된 데이터는 콜랩스 단계(collapse stage)를 통해 압축되고 리듀스 단계(reduce stage)에서 최종 답변을 예측합니다.
- ***Performance Highlights***: InfiniteBench 실험 결과에 따르면 LLM×MapReduce는 성능과 효율성 면에서 기존의 오픈 소스 및 상용 긴 문맥 LLMs와 비교할 때 우수한 성능을 보입니다. 특히, LongAgent와 같은 기존의 분할 정복 프레임워크보다 개선된 성능을 나타냅니다.

### [Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation](https://arxiv.org/abs/2410.13198)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13198.png)

Vote: 1

Authors: ['Jian Xue', 'Michael Levit', 'Sreyan Ghosh', 'Peidong Wang', 'Mohammad Sadegh Rasooli', 'Dinesh Manocha', 'Jinyu Li']

- ***What's New***: 이 논문은 DARAG(Data- and Retrieval-Augmented Generative Error Correction)라는 새로운 접근법을 제안합니다. 이는 ASR(Automatic Speech Recognition) 시스템의 오류 수정을 개선하기 위해 설계되었습니다. DARAG는 훈련 데이터에 포함된 오류 유형의 다양성과 특성을 확장하여 GEC(Generative Error Correction) 모델의 성능을 향상시킵니다.
- ***Technical Details***: DARAG는 두 가지 주요 개선점을 소개합니다. 첫째, LLMs(Large Language Models)와 text-to-speech 모델을 사용하여 오류 발생을 모의 실험하고, 이를 통해 생성된 합성 데이터를 GEC 모델의 훈련 데이터셋에 추가합니다. 둘째, Named Entities(NEs)의 수정을 개선하기 위해 Retrivial-Augmented Correction(RAC)을 도입하여, 데이터베이스에서 유사한 NEs를 검색하여 입력 데이터에 추가합니다.
- ***Performance Highlights***: 다양한 데이터셋과 환경에서 실험한 결과, DARAG는 ID(In-Domain) 환경에서 8% - 30%의 성능을, OOD(Out-Of-Domain) 환경에서 10% - 33%의 개선을 달성했습니다. 구성 요소를 제거한 실험에서도 성능이 감소하였지만, 이는 대부분의 NEs가 테스트 중에 만나는 NEs와 일치하지 않기 때문임을 시사합니다.

### [From Commands to Prompts: LLM-based Semantic File System for AIOS](https://arxiv.org/abs/2410.11843)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11843.png)

Vote: 0

Authors: ['Wenyue Hua', 'Yongfeng Zhang', 'Yongye Su', 'Wujiang Xu', 'Mengnan Du', 'Mingyu Jin', 'Chaoji Zuo', 'Zeru Shi', 'Dong Deng', 'Zirui Liu', 'Yujie Ren', 'Kai Mei']

- ***What's New***: 이 논문은 명령(Commands)을 자연어 프롬프트(Prompts)로 전환하여 AIOS를 위한 대형 언어 모델(LLM) 기반 의미론적 파일 시스템(Semantic File System; LSFS)을 제안하고 있습니다. 이를 통해 사용자가 자연어 프롬프트를 통해 파일을 관리할 수 있도록 하여 전통적인 명령 기반 파일 시스템의 한계를 극복합니다.
- ***Technical Details***: LSFS는 벡터 데이터베이스(Vector Database)를 사용하여 파일에 대한 의미론적 인덱스를 생성합니다. 파일은 의미론적 특성에 기반하여 저장되며, 다양한 시스템 호출(Syscalls)과 API가 설계되어 파일 CRUD(Create, Read, Update, Delete), 그룹 및 결합 작업을 지원합니다. 또한 자연어 처리를 통해 사용자의 프롬프트를 시스템 호출로 변환하는 LSFS 파서(Parser)를 제공합니다.
- ***Performance Highlights***: 실험 결과, LSFS는 전통적인 파일 시스템보다 사용 편의성, 기능적 다양성 및 파일 작업의 정확성과 효율성에서 뛰어난 성능을 나타냈습니다. 특히 복잡한 파일 검색(semantic file retrieval) 및 롤백(rollback) 작업에서 더 높은 정확도와 효율성을 보였습니다. LLM 통합으로 지능형 파일 관리 작업, 예를 들어 콘텐츠 요약이나 버전 비교가 가능해졌습니다.

### [DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](https://arxiv.org/abs/2410.10819)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10819.png)

Vote: 5

Authors: ['Shang Yang', 'Jiaming Tang', 'Haotian Tang', 'Jingwei Zuo', 'Yao Fu', 'Song Han', 'Guangxuan Xiao', 'Junxian Guo']

- ***What's New***: DuoAttention은 대형 언어 모델(LLM)이 긴 문맥을 효율적으로 처리할 수 있도록 'Retrieval Heads'와 'Streaming Heads'의 개념을 도입한 새로운 프레임워크입니다. 이 접근 방식을 통해 메모리 사용량이 줄어들면서도 긴 문맥 처리 능력을 유지할 수 있습니다.
- ***Technical Details***: Retrieval Heads는 문맥적으로 중요한 토큰을 처리하는 데 필요하며 모든 토큰에 대해 전부 주의를 기울입니다. 반면, Streaming Heads는 최근 토큰과 주의 싱크만 집중하므로 더 가벼운 상수 길이의 KV 캐시를 사용합니다. DuoAttention은 최적화 기반 알고리즘과 합성 데이터를 사용하여 Retrieval Heads를 정확하게 식별합니다.
- ***Performance Highlights***: DuoAttention은 MHA 모델에서 메모리 사용량을 최대 2.55배, GQA 모델에서 1.67배 줄였으며, 디코딩 속도는 MHA 모델에서 최대 2.18배, GQA 모델에서 1.50배 빠르게 수행되었습니다. 또한, 8-bit 가중치와 4-bit KV 캐시 양자화와 결합하여 Llama-3-8B 모델이 3.3백만 개의 컨텍스트 토큰을 단일 A100 GPU에서 처리할 수 있도록 지원합니다.

### [FLARE: Faithful Logic-Aided Reasoning and Exploration](https://arxiv.org/abs/2410.11900)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11900.png)

Vote: 0

Authors: ['Pasquale Minervini', 'Erik Arakelyan', 'Patrick Lewis', 'Isabelle Augenstein', 'Pat Verga']

- ***What's New***: FLARE는 자연어 질문 응답(QA) 및 추론 작업에서 문제 공간을 과제 분해를 통해 탐색하는 새로운 해석 가능한 접근 방식을 소개합니다. 이 방법은 대형 언어 모델(LLMs)을 사용해 솔루션 계획을 세우고, 논리 프로그래밍 코드를 통해 쿼리를 사실과 술어로 소프트-형식화하며, 정의된 공간을 통해 철저한 멀티홉 탐색을 시뮬레이션함으로써 중간 추론 과정의 신뢰성을 계산하고 외부 솔버 없이 멀티홉 탐색의 단계를 분석할 수 있게 합니다.
- ***Technical Details***: FLARE는 세 가지 모듈(planning, code generation, searching)을 통해 자연어 쿼리를 수행합니다. 각 쿼리에서 대형 언어 모델(LLM)은 초기 계획을 생성하고, 계획에 기반하여 Prolog 코드를 작성하며, 코드 실행을 시뮬레이션하여 문제 공간을 탐색합니다. Prolog는 선언적 프로그래밍 패러다임으로 탐색 트리에서 서브골을 분해 및 탐색하는 데 유용합니다. 시뮬레이션된 탐색은 사실과 관계를 세분화할 수 있게 하며, FLARE는 검색 경로를 비교하여 추론의 신뢰성을 측정합니다.
- ***Performance Highlights***: FLARE는 9개의 다양한 추론 벤치마크 중 7개에서 최첨단(State-Of-The-Art)의 결과를 기록했으며, 평균적으로 이전 CoT 방식을 능가하는 28%의 성능 향상을 보였습니다. 또한 FLARE는 코드 생성에 특화된 모델에게도 유리하여 F-CoT와 비교 시 평균 16%의 정확도 증가를 제공합니다. 이는 FLARE가 멀티홉 및 심볼릭 추론을 요구하는 데이터셋에서 더 큰 장점을 보여줍니다.

### [FlatQuant: Flatness Matters for LLM Quantization](https://arxiv.org/abs/2410.09426)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09426.png)

Vote: 1

Authors: ['Wulong Liu', 'Haoli Bai', 'Xin Jiang', 'Chun Yuan', 'Xianzhi Yu', 'Lu Hou', 'Jun Yao', 'Han Bao', 'Yuening Li', 'Ruikang Liu', 'Jiaxin Hu', 'Yuxuan Sun', 'Kang Zhao']

- ***What's New***: FLATQUANT은 대형 언어 모델(LLM)의 양자화를 위한 새로운 후처리 양자화 방법으로, 각 선형 계층에 최적화된 학습 가능한 아핀 변환(learnable affine transformations)을 사용합니다. 이를 통해 가중치와 활성화 값의 분포를 고르게 하고 양자화 오류를 줄이는 것이 가능합니다.
- ***Technical Details***: FLATQUANT은 크로네커 분해(Kronecker decomposition)를 활용하여 변환 행렬을 최적화하고, 모든 연산을 단일 커널로 통합하여 실행 오버헤드를 줄입니다. 여러 양자화 설정에 적합하며, LLaMA-2/3 모델 등 다양한 작업에서 평가되었습니다.
- ***Performance Highlights***: FLATQUANT은 W4A4 양자화에서 LLaMA-3-70B 모델에 대해 1% 미만의 정확도 하락을 이루며, 최신 양자화 벤치마크를 설정합니다. QuaRot 대비 2.3배 속도 향상 및 1.7배 디코딩 속도 향상을 달성하였습니다.

### [Latent Action Pretraining from Videos](https://arxiv.org/abs/2410.11758)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11758.png)

Vote: 1

Authors: ['Sejune Joo', 'Seonghyeon Ye', 'Reuben Tan', 'Dieter Fox', 'Baolin Peng', 'Minjoon Seo', 'Luke Zettlemoyer', 'Joel Jang', 'Yu-Wei Chao', 'Jianwei Yang', 'Bill Yuchen Lin', 'Lars Liden', 'Byeongguk Jeon', 'Jianfeng Gao', 'Kimin Lee', 'Ajay Mandlekar']

- ***What's New***: Latent Action Pretraining for general Action models (LAPA)은 인터넷 규모의 비디오 데이터를 활용하여 로봇 행동 레이블 없이 VLA (Vision-Language-Action) 모델을 사전훈련하는 무감독 방법입니다. 이는 현재의 VLA 모델이 대규모 비디오 데이터에서 추론된 잠재 행위(discretized latent actions)를 학습하도록 함으로써 기존의 인간과 로봇 간격에서의 한계를 효과적으로 극복합니다.
- ***Technical Details***: LAPA는 VQ-VAE 기반의 목표를 사용하여 연속 이미지 프레임 간의 잠재적 행동을 학습하기 위해 행동 양자화 모델(action quantization model)을 훈련시킵니다. 이후 이 잠재적 행동을 비디오 관찰 및 작업 설명을 바탕으로 예측하도록 VLA 모델을 사전 훈련하며, 마지막으로 작은 로봇 조작 데이터에 미세 조정하여 잠재 행동을 실제 로봇 행동으로 매핑합니다.
- ***Performance Highlights***: 실험 결과는 LAPA가 대규모 비디오에서 행동 없는(actionless) 데이터를 사용하는 기존 방법들보다 로봇 조작 정책을 훈련함에 있어 뛰어난 성능을 보인다는 것을 보여줍니다. 또한 LAPA는 실세계 조작 작업에서 최신의 VLA 모델인 OPENVLA보다 +6.22% 더 나은 성능을 기록하면서 30배 이상의 사전훈련 효율을 달성했습니다. 이러한 결과는 LAPA가 다양한 로봇 데이터셋에서 로봇의 환경 중심 행동을 효과적으로 캡처함을 시사합니다.

## Daily Papers (2024-10-18)

### [Retrospective Learning from Interactions](https://arxiv.org/abs/2410.13852)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13852.png)

Vote: 1

Authors: ['Yiwei Chen', 'Zizhao Chen', 'Anne Wu', 'Mustafa Omer Gul', 'Yoav Artzi', 'Gloria Geng']

- ***What's New***: RESPECT 방법은 대화의 과거 상호작용에서 얻은 신호를 활용하여 대규모 언어 모델(LLM)이 계속해서 학습할 수 있게 합니다. 이 접근 방식은 외부 주석 없이 다중 턴 상호작용에서 신호를 추출하여 적응적 학습을 가능하게 합니다.
- ***Technical Details***: RESPECT는 사용자의 지시를 기반으로 하여 LLM이 다중 턴 추론 작업을 수행하도록 설계되었습니다. 이 방법은 이전 상호작용의 피드백 신호를 복기(retrospection)하고 이에 기반하여 학습을 진행합니다. 구체적으로, 각 상호작용 후 LLM은 자신의 행동을 재검토하여 피드백을 디코딩합니다. MULTIREF라는 새로운 다중 턴 상호작용 시나리오에서 이 방법이 적용되며, 사용자가 3D 도형들로 구성된 난해한 문제를 해결하도록 LLM에게 지시합니다.
- ***Performance Highlights***: 실험 결과, RESPECT 방법을 적용한 후 LLM의 작업 완료 비율이 31%에서 82%로 증가했습니다. 이 과정에서 피드백을 디코드하는 정확성은 시간에 따라 안정적으로 유지되었으며, LLM은 다중 턴 상호작용에서의 성능을 개선하는 데 성공했습니다.

### [AERO: Softmax-Only LLMs for Efficient Private Inference](https://arxiv.org/abs/2410.13060)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13060.png)

Vote: 0

Authors: ['Brandon Reagen', 'Nandan Kumar Jha']

- ***What's New***: AERO는 비선형성(Nonlinearity)을 제거하고 더 효율적인 개인 정보 추론(Private Inference; PI)을 위한 Softmax-only LLM 아키텍처를 제안합니다. 이 연구는 기존 대형 언어 모델(Large Language Models; LLMs)의 아키텍처를 최적화하고, FLOPs(count)을 줄이는 데 중점을 두었습니다.
- ***Technical Details***: AERO는 네 단계로 구성된 아키텍처 최적화 프레임워크로, Transformer 기반 LLM의 기존 아키텍처를 비선형성을 제거하여 재구성합니다. 구체적으로, LayerNorm, GELU와 같은 비선형 컴포넌트를 제거하고 FLOPs 수를 줄이는 설계를 구현했습니다. Softmax-only 아키텍처는 비선형적인 활성화 함수가 없는 경우에 최적의 성능을 발휘할 수 있도록 설계되었습니다. 이 과정에서 새로운 엔트로피 정규화(Entropy Regularization) 기법을 도입하여 Softmax-only 모델의 성능을 향상시킵니다.
- ***Performance Highlights***: AERO는 통신 비용을 최대 4.23배 줄이고, 지연(latency)을 1.94배 단축시킬 수 있습니다. 표준 벤치마크에 따르면, Softmax-only 모델은 perplexity에서 기존의 SOTA(상태 최선; State-of-the-Art) 모델에 비해 6%에서 8% 향상된 성능을 기록했습니다.

