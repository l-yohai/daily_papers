## Daily Papers (2024-10-11)

### [MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code](https://arxiv.org/abs/2410.08196)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08196.png)

Vote: 27

Authors: Houxing Ren, Ke Wang, Mingjie Zhan, Zimu Lu, Junting Pan, Weikang Shi, Hongsheng Li, Aojun Zhou

- **What's New**: 이 논문은 LLMs(대형 언어 모델)의 수학적 추론 능력을 향상시키기 위해 기존의 수학 관련 코드에 자연어 추론 단계를 추가하는 새로운 방법을 제안합니다. 특히, 수학적 텍스트에서 LaTeX 표현식을 추출하여 Python 코드 스니펫으로 변환하고, 실행 가능한 코드만 남기는 방식으로 데이터를 생성합니다.
- **Technical Details**: 데이터 처리 파이프라인은 두 가지 주요 단계로 구성됩니다. 먼저 수학 관련 데이터세트를 모으고 필터링하는 단계와, 이 데이터를 활용해 수학적 추론 단계와 대응되는 Python 코드를 생성하는 단계입니다. 19.2B 토큰의 MathCode-Pile을 생성하며, 이는 웹 데이터, 합성 데이터, 코드, 교과서로 구성됩니다. 이 과정에서 모델 Llama-3.1-70B-Instruct를 사용해 복잡한 수학적 표현을 추출하고, 이를 Python 코드로 변환합니다.
- **Performance Highlights**: MathCode-Pile을 사용하여 Llama-3-8B, DeepSeekMath-7B 등 4개의 인기 있는 기저 모델의 성능을 검증한 결과, 수학 벤치마크에서 성능이 크게 향상되었습니다. 특히 MathCoder2-Llama-3-8B는 MATH에서 4-shot 정확도가 38.4%, GSM8K에서 69.9%로 각각 3.1%와 4.1% 향상되었습니다.

### [PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs](https://arxiv.org/abs/2410.05265)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05265.png)

Vote: 25

Authors: Jiahao Wang, Yi Bin, Yi Liu, Ping Luo, Wenqi Shao, Mengzhao Chen

- **What's New**: PrefixQuant이라는 새로운 기법을 제안하며 Token-wise outliers(잔차)를 제거하여 효율적인 per-tensor static quantization을 구현합니다. 기존의 방법들은 토큰별 동적 양자화를 사용했으나, PrefixQuant은 고정된 토큰 위치에서 자주 출현하는 잔차를 미리 처리함으로써 추가적인 재훈련 없이도 LLM의 퍼포먼스를 크게 향상시킵니다.
- **Technical Details**: PrefixQuant은 LLM의 토큰 시퀀스 내에 고정된 위치에 주로 나타나는 잔차 토큰을 미리 체크하고 이들을 KV 캐시에 저장해 둠으로써 방대하고 극단적인 경향성을 방지합니다. 이를 통해 per-tensor static quantization에서의 양자화 오류를 예방하고 있습니다. 추가적으로, 가중치와 활성화의 양자화 파라미터를 동시에 최적화하는 block-wise fine-tuning 최적화를 도입하여 성능을 증대시킵니다.
- **Performance Highlights**: PrefixQuant은 Llama-2-7B에서 4-bit per-tensor static activation quantization을 통해 5.91 perplexity를 달성하며, 이는 기존의 QuaRot의 17.95 perplexity를 크게 상회하는 성과입니다. Fine-tuning 시 WikiText2에서는 7.43 perplexity, 다섯 가지 일반적 상식 추론 과제에서는 평균 71.08%의 정확도를 기록했습니다. 이는 QuaRot에 비해 0.98 perplexity 및 5.98 포인트의 정확도 향상을 제공합니다. 또한, W4A4 quantization에서 1.60배에서 2.81배 빠른 속도로 FP16 모델을 능가하며, QuaRot 모델보다 1.2배에서 1.3배까지 우수한 속도를 기록했습니다.

### [MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents](https://arxiv.org/abs/2410.03450)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03450.png)

Vote: 23

Authors: Zongqing Lu, Junpeng Yue, Börje F. Karlsson, Xinru Xu

- **What's New**: 이번 논문에서는 복잡한 환경에서 embodied agents가 더 나은 성능을 보일 수 있도록 멀티모달 대형 언어 모델(Multimodal Large Language Models, MLLM)이 효과적으로 grounding할 수 있는 방법을 제안합니다. 이를 위해 새로운 접근법인 MART(MLLM As ReTriever)를 통해 MLLM을 진화시키고, Trajectory Abstraction 메커니즘을 소개합니다.
- **Technical Details**: 기존의 유사성 기반 검색 방식의 한계를 극복하기 위해, 제안된 MART는 상호작용 학습을 통합하여 주어진 작업에 효과적인 trajectory를 우선시하는 retriever 모델을 개발합니다. LLaVA와 같은 MLLM을 Bradley-Terry 모델로 이전 학습 시나리오와의 상호작용에서 수집된 선호 피드백 데이터를 사용하여 세부 조정합니다. 또한 Trajectory Abstraction은 MLLM의 요약 기능을 활용해 tokens 수를 줄이며 중요한 정보를 보존해, 효율성을 높입니다.
- **Performance Highlights**: 다양한 환경에서 실시한 실험을 통해 MART는 기존 모델들보다 10% 이상 높은 작업 성공률을 나타내며, unseen environments에서의 우수한 성능을 입증하였습니다. MART는 종합 실험에서 높은 성능 향상을 보여주며, 복합 환경에서의 새로운 멀티모달 검색 패러다임을 제시합니다.

### [DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models](https://arxiv.org/abs/2410.08207)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08207.png)

Vote: 15

Authors: Chaowei Tan, Quan Dao, Dimitris Metaxas, Di Liu, Bo Liu, Faez Ahmed, Song Wen, Akash Srivastava, Minhao Bai, Hongdong Li, Ligong Han, Xiaoxiao He, Kang Li, Felix Juefei-Xu, Han Zhang, Junzhou Huang, Martin Renqiang Min

- **What's New**: 이 논문은 DICE (Discrete Inversion for Controllable Editing)를 소개합니다. 이는 기존의 연속형 확산 모델(continuous diffusion models)을 위한 반전 알고리즘(inversion algorithm)을 이산 확산 모델(discrete diffusion models)에 확장한 최초의 방법으로, 멀티노미얼(multinomial) 확산 및 마스크 생성 모델(masked generative models)을 포함합니다. 이 방법은 기록된 잔차(residuals)를 활용하여 이산 데이터의 정확한 재구성과 제어된 편집을 가능하게 하며, 사전 정의된 마스크와 주의 맵(manipulation) 없이도 조정된 콘텐츠 조작을 수행할 수 있습니다.

### [Benchmarking Agentic Workflow Generation](https://arxiv.org/abs/2410.07869)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07869.png)

Vote: 13

Authors: Zhisong Qiu, Huajun Chen, Xiaobin Wang, Fei Huang, Pengjun Xie, Shuofei Qiao, Yong Jiang, Ningyu Zhang, Runnan Fang

- **What's New**: 이 논문은 선형적으로 불가능한 아이템 기반의 추천 시스템(item-based recommender systems)에 효과적이고 계산적으로 효율적인 차원 감소(dimensionality reduction) 기법을 적용하여 개선하는 방법을 제안합니다. 이를 통해 대규모 데이터 셋에서의 성능을 향상시키기 위한 체계적 접근법을 제시합니다.
- **Technical Details**: 이 연구에서는 행렬 분해(matrix factorization)와 같은 기존의 기술을 활용하여 차원 감소를 수행하고, 유사도를 기반으로 하는 선형 예측 모델(linear prediction model)을 학습합니다. 특히, 스케일(scale)을 고려하여 간단하고 직관적인 모델링을 가능하게 합니다. 이 방법은 사용자와 아이템 간의 복잡한 상호작용을 보다 효율적으로 캡처할 수 있게 합니다.
- **Performance Highlights**: 제안된 방법은 기존의 아이템 기반 추천 시스템보다 더 나은 성능을 보여주었으며, 대규모 데이터 셋에서도 높은 효율성을 유지하는 것으로 확인되었습니다. 실험 결과에서, 학습 시간과 메모리 사용량을 크게 줄이면서 예측 정확도를 향상시킬 수 있음을 보여주었습니다.

### [Agent S: An Open Agentic Framework that Uses Computers Like a Human](https://arxiv.org/abs/2410.08164)

![](/avatars/05abee0b6317f100923936ca2099e9eb.svg)

Vote: 9

Authors: Jiuzhou Han, Xin Eric Wang, Saaket Agashe, Ang Li, Shuyu Gan, Jiachen Yang

- **What's New**: Agent S라는 새로운 에이전틱 프레임워크가 소개되었습니다. 이 프레임워크는 GUI 기반의 운영 체제 제어 작업을 더 효과적으로 해결하기 위해 경험이 증강된 계층적 계획 수립 및 지속적인 메모리 업데이트, 그리고 MLLM 기반 GUI 에이전츠를 위한 에이전트-컴퓨터 인터페이스를 통합합니다.

### [Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow](https://arxiv.org/abs/2410.07303)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07303.png)

Vote: 9

Authors: Mengdi Wang, Hongsheng Li, Fu-Yun Wang, Ling Yang, Zhaoyang Huang

- **What's New**: 새로운 연구는 시각적 생성 분야에서 큰 혁신을 이룬 Diffusion 모델의 속도 문제를 해결하기 위한 방법으로 'Rectified Flow'를 제안합니다. 이 방법은 기존의 DDPM (Denoising Diffusion Probabilistic Models) 방식을 개선하여 더 적은 단계로 고품질 이미지를 생성할 수 있도록 설계되었습니다.
- **Technical Details**: Rectified Flow의 핵심 구성 요소는 세 가지로 나뉩니다. 첫째, 'Flow-Matching' 기반의 Diffusion 형태를 채택하여, 중간 상태를 보다 직관적으로 표현합니다. 둘째, '𝒗-prediction' 기법을 사용하여 모델이 예측하는 요소를 단순화하고, 𝒗는 기본 데이터 𝐱0에서 ϵ를 뺀 값으로 정의됩니다. 마지막으로, 'Rectification'은 생성된 데이터와 잡음을 선형적으로 결합하지 않고, 프리트레인된 디퓨전 모델로 생성된 𝐱^0와 미리 수집된 잡음을 사용하여 보다 정밀한 훈련을 가능케 합니다.
- **Performance Highlights**: Rectification 기법을 통해 생성 품질이 높은 저단계 레짐(low-step regime)에서도 크게 향상되며, 표준 Diffusion 모델의 유연성을 여전히 유지할 수 있습니다. 이런 방식은 특히 생성 속도가 중요한 응용 분야에서 강력한 성능 향상을 기대할 수 있습니다.

### [SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe](https://arxiv.org/abs/2410.05248)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05248.png)

Vote: 7

Authors: Yuxin Xiao, Shujian Zhang, Sanqiang Zhao, Wenxuan Zhou, Marzyeh Ghassemi

- **What's New**: 이번 논문에서는 SFTMix라는 새로운 방법론을 제안하며, 대규모 언어 모델(LLM)을 인스트럭션 튜닝(instruction tuning)하는 성능을 기존의 NTP(next-token prediction) 패러다임을 넘어 향상시킬 수 있음을 보여줍니다. SFTMix는 잘 구성된 데이터셋에 의존하지 않고 LLM의 자신감 분포를 이용하여 데이터를 더 효과적으로 해석하고 튜닝 과정에서 그 자신감 수준을 이용해 데이터를 분할하여 인스트럭션 튜닝의 효율성을 높입니다.
- **Technical Details**: SFTMix는 '믹스업(Mixup) 기반 정규화'를 도입하여 서로 다른 자신감 수준을 가진 데이터 쌍을 선형적으로 보간하여 인스트럭션 튜닝 과정에서 더 나은 일반화를 도모합니다. 이를 통해 자신감이 높은 예시에 과도하게 적합되는 것을 방지하고, 상대적으로 자신감이 낮은 예시에 대한 일반화 능력을 향상시킵니다. 특히, SFT 데이터셋을 자신감 있는 부분과 자신감 없는 부분으로 나누고, 이 둘을 믹스업 방법론을 통해 재구성하여 튜닝을 진행합니다.
- **Performance Highlights**: SFTMix는 다양한 LLM 계열과 데이터셋 크기에서 NTP 기반의 인스트럭션 튜닝을 뛰어넘는 성능을 보였습니다. 특히, MT-Bench와 AlpacaEval-2에서의 성과뿐만 아니라 헬스케어 도메인에서도 뛰어난 정확도 상승을 보여주며, 4개의 벤치마크에서 평균 1.5% 향상을 기록했습니다. 이러한 성과는 다양한 케이스에 대한 넓어진 적용 가능성을 제시합니다.

### [Intriguing Properties of Large Language and Vision Models](https://arxiv.org/abs/2410.04751)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04751.png)

Vote: 7

Authors: Young-Jun Lee, Byungsoo Ko, Yechan Hwang, Ho-Jin Choi, Han-Gyu Kim

- **What's New**: LLVM(Large Language and Vision Models)는 각종 작업에서 놀라운 일반화 성능을 보여줍니다. 이 모델들은 사전 학습된 비전 인코더(vision encoder)와 대형 언어 모델(LLM: Large Language Model)을 간단한 크로스-모달 정렬 모듈을 통해 결합하는 '모델 스티칭(model-stitching)' 개념을 기반으로 설계되었습니다. 이 방식은 비디오, 오디오 등의 다른 모달리티 영역으로도 확장되고 있습니다.
- **Technical Details**: 본 연구에서는 다양한 조건 아래에서 LLVM의 구조를 분석합니다. 연구는 퍼머테이션, 차단, 합성 이미지 등의 조건을 포함합니다. 10개의 다양한 벤치마크에서 LLaVA 시리즈를 평가한 결과, LLVM의 이전 순서가 무작위로 섞여도 성능이 크게 저하되지 않음을 보여줍니다. 이는 퍼머테이션에 대해 불변성(permutation-invariant)을 가지는 것을 나타냅니다.
- **Performance Highlights**: LLVM은 합성된 버전의 데이터셋을 이용할 때 성능이 크게 떨어지지 않았고(약간의 감소율 1.8%), 일부 작업에서는 이미지의 전체 정보를 다 보지 않고도 문제를 해결할 수 있었습니다. 하지만, 이미지 분류에서는 최대 20%의 성능 감소 현상(치명적 망각)도 나타났습니다.

### [Progressive Autoregressive Video Diffusion Models](https://arxiv.org/abs/2410.08151)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08151.png)

Vote: 7

Authors: Feng Liu, Yicong Hong, Yang Zhou, Zhan Xu, Difan Liu, Hao Tan, Desai Xie, Arie Kaufman

- **What's New**: 새로운 Video Diffusion 모델의 발전은 더 긴 길이의 비디오를 생성할 수 있는 가능성을 제시합니다. 전통적인 비디오 생성 모델의 제약을 넘어, 최대 1분 길이의 비디오를 부드럽고 연속적인 장면 전환과 함께 생성할 수 있습니다. 이 모델은 기존의 네트워크 구조 변경 없이, 노이즈 스케줄링 변화와 사전 훈련된 모델의 파인튜닝을 통해 구현됩니다.
- **Technical Details**: 이 연구는 점진적인 자회귀(auto-regressive) 추론 절차를 통해 비디오 프레임을 노이즈 수준이 점진적으로 증가하는 방식으로 복원합니다. 이는 주목(attention) 창에서 큰 중첩을 허용하며, UNet 기반 또는 Diffusion Transformer 기반의 백본을 그대로 사용하면서 훈련이 필요 없는 방식으로 동작할 수 있습니다. 
- **Performance Highlights**: 이 새로운 방법은 60초, 총 1440 프레임의 긴 비디오 생성에서 최고 수준의 프레임 품질, 모션 다이내믹스 및 일관된 장면 전환을 유지합니다. 벤치마크와의 비교에서, 제안된 방법은 모든 측면에서 우수한 성능을 나타냈으며, 다양한 기존 비디오 모델에 보편적으로 적용 가능합니다.

### [Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning](https://arxiv.org/abs/2410.06508)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06508.png)

Vote: 6

Authors: Dong Yu, Dian Yu, Haitao Mi, Ye Tian, Furong Huang, Linfeng Song, Xiyao Wang, Baolin Peng

- **What's New**: 이 논문은 AlphaLLM-CPL이라는 새로운 pairwise offline training 프레임워크를 제안합니다. 이는 LLM (Large Language Models)이 Monte Carlo Tree Search (MCTS) 동작 증류를 통해 자체 개선을 가능하게 합니다. AlphaLLM-CPL은 MCTS에서 생성된 트래젝토리를 더 효율적으로 활용하여 LLM의 추론 능력을 강화합니다.
- **Technical Details**: AlphaLLM-CPL은 두 가지 주요 구성 요소로 이루어져 있습니다: (a) Trajectory Pair Extraction(궤적 쌍 추출): MCTS가 생성한 다중 궤적에서 궤적 쌍을 형성하여 궤적 값에 따라 구성합니다. 단계별 궤적 쌍도 추출하여 MCTS 동작 증류를 강화합니다. (b) Curriculum Preference Learning(커리큘럼 선호 학습): 추출된 궤적 쌍의 리플레이 버퍼를 생성하여, 오프라인 학습 에포크마다 LLM의 학습에 중요한 궤적 쌍의 우선순위를 조정합니다.
- **Performance Highlights**: AlphaLLM-CPL을 적용한 결과, GSM8K 벤치마크에서 LLaMA2-7B와 Mistral-7B의 성능이 각각 150%와 48.8% 개선되었습니다. MATH 벤치마크에서는 LLaMA3-8B-Instruct의 성능이 17.4% 향상되었습니다. 이는 기존 MCTS 행동 증류 방법에 비해 명확한 성능 우위를 보여줍니다.

### [Emergent properties with repeated examples](https://arxiv.org/abs/2410.07041)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07041.png)

Vote: 6

Authors: Julia Kempe, François Charton

- **What's New**: 이 논문에서는 신경망 훈련에서 데이터 반복 사용의 긍정적 영향을 살펴보고, 제한된 훈련 예산(training budget) 내에서 데이터 예산(data budget)을 최대화하는 전통적인 관점을 도전합니다. 연구는 세 가지 수학적 작업에서 반복 샘플의 영향을 조사하며, 반복된 예제가 훈련 모델의 성능 향상에 어떻게 기여할 수 있는지를 보여줍니다.
- **Technical Details**: 논문에서는 반복 사용된 예제가 제한된 훈련 예산 내에서 어떻게 모델 성능을 향상시킬 수 있는지를 실험합니다. 두 가지 주요 발견은 'Repetition Helps'(반복이 도움이 된다)와 'Two-Set Training'(이중 세트 훈련)입니다. 제한된 데이터 예산 내에서 데이터를 반복해 사용했을 때, 훈련 속도와 성능이 크게 향상된다는 내용을 다룹니다. 이러한 접근 방식은 일반적인 커리큘럼 학습과 달리 임의의 서브셋(부집합)을 사용하는 것이 특징입니다.
- **Performance Highlights**: 고정된 훈련 예산에서, 훈련 데이터 예산이 작은 모델들이 큰 데이터 예산을 가진 모델보다 더 나은 성능을 보였습니다. 또한, 두 세트 접근법을 통해 학습 속도와 성능이 개선되었습니다. 연구는 반복된 예제를 사용할 때, 그들의 빈도가 어떻게 미니배치에서 혼합되어야 하는지를 강조합니다. 이러한 반복 방식은 모델 성능을 개선하고, 학습을 가속화하며, 새로운 작업을 배우거나 작은 모델을 더 효과적으로 활용할 수 있게 합니다.

### [Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality](https://arxiv.org/abs/2410.05210)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05210.png)

Vote: 6

Authors: Youngtaek Oh, Dong-Jin Kim, Junmo Kim, In So Kweon, Jae Won Cho

- **What's New**: 이 논문은 향상된 구성적 추론 능력을 유지하면서 다중 모달 작업 성능을 보존하는 새로운 VLM(vision-language model)의 파인튜닝(fine-tuning) 프레임워크를 제안합니다. 제안된 프레임워크는 글로벌 하드 네거티브 손실(global hard negative loss)로 인한 성능 저하를 완화하고, 두 가지 주요 혁신을 도입합니다: (1) 로컬 하드 네거티브(Local Hard Negative, LHN) 손실 - 이미지 패치와 텍스트 토큰 간의 밀도 정렬을 활용하여 더 나은 구성적 이해를 제공합니다. (2) 선택적 보정 정규화(Selective Calibrated Regularization, SCR) - 비슷하게 인코딩된 하드 네거티브와 원본 텍스트 때문에 발생하는 혼란을 줄이기 위해 고안되었습니다.

### [GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models](https://arxiv.org/abs/2410.06154)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06154.png)

Vote: 5

Authors: Zhuoyuan Mao, Saurav Jha, Horst Possegger, Rogerio Feris, Wei Lin, Sivan Doveh, James Glass, Shiqi Yang, Yuki Mitsufuji, Mengjie Zhao, Michael Dorkenwald, Paul Gavrikov, Leonid Karlinsky, Hiromi Wakaki, M. Jehanzeb Mirza

- **What's New**: 최근 대형 언어 모델(LLM)과 비전-언어 모델(VLM)의 발전으로 자연어 프롬프트를 통한 최적화 가능성이 열리고 있습니다. 전통적인 기울기 기반 최적화와 달리, 이 연구에서는 비전-언어 모델을 위한 적합한 자연어 프롬프트를 발견하는 방식으로 최적화를 프레이밍하였습니다. 이를 통해 다운스트림 비전 작업의 성능을 향상시키고자 합니다. 연구에서는 GLOV라는 프롬프트 검색 기술을 제안하며, 메타-프롬프트와 임베딩 공간 가이던스를 활용해 VLM의 프롬프트 최적화를 이끌어냅니다.
- **Technical Details**: GLOV의 핵심은 메타-프롬프트가 다운스트림 작업 관련 설명과 이전에 최적화된 프롬프트에서 유도된 컨텍스트 내 예제를 포함하는 LLM을 반복적으로 질의하는 것입니다. 이를 통해 LLM의 출력을 원하는 방향으로 안내합니다. 또한, 각 최적화 단계에서 언어 생성을 명시적으로 비편향시켜 LLM을 긍정적인 프롬프트 방향으로 유도합니다. 이는 LLM의 활성화 공간에 긍정적이고 부정적인 프롬프트에서 파생된 숨겨진 상태 오프셋 벡터를 추가하여 이루어집니다.
- **Performance Highlights**: ImageNet 테스트 세트에서 GLOV는 CLIP와 LLaVa-OV에 대해 각각 2.6%와 15.2%의 성능 향상을 달성했습니다. 연구는 이미지 분류와 비주얼 질문 응답(VQA)과 같은 다양한 다운스트림 작업을 통해 GLOV의 범용성을 입증했습니다. 듀얼 인코더 모델과 인코더-디코더 구조에서 GLOV는 각각 15.0%(3.81% 평균)와 57.5%(21.6% 평균)의 정확도 향상을 보여줍니다.

### [Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates](https://arxiv.org/abs/2410.07137)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07137.png)

Vote: 4

Authors: Jing Jiang, Tianyu Pang, Xiaosen Zheng, Chao Du, Qian Liu, Min Lin

- **What's New**: 이번 연구는 대규모 언어 모델(LLMs, Large Language Models)의 자동 평가 시스템을 악용하여 높은 승률을 얻고, 이를 홍보에 사용하는 잠재적인 문제를 다루고 있습니다. 특히, 'null models'라는 방법을 사용하여 자동 평가자를 속이고, 높게 평가받을 수 있는 방법을 제안하였습니다.
- **Technical Details**: 연구에서는 자동 평가 시스템인 AlpacaEval 2.0과 MT-Bench 등을 활용하여 'null models'를 통해 고정된 출력을 생성하여 시스템을 테스트했습니다. 또한, ChatGPT를 사용하여 설득력 있는 고정 응답을 생성하였으며, 이러한 응답이 자동 평가자에 의해 높은 평가를 받을 수 있는지를 평가하였습니다. 예를 들어, 항상 같은 'Pick me!'라는 출력을 반환하는 NullModel을 사용하여 테스트가 진행되었습니다. 진화된 구조적 치팅 응답이 어떻게 LLM 평가 시스템의 약점을 이용할 수 있는지를 보여주었습니다.
- **Performance Highlights**: 연구는 AlpacaEval 2.0 시스템이 설득력 있는 고정 응답에 강력하며, 1% 미만의 승률을 할당하는 것을 보여주었습니다. 그러나 구조적으로 설계된 치팅 응답은 평가 시스템을 혼란스럽게 하여 최대 76.8%의 승률을 얻을 수 있음을 밝혔습니다. 또한, GPT-4 결과를 기반으로 더욱 전이 가능한 프리픽스를 추가하여 자동 평가 시스템에 더 나은 성능을 발휘하도록 최적화하였습니다.

### [Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations](https://arxiv.org/abs/2410.08049)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08049.png)

Vote: 4

Authors: Xiaohan Ding, Xiangyu Yue, Yiyuan Zhang

- **What's New**: 이 논문은 대규모 커널을 사용한 ConvNet(Convolutional Neural Network)의 설계를 제안하여 전역 모델링의 가능성을 탐구합니다. 기존 ViT(Vision Transformer)에 의해 제공되는 전역 주의 메커니즘을 구현하면서도 계산 복잡성을 낮추고 추론 속도를 가속화하는 접근법을 소개합니다.
- **Technical Details**: 전통적인 작은 커널 기반 ConvNet과 달리, 제안된 대규모 커널 ConvNet 아키텍처는 수용장의 확대, 공간 패턴의 추상 계층화, 모델의 일반 표현 능력을 증가시키기 위해 세 가지 효과를 분리합니다. 이를 통해 큰 커널의 강점을 활용하여 깊게 가지 않고 넓게 볼 수 있는 모델을 설계합니다. 또한, 다양한 모달리티(모델화가능성)에서의 일반화 및 멀티모달 특징 융합을 위한 설계를 포함하고 있습니다.
- **Performance Highlights**: 제안된 UniRepLKNet 아키텍처는 ImageNet 분류 등에서 기존의 대형 커널 ConvNets 및 강력한 최신 아키텍처들과 비교하여 높은 정확성과 효율성을 자랑합니다. 특히, 객체의 전체 모양을 기반으로 예측하여 인간의 시각 시스템과 일치되는 더 나은 일반화를 보여줍니다. 또한, 다중 모달리티에서 높은 성능을 발휘하며, 큰 비전-언어 모델 벤치마크에서도 뛰어난 결과를 기록하였습니다. 이를 통해 원래의 ConvNet 영역에서의 '컴백'을 달성하였고, 오디오 및 시간 시계열 데이터 등 새로운 영역으로의 확장 가능성을 부각시키고 있습니다.

### [Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System](https://arxiv.org/abs/2410.08115)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08115.png)

Vote: 4

Authors: Cheng Yang, Maosong Sun, Jiarui Yuan, Weize Chen, Chen Qian, Zhiyuan Liu

- **What's New**: Optima라는 새로운 프레임워크는 LLM 기반의 다중 에이전트 시스템(MAS)을 최적화하여 효율적인 통신과 과제 수행 효과성을 동시에 향상시키는 것을 목표로 합니다. Optima는 생성, 랭크, 선택, 훈련의 반복적인 과정과 작업 성능, 토큰 효율성, 그리고 통신 해석 가능성을 균형있게 맞추는 보상 함수를 통합하여 MAS의 집단지능을 활용할 수 있습니다.
- **Technical Details**: Optima는 반복적인 생성-랭크-선택-훈련 패러다임에 기반하여 MAS에서의 에이전트 통신과 과제 완료의 효율성을 높입니다. 기초 모델 ℳbase와 작업 데이터셋 𝒟를 사용하여 각각의 작업 인스턴스에 대해 N개의 대화 경로를 표본화하며, 보상 함수 R에 의해 평가됩니다. 이 보상 함수는 작업 성능 측정치와 토큰 효율성 측정을 포함합니다. Monte Carlo Tree Search (MCTS)-영감을 받은 기술을 DPO 데이터 생성에 통합하여 대화 턴을 트리 노드로 개념화하여 다양한 상호작용 경로를 탐색합니다.
- **Performance Highlights**: Optima는 두 가지 다중 에이전트 설정인 정보 교환 및 토론에서의 다양한 작업에 대해 평가되었습니다. 기초 모델로 Llama 3 8B를 사용하여, Optima는 단일 에이전트와 MAS 기준치들을 꾸준히 뛰어넘어 토큰 사용을 최대 90% 줄이고 작업 성능을 2.8배 증가시켰습니다. 토큰 사용량 감소를 통해 동일한 계산 제약 내에서 더 많은 샘플을 사용할 수 있게 되어, 더 나은 추론-시간 스케일링 법칙을 가능하게 합니다.

### [WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents](https://arxiv.org/abs/2410.07484)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07484.png)

Vote: 3

Authors: Guodong Long, Yijun Yang, Jing Jiang, Chengqi Zhang, Siyu Zhou, Deheng Ye, Tianyi Zhou

- **What's New**: 이 연구에서는 LLM (Large Language Models) 에이전트를 더 효율적이고 안전하게 만들기 위한 'World Alignment by ruLe LEarning (WALL-E)'이라는 뉴로심볼릭 세계 모델을 제안합니다. 이 모델은 환경과의 상호작용을 통해 새롭게 학습된 규칙들을 사전학습된 LLM에 통합하여 LLM의 확률적, 추론적 지식과 규칙의 하드 제약을 결합합니다. 이렇게 하면 소수의 보완적인 규칙만으로도 환경의 동학에 맞춘 LLM 에이전트를 구축할 수 있습니다.
- **Technical Details**: WALL-E는 LLM의 귀납적 추론과 코드 생성 능력을 활용하여 환경 상호작용을 통한 루프 기반 규칙 학습을 진행합니다. 매 반복에서 환경과 상호작용하여 실제 궤적을 수집하고, 이를 LLM 기반 세계 모델과 비교하여 불일치를 분석해 새로운 규칙을 추출하거나 기존 규칙을 수정하며, 최대 커버리지 문제를 해결하여 불필요한 규칙을 가지치기합니다. 이 과정을 통해 LLM과 규칙 조합이 정확한 세계 모델로서 기능하도록 합니다. 또한, 모델 기반의 강화학습 대신 모델 예측 제어(MPC) 접근을 사용하여 매 단계에서 최적의 행동을 미리 검색합니다.
- **Performance Highlights**: WALL-E는 Minecraft와 ALFWorld 같은 복잡한 오픈 월드 환경에서 평가되었으며, 에이전트가 자유롭게 탐색하고 복잡한 작업을 목표로 할 수 있게 했습니다. 규칙 학습을 통해 인간의 개입을 줄이고 성능을 개선하는 데 성공했으며, 이를 통해 안전하고 최적화된 에이전트 경로를 생성할 수 있는 가능성을 제시하였습니다.

### [DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation](https://arxiv.org/abs/2410.08159)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08159.png)

Vote: 3

Authors: Yizhe Zhang, Jiatao Gu, Yuyang Wang, Josh Susskind, Shuangfei Zhai, Dinghuai Zhang, Qihang Zhang, Navdeep Jaitly

- **What's New**: 최근 딥 생성 모델(deep generative models)에서의 발전으로 인해 시각적 합성(visual synthesis) 분야에서 눈에 띄는 성과가 있었으며, 이 중 확산 모델(diffusion models)이 고품질 이미지 생성의 주도적인 접근 방식으로 부상하고 있습니다. 그러나 기존의 확산 모델은 고해상도 이미지에 직접 훈련하기 어려워, 캐스케이딩(cascading) 모델, 멀티스케일(multiscale) 접근법 또는 낮은 해상도의 오토인코더(autocoder) 코드로 이미지 전처리가 필요하다는 한계를 가지고 있습니다. 이러한 한계를 해결하기 위해, 우리는 Denoising AutoRegressive Transformer (DART)을 제안합니다. 이 모델은 비마코프(non-Markovian) 확산 프레임워크와 자동회귀 모델링을 통합하여 기존 방법보다 효율적이고 유연한 이미지 생성을 가능하게 합니다.
- **Technical Details**: DART는 비마코프 확산 모델(non-Markovian diffusion model)의 일부로서, 전체 생성 궤적을 활용하여 훈련 및 추론 중에 이전 생성을 완전히 이용할 수 있습니다. DART는 두 가지 주요 개선을 도입합니다: (1) 토큰 수준의 자동회귀 모델링(DART-AR), 이는 이미지 토큰 간의 의존성을 자동회귀적으로 캡처하여 세밀한 제어와 향상된 생성 품질을 가능하게 하고, (2) 흐름 기반의 정련 모듈(flow-based refinement module, DART-FM)은 모델의 표현력을 향상시키고, 디노이징 단계들 간의 전환을 매끄럽게 만듭니다.
- **Performance Highlights**: DART는 클래스 조건부(class-conditioned)와 텍스트-투-이미지(text-to-image) 생성 작업 모두에서 경쟁력 있는 성능을 제공합니다. 이 모델은 고품질의 제어 가능한 이미지 합성을 위한 확장 가능하고 통합된 접근 방식을 제공하며 전통적인 확산 모델에 비해 효율적이고 유연한 대안을 제시합니다.

### [Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition](https://arxiv.org/abs/2410.05603)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05603.png)

Vote: 3

Authors: Saurabh Agarwal, Ziqian Lin, Grigorios G Chrysos, Samet Oymak, Kangwook Lee, John Cooper, Zheyang Xiong, Dimitris Papailiopoulos, Zack Sifakis, Angeliki Giannou, Liu Yang, Vasilis Papageorgiou, Albert Ge, Ziyang Cai

- **What's New**: 이 논문은 자연어 처리(NLP)에서의 문장 의미유사성(Sentence Semantic Similarity) 측정 방법을 개선하기 위한 새로운 접근 방식을 제안합니다. 저자들은 최근의 deep learning 기반 모델들이 가지고 있는 한계를 극복하기 위해 새로운 알고리즘을 개발하였습니다.
- **Technical Details**: 제안된 방법은 기존의 pre-trained language models를 활용하면서도 기하학적인 embedding space의 조정을 통해 더욱 정밀한 의미 매칭을 가능케 합니다. 이를 위해, 논문은 각 문장의 의미적 특징을 다차원 체계(multi-dimensional framework)로 분석하고, 다양한 차원(features)을 종합하는 기법을 제시합니다.
- **Performance Highlights**: 실험 결과, 새로운 알고리즘을 통해 문장 의미유사성을 평가한 결과는 기존의 방법보다 유의미하게 높은 정확도를 보여줍니다. 특히, benchmark datasets에서의 비교에서는 이전 최고 성능 모델들을 초과하는 결과를 나타냈습니다.

### [LPZero: Language Model Zero-cost Proxy Search from Zero](https://arxiv.org/abs/2410.04808)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04808.png)

Vote: 2

Authors: Lujun Li, Xuebo Liu, Qiang Wang, Xiaowen Chu, Zhenheng Tang, Peijie Dong, Xiang Liu

- **What's New**: 신경망 아키텍처 설계에 대한 전문가의 의존성을 줄이기 위해 Zero-shot Neural Architecture Search (NAS)을 자동화하는 LPZero 프레임워크가 도입되었습니다. 이 프레임워크는 언어 모델에 특화된 새로운 Zero-cost (ZC) 프록시 탐색 공간을 설계하고, Rule-based Pruning Strategy (RPS)를 사용하여 프록시 검색 효율성을 향상시킵니다.
- **Technical Details**: LPZero는 기존의 ZC 프록시들을 통합하여 새로운 탐색 공간을 설계하고 유전 프로그래밍을 사용하여 새로운 프록시를 발견하는 데 중점을 둡니다. 프록시들은 Activation, Jacobs, Gradients, Head, Weight, Softmax의 6가지 입력 유형으로 분류되며, 각 입력은 단항 연산과 이항 연산을 통해 변형된 후 잠재적인 프록시로 결합됩니다. 이러한 접근은 Spearman’s ρ 또는 Kendall’s τ를 사용하여 평가됩니다.
- **Performance Highlights**: FlexiBERT, GPT-2, LLaMA와 같은 벤치마크에서 LPZero에 의해 식별된 프록시들은 기존 방법들보다 더 우수한 성능을 보였으며, 제안된 접근법의 효과성을 입증하였습니다.

### [MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting](https://arxiv.org/abs/2410.07707)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.07707.png)

Vote: 2

Authors: Jiahao Lu, Hanzhi Chang, Yanzhe Liang, Tianzhu Zhang, Ruijie Zhu, Wenfei Yang, Yongdong Zhang, Jiacheng Deng

- **What's New**: 본 연구에서는 새로운 변형 가능한 3D Gaussian 프레임워크인 MotionGS를 제안하여 3D Gaussian의 변형에 명시적 동작 가이던스를 제공하고, 실시간 렌더링을 지원하는 고품질의 동적 장면 복원을 달성했습니다. MotionGS는 optical flow의 동작 전조를 추출해 3D Gaussian의 변형을 제한하며, 이를 통해 더욱 효과적으로 동적 장면을 재구성합니다.
- **Technical Details**: MotionGS는 optical flow를 디커플링하는 모듈을 포함하여 카메라 흐름과 객체의 운동 흐름을 분리합니다. 이 운동 흐름은 3D Gaussian의 변형을 직접적으로 제한하는 데 사용됩니다. 촬영 시에 발생하는 카메라 자기운동과 객체의 동작에서 발생하는 흐름을 구분하여 3D Gaussian의 변형을 더 명확하게 제어할 수 있습니다. 또한, 카메라 포즈 개선 모듈을 통해 3D Gaussian을 고정한 상태에서 포토메트릭 일관성 손실(photometric consistency loss)을 사용해 카메라 포즈에 대해 역전파합니다. 이 과정을 통해 3D Gaussian과 카메라 포즈를 번갈아 최적화하여 렌더링 품질을 향상시킵니다.
- **Performance Highlights**: 제안된 방식은 NeRF-DS 및 HyperNeRF 데이터셋에서 뛰어난 성능을 발휘하며, 주어진 동적 장면 재구성 작업에서 최신의(state-of-the-art) 결과를 입증했습니다. MotionGS는 특히 3D Gaussian의 변형을 위한 명시적 동작 지침 제공을 통한 렌더링 품질 및 실시간 지원에서 우수한 성능을 보였습니다.

### [Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models](https://arxiv.org/abs/2410.05269)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05269.png)

Vote: 1

Authors: Ninareh Mehrabi, Rahul Gupta, Aram Galstyan, Fei Wang, Kai-Wei Chang, Palash Goyal

- **What's New**: 이번 논문에서는 대규모 언어 모델(LLMs) 기반의 데이터 생성 방법을 개선하기 위해 'Data Advisor'라는 새로운 접근 방식을 제안합니다. Data Advisor는 목표 데이터셋의 가이드라인을 동적으로 통합하여 데이터 품질과 범위를 향상시키며, 다양한 목표에 맞춘 LLM 정렬을 가능하게 합니다. 특히 안전성 조정(safety alignment) 분야에서 데이터 생성기의 전체 데이터셋 통계에 대한 인식 능력을 증가시켜, 부족한 부분을 식별하고 추가적인 데이터 생성을 위한 조언을 제공합니다.
- **Technical Details**: Data Advisor는 데이터 생성 과정에서 지향해야 할 데이터셋의 목적, 중점 사항 및 추가 요구사항을 포함한 원칙들을 설정합니다. 이러한 원칙들은 Constitutional AI의 개념과 유사하게 AI의 행동을 지배하는 지침 기반으로 데이터 생성을 안내합니다. 데이터 요약 과정에서는 데이터의 다양한 관점에서의 분포를 포괄하는 요약 보고서를 생성하며, 생성된 데이터의 새로운 인스턴스와 이전 요약을 입력으로 사용하여 요약을 갱신합니다. 이 과정은 반복 가능하며, 데이터셋의 진화와 특성을 보다 효율적으로 관리할 수 있습니다.
- **Performance Highlights**: Data Advisor를 사용하여 10,000개의 안전성 조정 데이터포인트를 생성했으며, 이를 추가적인 설명 튜닝 데이터셋과 통합하여 세 개의 기초 LLM(Mistral, Llama2, Falcon)을 훈련했습니다. 이러한 정렬된 모델들은 다양한 문제에 대해 뛰어난 안전성을 유지하면서도 전체 모델의 유용성을 저하시키지 않았습니다. 이로써 Self-Instruct와 같은 기존의 데이터 생성 방법보다 나은 성능을 보였습니다.

### [Vector-ICL: In-context Learning with Continuous Vector Representations](https://arxiv.org/abs/2410.05629)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05629.png)

Vote: 1

Authors: Chandan Singh, Jingbo Shang, Liyuan Liu, Jianfeng Gao, Yufan Zhuang

- **What's New**: 이번 논문에서는 대형 언어 모델(LLMs)이 자연어 텍스트가 아닌 연속 벡터(continuous vectors)에서도 in-context learning(ICL)을 수행할 수 있는지 탐구합니다. 이는 연속 데이터가 있는 센서 읽기, 재무 시계열 데이터, 과학적 측정과 같은 다양한 모달리티에도 적용할 수 있음을 시사합니다. 이 기법은 Vector-ICL이라고 불리며, LLM의 임베딩 공간(embedding space)과 연속 데이터 간의 다리 역할을 합니다.
- **Technical Details**: Vector-ICL의 주요 기술적 요소는 임베딩 투사(embedding projection)을 통해 연속 데이터를 박스 토큰(box tokens)으로 변환하는 것입니다. 이를 위해 데이터셋을 특정 인코더(encoder)로 추상화된 표현 형태로 변환하고, 이는 다시 '박스 토큰' 또는 '투사된 임베딩'으로 변환됩니다. 이는 일반적인 선형 투사(linear projection)를 이용하거나, 비정형 데이터(예: 시계열, 그래프 등)를 위한 비선형 변환도 사용할 수 있습니다.
- **Performance Highlights**: Vector-ICL은 텍스트 재구성, 수학적 문제 해결 등 다양한 분야에서 기존의 few-shot ICL 및 도메인 특정 모델이나 튜닝을 능가하는 성능을 보여주었습니다. 특히 다중 토큰에 걸친 큰 수를 처리할 때 함수 근사(function approximation)에 효과적이며 대형 언어 모델의 수치 추론 능력을 강화할 수 있습니다. 또한 텍스트 분류, 요약, 분자 캡션, 뇌 fMRI 복원 및 분류, 시계열 분류, 그래프 분류 등 다양한 작업에서도 경쟁력 있는 결과를 보여주었습니다.

### [Zebra: In-Context and Generative Pretraining for Solving Parametric PDEs](https://arxiv.org/abs/2410.03437)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03437.png)

Vote: 1

Authors: Patrick Gallinari, Thomas X Wang, Louis Serrano, Pierre Erbacher, Armand Kassaï Koupaï

- **What's New**: Zebra라는 새로운 프레임워크가 소개되었습니다. 이 프레임워크는 in-context pretraining(ICP)을 기반으로 하여 파라미터 의존편미분방정식(PDEs)을 풀기 위한 것 입니다. Zebra는 neural solvers가 새로운 상황에 빠르게 적응할 수 있도록 조건을 학습하도록 설계되었습니다. 이는 새로운 파라미터 값을 예측하는 방법입니다.
- **Technical Details**: Zebra는 생성적 자귀 회귀(generative autoregressive) 방식을 활용합니다. 이 프레임워크는 encode-generate-decode 구조로 구성되며, 벡터 양자화 변이 자동 인코더(VQ-VAE)를 사용하여 물리 상태를 압축하고 다시 원래의 물리 공간으로 복구합니다. 또한 in-context learning 속성의 이점을 활용하여 모델이 다양한 크기의 문맥 토큰 입력을 처리할 수 있도록 새로운 프리트레이닝 전략을 개발하였습니다.
- **Performance Highlights**: Zebra는 두 가지 명확한 평가 환경에서 다양한 파라미터 의존편미분방정식을 평가합니다. 첫 번째는 초기 조건이 다른 유사한 동작의 문맥 궤적으로부터 역학을 추론하는 상황이며, 이는 일회성(one-shot) 환경을 나타냅니다. 두 번째 시나리오에서는 목표 궤적의 제한된 수의 과거 프레임만 이용할 수 있어, 모델이 이 입력만으로 역학을 추론해야 합니다. Zebra는 이러한 다양한 평가 상황에서 경쟁력 있는 성능을 보여줍니다.

