## Daily Papers (2024-10-16)

### [Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free](https://arxiv.org/abs/2410.10814)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10814.png)

Vote: 19

Authors: Ziyue Li, Tianyi Zhou

- **What's New**: 이 연구는 Mixture-of-Experts (MoE) 아키텍처에서 라우터를 활용하여 고품질 임베딩을 추출할 수 있는 방법을 제안합니다. MoE는 최신 LLMs (Large Language Models)에서 사용되며, 라우팅 가중치(Routing Weights, RW)가 전통적으로 사용하는 히든 스테이트(Hidden State, HS)와 보완적으로 작용할 수 있음을 발견하였습니다.
- **Technical Details**: MoE는 입력을 특화된 전문가들에게 동적으로 할당하는 라우터가 핵심 구성요소입니다. 논문은 RW와 HS를 결합하여 'MoE Embedding (MoEE)'라는 훈련 없이 컨텍스트가 풍부하고 포괄적인 임베딩을 제안합니다. RW와 HS의 단순 연결 또는 가중치합을 통해 다양한 임베딩 품질 향상이 가능합니다.
- **Performance Highlights**: MoEE는 Massive Text Embedding Benchmark (MTEB)에서 다양한 임베딩 관련 과제에서 우수한 성능을 발휘했습니다. 특히, MoEE (sum)은 의미적 텍스트 유사성, 분류, 클러스터링 등 입력에 대한 심층 이해가 필요한 과제에서 두드러진 개선을 보여줍니다. RW와 HS 각각이 실패하는 사례에서의 오류 분석은 결합된 방법이 임베딩 성능을 크게 향상시킬 수 있음을 시사합니다.

### [What Matters in Transformers? Not All Attention is Needed](https://arxiv.org/abs/2406.15786)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.15786.png)

Vote: 19

Authors: Ang Li, Shwai He, Guoheng Sun, Zheyu Shen

- **What's New**: 이번 연구에서는 Transformer 기반 대형 언어 모델(LLM)의 효율성을 높이는 새로운 접근 방식을 제안합니다. Transformer의 주요 구성 요소인 Block, MLP, 및 Attention에서 불필요한 중복을 체계적으로 탐색하여 효율성을 높이는 연구를 진행했습니다. 새로운 'Joint Layer Drop' 방법론을 통해 MLP 레이어와 Attention 레이어를 함께 제거하여 성능을 유지하면서 모델의 크기 및 연산 비용을 절감할 수 있음을 보여줍니다.
- **Technical Details**: Transformer 모델은 여러 개의 쌓인 블록으로 구성되며 각 블록은 MLP 레이어와 Attention 레이어를 포함하고 있습니다. 이 연구에서는 각 레이어의 중복도를 평가하기 위해 유사성 기반 메트릭을 사용하였으며, 이를 토대로 'Block Drop' 및 'Joint Layer Drop' 기법을 개발했습니다. 특히, 'Attention Drop' 방법은 훈련 없이도 불필요한 Attention 레이어를 효과적으로 제거할 수 있습니다.
- **Performance Highlights**: Llama-2-70B 모델의 Attention 레이어를 50% 감소시키고도 성능은 2.4%만 떨어지는 반면, 48.4%의 속도 향상을 달성했습니다. Attention 레이어의 중복성은 훈련 과정 전반에 걸쳐 높은 상태를 유지하여, 이를 관련 디자인에 반영할 수 있는 귀중한 통찰력을 제공합니다.

### [MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation](https://arxiv.org/abs/2410.11779)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11779.png)

Vote: 18

Authors: Haoming Xu, Chenxi Wang, Huajun Chen, Bozhong Tian, Ningyu Zhang, Xiang Chen, Shumin Deng

- **What's New**: 최신 연구에 따르면, 다중모달 대형언어모델(Multimodal Large Language Models, MLLMs)은 일반 인공지능(AGI)로 가는 가능성을 보여주고 있습니다. 하지만, 이 모델들은 환각(hallucination) 현상을 겪고 있으며, 이는 존재하지 않는 이미지에 대한 설명을 생성하거나 실제 보이는 객체를 무시하는 문제를 일으킵니다.
- **Technical Details**: 연구팀은 MLLMs의 환각 메커니즘을 분석하기 위해 실험을 진행했습니다. 모델은 초기 레이어에서 객체를 인식할 수 있지만, 후속 레이어에서 이 인식이 억제되어 환각을 일으킨다는 것을 발견했습니다. 이 연구에서는 MLLMs의 선행 레이어가 실제 정보에 대해 더 높은 신뢰도를 가지는지를 파악하기 위해 Dynamic Correction Decoding(DeCo)을 제안합니다. DeCo는 이전 레이어의 지식을 사용해 환각을 완화하도록 설계되었습니다.
- **Performance Highlights**: DeCo는 InstructBLIP, MiniGPT-4, LLaVA, Qwen-VL 등의 모델에 결합되어 평균 10.8%의 환각 억제율을 기록했습니다. 이는 이미지 설명 및 시각적 질문 응답 데이터셋에서 우수한 성능을 보였으며, 지연시간 측면에서도 기존의 VCD, OPERA 등의 기준선을 능가하는 결과를 보여주었습니다.

### [MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models](https://arxiv.org/abs/2410.11710)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11710.png)

Vote: 16

Authors: Jiaheng Liu, Pei Wang, Wenbo Su, Bo Zheng, Hangyu Guo, Ge Zhang, Junran Peng, Chenchen Zhang, Jiakai Wang, Zhaoxiang Zhang, Yanan Wu, Zhongyuan Peng, Xiaoshuai Song, Ken Deng, Zekun Wang

- **What's New**: MTU-Bench는 Tool Learning의 한계를 해결하기 위해 개발된 새로운 데이터셋으로, MTU-Instruct와 MTU-Eval을 포함합니다. 이는 다양한 도구 사용 시나리오를 숙달시키고 평가하기 위한 자동 데이터 합성 파이프라인을 제공합니다.
- **Technical Details**: MTU-Bench는 기존 대화 데이터셋을 활용하여 고품질의 도구 사용 데이터셋으로 변환하는 자동화된 시스템입니다. 이를 위해 다섯 가지 주요 단계로 데이터셋을 구성하며, 이러한 데이터는 GPT-4를 이용해 검증됩니다. MTU-Instruct는 학습을 위한 데이터셋이고, MTU-Eval은 다양한 정밀 메트릭을 통해 도구 사용 능력을 평가합니다.
- **Performance Highlights**: MTU-Bench를 통해 학습된 모델인 MTU-LLaMA는 여러 시나리오와 메트릭에서 뛰어난 성능을 보입니다. 특히, 다중 회전 대화 시나리오와 다중 도구 설정에서 더욱 효과적임을 입증했습니다. 이는 LLM의 도구 사용 능력을 크게 향상시킬 수 있는 잠재력을 제공합니다.

### [LLM$\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models](https://arxiv.org/abs/2410.09342)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09342.png)

Vote: 15

Authors: Rongqiao An, Chong Li, Xinyi Chen, Zihan Zhou, Shuo Wang, Haoyu Wang, Xu Han, Zhili Li, Zhiyuan Liu, Yu Chao, Qi Shi, Zhixing Tan, Xiaodong Shi, Maosong Sun

- **What's New**: 이 논문은 LLM××MapReduce라는 새로운 프레임워크를 소개합니다. 이는 장문의 텍스트를 효율적으로 처리하기 위한 훈련 없이 사용할 수 있는 프레임워크로, divide-and-conquer 방식을 활용하여 짧은 문맥 윈도우를 가진 모델도 긴 컨텍스트를 효과적으로 다룰 수 있도록 합니다.
- **Technical Details**: 프레임워크는 Map, Collapse, Reduce의 세 단계로 구분됩니다. 'Map' 단계에서 LLM을 사용하여 각 청크의 필요한 정보를 추출하고, 'Collapse' 단계에서 매핑된 결과를 압축하여 LLM의 문맥 윈도우 내에 유지합니다. 마지막으로 'Reduce' 단계에서 각 청크의 출력을 통합하여 최종 답변을 예측합니다. 특히 중요한 기술 요소로는 구조적 정보 프로토콜과 컨텍스트 내 신뢰도 보정 메커니즘이 있습니다. 이는 청크 간 의존성 및 충돌 문제를 효과적으로 해결하는 데 도움을 줍니다.
- **Performance Highlights**: 제안된 LLM××MapReduce 방법은 여러 장문 텍스트 벤치마크에서 폐쇄형 및 오픈소스 LLM들을 성능 및 효율성 면에서 능가합니다. 추가적인 실험을 통해 프레임워크의 각 구성 요소가 전체 성능에 어떻게 기여하는지를 검증하였습니다.

### [Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts](https://arxiv.org/abs/2410.10626)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10626.png)

Vote: 13

Authors: Benyou Wang, Yuping Zheng, Juhao Liang, Guorui Zheng, Nuo Chen, Xidong Wang

- **What's New**: 이 연구는 의료 분야의 대형 언어 모델(LLM)의 다국어 확장을 통해 세계적인 의료 불평등 문제를 해결하고자 합니다. 12개의 주요 언어를 포함하여 데이터셋을 확장하고, 혼합 전문가 구조와 혼합 라우팅 방식을 도입하여 더 나은 성능과 확장성을 달성합니다.
- **Technical Details**: 주요 기술적 접근 방식으로는 전문가 모델의 모듈화와 하이브리드 라우팅이 있습니다. Mixture of Experts (MoE) 구조를 통해 언어별 전문가를 설계하여 언어에 따라 지식을 더 효과적으로 처리합니다. 또한, 'Post-MoE' 아키텍처를 활용하여 후반 레이어에서만 MoE 구조를 적용함으로써 성능을 개선합니다.
- **Performance Highlights**: 12개의 주요 언어에 대한 성능은 안정적으로 유지되었으며, 자원이 적은 언어의 성능도 추가적인 학습 없이 개선되었습니다. 향후 50개의 언어로 모델을 확장하는 데 성공적이며, 추가적인 파라미터 없이 언어 간 일반화를 유지하거나 개선할 수 있습니다.

### [LVD-2M: A Long-take Video Dataset with Temporally Dense Captions](https://arxiv.org/abs/2410.10816)

![](/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg)

Vote: 12

Authors: Jiashi Feng, Tianwei Xiong, Zhijie Lin, Daquan Zhou, Yuqing Wang, Xihui Liu

- **What's New**: 이 논문에서는 장시간 동안의 동영상 생성에서 시간적 일관성과 역동적인 콘텐츠를 유지하는 데 어려움을 다루고자, LVD-2M이라는 새로운 데이터셋을 소개합니다. 이 데이터셋은 10초 이상의 장시간 동영상을 대상으로 하며, 장면 전환 없는 롱테이크(long-take) 동영상과 풍부한 모션 및 다양한 내용을 포함합니다. 또한, 시간적으로 밀도 있는 캡션이 포함되어 있어, 긴 동영상 생성을 위한 트레이닝에 활용될 수 있습니다.
- **Technical Details**: 데이터셋 구축을 위해 저자들은 자동화된 데이터 필터링 파이프라인을 개발하였습니다. 이 파이프라인은 씬컷(scene cut) 감지와 옵티컬 플로우(옵티컬 흐름; optical flow) 추정과 같은 저차원 필터링 툴과 VLMs(비디오 언어 모델; Video Large Language Models)를 사용하는 의미론적 필터링 툴을 결합하여 고품질의 장시간 동영상을 선별합니다. 그 후, LLaVA-v1.6-34B와 Claude3-Haiku 같은 대형 언어 모델을 사용하여, 계층적 캡셔닝을 통해 시간적으로 밀도 있는 캡션을 생성합니다.
- **Performance Highlights**: 이 데이터셋의 효과성은 인간 평가와 이전에 학습된 영상 생성 모델을 LVD-2M으로 파인튜닝한 실험을 통해 입증되었습니다. 실험 결과, 이 데이터셋으로 학습된 모델은 큰 모션이 있는 장시간 동영상을 생성하는 데 있어서 성능이 향상되었으며, 장면 전환이 부드러운 롱테이크 영상을 생성하는 능력이 개선되었습니다.

### [SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI](https://arxiv.org/abs/2410.11096)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11096.png)

Vote: 12

Authors: Yuzhou Nie, Bo Li, Wenbo Guo, Yu Yang, Zhun Wang, Yuheng Tang, Dawn Song

- **What's New**: 새로운 연구는 코드 생성 인공지능(Code GenAI)의 보안 위험성을 평가하기 위해 새로운 평가 플랫폼 SecCodePLT를 도입하였습니다. 이 플랫폼은 코드 생성 AI의 불안전한 코딩과 사이버 공격 지원(attack helpfulness)을 종합적으로 평가할 수 있도록 설계되어, 기존 평가 지표들의 한계를 넘어섭니다.
- **Technical Details**: SecCodePLT는 두 가지 주요 지표로 구성됩니다. 첫째, 불안전한 코딩(insecure coding) 부분에서는 MITRE의 CWE(Common Weakness Enumeration)를 기반으로 두 단계의 데이터 생성 파이프라인을 도입하였습니다. 이 방법은 LLM 기반의 돌연변이(mutators)를 통해 데이터의 확장성과 품질을 보장하며, 동적 테스트를 기반으로 한 하이브리드 평가 메트릭을 적용합니다. 둘째, 사이버 공격 지원 평가에서는 MITRE ATT&CK 프레임워크를 따라 모델이 다양한 공격 단계를 수행할 능력을 평가합니다. 각 공격 단계에 맞게 모델에게 실행 가능한 공격을 생성하도록 유도하는 맞춤형 프롬프트를 설계하였습니다.
- **Performance Highlights**: SecCodePLT를 통해 최신의 오픈 및 클로즈드 소스 모델에 대한 평가를 수행한 결과, 더 큰 모델이 불안전한 코드 생성 위험을 상대적으로 적게 가질 수 있음을 밝혔습니다. 일부 모델은 종단간(end-to-end)의 공격을 생성할 수 있으며, 이는 즉각적으로 악용될 위험성을 나타냅니다. 새로운 플랫폼을 사용하여 최신 코드 에이전트인 Cursor의 보안 취약점을 발견하였으며, 특정 CWE에 대해 실패를 보였습니다. 이는 코드 생성 인공지능의 보안 위험성을 정밀하게 평가할 수 있는 첫 번째 플랫폼입니다.

### [EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation](https://arxiv.org/abs/2410.09704)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09704.png)

Vote: 10

Authors: Susan Cheng, Bryan He, David Ouyang, Milos Vukadinovic, Xiu Tang, Paul Cheng, Debiao Li, Neal Yuan

- **What's New**: 이 연구는 새로운 방법론을 소개하여 다양한 컴퓨터 비전 분야에서 성능을 향상시키고자 합니다. 최신 딥러닝 아키텍처(architecture)를 활용하여, 상대적으로 적은 데이터에서도 높은 정확도를 달성할 수 있는 모델을 제안합니다.
- **Technical Details**: 제안된 모델은 Transformer 기반의 네트워크 구조를 채용하여 정보를 병렬적으로 처리하며, 특히 attention 메커니즘을 강화해 정보의 가중치를 적절하게 조절합니다. 데이터 증강(data augmentation) 기법을 활용하여 학습 데이터를 다양화하고, 모델의 일반화 능력을 강화했습니다.
- **Performance Highlights**: 객관적인 비교평가에서, 제안된 모델은 기존의 SOTA(State-of-the-Art) 모델들보다 다양한 벤치마크 데이터셋에서 최대 5%의 성능 향상을 보였습니다. 특히, 로우 샷(less data) 상황에서도 안정적인 성능을 보이는 것이 특징입니다.

### [Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices](https://arxiv.org/abs/2410.11795)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11795.png)

Vote: 10

Authors: Mingjie Ma, Liangliang Zhao, Guoli Jia, Gaofeng Liu, Jianjun Li, Zhiyuan Ma, Bowen Zhou, Yichao Ma, Kaiyan Zhang, Yuzhu Zhang

- **What's New**: 최근 몇 년 동안 diffusion models (DMs)의 눈에 띄는 성공이 관찰되었으며, 이는 다양한 화려한 생성 콘텐츠의 등장을 동반하고 있습니다. DMs가 이미지 합성에서 GAN을 넘어선 후, 이미지 합성, 비디오 생성, 오디오 합성, 3D 렌더링 및 생성 등 다양한 다운스트림 응용 분야에서 촉망받는 알고리즘으로 자리 잡았습니다. 이러한 배경을 바탕으로 DMs는 LLMs와 견줄 만큼 경쟁력 있는 상대가 되었으며, 두 가지 모두 생성 AI 커뮤니티에서 주요한 다이아몬드로 부상하고 있습니다.
- **Technical Details**: DMs의 효율성과 성능을 향상시키기 위한 많은 연구는 샘플링 절차, 조건부 가이던스, 우도 최대화 및 일반화 능력에 집중해 왔습니다. DMs는 기존의 generative models인 Variational AutoEncoders (VAEs)와 Generative Adversarial Networks (GANs)보다 이론적 근거가 더욱 밀집되어 있습니다. 최신 기술 동향으로는 self-attention과 깊이 있는 scalable architecture의 장점에 의해 LLMs가 강력한 언어 능력을 획득한 반면, DMs는 여전히 scalable 어려움에 직면하고 있으며 이는 대규모 딥 생성 훈련과 emergent abilities를 지탱하는 데 중요하게 작용할 것입니다.
- **Performance Highlights**: Sora의 최근 출현은 비디오 모델을 세계 시뮬레이터로 취급함으로써 생성 모델의 지능적 출현 능력을 정점에 도달하게 했습니다. Sora는 아직 오픈 소스가 아니며 지능적 출현 메커니즘은 명확하지 않습니다. 이 외에도 DMs 관련된 기존 서베이들은 DMs의 전반적인 원칙과 실천을 깊이 있게 요약하는 데 한계가 있으며, 이는 미래 연구를 위한 빠른 이해와 응용에 도움을 줄 수 있습니다.

### [NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models](https://arxiv.org/abs/2410.11805)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11805.png)

Vote: 9

Authors: Hao Xiong, Xiang Zhang, Mengsong Wu, Wenliang Chen, Tong Zhu, Han Han

- **What's New**: 이번 논문에서는 NesTools라는 새로운 데이터셋을 소개합니다. NesTools는 복잡한 문제 해결을 위해 여러 도구의 중첩 호출(nested tool calls)을 처리할 수 있는 대규모 데이터셋을 자동으로 생성하여 LLMs의 학습 및 성능 평가에 도움을 주고자 합니다. 이 데이터셋은 현재 사용 가능한 데이터셋보다 더 다양한 예제를 포함하고 있습니다.

### [GS^3: Efficient Relighting with Triple Gaussian Splatting](https://arxiv.org/abs/2410.11419)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11419.png)

Vote: 8

Authors: Chong Zeng, Hongzhi Wu, Zoubin Bi, Yixin Zeng, Kun Zhou, Fan Pei, Xiang Feng

- **What's New**: 이번 연구에서는 공간 및 각도상의 Gaussian과 트리플 스플래팅(triple splatting) 방법을 기반으로 한 새로운 표현법을 제안합니다. 이는 여러 관점에서의 새로운 조명과 화면 합성을 실시간으로 높은 품질로 제공하며, 500~2000개의 다중 뷰 입력 이미지를 기반으로 합니다. 기존의 구면 조화 함수(spherical harmonics) 대신 램버시안과 미세각도 가우시안(differentiable anisotropic spherical Gaussian)을 사용하여 복잡한 외관을 설명하며, 자체 그림자를 지원하기 위한 공간 가우시안을 빛의 방향으로 스플래팅합니다. 또한 글로벌 일루미네이션을 보정하기 위해 다층 퍼셉트론(MLP)을 활용하여 RGB 튜플을 계산합니다.
- **Technical Details**: 제안된 방법은 기본적인 공간 가우시안에 구면 조화 함수 대신, 람버시안(lambertian)과 각도 가우시안의 혼합을 사용하여 복잡한 외관을 설명합니다. 이는 마이크로 페이싯 노말 분포를 나타냅니다. 자체 그림자를 효율적으로 지원하기 위해, 공간 가우시안을 빛의 방향으로 스플래팅하여 기존 화면 공간 스플래팅과 동일한 고성능 파이프라인을 재사용합니다. 또한 글로벌 일루미네이션과 같은 다른 효과를 보상하기 위해, 추가적인 MLP를 사용하여 색상 값을 계산하고, 카메라를 향해 세 가지 요소를 스플래팅하고 혼합하여 이미지를 생성합니다. 이 과정은 입력 사진과의 차이를 활용해 모든 과정을 최적화합니다.
- **Performance Highlights**: 제안된 표현법은 다양한 기하학 및 외관 변화에 대해 효과적입니다. GS와 비교했을 때 트레이닝 및 실시간 실행과 관련한 계산량이 약간 증가했음에도 불구하고, 새로운 조명 및 보기 조건에서 높은 성능과 품질의 합성 결과를 얻을 수 있습니다. 이러한 결과는 품질과 성능 측면에서 최첨단 기술과 비교하여 유리합니다. 이 접근법은 합성 또는 복원된 객체의 렌더링 이미지뿐만 아니라 스마트폰이나 프로페셔널 라이트스테이지로 촬영한 사진까지 다양한 입력 데이터를 처리할 수 있습니다.

### [Agent-as-a-Judge: Evaluate Agents with Agents](https://arxiv.org/abs/2410.10934)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.10934.png)

Vote: 6

Authors: Dylan Ashley, Changsheng Zhao, Zechun Liu, Jürgen Schmidhuber, Dmitrii Khizbullin, Yangyang Shi, Mingchen Zhuge, Ernie Chang, Yuandong Tian, Wenyi Wang, Yunyang Xiong, Raghuraman Krishnamoorthi, Vikas Chandra

- **What's New**: 최근 대규모 공간적 시스템은 단순한 문제를 해결하는 것에서 현실적인 문제를 해결하는 단계로 발전하였습니다. 그러나 기존의 시스템 평가 방법은 이러한 발전 속도를 따라가지 못하고 있는데, 특히 중간 단계를 평가하는 피드백 부족이 문제로 지적됩니다. 이에 따라 우리는 Agent-as-a-Judge라는 새로운 프레임워크를 제안하여 대리 시스템 자체를 평가하는 대리 시스템을 활용하도록 하였습니다. 이를 통해 코딩 생성 시스템의 평가를 개선하고자 합니다.
- **Technical Details**: DevAI라는 새로운 데이터셋을 소개하며, 이는 AI 앱 개발을 위한 실제적인 개발 태스크 55555555개를 포함합니다. 여기에는 요구 사항과 선택적인 선호 사항들이 포함되어 있으며, 개발 프로세스 전반을 평가할 수 있도록 설계되었습니다. 또한 Agent-as-a-Judge는 개발 과정 중 다양한 중간 피드백을 제공할 수 있도록 하여 기존의 사람 평가나 LLM-as-a-Judge보다 더 정교하고 경제적입니다.
- **Performance Highlights**: DevAI에서 인간 판정과 Agent-as-a-Judge를 비교한 결과, Agent-as-a-Judge는 인간 판정(90% 일치)보다 높은 일치를 보였으며 LLM-as-a-Judge(70% 일치)보다 더 정확했습니다. 또한 사람 측정 대비 비용과 시간을 절약할 수 있었습니다. 결과적으로 DevAI에서 코드 생성을 담당하는 Agentic 시스템은 더욱 현실적인 도전에 잘 대응함을 보여주고 있습니다.

### [SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning](https://arxiv.org/abs/2410.09754)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09754.png)

Vote: 6

Authors: Donghu Kim, Jaegul Choo, Peter Stone, Hyunseung Kim, Dongyoon Hwang, Jun Jet Tai, Kaushik Subramanian, Hojoon Lee, Peter R. Wurman, Takuma Seno

- **What's New**: 이 논문에서는 심플리시티 바이어스(simplicity bias)를 명시적으로 내재시켜 딥 강화 학습(deep reinforcement learning, RL)에서 파라미터를 효과적으로 확장할 수 있는 SimBa 네트워크를 소개합니다. 이 네트워크는 관측(normalization) 레이어, 사전-레이어 정규화 잔여(feedforward) 블록, 출력 레이어 전 정규화를 가지며 간결한 함수로 수렴하도록 설계되었습니다.
- **Technical Details**: SimBa는 심플리시티 바이어스를 측정하기 위해 Fourier decomposition을 활용하여 네트워크 출력에 대한 Fourier 변환을 수행하고, 낮은 주파수 함수일수록 높은 심플리시티 스코어를 부여합니다. 이를 통해 Nework가 단순하고 일반화 가능한 함수로 수렴하는 경향을 분석하였습니다.
- **Performance Highlights**: SimBa를 Soft Actor-Critic (SAC)과 같이 사용하여 인간과제 3가지에서 MLP보다 성능이 우수함을 확인했습니다. 특히, 네트워크 매개변수가 증가함에 따라 성능이 꾸준히 개선되는 것을 보여주었습니다. 또한, SimBa는 off-policy와 on-policy 모델프리 강화학습 알고리즘에서 시뮬레이션 효율성을 일관적으로 향상시켰으며, 컴퓨팅 자원이 많이 요구되지 않아 다양한 알고리즘에 쉽게 통합될 수 있습니다.

### [Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation](https://arxiv.org/abs/2410.08001)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.08001.png)

Vote: 3

Authors: Heming Cui, Qingwen Bu, Maoqing Yao, Li Chen, Yu Qiao, Hongyang Li, Jisong Cai, Jia Zeng

- **What's New**: 이 논문에서는 로봇 제어를 위한 새로운 접근법인 'RoboDual'을 제안합니다. RoboDual은 System-1과 System-2의 듀얼 시스템 아이디어에 기반하여, 'Generalist' 모델과 'Specialist' 모델을 결합합니다. 이를 통해 다양한 작업과 환경에서 높은 수준의 일반화 가능성과 효율적인 조작을 모두 달성하는 것을 목표로 하고 있습니다.
- **Technical Details**: RoboDual은 대규모 사전 학습된 OpenVLA를 'Generalist' 정책으로 사용하여 높은 수준의 작업 이해 및 일반화 가능성을 제공합니다. 그리고 다중 감각 입력과 Generalist의 출력으로 학습된 확산 트랜스포머(Scalable Diffusion Transformer)를 'Specialist'로 사용하여 실시간 제어를 가능케 합니다. 두 모델은 통합된 조건 메커니즘을 통해 원활하게 협력하며, Generalist 모델의 잠재 표현 및 이산화된 액션 출력은 Specialist가 새로운 작업이나 환경에 적응하도록 돕습니다.
- **Performance Highlights**: RoboDual은 CALVIN 데이터셋에서 일반적인 Generalist 모델보다 12%의 성능 향상을 나타냈으며, 실제 로봇 환경에서도 Specialist 및 Generalist 모델의 성능을 훨씬 뛰어넘는 결과를 보여주었습니다. 이 접근 방식은 다양한 작업에서 Specialist-와 Generalist-만을 사용하는 모델들과 비교해 우월한 성능을 발휘함을 광범위한 실험을 통해 입증했습니다.

### [Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt](https://arxiv.org/abs/2410.09745)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.09745.png)

Vote: 2

Authors: Chengguang Gan, Tatsunori Mori

- **What's New**: 이 논문에서는 Mutual Reinforcement Effect (MRE)이라는 개념을 소개하며, 텍스트 분류와 개체명 인식(NER) 작업을 혼합 데이터셋에서 함께 수행하는 방법론을 제시합니다. MRE는 문장 수준의 분류 작업과 단어 수준의 정보 추출(IE)을 병행하여 이들의 성능을 동시에 향상시키는 효과를 도모합니다.
- **Technical Details**: MRE는 문장 감성 분석 작업에서 예를 들어, 문장 전체의 감성이 부정적으로 분류되면 부정적인 단어들의 출현 가능성이 높아지며, 반대로 부정적인 단어들이 발견되면 문장 전체의 감성도 부정적으로 분류되는 식으로 양방향 강화 학습 방식입니다. 기존의 'Sentence-to-label' 프레임워크 이후로, T5 모델을 사용한 General Information Extraction Large Language Model (GIELLM)으로 진화했습니다. 이 모델들은 대규모 언어 모델(LLM)의 도입으로 다양한 텍스트 레벨 및 단어 레벨 작업을 단일 모델로 효율적으로 처리할 수 있게 되었습니다.
- **Performance Highlights**: 제안된 MRE 프레임워크는 각각의 작업을 독립적으로 학습한 모델에 비해 통합 작업에서 일관되게 우수한 성능을 보여주며, 이는 MRE의 상호 강화 효과의 시너지 이점을 입증합니다. 더 나아가, 단어 수준의 정보를 유능한 음성화(Knowledgeable Verbalizer)로 활용하여 소수 샘플의 텍스트 분류 문제에서 성능을 상당히 향상시켰습니다.

### [Towards Natural Image Matting in the Wild via Real-Scenario Prior](https://arxiv.org/abs/2410.06593)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.06593.png)

Vote: 1

Authors: Peng-Tao Jiang, Qianru Sun, Ruihao Xia, Bo Li, Hao Zhang, Yang Tang, Yu Liang, Pan Zhou

- **What's New**: 이번 연구에서는 복잡한 자연 이미지를 기반으로 하는 COCO-Matting 데이터셋과 새로운 Semantic Enhanced Matting (SEMat) 프레임워크를 제안합니다. 이는 기존의 상호작용 매팅 기법의 단점을 극복하고자 설계되었으며, 특히 COCO-Matting 데이터셋은 잘 알려진 COCO 데이터셋을 기반으로 38,251 개의 인스턴스 레벨 알파 매트를 생성하여 복잡한 자연적 시나리오를 잘 반영합니다.
- **Technical Details**: SEMat 프레임워크는 Feature Aligned Transformer (FAT)와 Matte Aligned Decoder (MAD)로 구성되어 있으며, FAT는 세그멘테이션과 매팅 간의 특징 불일치를 해결하고, MAD는 매팅에 특화된 물체들(예: 연기, 그물, 실크)을 예측할 수 있도록 설계되었습니다. 또한, 정규화 손실과 트리맵 손실과 같은 효과적인 학습 목표를 설정하여 동결된 네트워크와 학습 가능한 네트워크 간의 예측 일관성을 유지하고, 매팅 로짓이 트리맵 기반의 의미론적 정보를 포함하도록 유도합니다.
- **Performance Highlights**: COCO-Matting 데이터셋과 SEMat 프레임워크를 사용한 결과, 기존의 최첨단 방법에 비해 관련 메트릭(MAD, MSE, Grad, Conn)에서 각각 11%, 8%, 11%, 10%의 상대적 향상을 보여줍니다. 또한, 다양한 데이터셋(P3M, AIM, RW100, AM, RWP636)에서 SmartMat과 비교하여 최대 45%까지 상대적 성능 향상을 달성하였습니다.

### [MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval](https://arxiv.org/abs/2410.11619)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11619.png)

Vote: -

Authors: Kate Sanders, Jimena Guallar-Blasco, Nolan King, Alexander Martin, Benjamin Van Durme, Kenton Murray, Reno Kriz, David Etter, Ronald Colaianni, Cameron Carpenter, Kelly Van Ochten, Eugene Yang, Hannah Recknor

- **What's New**: MultiVENT 2.0 데이터셋이 발표되었습니다. 이 데이터셋은 218,000개 이상의 비디오와 3,900개가 넘는 수동으로 작성된 쿼리를 포함하여 세계 이벤트에 대한 정보를 비주얼 콘텐츠, 오디오, 임베디드 텍스트, 텍스트 메타데이터로부터 추출할 수 있도록 설계되었습니다. 비디오들은 아랍어, 중국어, 영어, 한국어, 러시아어, 스페인어를 포함한 다양한 언어로 구성되어 있으며, 뉴스 방송에서부터 개인이 핸드폰으로 촬영한 원본 영상까지 다양합니다.
- **Technical Details**: MultiVENT 2.0 데이터셋은 다국어 비디오 검색 작업을 위해 만들어졌으며, 복잡한 비주얼-언어 모델(Vision-Language Models, VLMs)을 평가하기 위해 3,900개 이상의 쿼리를 포함하고 있습니다. 이 모델들은 주로 이벤트 중심의 쿼리를 목표로 했고, 각 모달리티와 언어 간의 정보를 효과적으로 처리하고 통합할 것을 요구합니다. VLM뿐만 아니라 단일-모달리티에 특화된 모델들도 평가되었습니다.
- **Performance Highlights**: 초기 결과에 따르면, 이 작업은 현재의 최첨단 비전-언어 모델에 상당한 도전 과제를 제공합니다. 단일-모달리티에 특화된 모델들은 각자의 모달리티에서 어느 정도의 잠재력을 보였지만, 복합적인 쿼리를 다루는 데에는 여전히 부족합니다. 이러한 결과는 기존 시스템들이 복잡한 비전-언어 작업을 다루기에는 아직 불충분하며, 효과적인 비디오 검색을 위해 더 견고한 멀티모달 시스템의 필요성을 강조합니다.

