## Daily Papers (2024-10-07)

### [Addition is All You Need for Energy-efficient Language Models](https://arxiv.org/abs/2410.00907)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.00907.png)

Vote: 82

Authors: Hongyin Luo, Wei Sun

- **What's New**: 이 논문에서는 AI 시스템의 에너지 소비를 줄이기 위해 ℒℒ (caligraphic_L-Mul) 알고리즘을 제안합니다. 이 알고리즘은 부동 소수점(floating point) 곱셈을 정수 덧셈으로 근사함으로써, 특히 대규모 언어 모델(LLM)에 필요한 계산을 줄여주는 방식을 제시합니다. ℒℒ (caligraphic_L-Mul) 알고리즘은 기존 모델에서 곱셈 연산을 대체하여 에너지 소비를 효과적으로 줄일 수 있습니다.
- **Technical Details**: ℒℒ (caligraphic_L-Mul)은 부동 소수점(floating point) 연산을 정수 덧셈으로 대체할 수 있는 효율적인 알고리즘입니다. 특히 부동 소수점 연산이 많은 에너지를 사용하는 반면, 정수 덧셈은 훨씬 적은 에너지를 소모한다고 알려져 있습니다. 이 알고리즘은 예를 들어 attention 메커니즘의 곱셈 연산을 대체하거나, 모든 매트릭스 및 엘리먼트 별 곱셈에 적용할 수 있습니다. PyTorch와 같은 딥러닝 프레임워크에서 이 알고리즘을 통해 기존 fp32 연산을 int32 덧셈으로 변환할 수 있으며, 이는 초기 에너지의 약 2.7%만 소비합니다.
- **Performance Highlights**: 실험 결과, ℒℒ (caligraphic_L-Mul)을 사용한 모델은 자연어 추론, 언어 이해, 비전(Task) 작업에서 거의 손실 없는 성능을 보였으며, 특히 비전 작업에서는 0.12% 향상된 정확도를 보여주었습니다. 사전 학습된 대규모 언어 모델(LLM)에 ℒℒ (caligraphic_L-Mul) 기반의 attention 메커니즘을 적용하여 추가 학습 없이 동일한 성능을 얻었습니다. 추가로, 피네튜닝을 통해 ℒℒ (caligraphic_L-Mul)과 표준 곱셈 간의 성능 갭을 극복할 수 있음을 보여주었습니다. 이 기술은 네이티브 구현이 부족해 현재 GPU에서 완전한 효율성을 발휘하지 못하지만, 특별한 아키텍처 디자인이 통합된 디바이스에서 성장 가능성을 가지고 있습니다.

### [NL-Eye: Abductive NLI for Images](https://arxiv.org/abs/2410.02613)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02613.png)

Vote: 14

Authors: Mor Ventura, Michael Toker, Nitay Calderon, Yonatan Bitton, Roi Reichart, Zorik Gekhman

- **What's New**: NL-Eye라는 벤치마크가 도입되었습니다. 이는 VLMs(Visual Language Models)의 시각적 유추 능력을 여러 이미지에 걸쳐 평가하는 목적으로 설계되었습니다. 이전 연구들은 주로 단일 장면 설정에서 VLMs의 적합성을 평가했지만, NL-Eye는 다양한 카테고리와 시간 관계에 걸쳐 이 능력을 평가합니다.
- **Technical Details**: NL-Eye는 두 가지 주요 작업, 'Plausibility Prediction'과 'Plausibility Explanation'을 통해 VLM의 유추 추론 능력을 평가합니다. 이 벤치마크는 시작점을 제공하는 premise 이미지를 바탕으로 두 개의 가설(hypothesis) 이미지를 통해 이 가설이 얼마나 그럴듯한지를 추론합니다. 예제는 물리적, 논리적, 감정적, 기능적, 문화적, 사회적 등 6개의 이유 카테고리로 분류되며, 가설이 시간적으로 언제 발생하는지에 대한 정보도 포함됩니다.
- **Performance Highlights**: 사람들은 NL-Eye 작업에서 높은 성과를 보여 약 85%의 정확도로 더 그럴듯한 가설을 선택했습니다. 그러나 VLMs는 유추 추론 작업에서 실패하여 무작위 기준선을 넘지 못하며, 정확한 설명을 제공하는 데 있어서도 50% 이상의 실패율을 보였습니다. 이는 VLMs가 이미지 해석의 부정확성 때문에 유추 추론에서 어려움을 겪고 있음을 시사합니다.

### [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02703.png)

Vote: 13

Authors: Matan Kalman, Yaniv Leviathan, Yossi Matias

- **What's New**: 이번 연구에서는 표준 어텐션 메커니즘을 확장하여 원하는 수준의 문맥만 유지할 수 있는 '선택적 어텐션(Selective Attention)' 모델을 제안합니다. 이는 특정 토큰이 다른 토큰에 대한 주의력을 줄임으로써 쓸모 없는 정보를 제거하는 방식으로 동작합니다. 특히 자연어 모델링과 여러 크기의 모델과 문맥에서 의미 있는 성능 향상을 보여줍니다.
- **Technical Details**: 선택적 어텐션은 표준 어텐션 메커니즘에 간단한 수정을 가하여, 토큰들이 서로 마스킹할 수 있는 능력을 추가했습니다. 이것은 N×N 소프트 마스크 행렬 S를 생성하여 동작하며, 이것을 통해 토큰 간 마스킹을 구현합니다. 주목할 점은 선택적 어텐션이 새로운 매개변수를 추가하지 않으며, 기존 어텐션 모듈의 헤드 결과를 재사용해 부가적인 계산 없이 구현한다는 점입니다.
- **Performance Highlights**: 선택적 어텐션을 활용한 모델은 변수 할당 문제 및 자연어 모델링에서 표준 트랜스포머에 비해 더 나은 성능을 보여줍니다. 특히, 선택적 어텐션을 사용한 트랜스포머는 변수에 대한 할당이 발생할 때마다 이전 할당을 마스킹하여 문제를 경감시킬 수 있습니다. 또한, 'Bar, ##ack, Obama'와 같은 예에서 모호성을 줄여주는 역할을 effectively 수행합니다. 이러한 기능들은 중복된 계산을 줄이고 모델의 추론 과정을 간소화하는 데 도움을 줍니다.

### [Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding](https://arxiv.org/abs/2410.01699)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01699.png)

Vote: 11

Authors: Yao Teng, Xihui Liu, Guohao Dai, Han Shi, Xian Liu, Xuefei Ning, Yu Wang, Zhenguo Li

- **What's New**: 이번 연구에서는 auto-regressive(text-to-image) 생성을 가속화하기 위해 새로운 확률적 Jacobi Decoding(SJD) 알고리즘을 제안했습니다. 이는 최근의 샘플링 기반 auto-regressive 모델들에게 적용 가능하며, 추가적인 훈련 없이 다수의 토큰을 병렬적으로 디코딩할 수 있도록 합니다.
- **Technical Details**: Speculative Jacobi Decoding (SJD)은 기존의 결정론적 Jacobi 디코딩을 발전시킨 알고리즘으로, 샘플링 기반 디코딩을 지원하도록 설계되었습니다. 각 타임스텝에서 드래프트 토큰 시퀀스에 대한 조건부 확률을 계산하고, 이를 기반으로 확률적 기준을 통해 어떤 드래프트 토큰을 채택할지를 결정합니다. 채택된 토큰은 고정된 프리필링 시퀀스에 추가됩니다. 또, 공간적 국부성을 고려한 토큰 초기화 전략을 제안하여 생성 과정을 더욱 가속화합니다.
- **Performance Highlights**: Anole와 Lumina-mGPT 모델에서 SJD를 적용했을 때 약 2배 이상의 생성 가속을 이루며, 이미지 품질의 손상 없이 성능을 발휘했습니다. 간단한 패턴이 포함된 시나리오의 경우 가속 비율이 3배 이상으로 증가할 수 있습니다. 이는 auto-regressive 모델의 샘플링 디코딩에 의존하는 텍스트 투 이미지 모델의 추론을 가속화하는 첫 번째 방법론으로 자리잡을 것입니다.

### [Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise](https://arxiv.org/abs/2410.03017)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03017.png)

Vote: 11

Authors: Susanna Loeb, Carly D. Robinson, Rose E. Wang, Dora Demszky, Ana T. Ribeiro

- **What's New**: 새로운 연구는 Tutor CoPilot라는 인공지능 기반의 시스템을 도입하여 교육 영역에서 전문 지식 스케일링을 시도합니다. 이 시스템은 실시간으로 튜터에게 전문가와 같은 제안을 제공하여, 소외된 커뮤니티의 학생들을 포함하여 K-12 교육의 질을 향상시키고 비용 효율적으로 전문 교육을 확장하는 데 중점을 둡니다.
- **Technical Details**: Tutor CoPilot는 LMs(Language Models)과 인간 전문가의 사고 과정을 결합하여, 특히 수학과 같은 복잡한 과목에서 실시간 교육 상황에 적합한 전문가 수준의 가이드를 생성합니다. 연구는 이를 달성하기 위해 'think-aloud protocols'을 활용하여 교육자들의 잠재된 추론을 추출하고 이를 LMs에 적용하는 방식을 채택했습니다.
- **Performance Highlights**: 랜덤화된 대조 시험 결과, Tutor CoPilot를 이용한 학생들의 학습 성과가 통제 그룹에 비해 4% 포인트 더 높은 마스터리(lesson mastery)를 달성했습니다. 특히 낮은 평가를 받은 덜 숙련된 튜터들이 Tutor CoPilot의 도움을 받아 학생들의 마스터리를 9% 포인트까지 향상시켰습니다. Tutor CoPilot는 고품질 교육 전략을 더 많이 사용할 가능성을 증가시켰으며, 튜터들 사이에서 유용하다는 피드백을 받았습니다. 시스템은 연간 $20의 비용으로 전통적인 훈련 프로그램에 비해 비용 효율적인 솔루션을 제공합니다.

### [RoCoTex: A Robust Method for Consistent Texture Synthesis with Diffusion Models](https://arxiv.org/abs/2409.19989)

![](/avatars/dd48dff0b639123c605b5c0ee10577d7.svg)

Vote: 8

Authors: Dumim Yoon, Junghyun Han, Jeonga Wi, Donggoo Kang, Junyoung Choi, Jiun Bae, Jangyeong Kim, Junho Gwon

- **What's New**: RoCoTex는 기존 텍스처 생성 기법의 문제를 해결하기 위해 설계된 새로운 텍스처 합성 방법입니다. 이 방법은 대칭 뷰 합성 전략과 지역 프롬프트를 사용하여 뷰 일관성을 크게 향상시키고, 다양한 ControlNets와 SDXL을 조합하여 고품질의 잘 정렬된 텍스처를 생성합니다.
- **Technical Details**: RoCoTex는 SDXL(Stable Diffusion XL)을 사용하여 더 큰 UNet 백본을 바탕으로 여러 ControlNets(깊이, 노멀, Canny 에지)를 활용해 객체의 기하학적 정보를 이해합니다. 또한, 픽셀 신뢰도를 바탕으로 한 텍스처 블렌딩 기법과 Differential Diffusion을 사용한 소프트 인페인팅 기법을 도입하여 솔기(seam) 문제를 줄입니다.
- **Performance Highlights**: RoCoTex는 기존 SOTA(State-of-the-Art) 기법들을 능가하여 텍스처 생성의 강건성과 일관성을 보여줍니다. 실험 결과, 기존 기법들에서 발생하던 뷰 불일치 문제와 돌출 텍스처(artifacts)를 효과적으로 감소시켰습니다.

### [A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond](https://arxiv.org/abs/2410.02362)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02362.png)

Vote: 7

Authors: Chandravardhan Singh Raghaw, Madhava Prasath J, Nagendra Kumar, Gaurav Duggal, Shubhi Bansal, Manikandan S, Sreeharish A, Sreekanth Madisetty, Mohammad Zia Ur Rehman

- **What's New**: 이 논문은 Mamba 모델을 중심으로 하는 state space models (SSM)와 그 응용 사례를 의료 분야에 집중하여 포괄적으로 논의합니다. 특히, Mamba가 S4, S5, S6 등의 새로운 구조와 함께 어떻게 고차원의 시퀀스를 간단하게 처리하면서도 기존의 CNN 및 Transformer의 한계를 극복하는지에 대해 설명합니다.
- **Technical Details**: Mamba는 Recurrent하게 동작하며 주목할만한 포인트는 attention을 사용하지 않으면서도 기존 Transformer와 유사한 성능을 유지한다는 점입니다. 이 모델은 Kalman 필터와 유사한 접근 방법을 사용하여, 1차원 입력 시퀀스를 여러 차원으로 변환한 후, 다시 1차원 출력 신호로 투영합니다. 이는 특히 장거리 종속성 처리가 핵심인 분야에서 유리합니다. 학습과정에서 FlashAttention 및 Linear Attention과 같은 최신 기법을 통해 계산 복잡성을 줄였습니다.
- **Performance Highlights**: Mamba는 복잡한 의료 이미징 및 진단에서 특히 우수한 성능을 보였습니다. CNN이나 Transformer 기반 모델이 처리가 어려운 장거리 종속성과 고복잡성을 갖는 데이터에 대해 효율적이고 정확한 결과를 보이며, 다양한 의료 이미지 세분화와 질병 진단 과제에서 검증되었습니다. 모델의 확장은 Figure 1과 관련 차트에 의해 강조되며, 2023년 말부터 2024년 사이 연구가 급증했습니다.

### [Erasing Conceptual Knowledge from Language Models](https://arxiv.org/abs/2410.02760)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02760.png)

Vote: 7

Authors: Samuel Marks, David Bau, Sheridan Feucht, Rohit Gandikota

- **What's New**: 이번 연구에서는 '언러닝(unlearn)' 개념을 보다 명확하게 정의하고, 언어 모델에서 원하는 개념을 지우는 새로운 방법인 Erasure of Language Memory (ELM)를 제안합니다. 이 방법은 텍스트 생성을 손상시키지 않고도 불필요하거나 유해한 지식을 효과적으로 제거할 수 있게 합니다.
- **Technical Details**: ELM은 모델의 특정 층에 대해 low-rank adaptation 방법을 활용하여 원래 모델과의 일치 목표를 유지하되, 지워질 개념에 속하는 텍스트에 대한 가능성을 줄이는 방향으로 파인튜닝합니다. 이를 통해 내부 표현을 효과적으로 제거할 수 있습니다. 또한 이 방법은 생성된 파인튜닝 데이터로 원활함을 유지하면서도 지워진 개념에 대한 정보가 추가되지 않도록 합니다.
- **Performance Highlights**: ELM은 WMDP 바이오 보안, 사이버 보안 및 문학적 개념을 다루는 다양한 벤치마크에서 테스트되었으며, 무작위적 공격과 다지선다 질문을 활용해 '무해성(innocence)', '특정성(specificity)', '원활성(seamlessness)' 기준에서 높은 성능을 입증했습니다. 특히, 이전의 다른 방법들과 비교하여 모든 기준에서 우수한 성과를 보였습니다.

### [MIGA: Mixture-of-Experts with Group Aggregation for Stock Market Prediction](https://arxiv.org/abs/2410.02241)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02241.png)

Vote: 5

Authors: Yinghao Wu, Heming Weng, Zhaojian Yu, Genesis Wang

- **What's New**: 이 논문은 새로운 MIGA (Mixture of Expert with Group Aggregation) 프레임워크를 도입하여 다양한 스타일의 주식을 처리할 수 있는 예측 모델을 제안합니다. MIGA는 전통적인 끝에서 끝 (end-to-end) 모델과 달리 전문화된 전문가들을 동적으로 전환하는 두 단계 설계를 채택하였습니다.
- **Technical Details**: MIGA 아키텍처는 두 주요 구성 요소인 라우터(router)와 전문가(experts)로 구성됩니다. 라우터는 각 데이터 포인트에 가중치를 할당하고, 전문가는 각자의 예측을 생성합니다. MIGA의 최종 출력은 모든 전문가의 예측을 가중 평균하여 산출됩니다. 이 프레임워크에서는 3가지 타입의 피처 인코더(convolution-based, recurrence-based, attention-based)를 사용하여 실험을 진행하였습니다.
- **Performance Highlights**: MIGA 모델은 중국의 3대 주요 주가 지수(CSI300, CSI500, CSI1000)에 대해 광범위한 평가를 거쳤으며, 모든 끝에서 끝 모델을 능가하는 뛰어난 성능을 보였습니다. 특히 CSI300 벤치마크에서는 MIGA-Conv 모델이 장기 단일 투자 포트폴리오에서 연간 초과 수익률 24%를 기록하며 주목할 만한 결과를 나타냈습니다.

### [CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction](https://arxiv.org/abs/2410.01273)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01273.png)

Vote: 4

Authors: Suhwan Choi, Minseo Kim, Jaeyoon Jung, Yongjun Cho, Hwiseong Park, Youngjae Yu, Sungjae Lee, Yubeen Park, Sungwoong Kim, Minchan Kim, Jiwan Chung, Myunchul Joe

- **What's New**: CANVAS (Commonsense-Aware NaVigAtion System)를 소개합니다. 이 새로운 프레임워크는 인간의 추상적인 지시를 로봇 내비게이션 계획으로 통합하여, 로봇이 모호하거나 불명확한 인간 지시 상황에서도 효과적으로 목표를 달성할 수 있도록 합니다. COMMAND 데이터셋도 함께 소개되며, 이는 공통 상식을 갖춘 내비게이션 로봇 트레이닝을 위한 종합적인 데이터셋입니다.
- **Technical Details**: CANVAS는 시각적 및 언어적 입력을 사용하여 로봇 내비게이션을 수행합니다. 이 시스템은 비전-언어 모델을 통해 인간의 스케치나 텍스트 지시를 점진적인 내비게이션 목표로 변환합니다. COMMAND 데이터셋은 48시간의 주행 데이터와 219km에 달하는 다양한 환경에서의 주행 결과를 포함하며, 이를 통해 모방 학습을 지원합니다. 또한 Frechet Distance(FD)를 활용하여 스케치 지시와 사람의 실제 주행 경로 간의 차이를 측정합니다.
- **Performance Highlights**: CANVAS는 모든 시뮬레이션 환경에서 ROS NavStack을 꾸준히 능가합니다. 특히 복잡한 과수원 환경에서는 효과적으로 내비게이션하여 규칙 기반 알고리즘에 의존하는 NavStack보다 우수한 성과를 보입니다. CANVAS는 시뮬레이션 데이터로만 훈련되었음에도 불구하고 강력한 Sim2Real 전송 능력을 입증하며, 실제 세계에서도 높은 성과를 보여줍니다. CANVAS는 인간의 지시와 일치하도록 경로를 조정하는 뛰어난 능력을 갖추고 있습니다.

### [NRGBoost: Energy-Based Generative Boosted Trees](https://arxiv.org/abs/2410.03535)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03535.png)

Vote: 3

Authors: João Bravo

- **What's New**: 최근 연구는 테이블 형식 데이터의 생성 모델링을 발전시켜 데이터 개인 정보 문제를 해결하고 새로운 가능성을 열어줍니다. 기존의 Random Forests 및 Gradient Boosted Decision Trees를 확장하여 제안된 NRGBoost는 에너지 기반 생성 부스팅 모델로, 각 부스팅 라운드에서 가능도를 최대화하는 로컬 2차 근사를 사용합니다. 이는 기존의 생성 모델들이 주로 샘플링에만 유용한 것과 차별화됩니다.
- **Technical Details**: 에너지 기반 모델(EBM)은 확률 밀도 함수의 로그를 특정한 정규화 상수 없이 직접 파라미터화합니다. EBM은 데이터 생성 분포를 근사화하기 위해 Kullback-Leibler divergence를 최소화하거나 기대 로그 가능도를 최대화하는 방식으로 학습할 수 있습니다. NRGBoost는 에너지 함수의 2차 근사치를 사용하여 부스팅 라운드 동안 로그 가능도의 증가를 최대화하는 방향으로 학습을 진행합니다.
- **Performance Highlights**: 실험 결과에 따르면 NRGBoost는 중소형 데이터셋에서 일반적인 Gradient Boosted Decision Tree(GBDT) 모델과 비슷한 수준의 판별 성능을 달성하며, 표본화 측면에서도 최고 수준의 생성 모델들과 경쟁력을 갖추고 있습니다. NRGBoost는 중산 소비자용 CPU를 이용해서도 몇 분 안에 학습이 가능하여 실용성을 높였습니다.

### [Horizon-Length Prediction: Advancing Fill-in-the-Middle Capabilities for Code Generation with Lookahead Planning](https://arxiv.org/abs/2410.03103)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03103.png)

Vote: 2

Authors: Yifeng Ding, Hantian Ding, Qing Sun, Shiqi Wang, Zijian Wang, Varun Kumar

- **What's New**: 최신 연구에서는 대규모 소스 코드 데이터를 학습한 Large Language Models (LLMs)이 코딩 관련 작업에서 상당한 진전을 보여주고 있습니다. 자연어 생성은 주로 좌에서 우(Left-to-Right, L2R) 접근을 따르지만, 코드 완성 시나리오에서는 Fill-in-the-Middle (FIM)의 중요성이 두드러집니다. 이는 주로 코딩의 반복적 성질, 즉 빈번한 수정과 삽입에 의해 발생합니다. 이 연구에서는 FIM의 성능을 향상시키기 위해 Horizon-Length Prediction (HLP)라는 새로운 학습 방식을 제안합니다. HLP는 모델이 생성할 토큰의 길이를 미리 예측하여 코드 교환의 완벽한 연결성을 유지합니다.
- **Technical Details**: 기존의 FIM 기능은 주로 uni-directional auto-regressive 모델을 이용하여 시퀀스의 순서를 재조정하거나 <end_of_insertion> 토큰을 사용하는 방식으로 구현되었습니다. 그러나 이러한 방법은 예측 시 수평선 길이가 확장됨에 따라 모델이 생성 과정에서의 마찰을 해결하는 데 충분하지 않았습니다. HLP는 현재 토큰의 숨겨진 상태를 기반으로 미래에 필요할 토큰 수를 예측하게 하는 보조 학습 목표를 도입하여 이러한 문제를 해결합니다. 이는 표준 next-token prediction (NTP)와 결합되어, 주어진 좌우 컨텍스트에 맞춘 자발적인 생성 종료를 목표로 합니다.
- **Performance Highlights**: HLP를 사용한 평가 결과, 다양한 파일 수준 및 저장소 수준의 FIM 벤치마크에서 최대 24%의 성능 향상을 기록했습니다. 이는 특정 작업-기반 프로세싱 없이도 가능합니다. 또한, 코드 추론 성능에서도 개선을 이루었습니다. HLP는 학습 부담이 거의 없으며, 추론 비용도 추가되지 않기 때문에 효율적입니다.

### [MLP-KAN: Unifying Deep Representation and Function Learning](https://arxiv.org/abs/2410.03027)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03027.png)

Vote: 1

Authors: Zhengqing Yuan, Yifeng Xie, Lichao Sun, Yunhong He

- **What's New**: 이 논문에서는 MLP-KAN이라는 새로운 프레임워크를 소개합니다. 이 시스템은 Mixture of Experts (MoE) 방법론을 활용하여 두 가지 학습 접근 방식을 단일 시스템으로 통합합니다. Multi-Layer Perceptrons (MLP)는 표현 학습의 전문가로, Kernel Attention Networks (KAN)는 함수 학습의 전문가로 역할을 합니다. MoE 메커니즘은 적절한 전문가로 입력을 효율적으로 라우팅하여 효율성과 성능을 크게 향상시킵니다.

### [AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark](https://arxiv.org/abs/2410.03051)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03051.png)

Vote: 1

Authors: Jeng-Neng Hwang, Omer Bar-Tal, Saining Xie, Vashisht Madhavan, Yilun Du, Wenhao Chai, Christopher D. Manning, Enxin Song, Chenlin Meng

- **What's New**: AuroraCap는 상세한 비디오 설명 생성 과제를 해결하기 위한 새로운 모델로 소개됩니다. 이는 기존의 대규모 언어 모델(LLM)과 비주얼 모델의 장점을 결합하여 효율적인 비디오 설명을 생성합니다. 이를 위해 최근 토큰 병합(Token Merging)과 같은 기법을 통해 비디오와 이미지 입력의 비쥬얼 토큰 수를 줄이는 효율적인 방법을 도입하고 디테일이 풍부한 비디오 캡션 작성을 위한 새로운 벤치마크 VDC(Video Detailed Captions)를 구축하였습니다.
- **Technical Details**: AuroraCap는 프리트레인된 LLM과 비주얼 모델을 효과적으로 활용하기 위해 MLP(Multi-layer Perceptron)를 사용하여 이미지 특징의 패치 토큰을 워드 임베딩 공간으로 투영합니다. 이를 통해 효율적인 피처 정렬이 가능하며, 토큰 병합(Token Merging) 기법을 도입하여 유사한 토큰을 결합, 전송 토큰 수를 줄임으로써 비쥬얼 입력의 처리량을 증가시킵니다. 또한, 비디오 상세 캡션을 위한 새로운 평가 메트릭인 VDCscore를 제안하여, 기존의 평가 메트릭의 한계를 극복하고 상세 캡션의 품질을 보다 효과적으로 점수화할 수 있습니다.
- **Performance Highlights**: AuroraCap는 기존 비디오 설명 생성 모델에 비해 더 긴비디오 입력을 지원하면서도 성능 저하가 미미하게 나타납니다. 또한, VDC 벤치마크에서 최첨단 성능을 보이며, 기존의 비디오 이해 벤치마크를 자세한 설명 능력을 시연하는 데 효과적이지 않은 문제를 해결합니다. 실험 결과, ViT로 생성된 원래 토큰에 비해 10%에서 20% 정도의 비쥬얼 토큰만 사용하여도 다양한 벤치마크에서 성능이 거의 떨어지지 않음을 보여주었습니다.

### [CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding Capabilities of CodeLLMs](https://arxiv.org/abs/2410.01999)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01999.png)

Vote: 1

Authors: Thang Phan Chau, Nghi D. Q. Bui, Nam V. Nguyen, Quang Pham, Dung Nguyen Manh, Thong T. Doan, Nam Le Hai

- **What's New**: 이 논문은 새로운 머신러닝(Machine Learning) 방법론이나 이론을 제시하며, 기존 방식에 비해 중요한 성능 개선을 이룬 연구에 대한 내용입니다. 특히 현대적인 AI 문제를 해결하기 위해 제안된 혁신적인 접근법을 분석한 글입니다.
- **Technical Details**: 기술적으로는 논문에서 제안한 알고리즘(Algorithm)이나 모델(Model)의 구조와 작동 방식에 대해 설명하고 있습니다. 이 모델은 특수한 데이터 구조를 사용하여 기존 모델들이 간과했던 문제들을 해결하려고 합니다. 구체적으로, 모델의 학습 방법과 데이터 전처리 기법을 통해 장점이 강화됩니다.
- **Performance Highlights**: 제안된 모델은 여러 벤치마크(Benchmark) 데이터셋에서 기존의 최첨단 기법 대비 주목할 만한 성능 개선을 이루었습니다. 특히 정확도(Accuracy)와 처리 속도 측면에서의 향상이 두드러집니다.

### [GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs](https://arxiv.org/abs/2410.03645)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03645.png)

Vote: 1

Authors: Lirui Wang, Minghuan Liu, Weinan Zhang, Huazhe Xu, Annabella Macaluso, Pu Hua, Yunfeng Lin

- **What's New**: 이 논문은 새롭게 개발된 트랜스포머(Transformer) 아키텍처에 대한 혁신적인 접근법을 제안합니다. 이 모델은 기존의 트랜스포머 구조를 개선하여 효율성과 성능을 동시에 극대화하고자 합니다. 특히, 대규모 데이터 셋을 다루는 데 있어 기존의 모델에 비해 성능 향상을 보여줍니다.
- **Technical Details**: 논문에서 제안된 메커니즘은 변형된 어텐션(attention) 메커니즘을 채택하여 계산 복잡성을 줄이는 동시에 정보의 정확한 처리를 가능하게 합니다. 또한, 여러 레이어(layer) 간의 데이터를 효율적으로 전파하는 새로운 방법론이 소개됩니다. 이로써 모델의 전반적인 학습 과정이 최적화됩니다.
- **Performance Highlights**: 제시된 모델은 대규모 벤치마크 테스트에서 기존의 모델과 비교하여 10% 이상의 성능 향상을 기록하였습니다. 특히 자연어 처리(NLP)와 관련된 여러 태스크에서 뛰어난 결과를 보여주었으며, 이에 따라 해당 분야에서의 활용 가능성을 넓히고 있습니다.

