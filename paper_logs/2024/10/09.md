## Daily Papers (2024-10-09)

### [$\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization](https://arxiv.org/abs/2410.04717)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04717.png)

Vote: 13

Authors: Francois Charton, Justin Wang, Dylan Zhang

- **What's New**: 이 연구는 대형 언어 모델(LLM)의 Instruction-Following 능력을 중점적으로 분석하여 모델이 다양한 지시를 어떻게 해석하고 처리하는지를 체계적으로 조사합니다. 특히 마르코프 알고리즘의 문자열 재작성과 같은 상징적 작업을 통해 지시 다양성이 모델의 일반화 능력에 미치는 영향을 연구합니다.
- **Technical Details**: 연구에서는 문자열 대체 작업을 이론적 컴퓨터 과학의 기본으로 삼아 데이터를 다양화하는 실험을 진행합니다. 이러한 접근법은 LLM이 구조화된 규칙 기반 변환을 처리하고 일반적인 규칙 기반 작업에서 모델의 적응 능력을 평가하는 방법을 개발합니다. 두 가지 문자열 대체 작업인 기본 대체와 조건부 대체를 사용하여 모델의 규칙 적용 능력과 조건부 의사결정 처리를 평가합니다.
- **Performance Highlights**: 연구 결과, 지시가 충분히 다양화된 경우에만 기존에 보지 못한 작업 의미에 대한 일반화가 진전될 수 있음을 발견했습니다. 단일 분야에 국한된 다양화는 강력한 일반화를 보장하지 않으며, 반대로 분야를 넘나드는 다양화가 새로운 지시에 대한 모델의 적응력을 상당히 향상시킵니다. 데이터 다양성의 중요성을 강조하며, 데이터 셋 확장이 필요한 경우 데이터 다양화를 통한 성능 개선이 데이터 크기를 단순히 늘리는 것보다 효과적임을 보여줍니다.

### [LongGenBench: Long-context Generation Benchmark](https://arxiv.org/abs/2410.04199)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04199.png)

Vote: 10

Authors: Xiang Liu, Xuming Hu, Xiaowen Chu, Peijie Dong

- **What's New**: 이 논문에서는 LLMs(Language Models)를 대상으로 한 새로운 벤치마크인 Long-context Generation Benchmark (LongGenBench)를 제안합니다. LongGenBench는 긴 문맥을 바탕으로 논리적인 흐름과 일관성이 유지되는 텍스트 생성을 평가하도록 설계되었습니다. 이는 LLMs가 다수의 질문을 포함한 질의를 포괄적으로 응답하도록 요구합니다.
- **Technical Details**: LongGenBench는 인기 있는 LLM 벤치마크로부터 데이터를 합성하여 데이터셋을 구성하고 입력 포맷을 새롭게 설계합니다. 이 벤치마크는 LLM에게 이전에 생성된 문맥의 정확성에 상관 없이 일관성을 유지해야 하는 능력을 평가합니다. 또한, Flash attention, sparse attention과 같은 보조 기술들이 긴 문맥에서의 효율성을 향상시키는데 사용됩니다.
- **Performance Highlights**: Gemini-1.5-Flash 모델은 긴 문맥 생성 작업에서 가장 낮은 성능 저하를 나타내며, GPT-4o 보다 우수한 성능을 보였습니다. 분석 결과, 높은 기본 줄세우기 점수를 가진 모델들은 일반적으로 긴 문맥 생성 작업에서도 성능 저하가 적었습니다. 예외적으로, LLaMA-3-70B-Instruct는 높은 기본 성능에도 불구하고 상당한 성능 저하를 겪었습니다. 다양한 모델 아키텍처는 서로 다른 성능 저하 경향을 나타냈으며, 같은 시리즈 내에서 더 큰 모델은 덜한 성능 저하를 보였습니다.

### [A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation](https://arxiv.org/abs/2410.01912)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01912.png)

Vote: 8

Authors: Liang Chen, Jinze Bai, Yichi Zhang, Sinan Tan, Weichu Xie, Tianyu Liu, Junyang Lin, Baobao Chang, Haozhe Zhao, Zefan Cai

- **What's New**: 새로운 2-차원 Autoregression (AR) 이미지 생성 패러다임인 DnD(Depth and dimensions)와 이를 뒷받침하는 DnD-Transformer 모델을 제안했습니다. 이는 기존 VQ 기반 AR 이미지 생성의 정보 손실 문제를 해결하면서도 계산 비용을 증가시키지 않는 방법으로, 이미지의 깊이 차원을 추가하여 더욱 정밀한 재구성을 가능하게 합니다.
- **Technical Details**: DnD Autoregression은 공간적 차원 외에도 깊이 차원을 도입하여 이미지 패치를 다양한 인과적 코스-투-파인(causal coarse-to-fine) 순서로 분해합니다. 이 접근법을 통해 기존 VQ-VAE의 한계를 극복하고 정보 손실을 줄입니다. DnD-Transformer는 백본 transformer 디코더 모델에 복수의 예측 헤드를 삽입하여 각 포워드 프로세스에서 추가적인 autoregressive 예측을 수행합니다.
- **Performance Highlights**: DnD-Autoregression은 ImageNet 256x256 생성에서 AR 베이스라인을 크게 능가하며, 텍스트가 풍부한 이미지의 무조건적 생성에서 역시 Diffusion 모델을 초과하는 성능을 보여줍니다. 특히, 잔차 이미지 분해를 이용할 때 정밀한 이미지 디테일의 복원에서 우수한 성능을 입증하였습니다. DnD-Transformer는 더 많은 코드를 예측하면서도 1D 방법보다 낮은 학습 교차 엔트로피 손실을 보여주었습니다.

### [RevisEval: Improving LLM-as-a-Judge via Response-Adapted References](https://arxiv.org/abs/2410.05193)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05193.png)

Vote: 7

Authors: Ruiming Tang, Xin Jiang, Fuyuan Lyu, Yufei Wang, Lifeng Shang, Yasheng Wang, Qiyuan Zhang, Tiezheng YU, Yuxin Jiang, Liangyou Li, Chuhan Wu, Chen Ma

- **What's New**: 이 연구는 LLM(대형 언어 모델)의 평가 역량을 개선하기 위해 'RevisEval'이라는 새로운 평가 패러다임을 제안합니다. 이 방법은 모델이 생성한 응답에 대해 적응된 참조(responses-adapted references)를 생성하고 이것을 평가에 활용하는 것을 목표로 합니다.
- **Technical Details**: RevisEval의 주요 구성 요소는 '응답에 적응된 참조 생성'과 '참조 기반 평가'입니다. 주어진 작업 지침과 평가 룰북에 따른 모델 응답을 기반으로 LLM 수정자(reviser)가 원래 응답을 수정하여 고품질의 관련성을 보장하는 참조를 생성합니다. 이러한 참조는 최종 평가를 안내하는 데 사용됩니다.
- **Performance Highlights**: RevisEval은 기존의 참조 없는 평가(reference-free)와 참조 기반 평가 방법보다 두드러진 성능을 보였습니다. 다양한 자연어 생성(NLG) 작업과 개방형 지침 추종 작업에서 수행된 실험에서 RevisEval은 더 높은 평가 정확도를 유지했습니다. BERTScore와 같은 클래식 메트릭의 정확도를 3%-10%까지 초과하는 성과를 보이며, 낮은 성능의 LLM을 사용할 때 기준보다 1.5% 더 나은 결과를 기록했습니다. GPT-4를 사용했을 때는 성능이 비교 가능했습니다.

### [DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search](https://arxiv.org/abs/2410.03864)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03864.png)

Vote: 5

Authors: Wenlin Yao, Ziyu Yao, Dian Yu, Dong Yu, Murong Yue, Haitao Mi

- **What's New**: 최근 논문은 새로운 방법 ‘\method’를 제시하여 LLMs (Large Language Models)가 주어진 질문과 문제 해결에 있어 최적의 리즌(reasoning) 액션을 선택할 수 있도록 합니다. 이는 원래의 모델을 변경하지 않고 질의의 특성에 따라 동적으로 가장 적합한 리즌 전략을 선택하는 것이 목표입니다.
- **Technical Details**: 기존의 instruction tuning과 prompt engineering은 각각 유연성 부족, 동일한 질문에도 동일한 전략 적용 등 한계가 있었습니다. \method는 두 가지 설정으로 구현됩니다: 첫째, 외부 플래너를 훈련하여 닫힌 소스 또는 고비용의 LLM에 최적의 리즌 액션을 제안; 둘째, 오픈 소스 및 소형 LLM은 자체적으로 리즌 액션 계획 기능을 내재화하여 스스로 계획 능력을 키우게 합니다.
- **Performance Highlights**: \method는 다양한 LLMs 및 리즌 작업 시나리오에서 기존의 정적 프롬프트 엔지니어링 기술 및 일반 instruction tuning 방법을 지속적으로 능가함을 증명했습니다. 각각의 구성 요소의 중요한 역할을 입증하는 종합적 ablation 스터디 및 리즌 액션 분포 분석에서 강력한 성능 향상을 확인했습니다. 추가적인 재정적 비용 부담도 최소화합니다. LLMs는 탐구 및 학습 과정을 통해 대규모 문제에 자체적으로 더 많은 계산 자원을 할당하는 능력을 자연적으로 개발할 수 있음을 보여줍니다.

### [Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models](https://arxiv.org/abs/2410.03290)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03290.png)

Vote: 5

Authors: Shizhe Diao, Haibo Wang, Weifeng Ge, Zhiyang Xu, Lifu Huang, Yixin Cao, Yufan Zhou, Qifan Wang, Yu Cheng

- **What's New**: 이 연구는 이전의 Video-LLM들이 직면했던 세분화된(세밀화된) 비디오 이해 문제를 해결하기 위해 'Grounded-VideoLLM'이라는 새로운 모델을 제안합니다. 이 모델은 정교한 시간적 변환(Temporal Grounding)을 통해 특정 순간을 인식하고 논리적으로 이해하는 데 중점을 두고 있습니다. 특히 두 가지 혁신적인 접근을 통해 Video-LLM의 한계를 극복하고자 합니다.
- **Technical Details**: Grounded-VideoLLM은 두 개의 주요 기술적 혁신을 중심으로 구축되었습니다: (1) 'Two-Stream Encoding'은 비디오를 공간적 및 시간적 성분으로 분해하고 각각을 전문 인코더로 처리하여 종합적인 비디오 표현을 생성합니다. (2) 시간 토큰(Temporal Tokens)을 도입하여 LLM의 어휘를 확장하며, 이는 비디오 내 상대적 시간 위치를 나타내고 LLM의 임베딩 공간과 통합됩니다. 이러한 접근은 비효율적인 숫자 텍스트 토큰화를 피하고 시간 및 텍스트 출력을 하나의 디스크리트 토큰 시퀀스로 예측할 수 있게 해줍니다. 또, 세 가지 단계의 훈련 전략을 통해 이미지 기반의 MLLM을 점진적으로 비디오 이해 모델로 발전시켰습니다.
- **Performance Highlights**: Grounded-VideoLLM은 전통적인 비디오 시간적 변환 작업뿐만 아니라 일반 비디오 이해 벤치마크에서도 기존 Video-LLM들을 능가하는 뛰어난 성능을 보여주었습니다. 특히 세밀한 비디오 이해를 위한 다양한 작업에서 우수한 성과를 입증하였습니다. 이는 세밀한 시간 지향적 모델링과 강력한 시간표현 역량을 통해 달성되었습니다. 또한, GPT-4의 도움을 받아 17K의 Gronded VideoQA 자가 데이터셋을 구축하여 모델의 시간적 추론 능력을 향상시켰습니다.

### [MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions](https://arxiv.org/abs/2410.02743)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02743.png)

Vote: 5

Authors: Yu Sun, Haoran Sun, Shuohuan Wang, Yekun Chai, Hua Wu, Huang Fang

- **What's New**: 최근 Large Language Models(LLMs)의 혁신이 코드 생성, 수학적 추론, 대화 지원 등 다양한 자연어 처리 작업에서 탁월한 성능을 입증했습니다. 그러나 LLM을 인류의 가치와 선호에 맞추는 것은 여전히 중요한 과제입니다. 이를 해결하기 위해 인간의 피드백을 통한 강화 학습(Reinforcement Learning from Human Feedback, RLHF)이 유망한 접근법으로 부상했습니다. 이러한 가운데, 본 논문은 새로운 프레임워크인 매크로 액션 RLHF(MA-RLHF)를 제안합니다. MA-RLHF는 매크로 액션(macro-action)을 도입하여 기존의 미세 조정 방식의 한계를 극복하고 LLM과의 조화를 더욱 강화합니다.
- **Technical Details**: 기존의 RLHF 방법은 개별 토큰 수준에서 결정을 최적화하며, 긴 시퀀스에서의 크레딧 할당 문제가 발생할 수 있습니다. MA-RLHF는 매크로 액션을 활용하여 토큰 수준이 아닌 고수준의 언어 구조로 학습을 진행합니다. 이러한 매크로 액션은 반강마르코프 결정 프로세스(Semi-Markov Decision Processes, SMDPs) 프레임워크 아래에서 의미가 있는 언어 구조를 활용하여 긴 시퀀스의 의사결정 복잡성을 감소시킵니다. 또한, MA-RLHF는 토크나이제이션을 역전시키는 과정으로, 토큰을 매크로 액션으로 병합하여 의사 결정 지점을 줄이고 크레딧 할당 문제를 완화합니다.
- **Performance Highlights**: 실험 결과, MA-RLHF는 기존의 토큰 수준 RLHF와 비교하여 보상 점수가 1.7배에서 2배 더 빠르게 수렴하며, 추가적인 계산 비용 없이 향상된 결과를 보였습니다. MA-RLHF는 2B에서 27B 파라미터에 이르는 모델 크기에 걸쳐 강력한 확장성을 나타냈으며, 다양한 실험 설정에서도 우수한 일반화 능력을 보였습니다.

### [ControlAR: Controllable Image Generation with Autoregressive Models](https://arxiv.org/abs/2410.02705)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02705.png)

Vote: 4

Authors: Xiaoxin Chen, Peize Sun, Shoufa Chen, Haocheng Shen, Longjin Ran, Zongming Li, Wenyu Liu, Xinggang Wang, Tianheng Cheng

- **What's New**: 새로운 연구는 이미지 생성 분야에서의 autoregressive 모델의 제어 가능성을 탐구합니다. ControlAR이라는 새로운 프레임워크를 제안하여 autoregressive 모델에 정밀한 제어를 추가하고 고품질 이미지를 생성할 수 있게 합니다.
- **Technical Details**: ControlAR은 control 노드를 시퀀스로 변환하는 control encoder를 사용하며, 제시된 conditional decoding을 통해 이전 이미지 토큰과 현재 control 토큰을 기반으로 새로운 이미지를 예측합니다. 이 접근 방식은 비전 트랜스포머(ViT)를 encoder로 사용하여 control 이미지를 시퀀스로 인코딩하고, 멀티 레졸루션(Multi-Resolution) 가능성을 추가합니다.
- **Performance Highlights**: ControlAR은 기존 최첨단 diffusion 모델과 비교하여 다양한 제어 가능한 이미지 생성에서 높은 성능을 보여줍니다. 특히, 이미지 해상도를 제어하는 능력을 통해 고정된 해상도가 아닌 임의의 해상도로 이미지를 생성할 수 있습니다. 제안된 multi-resolution ControlAR는 멀티 스케일 훈련을 통해 다른 해상도의 이미지 품질을 향상시킵니다.

### [Hyper-multi-step: The Truth Behind Difficult Long-context Tasks](https://arxiv.org/abs/2410.04422)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04422.png)

Vote: 4

Authors: Yijiong Yu

- **What's New**: 최근 긴 문맥 처리 언어 모델(long-context language models, LCLMs)이 주목받고 있으며, 특히 GPT-4o-128k와 Gemini-1.5-1000k 같은 모델들이 주목받고 있습니다. 하지만 이러한 모델들이 복잡한 작업에서 여전히 한계를 보이고 있음을 이번 연구에서는 강조하고 있습니다. 특히 '다중 매칭 검색(multi-matching retrieval)'과 '논리 기반 검색(logic-based retrieval)'이 긴 문맥 작업에서 주요한 어려움의 원천으로 식별되었습니다.
- **Technical Details**: 연구에서는 기존 LCLMs의 성능을 평가하기 위해 두 개의 합성 데이터셋을 만들어 실험을 수행했습니다. '키-값 쌍(Key-Value Pair) 검색'과 '학생 이력서 검색(Student Resume Retrieval)'을 통해 검증했으며, 실험 결과 현재 LCLMs로는 이러한 검색 작업이 매우 어렵다는 것을 확인했습니다. 이와 반대로 일반적인 검색 또는 공식적인 다중 단계 검색은 상대적으로 더 쉬운 것으로 나타났습니다.
- **Performance Highlights**: 다중 매칭 검색과 논리 기반 검색 작업에서 LCLMs가 적절히 성능을 발휘하지 못하는 것으로 평가되었습니다. 특히 '하이퍼-다중 단계(hyper-multi-step)'로 정의된 문제가 LCLMs에게는 중대한 도전 과제임을 보여주었으며, 이는 문제의 분할이 가능해 보이지만 실제로는 여러 독립적 단계를 필요로 하는 문제입니다. 이러한 문제는 기존 기술들, 예를 들어 Retrieval-Augmented Generation(RAG), Chain-of-Thought(CoT)과 LCLMs으로 해결되지 않았습니다.

### [TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention](https://arxiv.org/abs/2410.05076)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05076.png)

Vote: 2

Authors: Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zhihao Jia, Zikun Li

- **What's New**: TidalDecode라는 새로운 알고리즘 및 시스템이 도입되어 있습니다. 이 시스템은 긴 문맥을 가진 LLM(Long Language Models)의 효율적이고 빠른 디코딩을 가능케 하는 position persistent sparse attention(PPSA)을 활용합니다.
- **Technical Details**: TidalDecode는 선택 기반 드문(attention)을 이용하여 각 단계에서 주목할 만한 점수의 토큰을 선택하는 기존 방법의 단점을 개선했습니다. 각 변환기 레이어에서 일관되게 같은 토큰들을 선택하여 토큰 선택의 오버헤드를 줄이고, 계산 및 메모리 소비를 감소시킵니다. PPSA에 특화된 GPU 커널과 시스템을 개발하여, 전체 인코딩 대기 시간을 2.1배까지 줄였습니다.
- **Performance Highlights**: TidalDecode는 기존의 드문(attention) 방법보다 성능과 효율성의 트레이드오프에서 우수한 성과를 보였습니다. Needle-in-the-Haystack, PG-19 및 LongBench 과제에 대해 LongChat-7b-v1.5-32k 및 Llama-3 모델 시리즈로 평가하여 높은 수치의 성능 향상을 확인했습니다.

### [EBES: Easy Benchmarking for Event Sequences](https://arxiv.org/abs/2410.03399)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03399.png)

Vote: 2

Authors: Igor Udovichenko, Egor Shvetsov, Dmitry Osin, Viktor Moskvoretskii, Evgeny Burnaev

- **What's New**: 이 논문에서는 EBES라는 새로운 벤치마킹 프레임워크를 소개하며, 이는 Event Sequences(EvS) 평가를 위한 포괄적인 시스템입니다. EBES는 데이터셋, 모델, 실험 프로토콜에 대해 통일된 인터페이스를 제공하여, EvS 평가 연구를 촉진하고 연구자들이 쉽게 접근할 수 있도록 합니다.
- **Technical Details**: EBES는 다양한 시나리오를 고려한 벤치마크 프로토콜을 설계하여 데이터셋과 모델의 중요한 속성을 강조합니다. 주요 기술적 접근 방법으로는 데이터 전처리를 최소화하여 원본 데이터를 최대한 보존하는 것이 있습니다. 이는 데이터 전처리가 모델 평가에 영향을 미치는 것을 방지하기 위한 것입니다. 또한 다양한 알고리즘 간의 공정한 비교를 위해 데이터를 단일 형식으로 변환하는 스크립트를 제공하고, Monte Carlo cross-validation을 통해 모델을 평가합니다.
- **Performance Highlights**: 본 연구는 다양한 방법을 기존의 데이터셋에서 다단계 평가 프로토콜을 통해 평가합니다. 이 접근 방식은 다양한 방법 간의 공정하고 일관된 비교를 보장하며, 통계적으로 유의미한 결과를 제공하는 것을 목표로 합니다. 결과를 바탕으로 데이터셋 사용 및 모델 평가 관련 가능성 있는 문제점을 포함한 미래 연구를 위한 권고사항을 제공합니다.
- **Benchmark Goals and Approaches**: 벤치마크의 주된 목표는 고품질 데이터셋을 사용하고, 기계 학습 모델의 강점을 정확하게 반영할 수 있는 평가를 설계하는 것입니다. EvS 모델링의 다양한 문제를 해결하기 위해 다섯 가지의 다양한 데이터셋을 선정하였고, 합성 펜듈럼 데이터셋은 시간 민감성 평가에 특히 유용합니다. 데이터셋의 다양성을 보장하고, 모델의 성능과 데이터의 상관관계를 분석하는 것이 목표입니다.
- **Preprocessing and Evaluation**: HPO(하이퍼 파라미터 최적화)와 Monte Carlo cross-validation은 벤치마크 설계의 핵심입니다. 이들은 다양한 설계와 하이퍼 파라미터를 평가하고 모델을 공정하게 비교할 수 있도록 합니다. 최종 모델 평가는 20회 실행하여 평균 및 표준 편차를 보고하며, 이는 모델이 처음부터 훈련된 후 테스트 세트에 대해 수행됩니다.

### [$ε$-VAE: Denoising as Visual Decoding](https://arxiv.org/abs/2410.04081)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04081.png)

Vote: -

Authors: Ziyu Wan, Han Zhang, Sanghyun Woo, Hartwig Adam, Yandong Li, Xuhui Jia, Long Zhao, Ting Liu, Boqing Gong

- **What's New**: 이 연구에서는 generative model(생성모델)의 최근 발전을 바탕으로 새로운 기술이나 이론을 제안하고 있습니다. 특히, 기존의 딥러닝(deep learning) 알고리즘들이 가지는 한계를 극복하기 위한 방법을 설명하고 있습니다.
- **Technical Details**: 논문은 transformer(트랜스포머) 구조를 개선하거나 하이퍼파라미터(hyperparameter) 튜닝(tuning) 같은 최적화 기법을 사용하여 성능을 개선했습니다. 또한, 데이터 전처리(pre-processing)와 모델의 추론 단계에 대한 혁신적인 접근을 소개합니다.
- **Performance Highlights**: 제안된 방법은 benchmark 데이터셋에서 기존의 방법들보다 월등한 성능을 보여줍니다. 특히, 특정 task에서의 accuracy(정확도) 및 efficiency(효율성)가 크게 향상되었습니다.

