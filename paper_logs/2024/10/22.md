## Daily Papers (2024-10-22)

### [Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers](https://arxiv.org/abs/2410.13184)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13184.png)

Vote: 1

Authors: ['Ang Li', 'Tao Ge', 'Bowei Tian', 'Shwai He', 'Guoheng Sun', 'Dong Yu', 'Xiaoyang Wang']

- ***What's New***: Router-Tuning은 Transformer에서 Dynamic-Depth를 가능하게 하는 새로운 단순하지만 효과적인 방법입니다. 이 연구는 전체 모델을 초기화하지 않고 Router만 미세 조정(fine-tuning)하여 정확도를 유지하면서 훈련 비용과 시간을 대폭 줄이는 데 중점을 두고 있습니다.
- ***Technical Details***: Router-Tuning은 가벼운 Router 네트워크를 미세 조정하는 접근 방식으로, 전체 모델을 업데이트하지 않고 동적 깊이를 구현합니다. 이 과정에서 Attention 레이어에 MindSkip을 적용하여 불필요한 레이어들을 선택적으로 건너뛰고, 필요한 곳에서는 컴퓨터 자원을 집중적으로 사용하여 효율성을 높입니다. Router는 입력의 중요도를 평가하는 점수 계산에만 관여하여 훈련 부담을 줄입니다.
- ***Performance Highlights***: Router-Tuning은 TPU A6000에서 실행 시 DLO보다 1000배 빠르며, 모델의 원래 성능을 약 99.8% 유지하면서 21%의 추론 속도 향상을 달성하였습니다. MindSkip을 통해 KV 캐시 크기도 크게 줄일 수 있으며, 인퍼런스 과정 전체에 걸쳐 메모리 사용량을 줄이는 데 기여합니다.

### [Zero-shot Model-based Reinforcement Learning using Large Language Models](https://arxiv.org/abs/2410.11711)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.11711.png)

Vote: 6

Authors: ['Abdelhakim Benechehab', 'Giuseppe Paolo', 'Youssef Attia El Hili', 'Balázs Kégl', 'Ievgen Redko', 'Maurizio Filippone', 'Ambroise Odonnat', 'Albert Thomas', 'Oussama Zekri']

- ***What's New***: 이 논문에서는 대형 언어 모델(Large Language Models; LLMs)의 제로샷 역량을 모델 기반 강화 학습(Model-based Reinforcement Learning; MBRL)에 적용하는 방법을 탐색합니다. 이를 위해 연속 상태 공간에서 동적 스마트 예측을 개선하기 위한 초기 개념 증명(proof-of-concept)과 이론적 분석을 제시합니다.
- ***Technical Details***: 논문에서는 다차원 데이터를 다루고 제어 신호를 통합하는 것이 LLMs를 이 환경에 배치하는 데 방해 요소임을 인지하고, 이를 해결하기 위해 Disentangled In-Context Learning (DICL)을 제안합니다. 이는 상태의 상호 의존성과 행동 정보를 인컨텍스트(In-Context) 궤적에 통합하여 연속 상태 공간의 강화 학습 환경에 ICL을 적용할 수 있는 새로운 방법론을 제시합니다.
- ***Performance Highlights***: 실험 결과는 제안된 방법이 샘플 효율성을 향상시키며, 불확실성 추정에 있어서도 잘 보정된 결과를 보였음을 강조합니다. 다양한 RL 환경에서 정책 평가와 오프라인 강화 학습에 있어서 의미 있는 성능 개선을 이루었습니다.

### [CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution](https://arxiv.org/abs/2410.16256)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16256.png)

Vote: 51

Authors: ['Haodong Duan', 'Songyang Zhang', 'Hongwei Liu', 'Alexander Lam', 'Kai Chen', 'Maosong Cao']

- ***What's New***: CompassJudger-1은 대형 언어 모델(LLMs)의 평가 및 발전을 돕는 최초의 오픈소스 통합 심판 모델(All-in-one Judge Model)입니다. 이 모델은 주어진 형식에 따른 평가, 비판 생성, 두 모델 간의 비교 등 다양한 평가 과제를 수행할 수 있습니다. 또한, CompassJudger-1은 JudgerBench라는 새로운 벤치마크를 통해 다양한 주관적 평가 태스크를 통합한 포괄적 평가를 제공합니다.
- ***Technical Details***: CompassJudger-1의 훈련 데이터는 공개된 심판 데이터, 자가 수집 평가 데이터, 보상 모델 수집 데이터 등 세 가지로 구성됩니다. 훈련 과정에서는 적절한 샘플링 전략이 사용되어 균형 있는 데이터 수집을 추구합니다. 특히, 전략적으로 생성적 평가 데이터를 확장하여 모델의 비판 능력을 강화합니다. 최적의 훈련 데이터 비율은 비판 데이터:보상 데이터:일반 SFT 데이터=1:3:1로 설정되었습니다.
- ***Performance Highlights***: CompassJudger 시리즈는 JudgerBench에서 GPT-4o와 비교하여 95% 이상의 유사성을 보이며, 여러 주관적 데이터세트에서 일관된 평가 성능을 발휘합니다. 특히, 수학, 추론, 코드 평가에서 더 높은 평가를 받으며, 이로 인해 모델의 판단 성능 향상에 긍정적인 영향을 미친 것으로 나타났습니다.

### [AutoTrain: No-code training for state-of-the-art models](https://arxiv.org/abs/2410.15735)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.15735.png)

Vote: 39

Authors: ['Abhishek Thakur']

- ***What's New***: AutoTrain은 개방형, 코드 없음(no-code) 도구/라이브러리로 다양한 작업에 대한 모델 훈련(또는 미세 조정)을 지원하는 솔루션입니다. Text Classification, Sequence-to-Sequence Task, Large Language Model(LLM) 및 Visual Language Model(VLM) 미세 조정은 물론 이미지 분류 및 회귀, 표형 데이터(Classification/Regression) 등의 작업도 가능하게 해줍니다.
- ***Technical Details***: AutoTrain Advanced 라이브러리는 커맨드 라인 인터페이스(CLI), 그래픽 사용자 인터페이스(GUI/UI), 파이썬 SDK를 제공하여 사용자들이 커스텀 데이터셋으로 모델을 훈련할 수 있게 합니다. 다양한 형식의 데이터셋을 업로드/사용할 수 있으며, 자세한 문서와 하이퍼파라미터 예시, 사용 사례가 제공됩니다. 주된 프레임워크로 PyTorch와 함께 트레이닝 작업을 이루며, Library는 Apache 2.0 라이선스를 통해 Github에서 제공됩니다.
- ***Performance Highlights***: AutoTrain은 사용자 선택에 따라 수십 만 개의 모델을 다룰 수 있으며, Hugging Face Transformers 라이브러리와의 호환성을 자랑합니다. 사용자는 프로젝트 구성(Project Configuration), 데이터셋 처리(Dataset Processor), 트레이너(Trainer) 같은 구성 요소를 통해, 실제 훈련을 관리하고 다양한 GPU에서의 분산 훈련을 지원합니다. 이로 인해 비전문가도 고성능 모델을 효율적으로 빌드할 수 있습니다.

### [Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation](https://arxiv.org/abs/2410.15748)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.15748.png)

Vote: 10

Authors: ['Shaonan Wu', 'Shuai Lu', 'Ping Wei', 'Nan Duan', 'Yeyun Gong']

- ***What's New***: Alchemy는 대형 언어 모델(LLM)을 통해 정리 증명(Theorem-Proving) 능력을 증진하기 위해 기호 변화(Symbolic Mutation)를 사용한 데이터 합성 프레임워크를 제안했습니다. Mathlib에서 110,000개의 정리를 약 6백만 개로 증대시키며, 새롭게 생성된 기호 정리를 통해 계속적인 사전학습과 지도학습 미세조정을 수행하여 정리 증명 성능을 향상시켰습니다.
- ***Technical Details***: Alchemy는 Lean에서 제공하는 기호 공간(Space)에서 직접 정리를 합성하며, 이를 일반 프로그래밍 언어의 함수 생성에 비유합니다. Mathlib4를 기본 데이터로 사용하고 Lean의 전술들을 변형 수단으로 활용하여 새로운 정리 문장을 구성합니다. 새로운 정리는 기존 정리 및 전술을 조합하여 증명 프로세스를 마무리하며 이를 Lean 증명 도구를 사용해 검증합니다.
- ***Performance Highlights***: 약 5%의 성능 향상을 Lean-dojo 벤치마크에서, 그리고 약 2.5%의 성능 향상을 miniF2F 경쟁 수준 벤치마크에서 이루었습니다. 이는 새로운 정리들이 LLM의 정리 증명 능력을 향상시키는데 효과적임을 시사하며, 생성된 데이터와 관련 코드도 오픈 소스로 제공됩니다.

### [SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree](https://arxiv.org/abs/2410.16268)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16268.png)

Vote: 48

Authors: ['Yuhang Zang', 'Xiaoyi Dong', 'Jiaqi Wang', 'Shuangrui Ding', 'Rui Qian', 'Dahua Lin', 'Yuhang Cao', 'Pan Zhang', 'Yuwei Guo']

- ***What's New***: SAM2Long은 기존의 SAM 2의 비디오 세분화 기능을 개선하여 트레이닝 없이 메모리 트리(memory tree) 구조를 도입함으로써 복잡하고 긴 비디오에서의 객체 세분화를 더욱 효과적으로 수행하는 방법을 제안합니다. 이는 각각의 프레임에서 세분화의 불확실성을 고려하여 다중 경로의 세분화를 통해 최적의 결과를 선택하는 방식입니다.
- ***Technical Details***: SAM2Long은 여러 메모리 경로를 유지하며 각 프레임에서 제시되는 다양한 마스크 후보 중 높은 누적 점수를 가진 경로를 선택합니다. 메모리 트리 구조를 활용하여 다양한 세분화 경로를 탐색하며, 각 경로의 메모리 은행과 누적 점수를 기반으로 후보지를 생성합니다. 불확실한 상황에서는 다양한 마스크 예측을 유지하여 오류 누적을 방지하는 전략을 사용합니다.
- ***Performance Highlights***: SAM2Long은 5개의 VOS 벤치마크에서 SAM 2보다 평균적으로 3.0점 향상된 성능을 나타내며, SA-V 및 LVOS와 같은 장기 비디오 세분화 벤치마크에서 최대 5.3점까지 개선되었습니다. SAM2Long은 여분의 파라미터나 추가적인 학습 없이도 이러한 성능 향상을 달성합니다.

### [SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation](https://arxiv.org/abs/2410.14745)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14745.png)

Vote: 36

Authors: ['Zhiping Xiao', 'Ming Zhang', 'Xiao Luo', 'Wei Ju', 'Junyu Luo', 'Xiusi Chen']

- ***What's New***: 이 논문에서는 대형 언어 모델(LLM; Large Language Models)의 적응을 위한 반지도 학습 미세조정 프레임워크를 제안합니다. SEMIEVOL이라는 새로운 프레임워크는 레이블이 있는 데이터와 없는 데이터를 모두 활용하여 모델 성능을 향상시키는 데 중점을 둡니다.
- ***Technical Details***: SEMIEVOL은 지식 전파와 선택을 위한 이중 수준 접근 방식을 채택합니다. 레이블이 있는 데이터를 활용해 LLM이 레이블이 없는 데이터에 대한 성능을 향상시킵니다. 지식 선택을 위해 협업 학습 메커니즘을 도입하여 더 높은 품질의 의사 응답(pseudo-response)을 선택합니다. 다양한 LLMs를 사용하여 서로 다른 설정으로 레이블이 없는 데이터에 대해 추론하고, 응답의 불확실성을 기반으로 레이블이 없는 데이터를 선택합니다.
- ***Performance Highlights***: SEMIEVOL은 GPT-4o-mini와 Llama-3.1을 포함한 여러 모델에서 실험이 진행되었습니다. MMLU, MMLU-Pro, ARC 등 7개의 일반 및 도메인 특화 데이터셋에 대해 실험한 결과, SEMIEVOL은 LLM의 성능을 일관되게 향상시키는 것으로 나타났습니다. 또한, 레이블이 있는 데이터의 부족과 많은 레이블이 없는 데이터가 있는 하이브리드 데이터 시나리오에서의 실용성을 강조했습니다.

### [PUMA: Empowering Unified MLLM with Multi-granular Visual Generation](https://arxiv.org/abs/2410.13861)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13861.png)

Vote: 41

Authors: ['Kun Wang', 'Chengqi Duan', 'Xihui Liu', 'Hongsheng Li', 'Hao Li', 'Hao Tian', 'Jifeng Dai', 'Xingyu Zeng', 'Rongyao Fang', 'Rui Zhao']

- ***What's New***: PUMA는 멀티 모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)을 멀티 그래뉴럴 시각적 생성 기능으로 강화하는 새로운 패러다임을 제안합니다. PUMA는 다양한 시각적 과제를 처리하기 위해 통합된 MLLM 프레임워크 내에서 여러 수준의 시각적 기능을 입력 및 출력으로 사용하여 그래뉴럴 요구 사항을 조화롭게 해결합니다.
- ***Technical Details***: PUMA는 세 가지 주요 모듈로 구성됩니다: 멀티 그래뉴럴 이미지를 추출하는 이미지 인코더, 멀티 스케일 이미지 기능을 점진적으로 생성하는 자동 회귀 MLLM, 그리고 여러 그래뉴레벨에서 MLLM 생성 기능을 디코딩하는 디퓨전 기반 이미지 디코더를 포함합니다. 두 단계로 구성된 학습 전략을 사용하여 사전 학습된 디퓨전 모델을 이미지 디코더로 세밀하게 조정하고, 멀티 스케일 인코더 기능을 감독하는 회귀 손실로 자동 회귀 MLLM을 훈련합니다.
- ***Performance Highlights***: PUMA의 f0 스케일 기능을 사용한 이미지 복원 성능은 PSNRr 18.16과 LPIPSr 0.2215로, 이는 SEED-LLaMA, SEED-X 및 Emu2와 대비되는 우수한 성능을 보여줍니다. 또한, 멀티 그래뉴럴 기능을 통해 다양한 텍스트-이미지 생성 및 세밀한 제어가 필요한 이미지 편집 작업에서 높은 성과를 나타냈습니다. 여러 테스트에서 PUMA의 성능은 최신 기법을 초과하거나 이에 필적했습니다.

### [Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs](https://arxiv.org/abs/2410.13394)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13394.png)

Vote: 1

Authors: ['Dilip Venkatesh', 'Anoop Kunchukuttan', 'Mohammed Safi Ur Rahman Khan', 'Mitesh M. Khapra', 'Raj Dabre', 'Sumanth Doddapaneni']

- ***What's New***: 이 논문은 다국어 LLMs의 평가를 위한 Cross-Lingual Auto Evaluation (CIA) Suite를 소개하고 있습니다. 수천 개의 언어로 된 다양한 작업에 대해 평가할 수 있는 광범위한 벤치마크 및 평가자 LLMs를 개발하였습니다.
- ***Technical Details***: CIA Suite는 다국어 평가를 위한 확장 가능한 프레임워크으로, HERCULE라는 교차 언어 평가 모델과 6개 언어로 된 500개의 인간 주석 지침을 포함하는 새로운 RECON 테스트 세트로 구성되어 있습니다. HERCULE 모델은 영어로 쉽게 참조 가능한 답변을 기반으로 응답에 점수를 할당하는 학습을 합니다.
- ***Performance Highlights***: HERCULE 모델은 독점 모델보다 인간 판단과 더욱 긴밀히 일치하며, 낮은 자원 시나리오와 보지 못한 언어에 대한 제로샷 평가에서도 효과적임이 실험을 통해 증명되었습니다. 다양한 언어에서 fine-tuning을 거친 모델이 대형 LLMs를 능가하는 성능을 보였습니다.

### [Pre-training Distillation for Large Language Models: A Design Space Exploration](https://arxiv.org/abs/2410.16215)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16215.png)

Vote: 12

Authors: ['Xin Lv', 'Hao Peng', 'Yushi Bai', 'Jiajie Zhang', 'Lei Hou', 'Juanzi Li', 'Zijun Yao']

- ***What's New***: 본 논문은 대형 언어 모델(Large Language Models; LLMs)의 사전 훈련 단계에서 지식 증류(Knowledge Distillation; KD)를 확장하여 사전 훈련 증류(Pre-training Distillation; PD)를 제안합니다. 이는 기존의 사후 훈련 단계에서의 KD와 달리 사전 훈련 단계에서 로그잇(logits) 기반의 KD를 적용하여 학생 모델의 훈련 효율성을 높이고 성능을 향상시키는 접근 방식입니다.
- ***Technical Details***: 논문에서는 사전 훈련 증류의 설계 공간을 4가지 측면에서 체계적으로 탐색하였습니다: (1) 로그잇 처리, (2) 손실 함수 선택, (3) 확장 법칙(Scaling Law), (4) 오프라인 혹은 온라인 로그잇. 각 측면은 로그잇 저장 공간 최소화와 학습 효율성을 높이기 위한 다양한 실험을 포함합니다. 이를 통해 최적 구성과 흥미로운 결론, 예를 들어 학생 모델이 클수록 사전 훈련 증류 이점이 더 크다는 점을 발견하였습니다.
- ***Performance Highlights***: 사전 훈련 증류 기법으로 학습된 1.9B와 3.8B 파라미터 학생 모델은, LM 손실만으로 훈련된 모델에 비해 평균 1.6%의 성능 향상을 나타냈습니다. 또한 더 많은 토큰 수로 학습할수록 증류 기법의 성능 개선이 일관되게 유지됩니다. 오프라인 로그잇을 사용한 경우의 성능이 최적이었으나, 적절한 설정에서 온라인 로그잇 역시 약간의 성능 향상을 보여주었습니다.

### [Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement](https://arxiv.org/abs/2410.15633)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.15633.png)

Vote: 6

Authors: ['Yunshui Li', 'Shuzheng Si', 'Kangyang Luo', 'Baobao Chang', 'Haozhe Zhao', 'Fanchao Qi', 'Maosong Sun', 'Chuancheng Lv', 'Gang Chen', 'Kaikai An']

- ***What's New***: GATEAU라는 신규 프레임워크가 제안되었습니다. 이 프레임워크는 Homologous Models’ Guidance (HMG)와 Contextual Awareness Measurement (CAM)를 활용하여 긴 문맥을 효과적으로 처리할 수 있도록 하는 고품질 데이터를 선택합니다. 특히 긴 문맥 간의 의존성을 모델링하여 LLMs의 성능을 향상시키는 데 중점을 둡니다.
- ***Technical Details***: HMG는 서로 다른 문맥 창을 가진 두 개의 homologous 모델의 복잡도 점수 차이를 비교하여 긴 의존성으로 인한 응답 생성의 난이도를 측정합니다. CAM은 LLM이 긴 입력의 중요한 부분에 집중하고 있는지를 평가하여 긴 입력 문맥을 이해하는 데 있어서의 어려움을 측정합니다.
- ***Performance Highlights***: GATEAU를 통해 선택된 데이터로 훈련된 모델은 LongBench 및 MT-Bench와 같은 벤치마크 테스트에서 일반적인 데이터 세트보다 최대 9% 더 높은 성능을 보였습니다. 이 실험 결과는 GATEAU가 긴 문맥 처리와 복잡한 지시를 따르는 데 얼마나 효과적인지를 보여줍니다.

### [Baichuan Alignment Technical Report](https://arxiv.org/abs/2410.14940)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14940.png)

Vote: 32

Authors: ['Yadong Li', 'Guosheng Dong', 'Yanjun Shen', 'Xu Li', 'Weipeng Chen', 'Yijie Zhou', 'Hao Liang', 'Wentao Zhang', 'Jianhua Xu', 'Haoze Sun', 'Fei Li', 'Miao Zheng', 'Yanzhao Qin', 'Chenzheng Zhu', 'Kun Fang', 'Youquan Li', 'Mingyang Chen', 'Tianpeng Li', 'Mingan Lin', 'Tao Zhang', 'Mang Wang', 'Bin Cui', 'Zenan Zhou', 'Fan Yang']

- ***What's New***: Baichuan Alignment은 Baichuan 시리즈 모델에 사용되는 정렬(Alignment) 기술을 처음으로 상세히 분석한 보고서입니다. 이 연구는 AI 연구 발전에 기여할 수 있는 귀중한 통찰력을 제공합니다. Baichuan-Instruct 모델은 사용자 경험에서 17%에서 28%까지 개선된 성능을 보여주며, Qwen2-Nova-72B와 Llama3-PBM-Nova-70B 모델은 기존 공식 버전에 비해 거의 모든 데이터셋에서 뛰어난 성능을 기록했습니다.
- ***Technical Details***: Baichuan Alignment는 세 가지 주요 단계로 구성됩니다: Prompt Augmentation System (PAS), Supervised Fine-Tuning (SFT), Preference Alignment입니다. 이 과정은 최적화 기법, 데이터 전략, 능력 강화와 평가 프로세스 등을 포함하고 있습니다. PAS 단계에서는 자동화된 프롬프트 엔지니어링(Prompt Engineering)을 통해 LLMs가 사용자 의도를 더 잘 이해하고 행동하도록 합니다. SFT 단계는 대화와 복잡한 작업을 처리할 수 있도록 다양한 고품질 데이터를 이용한 학습을 지원하며, Preference Alignment는 모델을 인간의 가치와 선호도에 맞추는 역할을 합니다.
- ***Performance Highlights***: Baichuan-Instruct 모델은 핵심 능력 평가에서 17%에서 28% 사이의 사용자 경험 향상을 보였습니다. Qwen2-Nova-72B 모델은 ArenaHard 벤치마크에서 성능을 48.1에서 75.1로 크게 향상시켰습니다. Llama3-PBM-Nova-70B도 Llama-3-70B-Instruct 모델에 비해 여러 벤치마크에서 탁월한 성능을 기록했습니다. Baichuan Alignment 기술은 향후 다양한 기초 모델에서의 성능 개선에 기여할 수 있음을 보여줍니다.

### [Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception](https://arxiv.org/abs/2410.12788)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.12788.png)

Vote: 15

Authors: ['Zhiyuan Ji', 'Zhiyu Li', 'Pengnian Qi', 'Jihao Zhao', 'Simin Niu', 'Bo Tang', 'Feiyu Xiong']

- ***What's New***: 본 논문은 Meta-Chunking이라는 새로운 개념을 도입하여, 문장과 문단 사이의 논리적 연관성을 기반으로 한 효율적인 텍스트 세분화 방법을 제안합니다. Retrieval-Augmented Generation(RAG)의 성능을 개선하기 위해 기존의 유사성 기반 방법의 한계를 극복하고 두 가지 전략, Margin Sampling Chunking과 Perplexity(혼란도, PPL) Chunking,을 통해 텍스트의 논리적 완결성을 보장하려고 합니다.
- ***Technical Details***: Meta-Chunking은 문단 내 문장들이 깊은 언어적 논리적 연관성을 갖는 집합체로 구성되어 있으며, PPL Chunking에서는 각 문장의 PPL을 계산하고 그의 분포 특성을 기반으로 텍스트 세분화를 수행합니다. Margin Sampling Chunking은 연속된 문장이 분할되어야 할지를 2진 분류의 확률 차이로 판단합니다. 또한, 다양한 텍스트의 복잡성을 고려해 동적 병합을 통해 텍스트 세분화의 정밀성과 조잡성의 균형을 맞추는 전략도 제안되었습니다.
- ***Performance Highlights***: Meta-Chunking은 전통적인 규칙 기반 및 유사성 세분화 방법과 비교하여 RAG 기반의 단일 홉 및 다중 홉 질문 응답 성능을 효율적으로 개선하였습니다. 예를 들어, 2WikiMultihopQA 데이터셋에서는 유사성 세분화에 비해 1.32 포인트 성능 개선을 기록하면서도 45.8%만의 처리 시간을 소비했습니다. 이는 최신 LLMs(Large Language Models)을 사용한 접근법에 비해 효율성과 비용면에서 우수한 성능을 보여줍니다.

### [In-context learning and Occam's razor](https://arxiv.org/abs/2410.14086)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.14086.png)

Vote: 1

Authors: ['Eric Elmoznino', 'Mahan Fathi', 'Leo Gagnon', 'Guillaume Lajoie', 'Sarthak Mittal', 'Tejas Kasetty', 'Tom Marty', 'Dhanya Sridhar']

- ***What's New***: 이 논문은 in-context learning(ICL)과 Occam's razor의 이론적 연결을 제시합니다. ICL은 문맥에서 과거 관찰로부터 학습하는 능력으로, 간단한 모델을 선호하는 meta-learning 알고리즘으로 설명됩니다.
- ***Technical Details***: 논문에서는 Kolmogorov complexity와 prequential coding이라는 데이터 압축 기법을 중점적으로 다룹니다. 이 기법은 ICL의 다음 토큰 예측 손실과 직접적으로 연결되며, 이를 통해 ICL이 모델의 training error와 복잡성을 동시에 최소화할 수 있음을 보여줍니다.
- ***Performance Highlights***: 실험 결과에 따르면, ICL은 데이터가 적은 경우 일반화에서 특히 뛰어난 성능을 보입니다. 그러나 현재의 ICL 방법은 대량 데이터 환경에서는 underfitting 문제로 인해 성능이 제한될 수 있으며, 모델 아키텍처에 따라 ICL의 성능이 크게 달라질 수 있습니다.

### [DM-Codec: Distilling Multimodal Representations for Speech Tokenization](https://arxiv.org/abs/2410.15017)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.15017.png)

Vote: 1

Authors: ['Tasnim Mohiuddin', 'Aman Chadha', 'Amin Ahsan Ali', 'Tariq Iqbal', 'Md Mofijul Islam', 'Md Mubtasim Ahasan', 'M Ashraful Amin', 'Md Fahim', 'A K M Mahbubur Rahman']

- ***What's New***: DM-Codec는 다중 모달의 언어 및 음성 표현을 종합적으로 결합한 새로운 음성 토크나이저입니다. 특히, 언어 모델(Language Model; LM)과 자기지도 학습 기반의 음성 모델(Self-Supervised Learning Model; SM)을 결합하여 음성의 맥락적 정보를 포착할 수 있습니다.
- ***Technical Details***: DM-Codec는 Residual Vector Quantizer(RVQ)를 활용한 Streamlined Encoder-Decoder 구조를 채택하며, 훈련 과정에서 LM과 SM을 이용한 증류 방법을 도입합니다. 첫 번째 증류 방법은 LM을 이용하여 맥락적 정보를 통합하며, 두 번째는 LM과 SM의 정보를 결합하여 음향, 의미론적, 및 맥락 정보를 포괄적으로 증류합니다.
- ***Performance Highlights***: LibriSpeech 벤치마크 테스트에서 DM-Codec은 최첨단 최적의 음성 토크나이저와 비교해 단어 오류율(WER)을 최대 13.46%까지 감소시키고, 단어 정보 손실률(WIL)을 9.82%까지 줄이며, 말의 품질을 5.84%까지, 이해도를 1.85%까지 향상시켰습니다.

### [CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy](https://arxiv.org/abs/2410.13218)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.13218.png)

Vote: 3

Authors: ['Jamie C. Chiu', 'Travis Labrum', 'Fei Fang', 'Zhiyu Zoey Chen', 'Xinlu Zhang', 'William Yang Wang', 'Xianjun Yang', 'Mian Zhang', 'Shaun M. Eack']

- ***What's New***: CBT-BENCH는 전문 심리 치료를 지원하기 위해 대형 언어 모델(Large Language Models; LLMs)의 잠재력을 평가하기 위한 새로운 벤치마크입니다. 이 연구는 인지 행동 치료(Cognitive Behavioral Therapy; CBT)를 지원하는 AI의 가능성을 탐구하는 최초의 체계적인 평가로, 세 가지 수준의 과제로 구성되어 있습니다.
- ***Technical Details***: CBT-BENCH는 다음과 같은 세 가지 수준으로 구성됩니다: I) 기본 CBT 지식 습득, II) 환자의 사고 과정 이해, III) 치료적 반응 생성. 각각의 수준은 다양한 데이터셋을 사용하여 LLM의 능력을 평가하도록 설계되었습니다. 특히, 실제 CBT 세션 데이터를 수집하는 것이 어려운 점을 고려하여 Deliberate Practice 방법론을 활용하여 모델의 능력을 테스트합니다.
- ***Performance Highlights***: 실험 결과, LLM들은 CBT 지식 문제를 암송하는 데는 잘 수행하였으나, 환자의 인지 구조에 대한 심층 분석과 효과적인 응답 생성이 요구되는 복잡한 실제 시나리오에서는 부족함을 드러냈습니다. 특히, 현 LLM들은 실질적인 치료 대화에서 필요한 결정적 기술을 보완하는 데 한계를 보였습니다.

### [Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training](https://arxiv.org/abs/2410.15460)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.15460.png)

Vote: 1

Authors: ['Marco Bonizzato', 'Juan David Guerra', 'Golnoosh Farnadi', 'Shahrad Mohammadzadeh', 'Reihaneh Rabbany']

- **What's New**: 이 연구에서는 대형 언어 모델(Large Language Models; LLMs)의 훈련 과정에서 발생하는 환상(hallucinations)을 줄이기 위해 새로운 훈련 프로토콜인 민감 뉴런 드랍아웃(Sensitive Neuron Dropout; SeND)을 제안합니다. 또한, 기존의 EigenScore 계산을 빠르게 대체할 수 있는 효율적인 비지도 학습 환상 감지 지표인 Efficient EigenScore(EES)를 개발했습니다.
- **Technical Details**: SeND는 LLM 훈련 중 민감한 뉴런(Sensitive Neurons)을 결정적으로 제거하여 변동성을 줄이도록 설계된 프로토콜입니다. 이는 훈련 데이터에서 뉴런의 변동성을 분석하여 민감한 뉴런을 찾아내고, 이후의 훈련 주기에서 이들을 제거하여 모델의 사실적 확신을 높입니다. EES는 Chebyshev 다항식과 DOS(Density of States)를 사용하여 계산 복잡성을 줄이며, EigenScore와의 높은 상관관계를 유지합니다.
- **Performance Highlights**: 실험결과, SeND를 사용한 모델은 정상적인 훈련에 비해 최대 40% 향상된 사실적 정확성을 보였습니다. 이는 특히 Wikipedia와 Medical 데이터셋에 조정된 LLMs의 사실적 정확성을 향상시키는 효율적인 방법으로 평가되었습니다. SeND는 다양한 데이터셋과 모델 크기에서의 테스트에서도 안정적인 성능 향상을 보여주었습니다.

### [Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos](https://arxiv.org/abs/2410.16259)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16259.png)

Vote: 3

Authors: ['Gengshan Yang', 'Shunsuke Saito', 'Andrea Bajcsy', 'Angjoo Kanazawa']

- ***What's New***: Agent-to-Sim (ATS)은 일상적인 비디오에서 3D 에이전트의 상호작용 행동 모델을 학습할 수 있는 새로운 프레임워크를 제시합니다. 이는 여러 카메라나 마커 기반 트래킹 없이도 지속적인 3D 행동을 비디오 관찰을 통해 학습할 수 있도록 합니다.
- ***Technical Details***: ATS는 비디오 데이터를 통해 4차원 공간(4D)의 에이전트 행동 모델을 생성합니다. 이를 위해, DINOv2와 같은 대형 이미지 모델을 사용하여 카메라와 에이전트를 변환하며 정렬하는 새로운 'coarse-to-fine' 등록 방법을 도입합니다. 이런 데이터로부터 에이전트의 지각과 움직임을 학습하여 인터랙티브한 시뮬레이션을 가능하게 합니다.
- ***Performance Highlights***: ATS는 여러 개의 휴대폰 비디오로부터 4D 재구성을 통해 피드백 기반의 행동 생성, 즉 관찰자의 움직임에 반응하는 동물이나 인간의 행동을 생성하는 것을 보여줍니다. 이 방법은 TotalRecon에 비해 더 높은 품질의 씬 재구성을 제공하며, 카메라 위치 추정에서 더 낮은 오류율을 나타냅니다.

### [Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant](https://arxiv.org/abs/2410.15316)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.15316.png)

Vote: 7

Authors: ['Alan Dao', 'Huy Hoang Ha', 'Dinh Bach Vu']

- ***What's New***: 이치고(Ichigo)는 음성과 텍스트 콘텐츠가 교차 섞여 있는 시퀀스를 생성하고 추론할 수 있는 혼합 모달(Mixed-Modal) 모델로, 음성을 디스크리트(Discrete) 토큰으로 양자화하여 음성과 텍스트 모두에 대해 통일된 트랜스포머(Transformer) 아키텍처를 사용하는 것이 특징입니다. 기존의 분리형 어댑터(Adapters) 없이 다양한 모달리티 간 통합된 추론과 생성을 가능하게 합니다.
- ***Technical Details***: 이치고는 음성을 디스크리트 토큰으로 양자화해 '초기 융합(Tokenized Early Fusion)' 접근 방식을 채택했습니다. 멀티턴 상호작용, 추론 작업, 거부 시나리오가 포함된 대규모 데이터를 활용해 사전훈련과 세련화된 미세 조정을 통해 모델을 발전시킵니다. WhisperVQ를 활용하여 음성을 512개의 코드북 차원을 가진 디스크리트 토큰으로 표현합니다.
- ***Performance Highlights***: 이치고는 음성 질문 응답 벤치마크에서 기존 오픈소스 음성 언어 모델을 능가하며, 특정 조건 하에 캐스케이딩 시스템과 비슷한 성능을 보입니다. 특히 첫 토큰 생성까지의 지연 시간이 111밀리초로, 현재 모델보다 현저히 낮습니다. 이는 실시간 응용 프로그램에 적합한 성능을 제공합니다.

### [RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style](https://arxiv.org/abs/2410.16184)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16184.png)

Vote: 20

Authors: ['Yixin Cao', 'Yantao Liu', 'Rui Min', 'Lei Hou', 'Juanzi Li', 'Zijun Yao']

- ***What's New***: RM-Bench는 언어 모델의 보상 모델(Reward Models)을 정교하고 스타일적인 변화를 감지하도록 평가하는 새로운 벤치마크입니다. 이러한 보상 모델은 인간 피드백을 기반으로 한 강화 학습(Reinforcement Learning from Human Feedback; RLHF) 및 추론 확장 법칙(Inference Scaling Laws)에서 언어 모델의 정렬 및 최적 응답 선택을 안내합니다. RM-Bench는 기존 모델들이 스타일 편향에 저항하는 능력을 평가합니다.
- ***Technical Details***: RM-Bench는 같은 언어 모델인 gpt-4o로 생성된 선택된 응답과 거부된 응답을 사용하여 보상 모델의 민감도를 평가합니다. 스타일 편향에 대한 강인함을 평가하기 위해 다양한 스타일(간결, 자세한 설명, Markdown 형식)의 답변을 생성하여 변형을 만듭니다. 이를 통해 프로세스 최적화(Proximal Policy Optimization; PPO) 과정을 거친 후 정책 모델과 높은 상관관계를 보입니다.
- ***Performance Highlights***: 최근 40개의 보상 모델을 RM-Bench로 평가한 결과, 최첨단 모델조차도 스타일 편향이 있는 환경에서 무작위 수준(50%)보다 낮은 평균 성능 46.6%를 기록했습니다. 이는 현재 보상 모델의 개선 여지가 크다는 것을 나타냅니다. DPO(Direct Policy Optimization) 모델은 순차 분류 모델보다 더 나은 성과를 보여주어 보상 모델의 유망한 후보임을 시사했습니다.

### [Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages](https://arxiv.org/abs/2410.16153)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16153.png)

Vote: 27

Authors: ['Graham Neubig', 'Xiang Yue', 'Akari Asai', 'Sathyanarayanan Ramamoorthy', 'Anjali Kantharuban', 'Yueqi Song', 'Lintang Sutawika', 'Seungone Kim', 'Simran Khanuja', 'Jean de Dieu Nyandwi']

- ***What's New***: PANGEA는 39개 언어를 포함하는 완전한 오픈소스 다국어 대형 멀티모달 언어 모델(LLM)입니다. 특히 다양한 문화적 맥락을 포괄하는 데이터셋을 활용하여, 기존의 서구 중심의 데이터셋에 비해 언어와 문화의 다양성을 증대시킨 점이 특징입니다.
- ***Technical Details***: PANGEA는 PANGEAINS라는 600만 개 이상의 명령어 데이터셋을 기반으로 학습되었습니다. 이 데이터셋은 고품질의 영어 지시문과 신중하게 기계 번역된 명령어들, 그리고 문화적으로 관련된 멀티모달 작업들을 포함합니다. PANGEABENCH라는 평가 스위트를 도입하여 14개 데이터셋, 47개 언어에 걸쳐 모델의 성능을 측정하였습니다. 특히 xChat과 xMMMU라는 새로운 벤치마크가 도입되었습니다.
- ***Performance Highlights***: PANGEA는 기존 오픈 소스 모델들보다 다국어 및 문화적으로 다양한 상황에서 뛰어난 성능을 보였습니다. 특히 xChat과 CVQA 벤치마크에서 멀티모달 이해 능력이 탁월했으며, 영어 및 다국어 인식에서도 평균적으로 경쟁 모델들을 10.8포인트 이상 앞서는 성능을 보여주었습니다.

### [How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold](https://arxiv.org/abs/2410.15002)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.15002.png)

Vote: 1

Authors: ['Royi Rassin', 'Chirag Shah', 'Preethi Seshadri', 'Hannaneh Hajishirzi', 'Jeff Bilmes', 'Arnav Das', 'Sahil Verma', 'Yanai Elazar', 'Gantavya Bhatt']

- ***What's New***: 이 논문은 텍스트-이미지 모델(text-to-image models)이 훈련 데이터 세트에서 얼마나 많은 '반 고흐'를 필요로 하는지, 즉 모방 임곗값(imitation threshold)을 찾는 문제를 다룹니다. 주어진 개념의 이미지 수가 모델의 모방 능력에 미치는 영향을 연구하며, 이를 통해 모델이 개념을 모방할 수 있는 최소한의 이미지 수를 결정합니다.
- ***Technical Details***: 연구에서는 인간 얼굴과 예술 스타일 두 가지 도메인에서 총 4개의 데이터 세트를 이용하여 3개의 텍스트-이미지 모델을 평가합니다. 연구는 MIMETIC2라는 효율적인 방법을 제안하여 다양한 이미지 수로 훈련 데이터를 구성하지 않고도 모방 임계값을 추정합니다. 이를 위해, 사전 학습된 모델을 사용하여 각 개념에 대한 모방 점수를 계산합니다.
- ***Performance Highlights***: 이 논문의 실험 결과에 따르면 모델의 모방 임계값은 도메인과 모델에 따라 다르며, 대개 200에서 600 이미지 사이로 나타났습니다. 이는 텍스트-이미지 모델이 데이터 세트에서 최소 약 200번 이상 등장해야 해당 개념을 모방할 수 있음을 시사합니다.

### [FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors](https://arxiv.org/abs/2410.16271)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.16271.png)

Vote: 56

Authors: ['Cheng Sun', 'Chin-Yang Lin', 'Chang-Han Yeh', 'Yu-Lun Liu', 'Chung-Ho Wu', 'Shih-Han Yen']

- ***What's New***: FrugalNeRF는 사전 학습된 선행 정보(learned priors) 없이 소수 이미지로부터 새로운 뷰를 빠르게 합성할 수 있는 혁신적인 프레임워크입니다. 다양한 스케일에서 가중치를 공유하는 복셀(voxel)을 사용하여 신속하게 장면을 학습하며, 외부 학습 데이터존재하다는 상황에서도 품질을 향상시킬 수 있는 적응적인 설계를 가지고 있습니다.
- ***Technical Details***: FrugalNeRF는 교차 스케일 기하학적 적응(cross-scale geometric adaptation) 방법을 통해 여러 스케일에서 재투영 오류(reprojection errors)에 기초해 가칭적으로 정확한 깊이 정보를 선택하여 학습을 유도합니다. 이 방법은 사전 학습된 선행 정보 없이 신속하게 수렴할 수 있게 하는 효과적인 학습 메커니즘을 제공합니다. LLFF, DTU, RealEstate-10K와 같은 데이터셋에서 실험 시 기존의 방법들보다 더 나은 성능을 보이며 초기에 중요한 기하학적인 구조를 확보하고 이후에 더 세밀한 구조를 학습할 수 있습니다.
- ***Performance Highlights***: FrugalNeRF는 기존의 소수 샷 NeRF 방법들보다 짧은 시간 안에 더 나은 품질의 이미지를 생성합니다. LLFF 데이터셋의 경우, PSNR과 LPIPS 기준으로 상당한 성능 향상을 보여주며, 특히 학습 시간이 대폭 줄어든 것이 특징입니다. DTU 데이터셋에서도 탁월한 시각적 결과를 보여주며, 단순한 배경을 가진 장면을 효과적으로 처리할 수 있음을 입증했습니다.

