## Daily Papers (2024-10-08)

### [Differential Transformer](https://arxiv.org/abs/2410.05258)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05258.png)

Vote: 53

Authors: Yutao Sun, Gao Huang, Li Dong, Tianzhu Ye, Yuqing Xia, Yi Zhu, Furu Wei

- **What's New**: 최근 연구가 Transformer 구조에 많은 관심을 보이는 가운데, 본 논문에서는 Diff Transformer라는 새로운 아키텍처를 제안합니다. 이는 대규모 언어 모델(large language models, LLMs)에 대해 기본 아키텍처 역할을 수행하며, 기존 Transformer의 주의(attention) 메커니즘을 개선하는 차별화된(differential) 메커니즘을 도입합니다. 이 새로운 메커니즘은 불필요한 주의 점수를 제거하고, 진짜 정보에 집중하기 위한 것입니
- **Technical Details**: Diff Transformer는 기존 Transformer의 softmaxattention을 차별화된(differential) 주의 메커니즘으로 대체합니다. 기본 아이디어는 노이즈 제거 이어폰(noise-canceling headphones) 또는 차동 증폭기(differential amplifiers)와 유사하게 두 신호 간의 차이를 이용하여 일반적인 노이즈를 '취소' 하는 것입니다. 구체적으로는 쿼리와 키 벡터를 두 그룹으로 나누어 두 개의 별도 softmaxattention맥스를 구하고 이를 뺀 결과를 주의 점수로 사용합니다.
- **Performance Highlights**: Diff Transformer는 기존 Transformer에 비해 모델 크기 또는 훈련 토큰의 65% 정도만 필요로 하며, 비슷한 수준의 언어 모델링 성능을 달성합니다. 다양한 다운스트림 작업에서도 Transformer를 능가하며, 특히 긴 문맥을 효과적으로 활용하는 데 뛰어난 성능을 보입니다. 중요한 정보 검색과 맥락 학습에서도 주목할 만한 성과를 나타냈으며, 모델 활성화에서 외곡성을 줄여 양자화(quantization)를 위한 새로운 기회를 제공합니다.

### [LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations](https://arxiv.org/abs/2410.02707)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02707.png)

Vote: 28

Authors: Zorik Gekhman, Hadas Orgad, Michael Toker, Yonatan Belinkov, Hadas Kotek, Idan Szpektor, Roi Reichart

- **What's New**: 이 논문은 대형 언어 모델(LLM)이 내부적으로 '진실성' 신호를 어떻게 인코딩하는지를 탐구하며, 이러한 신호가 모델의 오류를 예측하고 완화하는 데 어떻게 활용될 수 있는지를 밝혀냅니다. 많은 연구들이 LLM이 생성하는 오류를 '환각(hallucination)'이라고 해석해 왔지만, 이 연구는 모델의 내부 표현에 집중하여 진실성과 관련된 다양한 정보를 예측할 수 있는 분류기를 훈련시켰습니다.
- **Technical Details**: 이 연구는 LLM의 내부 표현을 분석하여 진실성과 관련된 여러 특징을 예측하는 분류기를 개발했습니다. 특히 '정확한 답변 토큰'에서 진실성 정보를 추출하는 것이 오류 감지 전략을 개선하는 데 중요하다는 것을 발견했습니다. 훈련된 분류기는 모델의 오류를 예측할 수 있으며, 이는 LLM의 내부 표현이 외부 행동과 어떻게 연결되는지를 이해하는 데 도움이 됩니다.
- **Performance Highlights**: 이 연구는 정확한 답변 토큰에서의 진실성 정보가 기존에 알려진 것보다 훨씬 강력하게 인코딩된다는 것을 밝혀냈습니다. 그러나 이 분류기는 다른 작업들, 특히 기술이 다른 작업들 간에는 일반화되지 않았음을 보여주었습니다. 따라서 실용적인 애플리케이션에서 학습 가능한 오류 탐지기를 배포할 때 주의가 필요합니다. 또한 모델의 내부 인코딩과 외부 행동 간의 불일치를 발견함으로써 모델의 오류를 감소시키는 새로운 전략 가능성을 제시합니다.

### [VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide](https://arxiv.org/abs/2410.04364)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04364.png)

Vote: 22

Authors: Jong Chul Ye, Bryan S Kim, Dohun Lee, Geon Yeong Park

- **What's New**: 최신 연구에서는 Text-to-image (T2I) 디퓨전 모델(diffusion model)을 넘어 높은 차원의 비디오 생성으로 그 적용 범위를 확대하려고 합니다. 'VideoGuide'라는 새로운 프레임워크를 제안하며, 훈련 또는 미세 조정 없이 미리 훈련된 텍스트-비디오(T2V) 생성 모델의 시간적 일관성을 개선할 수 있습니다. VideoGuide는 미리 훈련된 비디오 디퓨전 모델(VDM)을 안내 역할로 사용하여 초기 역디퓨전 샘플링 전 단계에서 시간적 품질을 개선합니다.
- **Technical Details**: VideoGuide는 기존의 미리 훈련된 VDM을 유연하게 선택하여 이를 안내 모델로 삼습니다. 안내 모델은 몇 단계의 노이즈 제거 과정에서 일관된 비디오 경로를 제공합니다. 이러한 샘플의 정보는 초기 샘플링 단계에서 도입되어, 생성 과정 전반에 걸쳐 더 나은 시간적 품질로 이끌어줍니다. VideoGuide는 임의의 미리 훈련된 VDM을 활용할 수 있어, 훌륭한 성과를 내지 못했던 모델들의 성능을 업그레이드 할 수 있습니다. AnimateDiff와 LaVie 두 가지 사례에서 VideoGuide의 적용을 논의합니다. AnimateDiff에서는 동작 모듈을 훈련하여 텍스트-이미지(T2I) 모델에 삽입할 수 있고, LaVie 역시 여러 기능을 제공하는 다면적 T2V 모델입니다. 이들의 시간적 일관성을 VideoGuide로 외부 모델을 통해 개선할 수 있습니다.
- **Performance Highlights**: VideoGuide는 기존의 텍스트와 비디오 생성 모델이 별도로 지닌 특성을 조합하여 새로운 시너지를 발휘합니다. 안내 모델의 노이즈 제거 샘플 정보를 통하여, 이전까지 생성하기 어려웠던 이미지를 생성할 수 있는 노이즈 프라이어(noise prior)를 제공하게 됩니다. 전반적으로 시간적 일관성과 움직임의 부드러움을 향상시키는 데 성공하였습니다.

### [FAN: Fourier Analysis Networks](https://arxiv.org/abs/2410.02675)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02675.png)

Vote: 21

Authors: Yongding Tao, Yihong Dong, Kechi Zhang, Jun Zhang, Ge Li, Jia Li, Jingjing Xu, Jing Su, Xue Jiang

- **What's New**: 최근 연구는 기존의 신경망이 주기성을 모델링하는 데 취약하다는 문제를 지적하며, 이를 해결하기 위한 새로운 접근 방식으로 Fourier Analysis Network (	exttt{	extbackslash modelname})를 제안하고 있습니다. 	exttt{	extbackslash modelname}는 Fourier Analysis를 기반으로 하여 신경망에 직접 주기적 패턴을 인코딩하는 새로운 프레임워크를 제공합니다.
- **Technical Details**: Fourier Analysis는 복잡한 함수 내주기에 대한 이해를 돕는 수학적 프레임워크입니다. 이 연구에서는 Fourier Series의 공식을 이용하여 신경망을 구성하고, 이를 기초로 	exttt{	extbackslash modelname}를 설계합니다. 주요 목표는 주기성을 나타내면서 입력-출력 쌍 간의 관계를 정확히 근사할 수 있는 함수 f(x)를 구축하는 것입니다. 이 네트워크는 기존의 MLP 및 Transformer 아키텍처와의 차별성을 강조합니다.
- **Performance Highlights**: 	exttt{	extbackslash modelname}는 기본 및 복잡한 주기 함수 학습에서 기존 신경망들(MLP, KAN, Transformer)을 초월하는 성능을 보여주었으며, 특히 OOD(Out-Of-Domain) 시나리오에서 탁월한 성능을 발휘했습니다. 실제 응용 분야에서도 개선된 성능을 보여주었으며, 이를 통해 주기성 모델링뿐 아니라 다양한 일반 작업에서도 경쟁력 있는 성능을 입증했습니다. 특히, 	exttt{	extbackslash modelname}는 타임 시리즈 예측 및 언어 모델링 작업에서 강력한 경쟁 모델들을 능가하는 성과를 거뒀습니다.

### [Named Clinical Entity Recognition Benchmark](https://arxiv.org/abs/2410.05046)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05046.png)

Vote: 14

Authors: Nasir Hayat, Wadood M Abdul, Clément Christophe, Tathagata Raha, Shadab Khan, Marco AF Pimentel, Muhammad Umar Salman, Praveen K Kanithi, Ronnie Rajan

- **What's New**: 의료 도메인에서 Named Entity Recognition (NER)은 비정형 의료 서사에서 구조화된 정보를 추출하는 데 중요한 역할을 합니다. 본 논문은 임상 NER(Clinical NER) 작업에서 다양한 언어 모델의 성능을 평가하고 벤치마킹할 수 있는 표준화된 플랫폼인 Clinical NER 리더보드를 소개합니다. 이는 개방된 임상 데이터셋을 활용하고 일관된 평가 지표를 구현하여 연구자들이 비교 분석을 하고 혁신을 촉진할 수 있도록 하는 것을 목표로 합니다.
- **Technical Details**: Clinical NER는 비정형 임상 텍스트에서 의료 엔티티를 식별하고 분류하는 작업을 포함합니다. 본 논문에서 제안한 리더보드는 F1-score와 같은 확립된 평가 지표를 주로 사용하며 임상 엔티티의 용어를 표준화하여 일관성 및 상호 운용성을 보장합니다. 또한, 다양한 의료 엔티티를 포괄하는 개방된 의료 벤치마크 데이터셋을 포함하고 있습니다.
- **Performance Highlights**: 최근 대규모 언어 모델(LLMs)을 활용한 연구들은 여러 NLP 작업에서 유망한 결과를 보여주었지만, 표준화된 평가 프레임워크가 부족하여 다양한 연구 및 데이터셋 간의 모델 성능 비교가 도전적이었습니다. 이러한 문제를 해결하기 위해 본 연구는 임상 NER 연구 분야의 혁신을 촉진하고 진행 상황을 주도하기 위한 중앙 집약적이고 투명한 플랫폼을 제공하며, 다양한 모델 아키텍처의 성능 비교를 가능하게 합니다.

### [ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery](https://arxiv.org/abs/2410.05080)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05080.png)

Vote: 13

Authors: Yuting Ning, Zeyi Liao, Frazier N. Baker, Huan Sun, Boshi Wang, Chen Wei, Benjamin Burns, Mingyi Xue, Botao Yu, Yu Su, Ziru Chen, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Yifei Li, Shijie Chen, Song Gao, Zitong Lu, Vishal Dey, Qianheng Zhang

- **What's New**: 본 논문에서는 데이터 기반 발견을 자동화하기 위한 언어 에이전트를 평가하는 새로운 벤치마크인 ScienceAgentBench를 소개합니다. 이 벤치마크는 과학적 발견의 전체 워크플로우를 자동화하기 전에 언어 에이전트가 각 단계를 코딩 작업으로도 수행할 수 있도록 지원합니다. 주목할 점은 각 작업을 코드 생성 문제로 공식화하여 과학자가 추가 수정 없이도 직접 사용할 수 있게 했다는 것입니다.
- **Technical Details**: ScienceAgentBench는 세 가지 주요 설계 원칙을 따릅니다: (1) 전문가와의 공동 설계 통해 과학적 진정성 확보, (2) 엄격한 등급 평가, (3) 다단계 품질 관리. 이 벤치마크는 4개의 분야(생물정보학, 계산화학, 지리정보과학, 심리 및 인지신경과학)의 44개의 피어 리뷰된 출판물에서 102개의 다양한 작업을 추출했습니다. 이는 Python 프로그램으로 자동 평가되며 평가 메트릭을 통해 프로그램의 실행 결과와 비용을 포함한 다양한 요소를 분석합니다.
- **Performance Highlights**: 다섯 개의 공개 가중치와 독점 LLM들을 평가했을 때, 단독으로 가장 성능이 좋은 에이전트는 32.4%의 작업을 성공적으로 해결할 수 있었지만, 전문가의 지식이 추가될 경우 34.3%까지 해결할 수 있었습니다. 이는 최근 Lu et al. (2024)의 작업을 자동화할 수 있다고 주장한 결과와는 대조적입니다. 또 다른 주요 결과로는 Claude-3.5-Sonnet이 self-debug 방식을 사용할 때, OpenHands CodeAct 대비 10.8% 더 많은 작업을 해결하고 API 비용도 17배 적게 소모한다는 점입니다.

### [Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents](https://arxiv.org/abs/2410.05243)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05243.png)

Vote: 13

Authors: Boyuan Zheng, Cheng Chang, Boyu Gou, Yu Su, Yiheng Shu, Yanan Xie, Huan Sun, Ruohan Wang

- **What's New**: 이번 연구에서는 인간과 유사한 구현 메커니즘을 가진 GUI 에이전트가, 즉 시각적 관찰 및 픽셀 수준의 조작만으로 디지털 환경을 탐색하는 능력을 중점적으로 다룹니다. 이를 위해 SeeAct 프레임워크에 기반한 새로운 SeeAct-V를 제안하였으며, 이는 HTML이나 접근성 트리 없이 스크린샷만을 사용하여 GUI 시각적 그라운딩을 수행합니다.
- **Technical Details**: SeeAct-V는 여러 플랫폼에서 작동할 수 있도록 설계되었습니다. 웹페이지에서 데이터를 합성하여 가장 큰 GUI 시각적 그라운딩 데이터셋을 구축하였고, 이 데이터를 기반으로 UGround라는 새로운 시각적 그라운딩 모델을 훈련했습니다. 이 모델은 스크린샷에서 직접 작동할 좌표를 산출합니다.
- **Performance Highlights**: UGround는 기존에 존재하던 모든 시각적 그라운딩 모델을 최대 20%의 성능 향상을 보여주었으며, SeeAct-V 에이전트는 현재 최첨단 에이전트와 비교해, 추가적인 텍스트 기반 입력 없이도 최소한 대등하거나 오히려 더 나은 성능을 보여줍니다. 이러한 결과는 인간처럼 디지털 세계를 탐색하는 GUI 에이전트의 가능성을 지원합니다.

### [TLDR: Token-Level Detective Reward Model for Large Vision Language Models](https://arxiv.org/abs/2410.04734)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04734.png)

Vote: 11

Authors: Guan Pang, Deqing Fu, Wang Zhu, Rui Wang, Robin Jia, Pengchuan Zhang, Tong Xiao, Lawrence Chen

- **What's New**: 이 논문에서는 시간에 따라 변화하는 의견 형성(dynamic opinion formation)의 복잡한 시스템을 설명하는 새로운 모델을 제시합니다. 주목할 만한 점은 이 모델이 사람들 간의 상호작용이 어떻게 집단 결정에 영향을 미치는지를 이해하는 데 초점을 맞추고 있다는 것입니다.
- **Technical Details**: 이 모델은 확률 모델(probabilistic model)과 미분 방정식(differential equations)을 결합하여 개인의 의견 전환과 상호작용을 수학적으로 설명합니다. 특히 각 개인의 의견은 환경 요인과 다른 사람들과의 상호작용에 의해 시간이 지남에 따라 업데이트 됩니다.
- **Performance Highlights**: 실험 결과, 제시된 모델은 기존의 다른 모델들에 비해 집단 의견의 변화를 보다 정확하게 예측할 수 있음을 보여주었습니다. 특히 다양한 시나리오에서 수렴(convergence) 속도와 의견 기반의 동적 변이를 효과적으로 모사할 수 있음을 입증했습니다.

### [Presto! Distilling Steps and Layers for Accelerating Music Generation](https://arxiv.org/abs/2410.05167)

![](/avatars/183d2143ba20e1cc8712c63c055aadd7.svg)

Vote: 11

Authors: Ge Zhu, Taylor Berg-Kirkpatrick, Zachary Novack, Julian McAuley, Nicholas J. Bryan, Jonah Casebeer

- **What's New**: 최근 오디오 도메인 생성 미디어에서 눈에 띄는 발전이 있었습니다. 이는 텍스트-오디오(Text-to-Audio)와 텍스트-음악(Text-to-Music) 생성 능력의 향상을 통해 나타났습니다. 본 논문에서는 새로운 접근법인 Presto를 소개합니다. 이는 듀얼 접근법으로, 점수 기반의 디퓨전 트랜스포머(score-based diffusion transformers)의 추론 속도를 가속화합니다. 특히, 샘플링 단계와 단계 당 비용을 줄이기 위한 다층 접근법을 제안합니다.
- **Technical Details**: Presto에는 세 가지 증류(distillation) 방법이 포함됩니다: (1) Presto-S - 연속 시간 모델의 유연성을 활용하여 GAN 기반으로 단계 추출(distillation)을 수행하는 새로운 분포 매칭(distillation) 알고리즘; (2) Presto-L - 증류 과정에서 숨겨진 상태의 분산을 더 잘 유지하도록 설계된 조건부 층 증류 방법; (3) Presto-LS - 실제 및 가짜 점수 기반 그래디언트 추정에서 층 증류를 분리하면서 층 단계 증류를 결합한 방법입니다.
- **Performance Highlights**: Presto-S는 4단계 추론으로 기본 TTM 디퓨전 샘플링 품질을 맞추는 첫 번째 방법입니다. Presto-L은 기존의 상태 기술(SOTA) 층 드롭핑 방법 및 기본 디퓨전 샘플링 보다 일관되게 성능 향상을 보여 줍니다. 결합된 층-단계 증류는 기본 모델을 10-18배 가속화하면서 다양성을 크게 향상시킵니다. 이는 관련 SOTA 모델보다 15배 빠른 성능을 보입니다.

### [GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2410.05229)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05229.png)

Vote: 10

Authors: Mehrdad Farajtabar, Keivan Alizadeh, Iman Mirzadeh, Samy Bengio, Hooman Shahrokhi, Oncel Tuzel

- **What's New**: 이번 연구에서는 대형 언어 모델(LLMs)의 수학적 추론 능력을 평가하기 위한 개선된 벤치마크 GSM-Symbolic을 소개합니다. 이는 기존의 GSM8K 데이터셋의 한계를 극복하기 위해 다양한 문제 변형을 생성하고 복잡성 수준을 조절하여 LLMs의 견고성과 추론 능력을 탐구할 수 있도록 설계되었습니다.
- **Technical Details**: GSM-Symbolic은 symbolic templates를 사용하여 GSM8K 문제의 다양한 변형을 생성합니다. 이는 단일 성능 메트릭을 넘어 LLMs의 수학적 추론 능력을 더 깊이 연구할 수 있는 기회를 제공합니다. 또한, GSM-NoOp 데이터셋을 도입하여 문제에 표면적으로 관련 있지만 실제로는 관련이 없는 정보를 추가함으로써 모든 최첨단 모델의 성능이 크게 감소하는 것을 보여줍니다. 이는 패턴 매칭에 의존하는 LLMs의 추론 능력의 한계를 시사합니다.
- **Performance Highlights**: 연구 결과, 다양한 설정에서의 LLMs의 성능을 보다 세밀하고 신뢰성 있게 평가할 수 있음을 입증했습니다. GSM-Symbolic에서 모든 모델의 성능이 하락한다는 사실은 데이터 오염 가능성을 암시합니다. 또한, LLMs는 표면적인 요소에는 더 견고하지만 숫자값의 변화에는 매우 민감하고, 문장의 복잡성이 증가할수록 성능 저하와 변동성이 증가한다는 것을 보여줍니다. GSM-NoOp 실험에서는 성능이 최대 65%까지 감소하였습니다. 이는 실질적인 문제 해결을 위한 관련 정보 식별 능력의 부족을 의미하며, 전반적인 추론 과정에서 깊이 있는 조사가 필요함을 강조합니다.

### [UniMuMo: Unified Text, Music and Motion Generation](https://arxiv.org/abs/2410.04534)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04534.png)

Vote: 10

Authors: Chuang Gan, Jiaben Chen, Kaizhi Qian, Kun Su, Han Yang, Gaowen Liu, Yutong Zhang

- **What's New**: UniMuMo라는 새로운 프레임워크는 음악, 모션, 텍스트의 모든 변형에서 콘텐츠를 생성할 수 있는 최초의 통합 모델을 제안합니다. 기존의 단방향 생성 작업과 달리, 다양한 조합의 생성 작업을 통합하여 효율적이고 비용 효과적인 솔루션을 제공합니다.
- **Technical Details**: UniMuMo는 세 가지 주요 단계로 구성됩니다: 음악-모션 공동 토크나이저, 음악-모션 변환기 디코더 모델, 음악-모션 캡셔닝용 T5 디코더. 첫 단계에서 모션을 음악 특징 공간으로 매핑하고, 두 번째 단계에서 두 가지 상호 조건화된 생성 스트림을 사용하여 음악과 모션을 동시에 생성하는 음악-모션 병렬 생성 체계를 도입합니다. 마지막 단계에서 T5 디코더를 미세조정해 음악과 모션 캡션 작업을 수행합니다.
- **Performance Highlights**: UniMuMo는 음악, 모션, 텍스트의 단방향 생성 작업에서 기존 최첨단 모델과 비교해 경쟁력 있는 성능을 발휘했습니다. 이는 이 접근 방식의 효과성과 다용성을 입증합니다.

### [MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion](https://arxiv.org/abs/2410.03825)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03825.png)

Vote: 9

Authors: Trevor Darrell, Deqing Sun, Forrester Cole, Varun Jampani, Junyi Zhang, Ming-Hsuan Yang, Charles Herrmann, Junhwa Hur

- **What's New**: 이 연구는 동적 장면의 기하(geometry) 추정을 개선하기 위해 DUSt3R라는 기존의 정적 장면 기하 추정 기법을 확장한 Motion DUSt3R (MonST3R)를 소개합니다. MonST3R은 포인트맵(pointmap)을 사용하여 동적인 물체를 포함한 장면에서도 기하를 직접 추정하는 새로운 접근법을 제안합니다.
- **Technical Details**: MonST3R는 DUSt3R의 포인트맵 표현을 활용하여 프레임마다 기하를 추정하고, 이들을 정적 요소를 기반으로 정렬하여 다중 프레임에서의 기하 정렬을 수행합니다. 또한 기존 DUSt3R의 학습 데이터셋의 한계를 극복하기 위해 적합한 소규모 데이터셋으로 재학습을 통해 동적 장면에 적응하는 것을 목표로 합니다. 이를 통해 MonST3R는 비디오 특정 과제에 최적화된 새로운 메서드들을 도입하여 성능을 향상시킵니다.
- **Performance Highlights**: MonST3R는 비디오 깊이(video depth) 추정 및 카메라 포즈(camera pose) 추정에서 뛰어난 성능을 보여줍니다. 특히, 기존의 최적화 기반 방법보다 속도가 빠르고 복잡한 시나리오에서 더욱 견고하며, 전문 기법들과의 경쟁력 있는 결과를 보여줍니다. 이를 통해 4D 재구성 등 다양한 다운스트림 과제에서도 유망한 결과를 얻을 수 있습니다.

### [MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs](https://arxiv.org/abs/2410.04698)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.04698.png)

Vote: 8

Authors: Hanze Dong, Amrita Saha, Ee-Peng Lim, Doyen Sahoo, Lei Wang, Yuhui Xu, Yalu Wang, Shan Dong, Caiming Xiong

- **What's New**: 최근 다양한 응용 분야에서 장문-맥락(long-context) 작업이 부각되고 있으며, 이를 평가하기 위한 새로운 수학적 추론 벤치마크인 MathHay가 도입되었습니다. MathHay는 대형 언어 모델의 긴 맥락 내 수학적 추론 능력을 평가하기 위해 자동화된 방법으로 생성됩니다. 이 벤치마크는 문서 수집, 질문 생성, 품질 관리, 헤이 스택(haystack) 구성의 4단계 과정을 통해 만들어집니다.
- **Technical Details**: MathHay는 4가지 난이도의 테스트 과제를 제공합니다: (1) 단일 단계, 단일 문서 (SSSD), (2) 다단계, 단일 문서 (MSSD), (3) 단일 단계, 다중 문서 (SSMD), (4) 다단계, 다중 문서 (MSMD). SSSD는 가장 단순하며 단일 문서와 계산 단계를 필요로 하고, MSMD는 가장 복잡하여 여러 문서와 계산 단계를 요구합니다. 벤치마크는 32K, 64K, 128K 입력 길이에 걸쳐 LLM의 추론 능력을 평가하도록 설계되었습니다.
- **Performance Highlights**: MathHay 벤치마크에 대한 실험 결과, 현재의 대형 언어 모델들은 긴 맥락에서의 수학적 추론 작업을 다루기 어려워하며, 개선의 여지가 많음을 보여주었습니다. 이는 LLM의 장문 맥락 시나리오에서의 수학적 추론 능력을 개선할 가능성을 시사합니다.

### [OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction](https://arxiv.org/abs/2410.04932)

![](/avatars/2638af4626e8a4e3a95f845b94ad94f6.svg)

Vote: 8

Authors: Ying-Cong Chen, Yingjie Cai, Weichao Qiu, Qing Lian, Jing He, Kaiqiang Zhou, Leheng Li, Bingbing Liu, Xu Yan

- **What's New**: 이 논문에서는 '인스턴스 수준의 사용자 지정이 가능한 공간 제어(spatial control with instance-level customization)'라는 문제를 해결하는 새로운 프레임워크를 제안합니다. 이를 통해 사용자가 지정한 위치에서 인스턴스를 생성하고, 해당 인스턴스의 속성을 사용자 정의 지침과 정확히 맞추는 것을 목표로 합니다.
- **Technical Details**: 제안된 프레임워크는 멀티모달 입력(multimodal input)을 사용하여 이미지 생성의 조절성을 향상시킵니다. 텍스트 프롬프트와 이미지 참조를 사용하여 인스턴스 수준에서의 특성을 사용자 정의할 수 있도록 하며, 이를 위해 '잠재 제어 신호(latent control signal, 𝐥𝐜𝐥𝐜)'를 도입합니다. 이 신호는 주어진 파노픽 마스크를 공간 제어로 사용하고, 텍스트 임베딩이나 이미지 임베딩을 𝐥𝐜𝐥𝐜로 변환하여 통합된 조건을 형성합니다. 또한, 불규칙한 이미지 아이덴티티를 처리하기 위해 공간 왜곡(spatial warping)을 제안합니다.
- **Performance Highlights**: MS COCO 및 DreamBooth 데이터셋을 이용한 광범위한 실험에서, 제안된 방법이 기존 방법들보다 우수한 이미지 품질 및 라벨 정렬을 달성했다고 보고하고 있습니다. 제안된 OmniBooth 프레임워크는 텍스트 설명과 이미지 참조를 포함한 멀티모달 컨트롤을 가능하게 하며, 다양한 설정 및 태스크에서 정밀한 정렬을 보장합니다.

### [LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning](https://arxiv.org/abs/2410.02884)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02884.png)

Vote: 7

Authors: Yuqiang Li, Di Zhang, Xiaoshui Huang, Wanli Ouyang, Dongzhan Zhou, Jingdi Lei, Shufei Zhang, Marco Pavone, Tong Che, Jianbo Wu, Tong Xie, Jiatong Li

- **What's New**: LLaMA-Berry is a new framework designed to tackle the challenges in mathematical reasoning by enhancing large language models using novel methods like SR-MCTS (Self-Refine applied to Monte Carlo Tree Search) and PPRM (Pairwise Preference Reward Model). This approach aims to improve both the efficiency and accuracy of reasoning paths in complex problem-solving tasks.
- **Technical Details**: SR-MCTS combines the structured search capabilities of Monte Carlo Tree Search with Self-Refine, where each solution is considered a state in a Markov Decision Process (MDP), and exploration is guided by logical refinement actions. PPRM leverages human feedback-inspired reinforcement learning to evaluate the quality of reasoning paths based on preference ranking, avoiding the unreliability of absolute scores. The Enhanced Borda Count (EBC) method aggregates local preferences to produce a global ranking score, enhancing decision-making capabilities.
- **Performance Highlights**: Evaluations on benchmarks like GSM8K and MATH showed that LLaMA-Berry surpasses baseline methods such as the Tree-of-Thought (ToT) and rStar in terms of search efficiency and performance. Notably, it matches proprietary models like GPT-4 Turbo in challenging tasks, implying broader applications across domains. This demonstrates the framework's potential to significantly uplift LLM reasoning abilities with limited additional data, enabling applications in fields like physics and chemistry.

### [TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles](https://arxiv.org/abs/2410.05262)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05262.png)

Vote: 7

Authors: Ke Fang, Simin Niu, Qingchen Yu, Zhiyu Li, Hanyu Wang, Zifan Zheng, Yunfeng Shi, Shichao Song

- **What's New**: 새로운 평가 방법인 TurtleBench는 대형 언어 모델(Large Language Models, LLM)의 논리적 추론 능력을 평가하기 위한 벤치마크로 제안되었습니다. TurtleBench는 사용자 추측을 바탕으로 LLM이 추론을 통해 옳고 그름을 판단하는 Turtle Soup 게임을 포함합니다. 이 설계는 모델의 기억 능력보다는 추론 능력을 중점적으로 평가하는 데 중점을 두며, 특히 외부 지식에 의존하지 않고 자체적으로 필요한 정보를 제공하여 평가의 신뢰성을 높입니다.
- **Technical Details**: TurtleBench의 데이터셋 구축 과정은 Turtle Soup 퍼즐 게임을 통해 사용자 추측 데이터를 수집하는 것으로 시작됩니다. Claude-3.5-Sonnet 모델이 사용자의 추측을 평가하여 '올바름', '틀림', '불확실함'으로 분류합니다. 추측 데이터의 중복 제거와 모호한 질문 제거 과정을 거쳐 최종적으로 1,532개의 품질 높은 데이터 항목이 포함된 데이터셋을 구축하였습니다. 벤치마크의 안정성을 위해 '불확실함'은 '틀림'으로 재분류했습니다.
- **Performance Highlights**: TurtleBench 벤치마크에서 OpenAI의 최신 모델과 기타 주요 LLM의 성과를 평가했습니다. 모델 평가에서는 0-shot과 2-shot 설정을 사용하였고, 일관된 매개변수 설정을 유지했습니다. 평가 결과로는 각 이야기별 평균 정확도, 전체 테스트 사례의 정확도, F1 Score가 보고되었습니다. 이 실험을 통해 여러 LLM이 TurtleBench에서 어느 정도의 추론 성능을 보이는지 확인할 수 있었습니다.

### [SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification](https://arxiv.org/abs/2410.05057)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05057.png)

Vote: 6

Authors: Jiawei Xu, Benjamin Feuer, Chinmay Hegde, Patrick Yubeaton, Niv Cohen, Govind Mittal

- **What's New**: 이번 연구에서는 데이터 큐레이션(data curation)의 중요성과 이를 연구 주제로 부각시키고자 합니다. 기존에 연구되어 온 데이터 필터링 전략 외에도 데이터 큐레이션을 독립된 연구 주제로 소개하며, 다양한 학습 방법과 하이퍼파라미터(hyperparameter)를 고정하여 데이터를 어떻게 큐레이션 할 수 있는지에 대한 새로운 접근을 제안합니다. NeurIPS 2023에서 도입된 DataComp 경쟁을 통해 데이터 큐레이션의 중요성을 제기하고 있습니다.
- **Technical Details**: 이 연구에서는 합리적 선택 이론(rational choice theory)을 활용하여 데이터 큐레이션 전략을 효용 함수로 공식화했습니다. 이를 통해, 마진 비용의 증가가 효용의 기대 이득을 어떻게 증가시키는지 설명합니다. Select라는 새로운 벤치마크를 소개하여 이미지 분류에서 다양한 데이터 큐레이션 방법의 효용을 평가합니다. 또한, ImageNet++라는 새로운 데이터셋을 활용하여 대규모 기준선 모델을 설정하고, 다양한 데이터 큐레이션 전략을 평가합니다.
- **Performance Highlights**: Select 벤치마크를 통해, 비용을 절감한 큐레이션 전략이 전문가의 라벨링(labelling) 데이터와 유사한 성능을 보이는 경우도 있다는 것을 알 수 있었습니다. 그러나 대부분의 지표에서는 여전히 전문가 라벨링이 더 우수했습니다. 이미지 간 큐레이션 방법이 텍스트에 의존한 방법보다 전반적으로 더 좋은 성과를 보였습니다. 라벨 노이즈와 라벨 불균형은 여전히 비용 효율적인 데이터 큐레이션의 성능에 중요한 제한 요소로 작용하고 있습니다. 이를 바탕으로 코드와 데이터셋을 공개하여 후속 연구와 재현 가능성을 높이고자 합니다.

### [Autonomous Character-Scene Interaction Synthesis from Text Instruction](https://arxiv.org/abs/2410.03187)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03187.png)

Vote: 6

Authors: Zi Wang, Siyuan Huang, Nan Jiang, Yixin Zhu, Yixin Chen, Hongjie Li, Zimo He

- **What's New**: 이 연구는 동적 3D 환경 내에서 언어 지도를 통한 캐릭터 모션 합성을 위한 새로운 접근 방식을 제안합니다. 인간의 직관적이고 맥락에 맞춘 모션을 재현하려는 시도로, 이 연구는 현재의 단편적인 시스템을 통합하여 한 번의 파이프라인에서 걸음걸이, 손 뻗기, 물체와의 상호작용(HOI) 과정을 통합합니다. LINGO라는 새로운 모션 캡처(MoCap) 데이터셋을 도입하고, VR 기반 셋업으로 16시간의 인간 동작 및 객체 상호작용을 포함하는 다양한 장면을 제공합니다.

### [What Matters for Model Merging at Scale?](https://arxiv.org/abs/2410.03617)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03617.png)

Vote: 4

Authors: Jonathan Lai, Prateek Yadav, Mohit Bansal, Manaal Faruqui, Tsendsuren Munkhdalai, Tu Vu, Alexandra Chronopoulou

- **What's New**: 이번 연구는 모델 합치기(model merging)의 확장성에 대한 통찰을 제공하는 것을 목표로 합니다. 모델 합치기는 두 개 이상의 전문가 모델을 결합하여 하나의 강력한 모델을 생성하는 과정입니다. 이 연구는 특히 모델 합치기의 대규모 적용 가능성을 평가하고, 그와 관련된 다양한 요인들이 모델의 성능에 미치는 영향을 조사합니다.
- **Technical Details**: 이번 연구에서는 다양한 모델 합치기 방법을 사용하여 실험을 진행했습니다. 구체적으로, 64B 파라미터까지 스케일링된 PaLM-2 모델과 그 지시조정(instruction-tuned) 변형을 사용했습니다. Averaging, Task Arithmetic, TIES-Merging, Dare-TIES 등 네 가지 합치기 방법을 실험했으며, 사전 학습된 모델과 instruction-tuned 모델의 차이점, 그리고 여러 전문가 모델의 조합이 성능에 미치는 영향을 분석했습니다.
- **Performance Highlights**: 첫째, 강력한 지시조정된 베이스 모델을 사용하는 것이 합쳐진 모델의 성능을 향상시키는 데 중요하다는 것을 발견했습니다. 둘째, 큰 모델일수록 합치기가 더 수월했습니다. 셋째, 모델 합치기는 제로샷 일반화를 크게 향상시켰으며, 합쳐진 전문가 수가 증가할수록 멀티태스크 학습 기준선을 능가하기도 했습니다. 끝으로, 큰 모델은 더 많은 수의 전문가 모델을 합칠 때 더 좋은 성능을 보였습니다.

### [SePPO: Semi-Policy Preference Optimization for Diffusion Alignment](https://arxiv.org/abs/2410.05255)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.05255.png)

Vote: 3

Authors: Dong-Jun Han, Daoan Zhang, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Jiebo Luo, Yu Dong, Christopher Brinton, Mingxiao Li, Pengcheng Chen, Guangchen Lan

- **What's New**: 본 논문은 시각 생성 모델(text-to-visual models)에서 인간의 선호에 맞게 정렬하기 위한 새로운 방법론, 즉 세미-정책 선호 최적화(Semi-Policy Preference Optimization, SePPO)를 제안합니다. 기존의 강화학습 기반 접근법은 인간 피드백을 바탕으로하여 모델을 훈련하지만, SePPO는 보상 모델이나 인간이 주석한 데이터를 사용하지 않고도 선호 정렬을 달성합니다. 이는 고비용의 피드백 데이터 수집을 줄이고 인프라 부담을 낮출 수 있습니다.
- **Technical Details**: SePPO는 각 반복에서 레퍼런스 모델을 선택하여 정책 탐색 공간을 확장하고, 생성된 응답의 품질을 평가하는 기준을 디자인하여 이 품질 정보에 기반한 선호 최적화를 조절하는 전략, 즉 앵커 기반 적응 플리퍼(Anchor-based Adaptive Flipper, AAF)를 사용합니다. AAF는 레퍼런스 모델이 현재 모델보다 '승리하는' 데이터 포인트를 생성할 확률이 높은 경우, 그 샘플을 학습에 포함시킴으로써 샘플 품질을 잘못 판단하여 발생할 수 있는 부정적인 영향을 피하고, 데이터 포인트의 품질을 향상시킬 수 있도록 합니다.
- **Performance Highlights**: SePPO는 텍스트-이미지 벤치마크에서 이전의 모든 최적화 방법을 초과하는 성과를 보였으며, 텍스트-비디오 데이터셋에서도 그 효과가 검증되었습니다. 또한, 정책 탐색 공간을 확장하여 보다 안정적인 학습 과정을 제공합니다.

### [Grounding Language in Multi-Perspective Referential Communication](https://arxiv.org/abs/2410.03959)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03959.png)

Vote: 3

Authors: Alane Suhr, Zineng Tang, Lingjun Mao

- **What's New**: 이 논문에서는 인간과 에이전트간의 상호작용을 통해 참조적 소통(Referential Communication)의 성공률을 높이기 위한 새로운 플랫폼을 소개합니다. 이 플랫폼은 포토리얼리스틱 3D 환경 내에서 에이전트 간의 다중 관점 참조 표현 생성과 이해를 연구할 수 있게 합니다. 이러한 환경에서 각각의 에이전트는 서로 다른 시점에서 환경을 관찰하며, 이 환경을 통해 참조 표현이 어떻게 생성되고 이해되는지를 탐구합니다.
- **Technical Details**: 제안된 플랫폼은 3D 씬(Scene)을 생성하고, 두 에이전트가 서로 다른 관점에서 참조 표현을 사용하는 형태로 설계되었습니다. 각 씬에는 후보 객체들이 있고, 그 중 하나가 목표 객체로 설정됩니다. 주요 임무는 화자가 의도한 목표 객체를 청자에게 성공적으로 전달하는 것입니다. 또 이 시스템은 시점 차이에 의한 과제 난이도를 조절할 수 있고, 에이전트들의 관점이나 목표 객체 배치를 수정할 수 있습니다.
- **Performance Highlights**: 여러 비전-언어 모델을 평가한 결과, 인간 청자와의 상호작용을 통해 개선된 LLaVA-1.5 모델이 참조 표현 생성 정확도를 크게 향상시켰습니다. 최초 58.9%의 성공률에서 69.3%까지 향상되었으며, 이는 GPT-4o 스피커 모델을 초과하는 결과입니다. 이러한 결과는 상호작용을 통한 학습이 다중 관점 참조 소통의 성공률을 높이는 데 효과적임을 보여줍니다.

### [Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach](https://arxiv.org/abs/2410.03160)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03160.png)

Vote: 1

Authors: Jean-michel Morel, Yaofang Liu, Yumeng Ren, Yang Liu, Tieyong Zeng, Aitor Artola, Xiaodong Cun, Raymond H. Chan

- **What's New**: 본 연구는 새로운 프레임 인식 비디오 확산 모델(Frame-Aware Video Diffusion Model, FVDM)을 제안합니다. 기존의 영상 확산 모델(Video Diffusion Models, VDM)들이 비디오 데이터를 단조롭게 처리하는 한계를 극복하기 위해, 각 프레임이 독립적으로 진화할 수 있도록 벡터화된 시간변수(Vectorized Timestep Variable, VTV)를 도입했습니다.
- **Technical Details**: 기존 확산 모델은 확률적 미분 방정식(Stochastic Differential Equations, SDEs)을 사용하여 데이터 분포에 점진적으로 잡음을 추가하고 이를 역으로 샘플링하는 방식으로 데이터를 생성합니다. 하지만 이러한 방법은 비디오 데이터의 복잡한 시간적 동적 구조를 효과적으로 모델링하기 어려웠습니다. 제안하는 FVDM은 각 프레임이 자신의 시간 궤적을 따라 진화할 수 있도록 하여 복잡한 시간적 의존성을 포착합니다. 이를 통해 보다 향상된 비디오 생성 성능을 제공합니다.
- **Performance Highlights**: 제안된 FVDM은 표준 비디오 생성에서의 품질 향상뿐만 아니라 이미지-비디오 전환, 비디오 인터폴레이션 및 장기간 비디오 생성 등 다양한 응용 분야에서 현행 최고 성능을 초과합니다. 이는 FVDM의 강력한 성능과 다재다능함을 입증합니다.

### [SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation](https://arxiv.org/abs/2410.03960)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.03960.png)

Vote: 1

Authors: Zhewei Yao, Samyam Rajbhandari, Yuxiong He, Aurick Qiao

- **What's New**: SwiftKV는 대규모 언어 모델(LLMs) 추론 속도를 크게 개선하기 위한 새로운 접근법을 제안합니다. 일반적인 기업 업무에서 입력 토큰의 수가 출력 토큰의 수를 크게 초과한다는 관찰을 바탕으로, SwiftKV는 네트워크의 후반 레이어를 건너뛰어 추론 과정에서 필요 없는 계산을 줄이고, KV 캐시 메모리 요구를 줄임으로써 더 큰 배치를 처리할 수 있도록 설계되었습니다.
- **Technical Details**: SwiftKV는 네 개의 주요 구성 요소로 구성됩니다. SingleInputKV는 모델을 재구성해 사전 채우기(prefill) 단계에서 후반 레이어를 건너뛰도록 하여, 필요없는 KV 캐시 계산을 줄이는 역할을 합니다. AcrossKV는 인접한 레이어의 KV projection을 하나로 통합하여, 메모리 절약을 가능하게 합니다. 또한, Knowledge Recovery를 통해 원본 모델의 예측 품질을 복구하며, distillation 기법을 사용합니다. 이러한 분석과 검증은 vLLM에 SwiftKV를 구현하여 완료되었습니다.
- **Performance Highlights**: SwiftKV는 도입된 기술로 기업의 업무 처리량을 최대 두 배로 증가시키고, 최초 출력 토큰까지의 시간(TTFT)과 출력 토큰당 시간을 각각 최대 50% 및 60%까지 줄입니다. Llama-3.1-70B에 대해, SwiftKV는 56.6%의 MFU 활용도를 달성하며, 메모리와 계산 비용 감소에도 품질 저하를 최소화하였습니다. 이 성능 향상은 다양한 벤치마크에서 테스트되어 검증되었습니다.

