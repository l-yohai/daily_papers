## Daily Papers (2024-08-20)

### [LongVILA: Scaling Long-Context Visual Language Models for Long Videos](https://arxiv.org/abs/2408.10188)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10188.png)

Vote: 31

Authors: Fuzhao Xue, Hongxu Yin, Yukang Chen, Zhijian Liu, Pavlo Molchanov, Xiuyu Li, Yuke Zhu, Song Han, Jan Kautz, Yunhao Fang, Yao Lu, Ligeng Zhu, Linxi Fan, Haotian Tang, Qinghao Hu, Shang Yang, Ethan He, Dacheng Li

- **What's New**: 이 논문은 인공지능 연구가 멀티 모달(다중 형태) 및 긴 문맥을 처리할 수 있는 모델로 빠르게 발전하고 있음을 강조합니다. 특히, LongVILA라는 새로운 모델이 제안되었습니다. 이 모델은 하나의 시스템 내에서 긴 문맥을 처리하는 비주얼 언어 모델을 훈련시키고 추론하는 것을 목표로 합니다.
- **Technical Details**: 이 논문에서는 긴 문맥과 멀티 모달을 지원하는 모델을 구축하기 위한 여러 기술적 도전을 다룹니다. 'Multi-Modal Sequence Parallelism (MM-SP)'이라는 새로운 시스템을 제안하여 긴 문맥 멀티 모달 언어 모델 훈련과 추론을 지원합니다. 이 시스템은 대규모 비주얼 언어 데이터를 모으고, 이를 통해 대규모 프리 트레이닝과 긴 문맥을 따르는 비주얼 언어 지시 데이터를 수집하고 훈련하는 구성 요소로 이루어져 있습니다.
- **Performance Highlights**: LongVILA 모델은 비디오 캡션 및 비디오 QA(질문 답변) 작업에서 경쟁력 있는 성과를 보여줍니다. 다양한 문맥 길이에서도 현재 공개된 최고 성능의 모델을 큰 마진으로 능가하는 결과를 보였습니다.

### [MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model](https://arxiv.org/abs/2408.10198)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10198.png)

Vote: 21

Authors: Chong Zeng, Xinyue Wei, Chao Xu, Hongzhi Wu, Linghao Chen, Hao Su, Ruoxi Shi, Minghua Liu, Mengqi Zhang, Isabella Liu, Zhaoning Wang, Xiaoshuai Zhang

- **What's New**: 이번 연구에서는 MeshFormer라는 새로운 오픈월드 희소 뷰 재구성 모델을 소개합니다. 이 모델은 임의 물체의 희소 포즈 이미지를 입력으로 받아 몇 초 만에 고품질의 3D 텍스처 메시를 생성합니다. MeshFormer는 기존의 3D 데이터를 '2D 평면'으로 표현하고 렌더링 손실만 최적화하는 '블랙 박스' 변환기 모델 대신, 네트워크 아키텍처, 감독 신호, 입력 가이던스 등의 3D 네이티브 요소들을 모델 설계에 포함하여 메시 품질과 학습 효율성을 크게 향상시킵니다.
- **Technical Details**: MeshFormer는 명시적인 3D 복셀(feature)을 표현하고 대규모 변환기(transformer)와 3D 희소 컨볼루션(convolution)을 결합한 새로운 아키텍처를 제안합니다. 트리플레인(triplane)이나 순수 변환기 모델과 달리 MeshFormer는 복셀 피처와 다중 뷰 기능 간의 정확한 투영 대응을 활용하여 더 빠르고 효과적인 학습을 가능하게 합니다. 또한, 우리는 표면 렌더링과 명시적인 3D 감독을 결합하여 고품질 메시 생성을 위한 단일 단계 학습 전략을 제안합니다. 이는 NeRF 기반의 표현을 파이프라인에 포함하지 않고 모델이 표면 렌더링과 추가적인 명시적 3D 감독을 통해 서명 거리 함수(SDF) 필드를 학습할 수 있도록 하여 더욱 빠른 학습을 가능하게 합니다.
- **Performance Highlights**: MeshFormer는 기존 방법들과 비교하여 단 8개의 GPU로 이틀 만에 훈련할 수 있으며, 그 결과 더 적은 리소스를 사용하여 동등하거나 더 나은 성능을 보여줍니다. 여러 2D 확산 모델과도 원활하게 통합되어 단일 이미지 또는 텍스트를 3D로 변환하는 다양한 작업을 수행할 수 있습니다. 마지막으로, 다중 뷰의 일반 정규 이미지를 입력으로 사용하여 중요한 기하학적 가이던스를 제공하고, 추가적인 3D 정규 텍스처를 예측하여 생성된 기하학을 향상시킵니다.

### [Segment Anything with Multiple Modalities](https://arxiv.org/abs/2408.09085)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.09085.png)

Vote: 12

Authors: Yun Xing, Heli Qi, Shijian Lu, Aoran Xiao, Naoto Yokoya, Weihao Xuan

- **What's New**: 최근 Segment Anything Model(SAM)은 일반 마스크 분할을 위한 최첨단 시각 기반 모델로 떠올랐습니다. 그러나 SAM은 대규모 RGB 이미지 마스크로 훈련되어 RGB 카메라에 맞춰져 있으며, 다른 시각 센서 모드에서는 제대로 작동하지 않을 때가 많습니다. 이에 따라 SAM의 한계를 극복하고 다중 모드 데이터 처리 능력을 확장하기 위해 MM-SAM이 제안되었습니다. MM-SAM은 다양한 센서 모듈과의 통합을 통해 세부 모드 간 분할 및 센서 융합을 가능하게 합니다.
- **Technical Details**: MM-SAM의 주요 기술적 세부 사항은 다음과 같습니다. 첫째, Unsupervised Cross-Modal Transfer(UCMT)을 통해 비지도 학습 방식으로 다양한 모드의 센서 데이터를 처리합니다. 이는 센서 모드에 특화된 패치 임베딩 모듈과 파라미터 효과적인 튜닝을 통해 이루어집니다. 둘째, Weakly-supervised Multi-Modal Fusion(WMMF)을 도입하여 선택적 융합 게이트를 통해 다중 모드 데이터의 적응적 융합을 가능하게 합니다. WMMF는 다양한 센서 모듈의 비레이블 방식 학습을 통해 적용성을 확장합니다. 또한, MM-SAM은 라벨 비효율적 적응을 위해 UCMT와 WMMF가 결합된 구조로 설계되었습니다.
- **Performance Highlights**: MM-SAM은 다중 모드 센서 데이터를 처리하면서도 높은 수준의 분할 능력을 보여줍니다. 이는 광범위하게 사용되는 센서 모듈들에 대해 뛰어난 성능을 입증했습니다. 특히, 비레이블 학습을 통해 데이터 수집 및 주석 작업의 노력을 크게 줄일 수 있습니다. 종합적으로, MM-SAM은 다양한 시각 인식 및 탐색 작업에 있어 강력한 다중 모드 융합 성능을 입증했습니다.

### [ShortCircuit: AlphaZero-Driven Circuit Design](https://arxiv.org/abs/2408.09858)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.09858.png)

Vote: 11

Authors: Dimitrios Tsaras, Antoine Grosnit, Lei Chen, Zhiyao Xie, Mingxuan Yuan, Haitham Bou-Ammar

- **What's New**: 이 연구는 새로운 딥러닝 모델을 통해 이미지 분류 (image classification) 및 객체 인식 (object detection)의 정확도를 향상시키는 방법을 제시하고 있습니다. 특히, Transformer 구조를 개선하여 더 나은 성능을 발휘합니다.
- **Technical Details**: 이 논문에서는 기존의 Convolutional Neural Networks (CNNs)를 강화한 새로운 모델인 Vision Transformer (ViT)를 소개합니다. 이 모델은 이미지 패치를 (image patches) 입력으로 사용하며, 각 패치가 Transformer의 입력 시퀀스가 됩니다. 이 모델은 또한 Position Embeddings (위치 임베딩)을 사용하여 각 패치의 공간 정보를 보존합니다.
- **Performance Highlights**: 제안된 Vision Transformer (ViT)는 유명한 데이터셋인 ImageNet에서 기존 모델보다 높은 정확도를 기록했습니다. 특히, 기존의 최고 성능 모델들과 비교했을 때, Top-1 Accuracy에서 1.5%의 향상을 보였습니다.

### [NeuFlow v2: High-Efficiency Optical Flow Estimation on Edge Devices](https://arxiv.org/abs/2408.10161)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10161.png)

Vote: 9

Authors: Hanumant Singh, Huaizu Jiang, Zhiyong Zhang, Aniket Gupta

- **What's New**: 최근 몇 년간 고정밀 광학 흐름 추정(optical flow estimation) 알고리즘 개발이 큰 진전을 보였습니다. NeuFlow v2는 이러한 최신 기술들을 활용하여 실시간 추정 성능을 유지하면서도 계산 비용을 최적화하는 새로운 모듈을 소개합니다. 이를 위해 백본(backbone)을 단순화하고 크로스 어텐션(cross-attention)과 글로벌 매칭(global matching) 모듈을 도입해 초기 광학 흐름을 추정하며, 효율적인 반복 정제(iterative refinement) 모듈로 성능을 개선합니다.
- **Technical Details**: NeuFlow v2는 다음 두 가지 주요 컴포넌트로 구성됩니다: 1) 백본에서는 불필요한 구성 요소를 제거하고 경량화된 CNN 기반 구조를 사용하여 멀티 스케일 이미지에서 저수준(low-level) 특징을 추출합니다. 2) 반복 정제 모듈(iterative refinement module)도 간단한 RNN 모듈을 사용하여 고정밀의 정제된 광학 흐름을 출력합니다. 크로스 어텐션과 글로벌 매칭 모듈은 1/16 스케일 특징에서 정보를 교환하여 큰 픽셀 변위를 처리합니다. 이 모든 모듈은 실시간 인퍼런스 속도를 유지하도록 설계되었습니다.
- **Performance Highlights**: NeuFlow v2는 512x384 해상도 이미지 기준으로 Jetson Orin Nano에서 20 FPS 이상의 실시간 성능을 달성합니다. 또한, FlyingThings 데이터셋에서 훈련하는 동안 실제 데이터로의 과적합을 방지하고, 장면의 일반화 성능을 크게 향상시켰습니다. 예를 들어, RAFT와 같은 기존 모델보다 적은 반복 단계를 사용하면서도 높은 정밀도를 유지합니다. 전체 결과는 실시간 성능과 낮은 계산 예산을 유지하며, 여러 데이터셋에서 뛰어난 성능을 입증합니다.

### [Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering](https://arxiv.org/abs/2408.09702)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.09702.png)

Vote: 8

Authors: Merlin Nimier-David, Zan Gojcic, Sanja Fidler, Nandita Vijaykumar, Ruofan Liang, David Acuna, Zian Wang

- **What's New**: 이 논문에서는 고품질의 사실적인 가상 물체 삽입을 위해 'DiPIR (Diffusion Prior for Inverse Rendering)'라는 새로운 접근 방식을 제안합니다. DiPIR은 대규모 Diffusion Models (DMs)이 학습한 이미지를 기반으로, 가상 물체와 환경 간의 상호작용을 정확하게 모델링하는 것을 목표로 합니다. 이는 특히 조명 추정 문제를 해결하고자 합니다.
- **Technical Details**: DiPIR은 세 가지 주요 기여를 통해 작동합니다. 첫째, 물리 기반 렌더러를 사용하여 빛과 3D 자산 간의 상호작용을 정확하게 시뮬레이션합니다. 둘째, 사전에 학습된 DM을 가볍게 개인화하는 방법을 제안합니다. 셋째, 이 개인화를 활용하여 훈련 안정성을 향상시키는 SDS 손실의 변형을 설계합니다. DM은 사람이 평가하는 것과 유사하게 작동하여 편집된 이미지를 입력으로 받아 물리 기반의 장면 속성으로 피드백 신호를 전달합니다.
- **Performance Highlights**: 실험 결과 DiPIR은 실내외 데이터셋 모두에서 객체 삽입을 위한 기존의 최첨단 조명 추정 방법을 능가하는 성능을 보였습니다. 이는 특히 낮은 동적 범위를 가진 소비자 장치로 단일 이미지를 캡처할 때 더욱 두드러집니다.

### [Factorized-Dreamer: Training A High-Quality Video Generator with Limited and Low-Quality Data](https://arxiv.org/abs/2408.10119)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10119.png)

Vote: 8

Authors: Tao Yang, Yangming Shi, Yunwen Huang, Feng Chen, Lei Zhang, Yin Zheng

- **What's New**: Factorized-Dreamer라는 새로운 모델을 제안하여, 공개된 비디오 데이터셋만을 사용해 고품질(HQ)의 비디오 생성이 가능한지 조사했습니다.
- **Technical Details**: Factorized-Dreamer는 T2I (Text-to-Image) 단계를 거쳐 텍스트/이미지-비디오(TI2V) 단계로 진행하는 분할 생성을 사용합니다. 이로 인해 복잡한 비디오 캡션의 필요성을 줄이고, 웹비드-10M(WebVid-10M)과 같은 데이터셋의 품질 낮은(LQ) 이미지 특징을 개선할 수 있습니다. 공간 모듈은 사전 학습된 T2I 모델을 사용하고, 픽셀 인식 교차 주의(PACA)와 T5 기반 텍스트 인코더를 사용하여 움직임을 인식합니다.
- **Performance Highlights**: 공개된 데이터셋인 웹비드-10M(WebVid-10M)으로 학습된 Factorized-Dreamer는 많은 자가 수집 데이터로 학습된 T2V 모델들과 비교하여 비슷하거나 더 좋은 성능을 보였습니다.

### [SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views](https://arxiv.org/abs/2408.10195)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10195.png)

Vote: 7

Authors: Chao Xu, Linghao Chen, Ang Li, Hao Su, Ruoxi Shi, Minghua Liu, Yulin Liu

- **What's New**: 3D 객체 재구성(3D object reconstruction)은 3D 콘텐츠 제작, 증강 현실(augmented reality), 가상 현실(virtual reality), 로봇 공학 등 다양한 분야에서 오랜 과제입니다. 최근 연구들은 단일 이미지나 텍스트를 3D로 변환하는 데 큰 진전을 이뤘습니다. 그러나 현실적인 상황에서는 높은 해상도의 이미지와 정밀한 카메라 데이터를 얻기 어려운 경우가 많아 실질적인 문제로 남아있습니다. 이에 새로운 접근 방식인 SpaRP을 제안합니다. 이는 다양한 개체 범주에서 포즈가 지정되지 않은 상태의 희소 뷰(sparse view) 입력을 처리할 수 있습니다.
- **Technical Details**: SpaRP은 최신 2D 확산 모델(diffusion model), 예를 들어 Stable Diffusion을 활용하여 임의의 개체 카테고리에 대해 포즈가 지정되지 않은 희소 뷰 입력을 처리합니다. 이 모델은 입력 이미지를 단일 이미지로 조합하여 그것을 조건부로 하여 3D 객체의 공간적 관련성과 카메라 포즈를 추론합니다. 구체적으로는 2D 확산 모델을 미세 조정(finetune)하여, NOCS 맵을 생성하게 하고 이는 서로 다른 뷰 간의 픽셀 대응을 처리하는 것입니다. 이를 통해 전통적인 PnP 알고리즘으로 카메라 포즈를 추출하고, 확산 모델은 고정된 카메라 포즈로 멀티뷰 이미지를 생성하여 3D 재구성 모듈에 제공합니다.
- **Performance Highlights**: SpaRP은 Objaverse 데이터셋을 통해 훈련되었으며, 포즈가 지정되지 않은 1~6개의 입력 뷰만으로 훈련되었습니다. 이 방법은 고가의 개별 형상 최적화 없이 약 16초 만에 3D 텍스처 메쉬와 카메라 포즈를 효율적으로 생성합니다. 기존의 단일 이미지 3D 변환의 애매한 부분을 효과적으로 극복하며, 세 가지 데이터셋에서 수행한 광범위한 평가에서 높은 충실도의 3D 메쉬와 정확한 포즈 추정 성능을 입증했습니다.

### [TraDiffusion: Trajectory-Based Training-Free Image Generation](https://arxiv.org/abs/2408.09739)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.09739.png)

Vote: 5

Authors: Rongrong Ji, Jiale Li, Jianzhuang Liu, Mingrui Wu, Huafeng Kuang, Oucheng Huang, Xinyue Cai, Jiayi Ji, Xiaoshuai Sun

- **What's New**: 최근 몇 년간 이미지 생성 분야에서 큰 진전이 있었습니다. 특히 웹에서 수집한 대규모 데이터셋을 학습한 모델들의 발전이 눈에 띕니다. 이러한 모델들은 텍스트를 조건으로 고품질 이미지를 생성하는 데 강력한 성능을 보여주고 있습니다. 하지만 텍스트 기반 제어는 종종 세밀한 조정이 부족해, 더욱 정밀한 조작을 위해 다른 조건 방법을 탐색하는 연구가 진행되고 있습니다. 이 논문은 텍스트-이미지 생성에서 'Trajectory-based control'(궤적 기반 제어)을 제안하여 이 문제를 해결하고자 합니다.
- **Technical Details**: 이 논문은 새로운 'Training-free (학습이 필요 없는) trajectory-conditioned image generation (궤적 조건 이미지 생성)' 방법을 제안합니다. 이 기술은 사용자에게 구체적인 궤적을 통해 이미지 요소의 위치를 안내할 수 있습니다. 이를 위해 'Distance Awareness Energy Function'(거리 인식 에너지 함수)를 도입하였습니다. 이 함수는 두 가지 주요 구성 요소로 이루어져 있는데, 첫 번째는 궤적으로 타겟을 안내하는 'Control function' (제어 함수)이고, 두 번째는 궤적과 관련 없는 영역의 응답을 줄이는 'Movement function' (이동 함수)입니다.
- **Performance Highlights**: 본 연구의 궤적 기반 접근 방법은 레이아웃 제어 이미지 생성에서 유망한 해결책을 제시합니다. 질적 및 양적 평가를 통해 이 방법이 뛰어난 제어 능력을 가지며, 생성된 이미지의 품질과 정확도 모두에서 현저한 향상을 이루었음을 보여줍니다. 또한, 본 방법은 임의의 궤적 입력에 대한 적응력을 보여주며, 객체 속성, 관계 및 주요 영역에 대한 정밀한 제어가 가능합니다.

### [Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges](https://arxiv.org/abs/2408.08946)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08946.png)

Vote: 4

Authors: Baixiang Huang, Kai Shu, Canyu Chen

- **What's New**: 저자 식별(AA)은 특정 텍스트의 저자를 판별하는 과정으로, 포렌식 조사, 테러리스트 추적, 범죄 조사 등 여러 분야에서 실질적인 응용이 가능합니다. 최근 LLMs(Large Language Models)의 발전은 텍스트 생성에 혁신을 가져왔지만, 이에 따라 텍스트의 진위 여부와 원본성을 식별하는 것이 더욱 어려워졌습니다. LLM이 생성한 텍스트와 인간이 작성한 텍스트를 구분하기 위한 새로운 방법이 필요합니다.
- **Technical Details**: 전통적으로 저자 식별은 스타일 분석(stylometry)을 통해 개별 필자의 고유한 글쓰기 스타일을 특징 추출(feature engineering)을 통해 분석했습니다. 그러나 최근 머신러닝 알고리즘의 발전으로 사전 훈련된 언어 모델(pre-trained language models)을 사용한 텍스트 임베딩(text embeddings) 추출이 가능해졌습니다. 또한 LLM 기반의 기법도 저자 식별에 도입되었습니다.
- **Performance Highlights**: LLMs의 급속한 발전은 텍스트 생성을 크게 개선하여 인간의 글쓰기와 유사한 수준의 유창하고 일관된 출력을 제공합니다. 저자가 인간인지, LLM인지, 혹은 두 가지가 혼합된 것인지 구분하는 것이 중요합니다. 이를 위해 저자 식별은 네 가지 문제로 체계적으로 분류될 수 있습니다: 인간 저자를 식별하는 문제, LLM 생성 텍스트를 감지하는 문제, 특정 LLM 또는 인간 저자를 식별하는 문제, 텍스트를 인간, LLM, 또는 두 가지가 공동 저자로 분류하는 문제입니다. 이러한 과제를 해결하기 위해 계속해서 새로운 방법이 개발되고 있습니다.

### [Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models](https://arxiv.org/abs/2408.08926)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08926.png)

Vote: 3

Authors: Leo Glikbarg, Polycarpos Yiorkadjis, Samantha Liu, Andy K. Zhang, Mike Yang, Nathan Tran, +, Rinnara Sangpisit, Derek Askaryar, Eliot Jones, Riya Dulepet, Donovan Jasper, Daniel Zamoshchin, Teddy Zhang, Neil Perry, Joey Ji, Vikram Sivashankar, Celeste Menders, Justin W. Lin, Gashon Hussein, Rishi Alluri, Pura Peetathawatchai, Ari Glenn

- **What's New**: 이 논문에서는 새로운 딥러닝 모델을 제안하고, 이는 자연어 처리(Natural Language Processing, NLP) 작업에서 상당한 성능 향상을 보여줍니다. 특히, 새로운 아키텍처는 주의 메커니즘(attention mechanism)과 트랜스포머(transformer) 네트워크를 결합하여 장문의 텍스트를 효과적으로 처리할 수 있습니다.
- **Technical Details**: 제안된 모델은 기본 트랜스포머 아키텍처를 기반으로 하며, 다중 헤드 자기 주의 메커니즘(multi-head self-attention mechanism)을 사용하여 입력 데이터의 다양한 부분에 동시에 집중할 수 있습니다. 또한, 멀티 레이어 퍼셉트론(MLP)을 사용하여 깊은 신경망의 표현 능력을 극대화합니다. 마지막으로, 학습 과정에서 배치 정규화(batch normalization)와 드롭아웃(dropout) 기법을 도입하여 과적합(overfitting)을 방지합니다.
- **Performance Highlights**: 제안된 모델은 공개된 여러 대규모 데이터셋에서 기존의 최첨단 모델들과 비교하여 월등한 성능을 보였습니다. 특히, 기계 번역(machine translation)과 문서 요약(text summarization) 작업에서 큰 향상을 보였으며, 실험 결과 평균적으로 5% 이상의 BLEU 점수(BLEU score) 개선을 이뤘습니다.

