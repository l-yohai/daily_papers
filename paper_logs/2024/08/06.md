## Daily Papers (2024-08-06)

### [MiniCPM-V: A GPT-4V Level MLLM on Your Phone](https://arxiv.org/abs/2408.01800)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.01800.png)

Vote: 42

Authors: Huarong Zhou, +, Haoye Zhang, Guoyang Zeng, Yuan Yao, Tianchi Cai, Zhihui He, Dahai Li, Hongji Zhu, Junbo Cui, Chongyi Wang, Weilin Zhao, Jie Cai, Ao Zhang, Qianyu Chen, Xu Han, Zhiyuan Liu, Zhensheng Zou, Jie Zhou, Tianyu Yu, Zhi Zheng, Haoyu Li, Shengding Hu

- **What's New**: 최근 멀티모달 대형 언어 모델(Multimodal Large Language Models, MLLMs)의 빠른 발전은 AI 연구와 산업의 판도를 크게 변화시키고 있습니다. 그러나 현재의 MLLMs는 수많은 파라미터와 높은 연산 비용 때문에 실질적인 응용에 한계가 있습니다. 이를 해결하기 위해, 더 효율적이고 경량화된 MLLMs의 개발이 필수적입니다. 이 논문에서는 MiniCPM-V라는 효율적인 MLLM 시리즈를 소개하며, 이는 모바일 기기와 같은 엔드 사이드(end-side) 기기에서 실행될 수 있습니다. 이는 사용자들의 일상 생활에 널리 사용될 수 있으며, 오프라인 시나리오에서도 강력한 성능을 발휘할 수 있습니다.
- **Technical Details**: MiniCPM-V 시리즈는 성능과 효율성 사이에서 균형을 유지하는 것을 목표로 합니다. 2024년 현재까지 세 가지 모델이 출시되었습니다. MiniCPM-V 1.0 2B는 모바일폰을 위해 설계된 최초의 MLLM 중 하나이며, MiniCPM-V 2.0 2B는 더 큰 모델을 능가하는 성능과 높은 해상도의 이미지 입력을 지원합니다. 마지막으로, MiniCPM-Llama3-V 2.5 8B는 GPT-4V-1106 등과 비교해 뛰어난 성능을 발휘하며, 고해상도 이미지 인식과 다국어 지원을 제공하는 등 탁월한 기능을 자랑합니다. 또한, 이 모델들은 효율적인 엔드 사이드 배포를 위해 양자화(quantization), 메모리 최적화(memory optimization), 컴파일 최적화(compilation optimization) 및 NPU 가속(NPU acceleration)을 통합하고 있습니다.
- **Performance Highlights**: MiniCPM-Llama3-V 2.5는 OpenCompass 평가에서 GPT-4V-1106, Gemini Pro 및 Claude 3을 능가하는 성능을 보여주며, OCRBench에서 뛰어난 광학 문자 인식(OCR) 능력을 자랑합니다. 이러한 성능은 신중한 아키텍처 설계와 데이터 및 트레이닝 레시피 덕분입니다. 또한, 믿을 수 있는 행동을 보여주는 RLAIF-V 및 RLHF-V 기술을 바탕으로 신뢰성을 향상시키고 있습니다. 미니CPM-V 시리즈는 다국어 지원 기능을 통합하여 30개 이상의 언어에서 우수한 다중 모드 실행 능력을 제공합니다.

### [Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining](https://arxiv.org/abs/2408.02657)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02657.png)

Vote: 21

Authors: Dongyang Liu, Peng Gao, Weifeng Lin, Le Zhuo, Hongsheng Li, Yu Qiao, Shitian Zhao

- **What's New**: 최신 연구에서는 Lumina-mGPT라고 불리는 새로운 디코더-온리 트랜스포머(Decoder-Only Transformer) 모델을 소개합니다. 이 모델은 효과적인 멀티모달 생성 사전학습(effective multimodal Generative PreTraining, mGPT)과 높은 품질의 고해상도 이미지를 생성할 수 있도록 설계되었습니다.
- **Technical Details**: 기존의 DALL-E, Stable Diffusion, SoRA와 같은 모델들이 연속적인 잠재 이미지 특징을 기반으로 높은 품질의 이미지 및 비디오 생성 성능을 보여주었다면, 새로운 Lumina-mGPT는 디코더-온리 구조와 효과적인 멀티모달 사전학습을 통해 이 문제를 해결하려고 합니다. 이는 큰 규모의 이미지-텍스트 데이터셋을 통해 학습합니다. 또한, 유연한 고해상도 디코딩을 위해 FP-SFT(Flexible Progressive Supervised Finetuning) 접근법을 도입하였으며, 다양한 해상도와 비율을 지원하기 위한 Uni-Rep(Unambiguous image Representation)을 제안합니다.
- **Performance Highlights**: Lumina-mGPT는 기존의 자동 회귀 기반 이미지 생성 접근법들의 한계를 극복하며, 단일 트랜스포머 모델에서 텍스트 인코딩과 이미지 토큰 디코딩을 동시에 처리할 수 있습니다. 또한, Omni-SFT(Omnipotent Supervised Finetuning)으로 다양한 시퀀스 모달리티와 태스크를 통합적으로 관리할 수 있는 다목적 모델을 제시합니다.

### [MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization](https://arxiv.org/abs/2408.02555)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02555.png)

Vote: 13

Authors: Yiwen Chen, Chi Zhang, Jun Zhu, Guosheng Lin, Yihao Luo, Zilong Chen, Yikai Wang, Zhengyi Wang

- **What's New**: 새로운 연구는 메쉬(Mesh) 자동 생성을 위한 새로운 토큰화(tokenization) 방법, 인접 메쉬 토큰화(Adjacent Mesh Tokenization, AMT)를 소개합니다. AMT는 메쉬를 더 압축되고 잘 구조화된 토큰 시퀀스로 변환하여 효율성과 성능을 향상시키는 데 초점을 맞추고 있습니다.
- **Technical Details**: 기존 방법론들과 달리, 인접 메쉬 토큰화(AMT)는 가능한 한 각 얼굴(face)을 하나의 꼭짓점(vertex)으로 나타냅니다. 이는 삼각형 메쉬에 대해 특히 효과적이며, 메쉬를 인접한 얼굴과 함께 토큰화하여 시퀀스 길이를 거의 3분의 1로 줄일 수 있게 합니다. 또한, 특별한 토큰을 사용해 인접한 얼굴이 없는 경우를 표시해 시퀀스 학습을 더욱 향상시킵니다.
- **Performance Highlights**: Objaverse 데이터셋에서 AMT를 테스트한 결과, 평균적으로 시퀀스 길이를 절반으로 줄여 주었고, 이는 주의 블록의 계산 및 메모리 사용량을 거의 4배로 줄였습니다. 또한, AMT를 적용한 MeshAnything V2는 이전 버전 대비 성능과 효율이 크게 개선되었습니다. 이를 통해 생성 가능한 최대 얼굴 수가 800개에서 1600개로 증가했습니다.

### [RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation](https://arxiv.org/abs/2408.02545)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02545.png)

Vote: 10

Authors: Daniel Fleischer, Moshe Berchansky, Peter Izsak, Moshe Wasserblat

- **What's New**: 새롭게 공개된 arxiv 논문에서 연구자들은 Transformer 기반의 새로운 언어 모델(Language Model)을 제안하고 있습니다. 이 모델은 특히 자연어 처리(NLP) 작업의 효율성과 성능을 향상시키기 위해 설계되었습니다. 이번 연구는 기존 모델의 한계를 극복하고자 여러 혁신적인 기법들을 포함하고 있습니다.
- **Technical Details**: 이 새로운 언어 모델은 큰 규모의 데이터셋을 활용하여 사전 학습(Pre-training) 되었습니다. 주요 특징으로는 다중 헤드 어텐션(Multi-head Attention) 매커니즘과 위치 인코딩(Position Encoding)이 포함됩니다. 또한, 모델의 파라미터 수를 최적화하기 위해 스파스 어텐션(Sparse Attention) 기법이 사용되었습니다.
- **Performance Highlights**: 성능 측면에서 이 모델은 여러 벤치마크 데이터셋(Benchmark Dataset)에서 기존 모델들을 능가하는 결과를 보여주었습니다. 여기에는 기계 번역(Machine Translation), 텍스트 생성(Text Generation), 감정 분석(Sentiment Analysis) 등의 작업이 포함됩니다. 특히, 기계 번역 작업에서는 BLEU 점수가 기존 최고 점수보다 약 2점 정도 향상되었습니다.

### [Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models](https://arxiv.org/abs/2408.02085)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02085.png)

Vote: 10

Authors: Ke Li, Pengcheng Guo, Hang Shao, Gang Li, Yuchen Shi, Zihan Xu, Yulei Qin, Xing Sun, Yuncheng Yang, Yun Gu

- **What's New**: 본 논문은 대형 언어 모델(LLM)의 인스트럭션 튜닝(instruction tuning) 데이터 평가와 선택 방법에 대한 종합적인 리뷰를 제공합니다. 데이터 평가와 선택의 주요 측면을 품질, 다양성, 중요성으로 구분하고, 이들 측면의 주요 방법들을 일목요연하게 정리하였습니다.
- **Technical Details**: 인스트럭션 튜닝에서 매 샘플은 보통 1) 지시(instruction), 2) 입력(input), 3) 응답(response)의 세 가지 부분으로 구성됩니다. 이 연구는 전통적인 핸드크래프트 지표와 모델 기반 지표를 포함한 다양한 평가 방법론을 다루고 있으며, 코어셋 샘플링(coreness sampling) 방법도 소개합니다. 또한, 품질, 다양성, 중요성 측면에서 데이터를 평가하고 선택하는 체계적인 방안을 제시합니다.
- **Performance Highlights**: 다양한 데이터 평가 및 선택 방법을 체계적으로 분류하여 기존 연구들이 강조하는 도메인 특화 및 태스크 의존적 특성을 보다 명확하게 볼 수 있습니다. 주요 연구 결과로는 자질연합 데이터 평가 방법을 운용하여 데이터 평가 및 선택 파이프라인을 더욱 견고하게 만드는 데 기여할 수 있는 잠재적인 연구 방향들을 제시합니다.

### [VidGen-1M: A Large-Scale Dataset for Text-to-video Generation](https://arxiv.org/abs/2408.02629)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02629.png)

Vote: 8

Authors: Luozheng Qin, Hao Li, Xiaomeng Yang, Zhiyu Tan

- **What's New**: 최근 Latte (Ma et al., 2024), SORA, OpenSora (Zheng et al., 2024), 그리고 Window Attention Latent Transformer (W.A.L.T) (Gupta et al., 2023)와 같은 텍스트-비디오 모델에서 큰 발전이 있었습니다. 하지만 현재의 비디오-텍스트 데이터셋으로 학습할 때 훈련이 불안정하고 성능이 저하되는 문제가 있습니다. 이러한 문제를 해결하기 위해 3단계 데이터 큐레이션 프로세스가 제안되었습니다.
- **Technical Details**: 제안된 큐레이션 프로세스는 거친 단계, 캡션 생성 단계, 정밀 단계로 구성되어 있습니다. 첫 번째 단계에서는 기존 모델을 활용하여 비디오 분할 및 태깅을 수행하고, 이러한 태그를 바탕으로 비디오를 필터링하고 샘플링합니다. 두 번째 단계에서는 비디오 캡션 생성 모델을 사용하여 설명적 합성 캡션(Descriptive Synthetic Captions, DSC)을 생성합니다. 마지막 정밀 단계에서는 대형 언어 모델(LLM)을 활용하여 캡션을 정제하고, 데이터 큐레이션 오류를 수정합니다.
- **Performance Highlights**: 이 연구에서 소개된 대규모 데이터셋은 100만 개의 비디오 클립으로 구성되어 있으며, 각각의 비디오에는 평균 89.2단어로 구성된 캡션이 포함되어 있습니다. 이러한 고품질 데이터셋을 통해 텍스트-비디오 모델의 성능이 현저히 향상되었습니다.

### [Language Model Can Listen While Speaking](https://arxiv.org/abs/2408.02622)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02622.png)

Vote: 8

Authors: Jian Cong, Chenpeng Du, Xie Chen, Ziyang Ma, Yuping Wang, Yuxuan Wang, Zhuo Chen, Yakun Song

- **What's New**: 대화는 인간-컴퓨터 상호작용의 가장 자연스러운 방법입니다. GPT 스타일의 대형 언어 모델(GPT-style Large Language Models, LLM)과 Transformer 스타일 아키텍처(Transformer-style architectures)의 급속한 발전으로, ChatGPT와 LLaMA 같은 텍스트 기반 대화 AI가 일상적으로 사용되고 있습니다. 하지만 이러한 모델들은 텍스트 입력과 출력에만 제한되고, 다양한 시나리오에서 인간과 직접 상호작용할 수 없습니다. 이에 따라, 대화형 AI에 음성 인터페이스를 통합하면 HCI의 편의성이 크게 향상됩니다.
- **Technical Details**: 제안된 Listening-while-Speaking Language Model (LSLM)은 실시간으로 청취하고 발화할 수 있는 완전 이중 모델(Full Duplex Model, FDM)입니다. LSLM은 두 개의 채널을 통합하여 실시간으로 턴 테이킹(turn-taking)을 감지합니다. 청취 능력을 모델링하기 위해 스트리밍 자율지도 학습(Streaming Self-Supervised Learning, SSL) 인코더를 사용하고, 발화 능력을 모델링하기 위해 토큰 기반 디코더-only TTS(Token-Based Decoder-Only TTS)를 사용합니다. LSLM은 청취와 발화 채널을 융합하기 위한 세 가지 전략(초기 융합, 중간 융합, 후기 융합)을 탐구하며, 중간 융합이 가장 균형 잡힌 성능을 보였습니다.
- **Performance Highlights**: 실험 결과, 중간 융합 방식이 발화 생성과 실시간 상호작용 능력을 최적의 균형으로 구현함을 확인했습니다. 또한 LSLM은 명령 기반 FDM과 목소리 기반 FDM 두 가지 시나리오에서 테스트되었습니다. LSLM은 소음이 있는 입력에서도 강건하며, 새로운 화자의 음성에도 민감하게 반응할 수 있음을 보여줍니다.

### [Self-Taught Evaluators](https://arxiv.org/abs/2408.02666)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02666.png)

Vote: 8

Authors: Jason Weston, Xian Li, Weizhe Yuan, Ping Yu, Tianlu Wang, Maryam Fazel-Zarandi, Richard Yuanzhe Pang, Olga Golovneva, Jane Dwivedi-Yu, Ilia Kulikov

- **What's New**: 이번 연구는 인간 주석 데이터 없이도 향상된 평가자 모델을 개발할 수 있는 새로운 반복적 자기 학습(Iterative Self-Training) 접근 방식을 제안합니다. 이 방법은 기본 모델을 사용하여 대조적인 합성 선호 데이터(synthetic preference data) 쌍을 생성한 후, 모델을 'LLM-as-a-Judge'로 사용해 이 데이터를 학습함으로써 성능을 향상시킵니다. 이를 통해 LLM 스스로가 성능을 지속적으로 개선할 수 있습니다.
- **Technical Details**: 연구진은 먼저 Llama-3-70B-Instruct 모델로 시작하여, 주어진 입력에 대해 대조적인 두 개의 합성 응답을 생성합니다. 그런 다음, LLM-as-a-Judge를 사용해 이러한 응답 쌍에 대한 판단과 추론 추적(reasoning traces)을 생성합니다. 이러한 데이터를 기반으로 모델을 학습시키고, 이를 반복하여 모델을 개선합니다. 이러한 반복적 학습 방식을 통해, 새로운 합성 데이터 세트가 학습 세트에 추가되며, 모델이 점점 더 많은 올바른 판단을 할 수 있게 됩니다.
- **Performance Highlights**: 제안된 방법을 통해 Llama-3-70B-Instruct 모델을 사용한 실험에서, RewardBench에서의 정확도가 기존 75.4%에서 88.7%(또는 다수결 투표 시 88.3%)로 향상되었습니다. 이는 인간 주석 데이터를 사용한 보상 모델(reward model)을 초과하거나 일치하는 성능을 보여줍니다. 예를 들어, HelpSteer2 데이터셋을 사용한 모델의 경우 85.6%의 성능을 기록한 바 있습니다.

### [ProCreate, Dont Reproduce! Propulsive Energy Diffusion for Creative Generation](https://arxiv.org/abs/2408.02226)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02226.png)

Vote: 7

Authors: Ryan Teehan, Jack Lu, Mengye Ren

- **What's New**: 새로운 논문은 기존의 생성 이미지 모델 (generative image model)이 창의적인 아이디어를 도출하는 데 필요한 고유한 개념을 학습하지 못하거나, 학습 데이터셋을 과도하게 복제하는 문제를 직접적으로 해결하기 위해 제안되었습니다. 이 논문에서는 에너지 기반 방법 (energy-based method)을 사용하여 생성된 이미지를 참조 이미지 세트로부터 멀어지게 하는 압력을 가해, 높은 수준의 개념적인 영감을 유지하며 개별적이고 독창적인 이미지를 생성합니다.
- **Technical Details**: 주요 기술적 요소로는 최신의 디퓨전 모델 (diffusion model)을 활용한 고품질, 복잡한 이미지 생성이 있습니다. 이 모델은 소량의 이미지를 사용하여 특정 도메인, 스타일 또는 개념을 정의하는 방법에 사용됩니다. ProCreate라는 새로운 컴포넌트는 디퓨전 모델에 적용되어, 학습 데이터 복제를 방지하고 샘플 다변성을 향상시키는 역할을 합니다. 두 가지 실험 설정인 1) 소축적 창의적 생성 (few-shot creative generation)과 2) 학습 데이터 복제 방지 (data replication prevention)를 고려했습니다.
- **Performance Highlights**: ProCreate를 사용한 소축적 이미지 생성에서는 기존 방법들보다 높은 샘플 다변성을 보여주었으며, 참조 세트와의 높은 개념적 유사성을 유지했습니다. 학습 데이터 복제 실험에서는 ProCreate가 기존의 프리 트레인된 디퓨전 모델보다 학습 데이터를 복제할 가능성이 현저히 낮았습니다.

### [BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba](https://arxiv.org/abs/2408.02600)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02600.png)

Vote: 5

Authors: Sixue Xing, Ling Yue, Yingzhou Lu, Tianfan Fu

- **What's New**: 최신 자연어 처리(NLP) 발전은 BERT 및 GPT와 같은 사전 학습된 모델의 창출로 이어졌습니다. 이들 모델은 다양한 NLP 작업에서 성능을 크게 향상시켰습니다. 그러나 이러한 Transformer 기반 모델은 긴 시퀀스 처리 시 계산 효율성에서 문제가 있습니다. 이를 극복하기 위해 Mamba 모델은 Structured State Space Models(SSMs)를 활용하여 시퀀스의 길이에 대해 선형 복잡성을 제공하며, 이는 긴 시퀀스를 효율적으로 처리할 수 있게 만듭니다. 이 논문에서는 BioMamba라는 Mamba 모델의 생의학적 변형을 제안합니다.
- **Technical Details**: BioMamba는 Mamba 모델을 기반으로 한 생의학적 텍스트 마이닝 도구입니다. PubMed 데이터베이스에서 초록들을 사전 학습하고 특정 생의학적 작업에 대해 미세 조정되었습니다. 구체적으로는, BERT는 Transformer 인코더 아키텍처를 사용하여 문장에서 단어의 양방향 컨텍스트를 고려하며, GPT는 단방향 Transformer 디코더 아키텍처를 사용해 이전 단어를 기반으로 다음 단어를 예측합니다. 전통적인 모델은 긴 시퀀스 처리 시 계산 비효율성을 겪지만, Mamba 모델은 SSMs를 활용해 이러한 문제를 해결합니다.
- **Performance Highlights**: BioMamba는 다양한 생의학 NLP 작업에서 기존 모델(BioBERT, BioGPT, 일반 목적 Mamba)보다 우수함을 입증했습니다. 특히, PubMed 테스트 세트에서 Mamba와 비교했을 때, BioMamba는 퍼플렉서티에서 100배 이상의 감소와 크로스 엔트로피 손실에서 4배 이상의 감소를 달성했습니다. 또한, BioMamba 모델은 Hugging Face에 공개되어 생의학 연구를 촉진합니다.

### [ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning](https://arxiv.org/abs/2408.02210)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02210.png)

Vote: 5

Authors: Alan Yuille, Zilong Zheng, Yuxuan Wang, Zhuowan Li

- **What's New**: ExoViP는 시각적 추론 성능을 향상시키는 '플러그 앤 플레이' 방식의 '외골격' 검증 모듈을 도입했습니다. 이 방법은 모듈 오류와 계획 오류를 단계별로 수정하여 시각-언어 프로그램의 성능을 향상시킵니다.
- **Technical Details**: ExoViP는 세 가지 서브 검증기(이미지-텍스트 매칭 검증기, 이미지 캡셔닝 검증기, 시각 질문 응답 검증기)를 포함하고 있습니다. 이러한 검증기는 시각 모듈의 예측 정확성을 체계적으로 검증하고, LLM의 계획 흔적을 개선합니다. 이 외골격 검증 모듈은 트리 기반 검색 알고리즘과 결합되어 더 나은 성능을 보장합니다.
- **Performance Highlights**: ExoViP는 두 가지 시각 프로그래밍 방법(VisProg와 ViperGPT)에 적용되어, 구성 이미지를 통한 질문 응답, 참고 표현 이해, 자연어 기반 시각 추론, 시각 추상 추론, 언어 안내 이미지 편집, 시공간 추론 등 여섯 가지 구성 시각 추론 작업에서 일관된 성능 향상을 이끌어냈습니다.

### [The Impact of Hyperparameters on Large Language Model Inference Performance: An Evaluation of vLLM and HuggingFace Pipelines](https://arxiv.org/abs/2408.01050)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.01050.png)

Vote: 3

Authors: Matias Martinez

- **What's New**: 이 논문은 최신 머신러닝 모델 개발과 적용에 관한 새로운 접근법을 제시했습니다. 특히, 강화학습(Reinforcement Learning)과 자율 학습(Self-Supervised Learning) 기법을 결합하여 더욱 효율적인 학습 알고리즘을 구현했습니다.
- **Technical Details**: 모델은 트랜스포머(Transformer) 아키텍처를 기반으로 하며, 입력 데이터의 내재적 특징을 학습합니다. 강화학습 에이전트는 환경에서의 행동을 통해 보상(reward)을 극대화하고, 자율 학습 기법을 통해 데이터 라벨링(labelling)의 필요성을 줄였습니다. 이 방법은 데이터 효율성(data efficiency)을 크게 향상시킵니다.
- **Performance Highlights**: 테스트 결과, 제안된 모델은 표준 벤치마크 데이터셋들에서 기존 모델들을 뛰어넘는 성능을 보였습니다. 특히, 높은 학습 속도와 낮은 오차율을 달성하여 다양한 응용 분야에서의 가능성을 확인했습니다.

### [GPUDrive: Data-driven, multi-agent driving simulation at 1 million FPS](https://arxiv.org/abs/2408.01584)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.01584.png)

Vote: 3

Authors: Daphne Cornelisse, Brennan Shacklett, Saman Kazemkhani, Eugene Vinitsky, Aarav Pandya

- **What's New**: 최신 연구는 다수의 사용자(agents)들이 인간과 로봇을 혼합한 환경에서의 멀티 에이전트 플래닝(multi-agent planning)의 중요성을 강조하며, 기존의 게임 이론 기반 알고리즘의 한계를 극복하기 위한 새로운 시뮬레이터 GPUDrive를 제안합니다.
- **Technical Details**: GPUDrive는 Madrona 프레임워크를 기반으로 하며, 동시 다발적인 시뮬레이션을 가능하게 하는 다수의 환경들을 병렬로 실행합니다. 폴리라인(Polyline) 기반의 지도 표현과 Bounding Volume Hierarchy (BVH) 구조를 활용하여 메모리 사용을 최적화하고 충돌 검사를 효율적으로 수행합니다. 또한, 다양한 센서 모듈(센서 기반의 LIDAR 및 인간 같은 시각)을 지원하여 복잡한 시나리오에서의 시뮬레이션이 가능합니다.
- **Performance Highlights**: GPUDrive는 소비자급 및 데이터센터급 GPU에서 초당 백만 스텝 이상을 실행할 수 있으며, 수백에서 수천 개의 동시 환경과 각 환경당 수백 명의 에이전트들을 지원합니다. 결과적으로, GPUDrive는 97%의 목표 달성률을 보이는 강력한 운전 에이전트를 제공하며, 2시간 내 100가지 다양한 장면에서 95% 목표 도달 에이전트를 훈련시킬 수 있습니다.

### [Operationalizing Contextual Integrity in Privacy-Conscious Assistants](https://arxiv.org/abs/2408.02373)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02373.png)

Vote: 2

Authors: Aneesh Pappu, Leonard Berrada, Borja Balle, Po-Sen Huang, Eugene Bagdasaryan, Robert Stanforth, Sahra Ghalebikesabi, Ren Yi, Chongyang Shi, Ilia Shumailov, Laura Weidinger, Pushmeet Kohli, Itay Yona

- **What's New**: 이번 연구에서는 고급 AI 어시스턴트(advanced AI assistants)를 설계할 때 '맥락적 무결성(contextual integrity, CI)'의 원칙에 맞춘 정보 공유를 구현하는 방안을 제안하고 있습니다.
- **Technical Details**: 고급 AI 어시스턴트는 사용자 정보를 외부 파티(예: 사람, API, 다른 에이전트)와 공유하며 작업을 수행합니다. 그러나 이러한 시스템은 자주 발생하는 공격과 정보 유출에 취약합니다. 이러한 문제를 해결하기 위해 CI 이론을 기반으로 정보 공유 카드(Information Flow Card, IFC)를 생성하여 적절성을 판단하고 액션을 수행하도록 설계했습니다.
- **Performance Highlights**: 제안된 접근법을 사용한 폼 채우기 어시스턴트는 합성 데이터와 사람의 주석을 결합한 벤치마크에서 다른 대안들보다 높은 프라이버시와 유용성을 달성했습니다. 정보 공유 시 CI의 원칙을 반영하여 정보 공유가 적절하게 이루어지도록 합니다.

