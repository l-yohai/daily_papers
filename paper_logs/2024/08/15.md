## Daily Papers (2024-08-15)

### [Seeing and Understanding: Bridging Vision with Chemical Knowledge Via ChemVLM](https://arxiv.org/abs/2408.07246)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.07246.png)

Vote: 14

Authors: Di Zhang, Jingdi Lei, Wenhai Wang, Zeying Hao, Cai Zhou, Junxian Li, Zhe Chen, Wei Li, Mao Su, Dongzhan Zhou, Qian Tan, Shufei Zhang, Weiyun Wang, Wanli Ouyang, Yuqiang Li, Wei Liu, Xunzhi Wang

- **What's New**: 화학 분야에서 연구와 분석 능력을 강화할 수 있는 새로운 모델인 ChemVLM이 제안되었습니다. 이 모델은 비전 트랜스포머(ViT), 멀티레이어 퍼셉트론(MLP), 대형 언어 모델(LLM)을 기반으로 하여, 화학 이미지와 텍스트 간의 포괄적인 추론을 수행할 수 있습니다.
- **Technical Details**: ChemVLM은 ViT-MLP-LLM 아키텍처를 따르며, InternVIT-6B를 이미지 특징 추출기로, ChemLLM-20B를 언어 모델로 사용합니다. 텍스트-이미지 모달 정렬을 위한 프로젝트를 수행하는 MLP도 포함되어 있습니다. 다양한 화학 이미지를 이해하고, 텍스트와 시각적 정보를 결합하여 복잡한 화학 데이터를 효과적으로 처리합니다.
- **Performance Highlights**: ChemVLM은 기존의 문서 중심 모델과 MLLM보다 훨씬 뛰어난 성능을 보여주었으며, 여러 기준 작업에서 최첨단(SOTA) 성과를 달성했습니다. 특히 GPT-4 비전 모델을 능가하는 성과를 기록하였습니다. 이 모델은 화학 이미지 이해와 텍스트-이미지 추론에서 상당한 장점을 갖추고 있습니다.
- **New Datasets**: 효율적이고 효과적인 모델 평가를 위해 ChemOCR, MMChemExam, MMChemBench라는 세 가지 혁신적인 데이터셋을 제공했습니다. 이 데이터셋은 화학 연구에 필요한 다양한 데이터 종류를 포괄하며, 화학 도메인 시각-언어 모델의 성능을 종합적으로 평가할 수 있는 벤치마크를 제공합니다.
- **Applications**: ChemVLM은 문서 지능, 분자 설계, 제약 연구와 같은 다양한 응용 분야에서 높은 잠재력을 가지고 있습니다. 이 모델은 지능형 문서 분석, 분자 설계 지원 및 약물 발견을 위한 능력을 크게 향상시킬 수 있습니다.

### [Generative Photomontage](https://arxiv.org/abs/2408.07116)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.07116.png)

Vote: 10

Authors: Jun-Yan Zhu, Ariel Shamir, Nupur Kumari, Sean J. Liu

- **What's New**: 최신의 텍스트-이미지 생성 모델들은 텍스트 프롬프트와 스케치를 입력 조건으로 이용하여 시각적으로 설득력 있는 이미지를 생성할 수 있지만, 사용자 의도를 정확히 반영하는 데 어려움을 겪습니다. 이 논문에서는 사용자에게 원하는 이미지를 합성할 수 있는 새로운 접근 방식을 제안합니다. 사용자는 여러 생성된 이미지 중에서 원하는 부분을 선택하여 최종 이미지를 구성할 수 있으며, 이를 'Generative Photomontage'라고 부릅니다.
- **Technical Details**: 제안된 프레임워크는 ControlNet을 사용하여 동일한 텍스트 프롬프트와 제어 입력으로 생성된 이미지 스택에서 시작합니다. 사용자는 간단한 브러시 스트로크를 통해 다른 이미지에서 원하는 부분을 선택할 수 있습니다. 핵심 아이디어는 이 이미지들이 동일한 입력 조건에서 생성된 공통 공간 구조를 공유한다는 점을 활용하여 최종 이미지로 합성하는 것입니다. 제안된 기술은 사용자의 입력을 받아 다중 레이블 그래프 기반 최적화를 통해 이미지 부분을 세그먼트화하며, 이후 최종 노이즈 제거 과정에서 이 부분들을 합성합니다.
- **Performance Highlights**: 사용자가 선택한 영역을 정확히 보존하면서 이들을 조화롭게 블렌딩할 수 있는 이 접근 방식은 기존의 픽셀-스페이스 블렌딩 방법을 능가합니다. 제안된 방법은 새로운 외형 조합 생성, 형태 불일치 수정, 아티팩트 감소, 프롬프트 정렬 향상 등의 다양한 응용 분야에서 시각적으로 설득력 있는 결과를 제공합니다.

### [InfinityMATH: A Scalable Instruction Tuning Dataset in Programmatic Mathematical Reasoning](https://arxiv.org/abs/2408.07089)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.07089.png)

Vote: 7

Authors: Yan Yan, Lin Li, Guang Liu, Bo-Wen Zhang

- **What's New**: 이번 연구에서는 수학적 추론을 통한 LLMs의 성능 평가 중요성을 강조하며, 수학 문제에 대한 논리적 일관성을 개선하고자 InfinityMath를 도입했습니다. InfinityMath는 '무한한 수학적 명령어 튜닝 데이터셋'으로, 수학 문제에서 수치 값을 추상적으로 분리해 여러 숫자들로 데이터셋을 확장합니다.
- **Technical Details**: InfinityMath의 데이터 구축에는 다단계 파이프라인이 포함됩니다. 수학 문제의 수치 값을 추출해 '범용 템플릿'을 생성하고, 이를 변형해 다양한 문제와 프로그램을 생성합니다. 이 과정에서 LLM(GPT-4 등)을 사용하여 범용 템플릿에서 벗어난 프로그램을 생성하며, 도서문자열(docstring)과 주석(comment)을 포함하여 명확성과 이해도를 높입니다. 이후, LLM 사용을 최소화하고 계산 비용을 줄이기 위해 다양한 프롬프트 템플릿을 비교하여 최적의 프롬프트를 개발했습니다.
- **Performance Highlights**: InfinityMath로 튜닝한 Llama2와 Aquila2 모델(7B 버전), 그리고 CodeLlama는 도메인 내부(in-domain)와 외부(out-of-domain) 벤치마크에서 일관된 우수 성능을 보였습니다. 또한, 논리적 불일치 문제에 대해 강화된 GSM8K+와 MATH+ 테스트셋을 사용한 결과, InfinityMath로 훈련된 모델이 다른 PoT 데이터셋을 사용한 모델보다 높은 정확도와 견고성을 나타냈습니다.

### [DeepSpeak Dataset v1.0](https://arxiv.org/abs/2408.05366)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.05366.png)

Vote: 6

Authors: Hany Farid, Matyas Bohacek, Sarah Barrington

- **What's New**: 이번에 소개된 오디오 및 비디오 데이터셋은 디지털 포렌식 커뮤니티를 지원하기 위해 설계된 것으로, 34343434시간의 실제 및 딥페이크(deepfake) 비디오가 포함되어 있습니다. 이 데이터셋은 사람들이 말하고 제스처를 취하는 영상을 담고 있습니다. 실제 비디오는 참가자의 동의를 얻어 녹화되었으며, 딥페이크 비디오는 얼굴 교체(face-swap), 립싱크(lip-sync), 오디오 딥페이크로 구성되어 있습니다. 이 데이터셋은 https://huggingface.co/datasets/faridlab/deepspeak_v1에서 필요한 경우 사용할 수 있습니다.
- **Technical Details**: 총 220명의 참가자가 Prolific 리서치 리크루트먼트 플랫폼을 통해 모집되었고, 참가자들은 자신이 녹화한 내용을 공개 데이터셋에 포함시킬 것에 동의했습니다. 참가자들은 표준 스크립트와 랜덤 스크립트를 읽고, 자유롭게 질문에 응답하며, 간단한 행동을 수행하는 등의 과정을 거쳤습니다. 녹화는 JavaScript와 Python으로 구축된 커스텀 웹 애플리케이션을 사용하여 이루어졌습니다. 모든 오디오/비디오 녹화는 .webm 형식으로 캡처되어, 나중에 .mp4 및 .wav 형식으로 변환되었습니다.
- **Performance Highlights**: 데이터셋은 총 5958개의 딥페이크 비디오가 3가지 얼굴 교체 모델과 2가지 립싱크 모델을 활용해 생성되었습니다. 이 데이터셋의 80%는 훈련용으로, 나머지 20%는 평가용으로 나뉘어졌으며, 얼굴 또는 목소리의 정체성이 겹치지 않도록 구성되었습니다. 딥페이크 생성에는 FaceFusion, FaceFusion + GAN, FaceFusion Live 등의 다양한 엔진이 사용되었습니다.

### [3D Gaussian Editing with A Single Image](https://arxiv.org/abs/2408.07540)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.07540.png)

Vote: 6

Authors: Song-Hai Zhang, Guan Luo, Fang-Lue Zhang, Xiao-Xiong Fan, Ying-Tian Liu, Tian-Xing Xu

- **What's New**: 이 논문에서는 사용자가 제공한 단일 이미지를 기반으로 3D 장면을 편집하는 새로운 방법을 제안합니다. 이는 '보이는 그대로 얻는다(What you see is what you get)'라는 철학을 따릅니다. 3D 장면은 3D Gaussian Splatting(3DGS) 방법을 사용하여 재구성되며, 이는 명시적 표현과 우수한 재건 품질을 제공합니다.
- **Technical Details**: 이 방법은 점 차이 최적화 과정으로 공식화됩니다. 사용자는 특정 관점에서 렌더링 된 이미지를 기반으로 편집된 이미지를 제공해야 합니다. 이 과정은 객체의 비강성 변형과 텍스처 변경을 포함할 수 있습니다. 특히, 3D Gaussian 표현(3DGS)을 사용하여 최적화 과정에서 포지셔널 손실을 도입하여 장거리 변형을 모델링하고 재파라미터화를 통해 그라디언트를 전파합니다.
- **Performance Highlights**: 논문은 단일 이미지 기반 3D Gaussian 장면 편집 방법을 최초로 제안하였고, 3DGS에 포지셔널 파생변수를 도입하여 장거리 변형을 캡처하고 그라디언트 전파를 가능하게 했습니다. 앵커 기반의 지극히 강성적인(As-Rigid-As-Possible, ARAP) 정규화 방법과 거칠게-자세히(coarse-to-fine) 최적화 전략을 제안하여 객체 수준의 기하학적 일관성을 유지하면서 3D 장면을 편집하는데 사용되었습니다.

### [Aquila2 Technical Report](https://arxiv.org/abs/2408.07410)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.07410.png)

Vote: 5

Authors: Zhengduo Zhang, Shuhao Gu, Xinya Wu, Bo-Wen Zhang, Guang Liu, Boyan Gao, Yulong Ao, Liangdong Wang, Jijie Li

- **What's New**: Aquila2 시리즈는 7억에서 70억까지의 파라미터 크기를 갖춘 이중언어 모델로써 새로운 HeuriMentor (HM) 프레임워크를 도입하여 훈련 효율성을 향상시켰습니다. 이 시스템은 Adaptive Training Engine(ATE), Training State Monitor(TSM), Data Management Unit(DMU)를 포함하며, 이를 통해 모델의 훈련 진행 상황을 모니터링하고 데이터 분포를 효율적으로 조정할 수 있습니다. Aquila2-34B 모델은 다양한 벤치마크 테스트에서 기존 모델들을 능가하는 성능을 보여줍니다.
- **Technical Details**: Aquila2 시리즈는 다음과 같은 기술적 특징을 가지고 있습니다:
1. **Tokenizer**: 10만 단어 크기의 Byte Pair Encoding (BPE) 토크나이저 사용.
2. **Group Query Attention(GQA)**: 전통적인 multi-head attention보다 효율적인 메커니즘 채택.
3. **Position Embedding**: Rotary Position Embedding (RoPE) 사용.
4. **ATE**: 최신 데이터 소스를 지속적으로 갱신하며, 다양한 모델 구조와 파라미터 규모에 맞춰 훈련 전략 최적화. 예: A100 40G 클러스터에서 A800 80G 클러스터로 유연하게 변경 가능.
5. **DMU**: 웹페이지 및 PDF 등에서 데이터를 수집하고, 철저한 deduplication 및 품질 필터링 실시.
- **Performance Highlights**: Aquila2-34B 모델은 21개의 다양한 데이터셋에서 LLaMA-2-70B를 포함한 다른 이중언어 모델보다 더 나은 평균 점수를 기록했습니다. 심지어 4-bit 양자화된 상태에서도 성능 저하가 최소화되었습니다. 또한, AquilaChat2-34B는 주관적 및 객관적 평가가 포함된 이중언어 벤치마크에서 LLaMA-2-70B와 그 대응 채팅 모델을 일관되게 능가했습니다.

### [PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform Generation](https://arxiv.org/abs/2408.07547)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.07547.png)

Vote: 4

Authors: Seong-Whan Lee, Ha-Yeong Choi, Sang-Hoon Lee

- **What's New**: 이번 연구에서는 PeriodWave라는 새로운 웨이브폼 생성 모델을 제안합니다. 이 모델은 고해상도의 웨이브폼 신호 생성을 위해 다양한 암시적 주기적 표현을 반영할 수 있습니다. 또한, 최적의 운송 경로를 사용하여 벡터 필드를 직접 추정할 수 있는 Flow Matching을 채택하여 빠른 샘플링을 가능하게 합니다. 저자들은 주어진 기간 수를 늘리면 전체 성능이 일관되게 향상된다는 것을 관찰했으나, 이는 느린 추론 속도를 유발할 수 있습니다. 이를 간단히 해결하기 위해, 기간별 배치 추론(period-wise batch inference)을 통해 병렬로 피드포워드 할 수 있는 기간-조건 유니버설 추정기(period-conditional universal estimator)를 제안합니다.
- **Technical Details**: PeriodWave는 멀티-프리퀀시 정보와 고해상도 웨이브폼 생성을 효율적으로 모델링하기 위해 이산 웨이브렛 변환(DWT, Discrete Wavelet Transformation)을 사용합니다. Flow Matching 기법을 활용하여 빠르고 효율적으로 벡터 필드(vector fields)를 추정하며, 이는 다양한 주기적 정보(implicit periodic representations)를 반영하는 데 중요한 역할을 합니다. 또한, Prime Number를 사용하여 중첩이 없는 다수 기간 추정기(multi-period estimator)를 채택해 성능을 최적화합니다.
- **Performance Highlights**: PeriodWave는 음성 및 분포 밖(out-of-distribution) 샘플에 대해 다른 공개된 강력한 베이스라인 대비 객관적이고 주관적인 메트릭에서 더 나은 성능을 달성했습니다. 특히, 피치 관련 메트릭(pitch distance, periodicity, V/UV F1 score)에서 유례없는 성능 개선을 보여줍니다. 또한, GAN 기반 모델들은 3주 이상의 학습 시간이 필요했던 반면, PeriodWave는 단 3일 만에 모델을 학습시킬 수 있습니다. 이번 연구는 고해상도 신호 모델링을 위해 Flow Matching을 성공적으로 활용한 첫 번째 접근법이며, 이를 통해 이전의 문제점을 해결하는 데 성공했습니다.
- **Code and Resources**: 모든 소스 코드와 체크포인트는 https://github.com/sh-lee-prml/PeriodWave에서 제공됩니다.

### [Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space](https://arxiv.org/abs/2408.07416)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.07416.png)

Vote: 3

Authors: Seoha Kim, Jeongmin Bae, Youngsik Yun, Hyunjee Lee, Youngjung Uh

- **What's New**: 이 논문은 3D 공간을 이해하는 새로운 방법을 제안하며, 특히 Neural Radiance Fields (NeRFs)와 3D Gaussian Splatting (3DGS)를 활용한 3D 볼륨 세그멘테이션에 중점을 둔다. 기존 연구들이 2D 마스크를 사용해 3D 객체를 이해하려 노력하는 반면, 본 연구는 3D 포인트의 의미론적 이해를 목표로 한다.
- **Technical Details**: 이 논문은 3D 포인트 언어 임베딩을 학습하는 새로운 방법을 제안한다. 멀티스케일 언어 임베딩 (multi-scale language embeddings)을 제거하고, 3D 점들의 임베딩을 직접 학습하도록 하여 시점 (viewpoint)에 따른 일관성을 높였다. 또한, 3D Gaussian Splatting (3DGS)에 사전 학습된 언어 필드 (pre-trained language field)를 전이시켜 첫 실시간 렌더링을 가능하게 했다.
- **Performance Highlights**: 제안한 방법은 기존 보다 2D와 3D 세그멘테이션 정확도에서 우수하며, 학습 및 렌더링 시간에서도 뛰어난 성능을 보인다. 이 방법은 이전 방법보다 27배 빠른 실시간 의미론적 렌더링을 달성하였다.
- **Evaluation**: 논문은 3D 이해 평가를 위해 기존의 mIoU 대신, 추정된 볼륨과 실제 볼륨 간의 일치를 F1-score로 측정하는 새로운 평가 프로토콜을 제안했다.

