## Daily Papers (2024-08-22)

### [TWLV-I: Analysis and Insights from Holistic Evaluation on Video Foundation Models](https://arxiv.org/abs/2408.11318)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11318.png)

Vote: 41

Authors: Jihwan Kim, Jiho Jang, Junwan Kim, Seokjin Han, Jongseok Kim, Jay Suh, Seungjoon Park, Minjoon Seo, JongMok Kim, Jaehyuk Yi, Daewoo Kim, Seongsu Ha, Jin-Young Kim, Hyeongmin Lee, Hyojun Go, Kyungjune Baek, Soonwoo Kwon, Raehyuk Jung, Jangwon Lee, GeunOh Kim, Aiden Lee

- **What's New**: 최신 논문에서는 비디오 이해 시스템을 개선하고 평가하는 새로운 방법을 제시하고, 종합적이고 공정한 평가 프레임워크와 비디오에서 출현하는 다양한 특징을 포괄적으로 이해할 수 있는 비디오 기초 모델(TWLV-I)을 도입했습니다. 이 모델은 동작(appearance)과 모션(motion) 모두를 인식할 수 있습니다.
- **Technical Details**: 기본적으로 Vision Transformer(ViT) 아키텍처를 사용하며, ViT-B(Base, 86M 파라미터)와 ViT-L(Large, 307M 파라미터)를 채택했습니다. 입력 비디오를 여러 패치로 나눈 후 트랜스포머를 통해 패치 단위의 빈 공간 임베딩을 생성합니다. 사전 학습 데이터셋으로는 Kinetics-710, HowTo360K, WebVid10M, 그리고 15M 공개된 이미지 데이터셋을 사용합니다. 프레임 샘플링에 대해선, 비디오의 길이에 관계없이 일정한 간격으로 N 프레임을 샘플링하는 방법(Uniform Embedding)과 몇 초 단위로 클립을 나눈 후 M 클립마다 N 프레임을 샘플링하는 방법(Multi-Clip Embedding)을 사용합니다. 동작 인식(Action Recognition)은 Kinetics-400, Something-Something-v2, Moments-in-Time, Diving-48, Epic-Kitchens 등의 벤치마크를 사용합니다.
- **Performance Highlights**: TWLV-I 모델은 동작 및 모션 중심의 액션 인식 벤치마크에서 두드러진 성능을 보여줍니다. 뿐만 아니라, Temporal Action Localization, Spatio-temporal Action Localization, Temporal Action Segmentation과 같은 다양한 비디오 중심 작업에서도 최첨단 성능을 보였습니다. 특히, 특정 작업을 위한 전문가 모델들과 비교했을 때도 경쟁력 있는 성능을 입증했습니다.

### [LLM Pruning and Distillation in Practice: The Minitron Approach](https://arxiv.org/abs/2408.11796)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11796.png)

Vote: 28

Authors: Bryan Catanzaro, Jan Kautz, Pavlo Molchanov, Marcin Chochowski, Sharath Turuvekere Sreenivas, Raviraj Joshi, Mohammad Shoeybi, Saurav Muralidharan, Mostofa Patwary

- **What's New**: 본 논문에서는 Minitron 모델 압축 전략을 이용하여 최신 대규모 언어 모델인 Llama 3.1 8B와 Mistral NeMo 12B 모델을 각각 4B 및 8B 파라미터로 압축했습니다. 특히 원본 데이터에 접근할 수 없는 상황을 고려하여 teacher 모델을 자체 데이터셋으로 미세 조정한 후 가지치기(pruning) 및 지식 증류(Knowledge Distillation)를 수행하는 방법론을 제안했습니다.
- **Technical Details**: 주요 접근 방식은 다음과 같습니다. 먼저, 목표 데이터셋으로 teacher 모델을 미세 조정(teacher correction)한 후 모델 가지치기 및 지식 증류를 통해 압축된 모델의 정확성을 회복합니다. 가지치기는 중요도 추정(Importance Estimation)을 통해 층(layer), 신경(neuron), 머리(head) 및 임베딩 차원에서 중요도를 계산하고, 이를 기반으로 가지치기를 수행합니다. 추후 정확성 회복을 위해 두 가지 재훈련 전략, 즉 전통적인 훈련과 지식 증류를 사용합니다. 지식 증류는 있는 그대로 가르치기(teacher) 모델과 가지치기된 학생(student) 모델 간에 KL Divergence 손실을 사용합니다.
- **Performance Highlights**: MN-Minitron-8B 모델은 원본 Mistral NeMo 12B 모델 대비 평균 1.2배 빠른 추론 속도를 제공합니다. Llama-3.1-Minitron-4B 모델은 깊이(depth) 가지치기 변형과 넓이(width) 가지치기 변형에서 각각 평균 2.7배, 1.8배 빠른 추론 속도를 보였습니다. 또한, MN-Minitron-8B 모델은 일반적인 언어 모델링 벤치마크에서 동일한 규모의 다른 모델들을 능가했습니다. Llama-3.1-Minitron-4B 모델 역시 teacher 모델과 이전 세대의 Minitron-4B 모델 대비 강력한 정확성을 보였습니다.

### [FocusLLM: Scaling LLM's Context by Parallel Decoding](https://arxiv.org/abs/2408.11745)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11745.png)

Vote: 10

Authors: Yutao Sun, Jianyong Wang, Tengyu Pan, Zixuan Wang, Zhichao Duan, Junjie Fang, Zhenyu Li, Yike Zhang, Rong Han

- **What's New**: FocusLLM은 대형 언어 모델(LLM)의 문맥 길이를 매우 효율적으로 확장할 수 있는 새로운 프레임워크를 제안합니다. FocusLLM은 특히 긴 텍스트를 처리하면서 높은 퍼포먼스를 유지할 수 있도록 병렬 디코딩을 활용하는 모델입니다. 이 모델은 대형 문서의 분석, 요약, 질문 응답 등의 작업에서 기억해야 할 정보를 효과적으로 활용할 수 있습니다.
- **Technical Details**: FocusLLM는 두 가지 주요 특징을 가지고 있습니다. 첫째, 오리지널 모델 파라미터를 동결(freeze)하여 일반화 능력을 유지합니다. 둘째, 병렬 디코딩을 통해 다양한 청크(chunk)에서 정보를 집계할 수 있도록 소수의 트레이너블 파라미터(trainable parameters)를 추가합니다. 긴 텍스트를 효율적으로 처리하기 위해 우리는 길이가 작은 청크들로 문서를 분할하고 각각의 청크에서 병렬 디코딩을 수행하여 최종적으로 종합된 결과를 도출합니다.
- **Performance Highlights**: FocusLLM은 긴 텍스트를 처리할 때 낮은 퍼플렉시티(perplexity)를 유지하면서도 정밀한 이해가 필요한 작업에서 뛰어난 성과를 보여줍니다. FocusLLM의 평가 결과, Longbench와 ∞-Bench 벤치마크에서 뛰어난 퍼포먼스를 기록했으며, 기존의 길이 확장 모델과 지속적 학습 모델을 능가하는 성과를 나타냈습니다.

### [TrackGo: A Flexible and Efficient Method for Controllable Video Generation](https://arxiv.org/abs/2408.11475)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11475.png)

Vote: 9

Authors: Haitao Zhou, Rui Nie, Changhu Wang, Qian Yu, Chuang Wang, Dongdong Yu, Jinxiao Lin

- **What's New**: 새로운 논문은 동영상 생성에서 'TrackGo'라는 새로운 접근 방식을 소개합니다. 이 접근 방식은 사용자가 제공하는 자유 형태의 마스크와 화살표를 사용하여 목표 영역과 이동 궤적을 정의하는 것입니다.
- **Technical Details**: TrackGo는 두 가지 주요 단계를 거칩니다: Point Trajectories Generation(포인트 궤적 생성)과 Conditional Video Generation(조건부 동영상 생성). 첫 번째 단계에서는 사용자 정의 마스크와 화살표에서 궤적을 자동으로 추출하고, 두 번째 단계에서는 Stable Video Diffusion Model (SVD)과 새로운 모션 정보를 인코딩하는 인코더를 기반으로 동영상을 생성합니다. TrackAdapter이라는 새로운 구성 요소를 사용하여, 기존의 시간적 자기-주의 (temporal self-attention) 레이어를 수정하여 모션 제어 정보를 효과적으로 통합합니다.
- **Performance Highlights**: TrackGo는 기존 모델보다 뛰어난 비디오 품질(FVD), 이미지 품질(FID), 모션 충실도(ObjMC)를 보여주며, 정교하고 효율적인 모션 제어를 제공합니다. 실험 결과에서 TrackGo는 더 높은 품질의 동영상을 생성하는 것으로 나타났습니다.

### [GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models](https://arxiv.org/abs/2408.11817)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11817.png)

Vote: 4

Authors: Jonathan Roberts, Samuel Albanie, Kai Han

- **What's New**: 최신 컴퓨팅 자원 증가, 활발해진 연구 커뮤니티 및 상업적 기회 증가로 인해 대규모 멀티모달 모델(Large Multimodal Models)의 성능이 빠르게 향상되고 있습니다. 특히 평가와 벤치마킹의 중요성이 커지고 있습니다. 이를 해결하기 위해 우리 팀은 2170개의 질문으로 구성된 새로운 벤치마크인 GRaph Analysis Benchmark(GRAB)을 소개합니다.
- **Technical Details**: GRAB의 모든 도표는 Matplotlib 라이브러리를 사용하여 합성적으로 생성되었습니다. 이는 질문의 복잡성을 제어하고, 정확한 진실값(ground truth)을 자동으로 제공하며, 데이터 오염 가능성을 줄이는 등 여러 가지 이점을 제공합니다. GRAB의 질문들은 평균(mean), 분산(variance), 사분범위(interquartile range), 기울기(gradient), 정지점(stationary point) 등의 그래프 속성들을 평가하는 다양한 문제 유형을 포함합니다. 20개의 폐쇄형 LMM을 평가한 결과, 최고 성능을 보인 모델조차 21.7%의 점수에 그쳤습니다.
- **Performance Highlights**: GRAB 벤치마크에 대해 평가된 20개의 LMM 중 대부분의 모델이 어려움에 직면하며, 최고 모델조차도 21.7%의 점수를 기록했습니다. 이는 현재 최첨단 모델들이 직면한 도전과제를 명확히 보여줍니다.

### [Iterative Object Count Optimization for Text-to-image Diffusion Models](https://arxiv.org/abs/2408.11721)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/1645002335184-617fb19badaa26f5e57e97cd.png)

Vote: 3

Authors: Idan Schwartz, Oz Zafar, Lior Wolf

- **What's New**: 본 논문에서는 텍스트-이미지 변환(dieffen) 모델에서 구체적인 오브젝트 개수를 정확하게 표현하는 새로운 방법을 제시합니다. 본 연구는 특히 이미지를 생성하는 과정에서 오브젝트 개수를 카운팅하기 위한 메커니즘을 도입하여, 주어진 개수를 정확하게 반영할 수 있도록 최적화된 수량 정렬 목표(quantity alignment objective)를 채택했습니다.
- **Technical Details**: 기술적인 측면에서는, 새로운 입력 토큰인 'counting token'의 임베딩을 최적화하여 원하는 오브젝트 개수를 달성하는 방법을 설명합니다. 이 과정은 생성된 이미지에 대해 카운팅 손실(counting loss)을 적용하고, 각 시간 단계에서 검출 모델의 스케일된 카운팅 손실 잠재력을 사용하여 반복적으로 실행됩니다. 최적화된 후 이 토큰은 추가 최적화 없이 다른 배경 및 클래스의 이미지를 생성하는 데 재사용할 수 있습니다. 이 방법은 파생 가능한 잠재 신호를 비파생 가능(non-derivable) 점수와 결합하여 어떤 검출 메커니즘을 기반으로 최적화할 수 있게 합니다.
- **Performance Highlights**: 우리의 방법은 최근 최신 이미지 생성 방법과 비교했을 때 품질과 정확도 측면에서 상당한 개선을 보여줍니다. 예를 들어, 초기 베이스라인에서 생성된 30개의 공을 최적화 과정을 통해 10개의 공으로 줄이는 과정을 성공적으로 시연했습니다. 또한 동일한 토큰을 다른 프롬프트에서도 정확하게 오브젝트 수를 유지하며 재사용할 수 있음을 입증했습니다.

### [Scaling Cross-Embodied Learning: One Policy for Manipulation, Navigation, Locomotion and Aviation](https://arxiv.org/abs/2408.11812)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11812.png)

Vote: 2

Authors: Sudeep Dasari, Homer Walke, Ria Doshi, Oier Mees, Sergey Levine

- **What's New**: 최신 기계 학습의 성공은 점점 더 다양한 멀티태스크 데이터에 일반 목적 모델을 훈련시키는 데서 나온 것입니다. 이 논문에서는 비전-언어(vision-language) 모델이 다양한 데이터와 작업에서 유의미하게 향상된 성능을 보여준 것처럼, 로봇 정책(robot policy)에서도 여러 환경과 구성의 데이터를 통해 일반 목적의 크로스 엠보디먼트(cross-embodied) 정책이 개별 로봇에 최적화된 정책보다 우수한 성능을 보인다고 주장합니다.
- **Technical Details**: 여러 로봇 시스템의 관찰(observations)과 행동(actions) 데이터를 시퀀스로 변환하여, 이를 처리할 수 있는 트랜스포머 기반 정책(transformer-based policy)을 제안합니다. 이 방법은 각각 관찰 종류 및 행동 타입에 따라 토큰화(tokenizing) 및 시퀀싱(arranging)을 통해 관찰과 행동을 처리합니다. 입력 토큰 시퀀스에는 행동 판독 토큰(action readout tokens)을 삽입하고, 이를 통해 각 행동 타입에 맞는 출력 벡터를 생성합니다. 이 정책은 언어 지시(input modalities)를 통해 작업을 받아들일 수 있으며, 목표 이미지(goal images)를 통해 작업을 수행할 수 있습니다.
- **Performance Highlights**: 900K의 궤적(trajectories)과 20개의 서로 다른 로봇 구성을 포함한 가장 큰 로봇 데이터셋을 통해 정책이 훈련되었습니다. 이 정책은 사족 로봇(quadruped)에서 3개의 카메라 품은 양팔 로봇(bimanual robot)까지 다양한 관찰 및 행동 타입의 로봇을 제어할 수 있습니다. 실제 세계에서의 실험 결과를 통해 이 정책은 단일 로봇 데이터로 훈련된 정책과 동일한 아키텍처를 사용하여도 유사한 성능을 보여주었고, 이전의 최고의 방법과 비교하여도 견줄 만한 성능을 보였습니다.

### [FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive Prompt Weighting](https://arxiv.org/abs/2408.11706)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11706.png)

Vote: 2

Authors: Mohan Sai Singamsetti, Liyao Jiang, Mohammad Salameh, Di Niu, Wei Lu, Fengyu Sun, Negar Hassanpour

- **What's New**: 최근 연구는 텍스트-이미지 생성(T2I) 디퓨전 모델들이 텍스트 프롬프트에 따라 사실적인 이미지를 생성하는 데 있어 인상적인 성과를 보여주었습니다. 이 논문에서 저자들은 프롬프트-이미지 정합성과 현실감을 향상시키기 위한 새로운 접근법으로 Adaptive Prompt Weighting (FRAP)을 소개합니다. FRAP는 디퓨전 모델의 역생성 과정 동안 각 토큰의 가중치를 적응적으로 조정하여 보다 정교한 이미지 생성과 높은 품질의 이미지를 동시에 달성합니다.
- **Technical Details**: FRAP는 UNet 모델의 반복 호출이 불필요한 온-더-플라이 최적화 알고리즘을 설계하여 이미지 생성을 진행합니다. 이 모델은 각 프롬프트 토큰의 가중치를 최적화하여 크로스-어텐션 맵을 기반으로 통합된 목표 함수를 최소화합니다. 또한 FRAP는 대규모 언어 모델(LLM) 기반 프롬프트 최적화기와 결합하여 프롬프트 정렬과 생성 품질 모두를 향상시키는 새로운 방법을 제시합니다.
- **Performance Highlights**: FRAP는 COCO-Subject와 COCO-Attribute와 같은 복잡한 프롬프트를 포함하는 데이터셋에서 최근 방법들보다 뛰어난 정합성을 보여주었으며, 단순 프롬프트에서는 유사한 성능을 유지했습니다. 또한 FRAP는 CLIP-IQA-Real 측정 기준으로 더 현실감 있는 이미지를 생성하였습니다. 그리고 프롬프트 최적화 방법과의 결합으로 정합성과 이미지 품질 모두 향상됨을 확인했습니다.

### [Out-of-Distribution Detection with Attention Head Masking for Multimodal Document Classification](https://arxiv.org/abs/2408.11237)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11237.png)

Vote: 1

Authors: Aman Chadha, Edwin Simpson, Christos Constantinou, Aaron Elkins, Georgios Ioannides

- **What's New**: 문서 분류에서 out-of-distribution (OOD) 감지의 어려움을 다루는 새로운 연구가 발표되었습니다. 연구자들은 종종 과소 대표된 문서 유형을 실제 환경에서 마주하게 되며, 이는 예측의 신뢰성과 정확성을 해칠 수 있습니다. 이를 해결하기 위해 새로운 데이터셋인 FinanceDocs와 주목할 만한 기술인 attention head masking (AHM)을 도입하였습니다.
- **Technical Details**: 연구에서는 OOD 데이터를 감지하기 위한 여러 방법론이 탐구되었으며, 크게 세 가지로 분류됩니다: (i) confidence-based methods, (ii) features/logits-based methods, (iii) distance/density-based methods. 본 연구는 transformer 모델의 self-attention 메커니즘을 활용하는 AHM 방식을 제안합니다. 이 방법은 특히, ID 데이터와 OOD 데이터 사이의 효과적인 분리를 위한 임베딩 생성에 중점을 둡니다.
- **Performance Highlights**: 연구 결과, AHM 방법은 대규모 평가에서 기존의 OOD 감지 기술을 능가하는 성능을 보였습니다. 두 개의 데이터셋, 즉 Tobacco3482와 새로 개발된 FinanceDocs를 사용한 실험에서 AUROC와 FPR 메트릭을 통해 평가되었습니다. 실험용 모델로는 LayoutLMv3가 사용되었으며, cross-dataset 및 intra-dataset OOD 실험 모두에서 우수한 결과를 보였습니다.

### [Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer](https://arxiv.org/abs/2408.08793)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08793.png)

Vote: 1

Authors: Simone Ricci, Federico Pernici, Alberto Del Bimbo, Niccolò Biondi

- **What's New**: 이 논문에서는 새로운 모델을 독립적으로 훈련하면서도 예전 모델과 호환되는 표현을 학습하는 방법을 제안합니다. 이 접근법은 새로운 모델의 성능을 유지하면서도 과거의 갤러리 데이터(re-indexing)를 다시 처리할 필요 없이 직접 비교할 수 있는 표현을 생성합니다.
- **Technical Details**: 이 방법은 새로운 정보가 추가된 모델의 표현 공간을 확장하는 것입니다. 이를 위해 정칙 변환(orthogonal transformation) 기능을 학습하여 호환성 있는 표현 공간의 기하학을 유지합니다. 새롭게 제안된 모델은 추가 변환이 이루어지기 전에 원래의 표현 공간에서 특징을 추출하므로 여러 모델 업데이트에도 직접 비교가 가능합니다.
- **Performance Highlights**: 이 방법은 CIFAR-100과 ImageNet-1k 데이터셋에서 실험을 통해 최고의 정확도를 달성했습니다. 이로써 새로운 모델이 이전 모델과 호환성을 유지하면서도 높은 성능을 보여줍니다.

### [Expanding FLORES+ Benchmark for more Low-Resource Settings: Portuguese-Emakhuwa Machine Translation Evaluation](https://arxiv.org/abs/2408.11457)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11457.png)

Vote: -

Authors: Rui Sousa-Silva, Henrique Lopes Cardoso, Felermino D. M. Antonio Ali

- **What's New**: 이번 연구는 저자들이 Emakhuwa 언어를 포함하도록 FLORES+ 평가 세트를 확장하는 내용을 다룹니다. Emakhuwa는 모잠비크에서 약 900만 명이 사용하는 저자원 언어로, 이번 연구는 Emakhuwa 언어 데이터를 구축하여 기계 번역(ML) 커뮤니티에서 활용할 수 있도록 합니다.
- **Technical Details**: 이 연구에서는 dev 세트와 devtest 세트를 번역하는 작업을 진행했습니다. 번역 작업에는 Matecat (CAT: Computer-Assisted Translation) 도구가 사용되었으며, 번역 품질을 보장하기 위해 신중한 언어 품질 검토와 전문가 의견 수렴 단계를 거쳤습니다. 번역 품질 평가는 Direct Assessment (DA) 파이프라인을 통해 이루어졌습니다.
- **Performance Highlights**: 번역 팀은 devtest와 dev 세트에서 총 2009개의 문장을 번역했습니다. 번역 평가는 번역의 적절성을 0에서 100점 사이로 평가하는 방법을 사용했으며, 점수 구간은 Wang et al. 2024 연구를 참고했습니다. 품질 보고서에 따르면, 두 번역 세트 간에 언어 품질과 용어 일관성에서 차이가 나타났습니다.

