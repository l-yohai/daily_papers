## Daily Papers (2024-08-05)

### [Medical SAM 2: Segment medical images as video via Segment Anything Model 2](https://arxiv.org/abs/2408.00874)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00874.png)

Vote: 15

Authors: Junde Wu, Yunli Qi, Jiayuan Zhu

- **What's New**: 최근의 연구는 SAM 2(Ravi et al., 2024)라는 실시간 객체 분할 모델이 발표되었습니다. 이 모델은 원래 이미지와 비디오 스트림에 적용되는 범용 모델로, 특히 의료 영상 분할에서 두드러진 성과를 보였습니다. 새로운 MedSAM-2는 SAM 2를 의료 영상에 맞게 변형한 모델로, 2D 및 3D 의료 영상 모두에서 뛰어난 성능을 보이며 'One-prompt Segmentation' 기능을 구현했습니다.
- **Technical Details**: MedSAM-2는 의료 영상을 비디오처럼 취급하는 혁신적 접근 방식을 통해 개발되었습니다. 이 모델은 2D 슬라이스 시퀀스를 처리하는 방식으로, 인접한 슬라이스 간의 연관성을 활용해 단일 슬라이스의 품질이 떨어져도 정확한 분할을 가능하게 합니다. SAM 2의 비디오 프레임 추적 및 객체 분할 능력을 3D 의료 영상에 채택하며, 'One-prompt Segmentation' 기능을 통해 특정 프레임에만 프롬프트를 제공하면 후속 이미지에서도 동일한 영역을 자동으로 분할합니다.
- **Performance Highlights**: MedSAM-2는 15개의 서로 다른 벤치마크와 26개의 특정 작업에서 평가되었으며, 모든 테스트 방법에서 뛰어난 성능을 발휘했습니다. 특히 'One-prompt Segmentation' 설정에서 다른 few-shot 및 one-shot 모델들보다 우수한 성능을 보여주었습니다. 이는 MedSAM-2의 놀라운 일반화 역량을 입증하는 것입니다.

### [POA: Pre-training Once for Models of All Sizes](https://arxiv.org/abs/2408.01031)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.01031.png)

Vote: 12

Authors: Jian Wang, Ming Yang, Lixiang Ru, Huimei He, Jiangwei Lao, Xin Guo, Yingying Zhang, Lei Yu, Guo Ye, Jingdong Chen

- **What's New**: 새로운 인공지능 연구 논문에서는 POA (Pre-training Once for All)라는 새로운 자가 지도 학습(self-supervised learning) 패러다임을 소개합니다. 이 방법은 한 번의 사전 학습만으로 다양한 크기의 모델을 동시에 생성할 수 있는 최초의 방법으로, 실제 응용 프로그램에서의 자원 제약을 고려하여 설계되었습니다.
- **Technical Details**: POA는 주로 교사-학생(self-distillation) 프레임워크를 기반으로 하며, 추가적으로 'Elastic Student'라는 혁신적인 구성 요소를 도입합니다. 이 'Elastic Student'는 매개 변수 공유를 통해 일련의 하위 네트워크를 포함하고 있습니다. 학습 과정에서 랜덤하게 선택된 매개 변수 하위 집합을 사용하여 원본 학생 네트워크와 유사한 출력을 생성하도록 학습됩니다. 교사는 학생의 매개 변수를 사용한 지수 이동 평균(EMA)으로 지속적으로 업데이트됩니다.
- **Performance Highlights**: POA의 효율성을 평가하기 위해 ViT, Swin Transformer, ResNet 등의 백본 아키텍처를 사용하여 다양한 실험을 수행했습니다. POA는 k-NN, 선형 탐사(linear probing) 및 객체 탐지와 의미론적 세분화와 같은 다운스트림 작업에서 기존의 최첨단(SOTA) 성능을 능가하는 결과를 보였습니다. 특히 POA는 하나의 학습 세션으로 다양한 크기의 모델을 사전 학습하고 높은 품질의 representation을 제공할 수 있습니다.

### [ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget](https://arxiv.org/abs/2408.00103)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00103.png)

Vote: 8

Authors: Edoardo Barba, Roberto Navigli, Pere-Lluis Huguet-Cabot, Riccardo Orlando

- **What's New**: 이 논문은 엔티티 링크(Entity Linking, EL)와 관계 추출(Relation Extraction, RE)이라는 두 가지 주요 정보 추출(Information Extraction, IE) 작업에 초점을 맞추고 있다. 연구진은 지금까지 이 두 작업 모두에서 성능, 추론 속도(Inference Speed), 유연성(Flexibility)이라는 세 가지 속성을 동시에 만족시키는 접근법이 부족하다는 점에 주목했다. 이를 통해 Retriever-Reader 패러다임을 활용해 같은 기반 아키텍처로 두 작업을 모두 개선하는 것이 가능함을 보여준다.
- **Technical Details**: ReLiK 시스템은 오픈 도메인 질문 응답(Open Domain Question Answering, ODQA) 시스템과 유사한 구조를 가지고 있다. 입력 텍스트에 대한 쌍엔코더(bi-encoder) 아키텍처인 Retriever가 해당 텍스트를 인코딩하고 외부 인덱스에서 가장 관련 있는 텍스트 구절을 검색한다. 그런 다음 Reader가 검색된 구절을 이용해 엔티티를 연결하고 관계를 추출한다. EL과 RE 모두에서 텍스트에 여러 질문이 동시에 포함될 수 있으며, 모든 관련 구절을 단일 전방 패스로 인코딩하고 추출하는 방식이다. 아키텍처는 크게 Retriever와 Reader의 두 가지 주요 구성 요소로 나뉜다.
- **Performance Highlights**: ReLiK는 비매개 메모리(non-parametric memory)를 활용해 성능을 유지하면서도 모델의 파라미터 수를 크게 줄여준다(추론 속도). 또한, 텍스트 표현(representations)을 사용하여 제로-샷(zero-shot)으로 보지 못한 엔티티와 관계도 처리할 수 있어 유연성이 증가했다. 이와 같은 혁신적인 입력 공식화(input formulation)를 통해 최신 언어 모델인 DeBERTa-v3의 문맥화 능력을 최대한 활용했다. 종합 실험을 통해 이 방식을 사용함으로써 모델의 최종 성능과 처리 속도를 모두 향상시켰음을 입증했다.

### [TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling](https://arxiv.org/abs/2408.01291)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.01291.png)

Vote: 6

Authors: Dong Huo, Yee-Hong Yang, Li Cheng, Zhihao Shi, Xinxin Zuo, Juwei Lu, Peng Dai, Zixin Guo, Songcen Xu

- **What's New**: TexGen의 발표로 3D 텍스처 생성에 새로운 방향이 제시되었습니다. 이 논문에서는 자동 텍스트 기반 3D 텍스처 합성 방법을 소개하며, 기존의 시퀀스 문제와 부드러운 텍스처 문제를 해결합니다.
- **Technical Details**: TexGen은 2D T2I 모델을 활용하여 텍스처를 생성합니다. 중점적으로, 멀티뷰 샘플링과 재샘플링(Resampling) 전략을 사용하여 RGB 이미지의 일관된 뷰 텍스처를 생성합니다. 각 디노이징(Denoising) 단계에서 UV 텍스처 맵을 업데이트하고 고빈도 프리어스를 사용해 잡음을 추정합니다. Attention-guided multi-view sampling 및 Text&Texture-guided noise resampling 기술을 통해 고품질 텍스처 생성이 가능해졌습니다.
- **Performance Highlights**: TexGen은 텍스처의 일관성과 디테일을 뛰어나게 유지하며, 기존 기술 대비 우수한 성능을 입증했습니다. 다양한 3D 객체에 텍스처를 적용한 결과, 매끄러운 텍스처와 높은 디테일이 나타났습니다. 이 프레임워크는 자연스럽게 텍스트 기반 텍스처 편집을 지원할 수 있습니다.

### [MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models](https://arxiv.org/abs/2408.01337)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.01337.png)

Vote: 3

Authors: Benno Weck, Emmanouil Benetos, Dmitry Bogdanov, Ilaria Manco, George Fazekas, Elio Quinton

- **What's New**: MuChoMusic는 Audio LLMs(대화형 언어 모델)의 음악 이해 능력을 평가하기 위한 최초의 벤치마크입니다. MuChoMusic은 사실에 기반한 음악 지식을 바탕으로 구성된 정답형 질문 세트를 통해 모델의 음악 이해도와 다양한 음악적 차원에서의 추론 능력을 측정합니다.
- **Technical Details**: MuChoMusic은 MusicCaps와 Song Describer Dataset에서 수집된 644개의 고유한 음악 트랙에 대해서 총 1,187개의 다지선다형(Multiple-choice) 질문을 포함하고 있으며, 이는 인간 주석자(Human Annotators)에 의해 철저히 검증되었습니다. MuChoMusic는 지식(knowledge)과 추론(reasoning) 두 가지 주요 카테고리로 나누어 음악 이해를 평가하며, 각각의 질문은 이 두 가지 차원에서 다양한 음악적 요소를 포괄합니다.
- **Performance Highlights**: MuChoMusic은 Audio LLMs의 평가를 위한 표준화를 제공하며, 기존의 텍스트 생성 평가 메트릭과 데이터셋의 품질 문제를 해결하는 것을 목표로 합니다. 이 벤치마크를 통해 모델의 음악 이해도를 신뢰성 있게 평가할 수 있는 방법을 제시하며, 이를 통해 향후 연구의 방향성을 제시합니다.

### [In-Context Example Selection via Similarity Search Improves Low-Resource Machine Translation](https://arxiv.org/abs/2408.00397)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00397.png)

Vote: 2

Authors: Armel Zebaze, Rachel Bawden, Benoît Sagot

- **What's New**: 대형 언어 모델(LLMs)을 활용한 문맥 학습(ICL)에서 예시 선택 방법이 성능에 미치는 영향을 분석하는 연구가 진행되었습니다. 특히 저자들은 예시 추출을 위해 유사성 검색 방식을 체계적으로 분석하고, 영어에서 프랑스어, 독일어, 스와힐리어 및 월로프로 번역하는 다양한 시나리오에서 다국어 문장 임베딩을 활용한 여러 유사성 메트릭을 벤치마킹했습니다.
- **Technical Details**: 연구진은 다국어 문장 임베딩(multilingual sentence embeddings)을 사용하여 예시를 선택하는 다양한 유사성 메트릭을 분석했습니다. 번역 모델은 영어에서 프랑스어, 독일어, 스와힐리어 및 월로프로 번역하며 고자원와 저자원 언어에 대한 성능을 비교했습니다. 특히 스와힐리어 번역에서는 다양한 선택 풀 구성에 대한 방법의 강건성을 평가했으며, LLM 기반의 번역 평가에서 발생할 수 있는 잠재적 문제점을 지적하고 더 적절한 평가 프로토콜을 제안했습니다.
- **Performance Highlights**: 연구 결과, 유사성 검색을 통한 예시 추출은 고자원 언어 번역에서는 무작위 샘플링보다 약간의 향상만 보였습니다. 그러나 저자원 언어로 번역할 때는 모든 메트릭에서 유의미한 성능 향상이 관찰되었습니다. 이러한 결과는 다양한 규모의 LLM에서도 일관되게 나타났습니다.

### [RelBench: A Benchmark for Deep Learning on Relational Databases](https://arxiv.org/abs/2407.20060)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.20060.png)

Vote: 2

Authors: Kexin Huang, Matthias Fey, Weihua Hu, Alejandro Dobles, Joshua Robinson, Jiaqi Han, Yiwen Yuan, Xinwei He, Zecheng Zhang, Rishabh Ranjan, Jan E. Lenssen, Jure Leskovec

- **What's New**: RelBench라는 새로운 벤치마크를 발표했습니다. RelBench는 e-커머스, Q&A 플랫폼, 의료 및 스포츠 데이터베이스와 같은 다양한 도메인에서 관계형 데이터의 종단 간 학습 가능한 모델을 지원하고자 합니다. 이는 관계형 심층 학습(Relational Deep Learning, RDL)을 최초로 구현하는 데 필요한 인프라를 제공합니다.
- **Technical Details**: RelBench는 기본적으로 다음과 같은 구성 요소들로 이루어져 있습니다: (i) 테이블이 primary-foreign key 관계로 연결된 관계형 데이터베이스, (ii) 각 데이터베이스에 대한 예측 작업 세트, (iii) open-source 소프트웨어 패키지. 이 패키지는 관계형 데이터베이스를 그래프 형식으로 변환하고, Graph Neural Networks(GNNs)를 사용하여 예측 모델을 학습하도록 설계되었습니다. 주요 구성 요소로는 heterogeneous temporal graph, deep learning model, temporal-aware training, task-specific loss 등이 포함됩니다.
- **Performance Highlights**: 이 초기 구현 결과, RDL 모델은 데이터 과학자가 수작업으로 특징을 엔지니어링하고 표형 모델에 피드한 것보다 더 높은 정확도를 보였습니다. 또한 시간 대비 96%가 감소하고, 코드 라인은 94% 감소했습니다. 이는 RDL의 중앙 약속을 실증하는 첫 번째 사례로, 관계형 데이터에 대한 완전한 종단 간 심층 학습 솔루션을 제시합니다.

### [Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models](https://arxiv.org/abs/2408.00113)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00113.png)

Vote: 2

Authors: Adam Karvonen, Samuel Marks, Logan Smith, Claudio Mayrink Verdun, Jannik Brinkmann, David Bau, Rico Angell, Benjamin Wright, Can Rager

- **What's New**: 최근 연구는 Sparse Autoencoders (SAEs)을 사용하여 언어 모델(LM)의 내부 구성을 분리된 표현으로 찾는 데 중점을 두고 있습니다. 이번 연구에서는 체스와 오델로 게임 대화록에 트레이닝된 LMs를 활용하여 새로운 지표 두 가지를 소개합니다: '보드 재구성' 및 '커버리지'입니다. 새로운 트레이닝 기법인 'p-annealing'도 제안합니다.
- **Technical Details**: 본 연구는 체스 및 오델로 게임 대화록을 단계적으로 예측하도록 트레이닝된 LMs를 사용합니다. 각 토큰은 게임 보드의 특정 위치를 나타내며, 이러한 모델은 내부적으로 보드 상태를 모델링하게 됩니다. SAE는 특정 보드 구성 또는 연구자가 지정한 후보 특징을 분류하는 데 사용됩니다. p-annealing 기법은 트레이닝 초기에는 L1-norm 기반 스파시티 페널티를 사용하고, 훈련 말기에는 비볼록 목표로 바꾸어 성능을 향상시킵니다.
- **Performance Highlights**: p-annealing을 사용한 SAE는 이전 기법보다 높은 성능을 보여주며, 더 많은 계산 자원을 요구하는 Gated SAE와 유사한 성능을 냈습니다. 또한, 연구에서 훈련된 500개 이상의 체스 및 오델로 모델을 공개합니다.

