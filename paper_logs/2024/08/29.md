## Daily Papers (2024-08-29)

### [Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders](https://arxiv.org/abs/2408.15998)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15998.png)

Vote: 44

Authors: Min Shi, Shijia Liao, Fuxiao Liu, Bryan Catanzaro, Shihao Wang, Andrew Tao, Jan Kautz, Zhiding Yu, Yaser Yacoob, Humphrey Shi, Karan Sapra, De-An Huang, Hongxu Yin, Guilin Liu, Subhashree Radhakrishnan

- **What's New**: 이번 논문은 큰 언어 모델(LLMs)의 성공을 시각적 인지 능력으로 확장하려는 시도로, 이를 통해 실세계에서 이미지를 이해하고 추론할 수 있는 멀티모달 큰 언어 모델(MLLMs)을 개발하는 데 중점을 둡니다. 특히, 'mixture-of-vision-experts' 전략의 효과와 최적화를 체계적으로 조사하여 다양한 비전 인코더 조합과 융합 방식을 탐구합니다.
- **Technical Details**: 기존 연구들은 CLIP와 같은 비전 인코더를 활용하여 시각적 표현을 텍스트 공간과 정렬시키는 방식으로 텍스트 임베딩에 시각적 토큰을 추가하는 디자인을 따르고 있습니다. 이 논문은 다양한 비전 인코더와의 융합 전략을 단계별로 비교하고, 더 높은 해상도 적응을 위해 다양한 융합 방법을 분석합니다. 또한, 기본 CLIP 인코더에 추가 비전 전문가를 추가하고, 각 인코더의 최적 조합을 점진적으로 식별하는 접근법을 사용합니다.
- **Performance Highlights**: 연구 결과, 비전 인코더를 훈련시키는 동안 잠금 해제하는 것이 중요하다는 것을 발견했습니다. 간단한 채널 연결은 가장 효율적이고 성능 좋은 융합 전략으로 나타났습니다. Eagle 모델은 다양한 벤치마크에서 최첨단 성능을 달성했으며, 특히 OCR 및 문서 이해 작업에서 현저한 장점을 보여주었습니다.

### [BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and Deduplication by Introducing a Competitive Large Language Model Baseline](https://arxiv.org/abs/2408.15079)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15079.png)

Vote: 42

Authors: Bingning Wang, Xin Wu, Jiaxin Mao, Yanjun Shen, Lei Su, Xiaonan Nie, Fan Yang, Mingan Lin, Jianhua Xu, Weipeng Chen, Guosheng Dong, Haoze Sun, Wentao Zhang, Da Pan, Shusen Zhang, Yufan Zhang, Yiding Sun, Zenan Zhou, Zheng Liang, Tianpeng Li

- **What's New**: 대규모 언어 모델(LLM)을 위한 BaichuanSEED 소개. 이 모델은 7B 매개변수를 가진 기초 모델로서 3T 양측 언어 토큰으로부터의 pre-training 및 지도 미세 조정을 거쳤습니다.
- **Technical Details**: BaichuanSEED는 Transformer 디코더 스택 아키텍처를 따르며, 32개의 레이어와 32개의 attention heads로 구성됩니다. 숨겨진 차원 크기는 4,096이고, 피드포워드 레이어 크기는 11,008입니다. 활성화 함수로는 SwiGLU를 사용하며, RMSNorm을 통해 교육 안정성을 향상시켰습니다. 상대적 위치 의존성을 모델링하기 위해 RoPE(Rotary Positional Embedding)를 사용합니다.
- **Performance Highlights**: BaichuanSEED는 과도한 최적화 없이도 Llama3와 Qwen-1.5 등의 최신 상업용 모델과 비슷한 성능을 보여줍니다. 특히 수학 관련 작업에서는 추가적인 개선의 여지가 있습니다. 여러 일반적인 최적화 방법을 적용하여 높은 지식 밀도의 데이터 비율 조정과 수학 및 코딩 능력 최적화를 시도하고 있습니다. 모델은 일관성 있고 예측 가능하며, 앞으로 더 많은 최적화 방법을 통합할 계획입니다.

### [Dolphin: Long Context as a New Modality for Energy-Efficient On-Device Language Models](https://arxiv.org/abs/2408.15518)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15518.png)

Vote: 30

Authors: Yihao Wang, Shuo Xin, Wei Chen, Zhiyuan Li

- **What's New**: 이 논문에서는 Dolphin이라는 새로운 디코더-디코더 아키텍처를 도입하여, 장문 컨텍스트(long context)를 에너지 효율적으로 처리하는 방법을 제시합니다. 기존의 기법이 메모리 관리와 처리 속도의 개선에 중점을 두었다면, Dolphin은 두 가지 크기의 디코더를 활용하여 에너지 소모와 지연 시간을 획기적으로 줄이는 성과를 이루었습니다. 이는 모바일 환경에서의 AI 응용 프로그램의 실용성을 크게 향상시킵니다.
- **Technical Details**: Dolphin은 0.5B 파라미터를 가진 작은 디코더와 7B 파라미터를 가진 큰 디코더를 결합해 긴 텍스트 컨텍스트를 처리합니다. 작은 디코더는 긴 컨텍스트 정보를 여러 개의 메모리 토큰(memory tokens)으로 압축하여 큰 디코더가 현재 쿼리와 연관된 문맥 이해 및 생성에 집중할 수 있도록 합니다. 또한, 이 접근법은 비전-언어 모델(VLMs)의 다단계 학습 절차를 참고하여 개발되었습니다.
- **Performance Highlights**: Dolphin 모델은 전통적인 장문 컨텍스트 처리 방식에 비해 에너지 효율은 10배, 지연 시간은 5배 개선되었습니다. 이러한 성과는 AI 기술의 도입을 촉진하며 모바일 컴퓨팅, IoT, 웨어러블 기술 분야에 큰 영향을 미칠 것으로 기대됩니다.

### [Distribution Backtracking Builds A Faster Convergence Trajectory for One-step Diffusion Distillation](https://arxiv.org/abs/2408.15991)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15991.png)

Vote: 13

Authors: Zhiyuan Yang, Zejian Li, Lingyun Sun, An Zhao, Guang Yang, Shengyuan Zhang, Chenye Meng, Ling Yang, Changyuan Yang

- **What's New**: 최근 다양한 분야에서 탁월한 성능을 보이는 생성 모델들이 등장했지만, 이들은 여전히 높은 생성 품질, 빠른 생성 속도, 높은 샘플 다양성을 동시에 달성하는 '트릴레마' 문제에 직면해 있습니다. 이러한 문제를 해결하기 위해, 우리는 Distribution Backtracking Distillation (DisBack) 기법을 제안합니다. DisBack은 전체 convergence 경로를 도입함으로써 기존의 score distillation 프로세스를 확장하고 더 빠른 속도로 효율적인 디스틸레이션을 달성합니다.
- **Technical Details**: 기존의 score distillation 기법은 주로 사전 학습된 diffusion 모델의 최종 결과만을 중점적으로 다뤄, 학생 생성기와 교사 모델 간의 convergence 경로의 중요성을 간과했습니다. 이러한 문제를 해결하기 위해 DisBack은 teacher 모델의 전체 디스틸레이션 과정을 소개하고, initial student generator에서 teacher 모델로 역행하는 degradation 경로를 사용하여 mismatch 문제를 줄입니다. DisBack은 degradation recording와 distribution backtracking 두 가지 단계로 구성되며, 이는 추가적인 계산 비용이 거의 들지 않습니다.
- **Performance Highlights**: DisBack은 기존 score distillation 방법에 비해 현저히 빠른 convergence 속도를 보이며, 더 높은 생성 성능을 제공합니다. 구체적으로, DisBack을 사용한 실험에서는 기존 메서드와 비교하여 향상된 수렴 속도와 유사한 생성 품질을 달성하였습니다. 이는 연구자들이 DisBack 훈련 전략을 기존 디스틸레이션 방법에 통합하는 것을 장려합니다.

### [Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models](https://arxiv.org/abs/2408.15915)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15915.png)

Vote: 12

Authors: Tong Wu, Yuncheng Yang, Jie Yang, Yun Gu, Yulei Qin, Gang Li, Pengcheng Guo, Hang Shao, Xing Sun, Zihan Xu, Yucheng Shi, Ke Li

- **What's New**: 본 논문에서는 K-shot 데이터와 공개된 LoRA 모델 및 데이터셋을 활용하여 대형 언어 모델(LLMs)의 전문 분야 과제 전문성을 향상시키는 새로운 접근법을 제안합니다. 이 방법은 기존의 일반적인 도메인 및 공통 과제에서 인간 선호도와의 정렬을 향상시키는 모델 선택 방식과 차별화됩니다.
- **Technical Details**: 제안된 파이프라인은 세 가지 주요 챌린지를 해결합니다. 첫째, K-shot 데이터를 활용하여 과제 해결에 높은 잠재력을 가진 모델을 식별합니다. 둘째, 오픈 소스 데이터셋에서 K-shot과 유사한 맥락을 가진 지침을 선택합니다. 셋째, 여러 LLM의 지식을 효과적으로 조율하는 적응형 토큰 게이팅 시스템을 구축하여 협업을 향상시킵니다. 이를 위해, 이 논문은 자체 LoRA 은행을 유지하고 다양성을 핵심 지표로 사용하는 모델 및 데이터 선택 전략을 개발했습니다.
- **Performance Highlights**: 논문에서는 다양한 LoRA 모델과 데이터셋을 활용한 Task Augmentation 파이프라인을 통해 대형 언어 모델의 전문 분야 과제 수행 능력 향상을 입증합니다. 제안된 MoE(Mixture-of-Experts) 시스템은 K-shot 데이터 증가를 통해 여러 전문가 모델 간의 협업과 적응력을 강화하며, 일반적인 단일 모델 접근법 대비 뛰어난 성능을 보여줍니다.

### [LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation](https://arxiv.org/abs/2408.15881)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15881.png)

Vote: 10

Authors: Haoyuan Li, Bolin Li, Hao Jiang, Fangxun Shu, Tao Zhong, Wanggui He, Guanghao Zhang, Si Liu, Hongsheng Li, Haonan Shi, Zhelun Yu, Long Chen, Chenning Xu, Siming Fu, Yue Liao, Le Zhuo

- **What's New**: 최신 연구는 대규모 언어 모델(Large Language Models, LLMs)의 고급 지시 따르기 및 추론 능력을 활용하여 다중 모드의 대규모 언어 모델(Multimodal Large Language Models, MLLMs)을 개발하는 데 중점을 둡니다. 이 연구는 특히 LLaVA-MoD111MoD라는 새로운 프레임워크를 도입하여 MLLM의 성능과 효율성 사이의 균형을 유지하는 것이 목표입니다.
- **Technical Details**: 연구는 주로 두 가지 도전 과제에 중점을 둡니다. 첫 번째는 작은 모델(s-MLLM)을 효과적으로 설계하여 큰 모델(l-MLLM)의 복잡한 지식을 흡수하는 것입니다. 이를 위해 연구진은 희소 전문가 구조(Sparse Mixture-of-Experts, MoE)를 활용하여 모델의 규모를 줄이면서도 복잡한 멀티모달 정보를 포착할 수 있는 능력을 유지합니다. 두 번째는 효과적인 지식 전이를 통해 작은 모델이 큰 모델의 지식을 효율적으로 습득하는 것입니다. 이를 위해 연구는 프로그레시브 디스틸레이션(Progressive Distillation) 전략을 제안합니다.
- **Performance Highlights**: LLaVA-MoD는 다양한 멀티모달 벤치마크에서 높은 성능을 보이며, 적은 최대 활성화된 파라미터와 낮은 계산 리소스를 사용합니다. 예를 들어, LLaVA-MoD-2B는 Qwen-VL-Chat-7B보다 평균 8.8% 더 높은 성능을 보이며, 훈련 데이터의 0.3%만 사용하고 훈련 가능한 파라미터의 23%만 사용합니다. 또한, 여러 환각 벤치마크에서 7B 및 13B 파라미터를 사용한 RLHF 기반 방법과 성능이 일치하며, 특히 Object HalBench에서 RLHF-V를 8.2% 포인트 이상 능가하는 환각율을 보여줍니다.

### [Efficient LLM Scheduling by Learning to Rank](https://arxiv.org/abs/2408.15792)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15792.png)

Vote: 7

Authors: Hao Zhang, Aurick Qiao, Ion Stoica, Yichao Fu, Runlong Su, Siqi Zhu

- **What's New**: 본 논문에서는 대형 언어 모델(LLM)의 효율적인 서비스 제공을 위한 새로운 스케줄링 방법을 제안합니다. 현재 대부분의 LLM 서비스는 First-Come-First-Serve(FCFS) 방식으로 스케줄링되어, 많은 요청이 헤드 오브 라인(HOL) 블로킹을 겪고 있습니다. 본 논문에서는 이러한 문제를 해결하기 위해 LLM 요청의 생성 길이를 정확히 예측하지 않고도, 상대적인 순서만 알면 효과적인 Shortest-Job-First(SJF)와 Shortest-Remaining-Time-First(SRTF) 스케줄링을 구현할 수 있음을 증명합니다.
- **Technical Details**: 제안된 방법은 Kendall의 타우(Kendall's Tau) 상관계수를 사용하여, 예측된 스케줄과 실제 최적 스케줄 간의 유사도를 측정합니다. 높은 Kendall's Tau 유사도는 낮은 지연 시간과 높은 처리량으로 이어집니다. 이를 위해 작은 보조 모델(e.g., OPT-125M)을 트레이닝하여 LLM 요청의 생성 길이를 순위 매기는 데 사용합니다. 이러한 접근 방식은 기존의 생성 길이 예측 방식보다 더 견고하고, 실제 시스템에 쉽게 통합될 수 있습니다.
- **Performance Highlights**: 제안된 방법을 사용하여, 대기 시간이 높은 대화형 애플리케이션에서 p90 지연 시간을 2.8배, 배치 생성에서 처리량을 6.5배 개선하는 성과를 거두었습니다. 또한, 본 방법은 현재 가장 앞선 서빙 시스템에 통합되어 중요 LLM 서빙 작업에서 성능을 크게 향상시켰습니다.

### [Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature](https://arxiv.org/abs/2408.15836)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15836.png)

Vote: 6

Authors: Yoav Goldberg, Uri Katz, Mosh Levy

- **What's New**: 이번 arXiv 논문에서는 새로운 딥러닝 모델 (Deep Learning Model)을 통해 특정 컴퓨터 비전 (Computer Vision) 문제를 해결하는 접근법을 제안합니다. 이 논문은 기존 방법론의 한계를 보완하고 성능을 향상시키기 위해 설계되었습니다.
- **Technical Details**: 논문의 주요 기술적 특징은 새로운 네트워크 아키텍처 (Network Architecture)와 하이퍼파라미터 최적화 (Hyperparameter Optimization)에 있습니다. 제안된 모델은 ResNet과 Transformer를 통합하여 고도의 특징 추출 능력을 가지고 있습니다. 또한, 학습 과정에서 데이터 증강 기술 (Data Augmentation)을 활용하여 모델의 일반화 성능을 향상시켰습니다.
- **Performance Highlights**: 제안된 모델은 다양한 벤치마크 데이터셋 (Benchmark Datasets)에서 테스트되었으며, 기존 최고 성능 모델 (State-of-the-Art) 대비 정확도 (Accuracy)와 효율성 (Efficiency)에서 우수한 결과를 보였습니다. 특히, ImageNet 데이터셋에서 1.5% 이상의 성능 향상을 기록하였습니다.

### [In-Context Imitation Learning via Next-Token Prediction](https://arxiv.org/abs/2408.15980)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15980.png)

Vote: 6

Authors: Gaurav Datta, Hui Li, Huang Huang, William Chung-Ho Panitch, Fangchen Liu, Lawrence Yunliang Chen, Ken Goldberg, Letian Fu

- **What's New**: 최근 로봇 정책(Policies)에 대해 학습 기반 단일 및 멀티 태스크 접근 방식이 점점 더 높은 성능을 보이고 있다. 특히, 비전과 언어 모델링 분야의 발전이 큰 기여를 하고 있다. 본 논문에서는 이러한 발전을 바탕으로 대규모 언어 모델(LLMs) 및 대규모 비전 모델(LVMs)의 '다음 토큰 예측(next-token prediction)' 방식을 로봇 학습에 적용한 'In-Context Robot Transformer (ICRT)'를 소개한다. ICRT는 새로운 환경 구성에서도 추가 학습 없이 주어진 시나리오를 신속히 파악하고 작업을 수행할 수 있다.
- **Technical Details**: ICRT 모델은 로봇의 규제 궤적(sensorimotor trajectories)을 문맥(context)으로 사용하여 새로운 작업을 수행한다. 이 궤적은 이미지 관찰, 로봇의 고유감각 상태, 액션으로 구성되며, 문맥을 통해 작업 프리미티브(task primitives)와 상호작용해야 할 객체를 암시적으로 인코딩한다. 모델은 주어진 시나리오로부터 이 정보를 추출하여 현재 환경에서 유사한 패턴의 액션을 실행한다. ICRT는 복잡한 손실 함수 또는 키 포인트 선택 없이 로봇 궤적을 직접 처리하여 연속적인 제어를 수행할 수 있는 단순한 프레임워크를 제공한다.
- **Performance Highlights**: ICRT는 Franka Emika 로봇을 활용한 다양한 난이도의 작업에서 물리적 실험을 통해 평가되었다. 결과에 따르면 ICRT는 주어진 프롬프트에 의해 지정된 보지 않은 작업도 성공적으로 수행할 수 있었다. 이는 ICRT의 효과적인 '문맥 학습(in-context learning)' 능력을 보여준다. 또한, ICRT는 다중 태스크와 문맥 학습 능력을 촉진시키는 새로운 로봇 데이터셋과 훈련 패러다임을 제공한다.

### [Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts](https://arxiv.org/abs/2408.15664)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15664.png)

Vote: 4

Authors: Xu Sun, Lean Wang, Chenggang Zhao, Huazuo Gao, Damai Dai

- **What's New**: 해당 논문은 대규모 언어 모델(LLM: Large Language Model)에서 파라미터를 확장하는 과정에서 발생하는 계산 비용을 관리하기 위한 새로운 부하 균형 전략인 Loss-Free Balancing을 제안합니다. 이 접근법은 추가적인 보조 손실(auxiliary loss) 없이 모듈의 부하를 균형 있게 유지할 수 있도록 설계되었습니다.
- **Technical Details**: 전통적인 보조 손실을 사용하는 부하 균형 전략과 달리, Loss-Free Balancing은 각 전문가의 부하 상태에 따라 게이트 점수(gating scores)를 조정합니다. 전문가별 편향(bias)을 추가하여 토큰 라우팅을 조정하며, 무거운 부하를 가진 전문가의 편향을 줄이고 가벼운 부하를 가진 전문가의 편향을 늘리는 방식으로 동적으로 업데이트합니다. 이를 통해 토큰이 고르게 분배되도록 합니다. 이 과정에서 비전망성 미래 토큰의 정보를 누설하지 않기 위해 이전 배치의 부하 정보를 사용하여 업데이트합니다.
- **Performance Highlights**: Loss-Free Balancing을 적용한 MoE 모델은 보조 손실을 사용하는 기존 방법보다 더 나은 검증 손실(validation loss)을 보였으며, 글로벌 및 배치 수준에서 더 나은 부하 균형을 달성했습니다. 또한, 이 방식은 전문가 병렬화(expert parallelism)와 자연스럽게 호환되어 매우 큰 MoE 모델의 훈련에 효과적입니다.

### [Towards Realistic Example-based Modeling via 3D Gaussian Stitching](https://arxiv.org/abs/2408.15708)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15708.png)

Vote: 4

Authors: Xiaogang Jin, Ziyi Yang, Sipeng Yang, Xiaoguang Han, Bingchen Gong, Xinyu Gao

- **What's New**: 3D 물체의 다양한 부품을 결합하여 새로운 물체를 생성하는 전통적인 컴퓨터 그래픽(CG) 모델링 기법을 발전시켜, 실제 세계에서 캡처된 부품들을 사용해 현실감 있는 모델링을 수행하는 방법을 제안합니다. 특히 Neural Radiance Fields(NeRF) 기술의 발전으로 인해 포토리얼리스틱한 3D 복원 및 렌더링이 가능해졌습니다. SeamlessNeRF (Gong et al., 2023)은 처음으로 매끄러운 병합 문제를 다루기 위해 설계되었으나, 실제 사례에서는 세밀한 편집이나 실시간 대화형 워크플로우를 지원하지 못하는 한계가 있습니다. 이러한 문제를 해결하기 위해, 우리는 3D Gaussian Splatting(3DGS)을 사용하여 상호작용이 가능한 편집과 다수의 부품을 스티칭하는 새로운 방법을 제안합니다.
- **Technical Details**: 우리가 제안한 방법은 3DGS의 점 기반 표현을 사용하여 세밀한 편집이 가능하며, 실시간 대화형 편집 환경을 제공합니다. SeamlessNeRF와 달리, 우리는 새로운 샘플링 기반 최적화 전략을 도입하여 색조뿐만 아니라 구조적 특성도 매끄럽게 전파할 수 있습니다. 두 가지 주요 단계로 나뉘어집니다. 첫 번째 단계(S-phase)에서는 휴리스틱 샘플링 전략을 사용하여 타겟 필드의 테두리에서의 구조적 특성을 고려하여 최적화합니다. 두 번째 단계(T-phase)에서는 소스 필드에서 집계된 특징 팔레트를 사용하여 타겟 필드를 튜닝합니다. 이 두 단계는 합동적으로 수행되며, S-phase에서 발생한 손실을 유지하면서 T-phase의 손실을 추가하여 최적화합니다.
- **Performance Highlights**: 우리의 방법은 실제 세계에서 복잡한 사례를 다루는 탁월한 능력을 가지며, 사용자가 실시간으로 상호작용할 수 있는 사용자 친화적인 GUI를 제공합니다. 3DGS를 사용하여 현실감 있고 매끄러운 부품 결합을 지원하는 최초의 작업으로, 부품 스티칭과 예제 기반 모델링의 한계를 극복합니다.

### [ReMamba: Equip Mamba with Effective Long-Sequence Modeling](https://arxiv.org/abs/2408.15496)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15496.png)

Vote: 3

Authors: Dongyan Zhao, Huishuai Zhang, Bei Li, Danlong Yuan, Jingang Wang, Xunliang Cai, Jiahao Liu

- **What's New**: 새로운 모델 ReMamba는 기존의 Mamba 모델의 한계를 극복하기 위해 제안되었습니다. ReMamba는 Mamba 모델의 정보를 효과적으로 압축하여 장문의 텍스트에서도 뛰어난 성능을 발휘하도록 설계되었습니다. 실험 결과, ReMamba는 LongBench와 L-Eval 벤치마크에서 기존 Mamba 모델보다 각각 3.2점과 1.6점 향상된 성능을 보였습니다.
- **Technical Details**: Mamba 모델은 시간 복잡성을 선형으로 유지하면서도 상수 메모리 요구를 충족시키기 위해 순환 추론 모드를 사용합니다. 그러나 이러한 구조는 RNN과 유사한 특성을 지니고 있어 장문의 텍스트에서 초기 입력 정보를 잘 유지하지 못하는 문제가 있었습니다. ReMamba는 첫 번째 순방향 패스에서 상위 k개의 숨겨진 상태를 선택하고 이를 두 번째 순방향 패스에서 상태 공간에 통합하는 전략으로 이러한 문제를 해결했습니다.
- **Performance Highlights**: ReMamba는 Mamba 모델의 장문 텍스트 처리 성능을 크게 향상시켜, Transformer와 거의 동등한 성능을 발휘할 수 있습니다. LongBench 벤치마크에서 3.2점, L-Eval 벤치마크에서 1.6점의 성능 향상이 확인되었습니다. 이러한 접근 방식은 Mamba2에서도 적용 가능하여, LongBench 벤치마크에서 1.6점의 성능 향상을 가져왔습니다.

### [TEDRA: Text-based Editing of Dynamic and Photoreal Actors](https://arxiv.org/abs/2408.15995)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15995.png)

Vote: 2

Authors: Marc Habermann, Heming Zhu, Christian Theobalt, Adam Kortylewski, Mohit Mendiratta, Basavaraj Sunagad

- **What's New**: 높은 현실성과 제어 가능성을 갖춘 전신 아바타의 텍스트 기반 편집을 가능하게 하는 첫 번째 방법인 TEDRA를 소개합니다. 이 방법은 디지털 아바타의 세부적인 주름 패턴을 유지하면서 동적성을 보장하여 텍스트 입력을 기반으로 한 3D 전신 아바타 편집을 실현합니다.
- **Technical Details**: TEDRA는 멀티뷰 비디오에서 훈련된 전신 아바타를 입력으로 받아 텍스트 기반 조건부 이미지 생성(diffusion model)을 이용하여 편집을 수행합니다. 핵심 기술은 Personalized Normal-Aligned Score Distillation Sampling (PNA-SDS)으로, 이는 사용자 맞춤형 편집을 보장하며 동적 세부 사항을 유지합니다. PNA-SDS는 두 개의 잠재적 확산 모델(하나는 개인화되었고 다른 하나는 사전 훈련됨)을 사용하여 반복적인 편집을 수행하고, 아바타의 렌더링 된 법선을 조건으로 하여 동적성을 보존하며 로컬 편집을 강화합니다.
- **Performance Highlights**: 종합적인 평가를 통해 TEDRA가 다양한 텍스트 기반 편집 작업을 성공적으로 수행하며 원래 아바타의 정체성을 유지함을 확인했습니다. 또한 TEDRA를 사용한 아바타의 애니메이션 결과는 시간적 일관성과 우수한 성능을 보여줍니다.

