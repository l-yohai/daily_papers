## Daily Papers (2024-08-16)

### [DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://arxiv.org/abs/2408.08152)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08152.png)

Vote: 21

Authors: Zhihong Shao, Z. Z. Ren, Wanjia Zhao, Z. F. Wu, Junxiao Song, Haocheng Wang, Xuan Lu, Liyue Zhang, Chong Ruan, Dejian Yang, Qihao Zhu, Huajian Xin, Wenjun Gao, Fuli Luo, Zhibin Gou, Bo Liu, Qiushi Du

- **What's New**: 최근 인공지능 분야에서의 대형 언어 모델의 발전이 수학적 추론과 정리 증명에 큰 영향을 미치고 있습니다. 특히 DeepSeek-Prover-V1.5라는 새로운 접근법을 도입하여, 기존의 증명 단계 생성과 전체 증명 생성 두 가지 전략을 통합하여 성능을 향상시켰습니다.
- **Technical Details**: DeepSeek-Prover-V1.5는 truncate-and-resume 메커니즘을 통해 증명 코드의 일부분을 생성하고, 오류가 발생하면 해당 부분을 잘라내고 새로 생성하는 방식을 사용합니다. 또한, Monte-Carlo Tree Search(MCTS)를 통합하여 성공적인 증명 세그먼트를 이어서 생성합니다. 더욱이, 본 연구는 증명 탐색의 보상 희소성 문제를 해결하기 위해 완전히 새로운 보상 없는 탐색 알고리즘을 제안하였습니다.
- **Performance Highlights**: DeepSeek-Prover-V1.5는 miniF2F 벤치마크에서 60.2%의 통과율을 기록하여, 이전 버전인 DeepSeek-Prover-V1의 50.0% 대비 10.2% 증가했습니다. 트리 탐색 기법을 통합하여 63.5%의 새로운 최고 기록을 세웠습니다. ProofNet에서는 검증 셋에서 21.6%, 테스트 셋에서 23.7%의 통과율을 기록하였으며, 트리 탐색 기법을 통합한 결과, 각각 25.4%, 25.3%로 성능이 향상되었습니다.

### [I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm](https://arxiv.org/abs/2408.08072)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08072.png)

Vote: 16

Authors: Chenghua Lin, Ge Zhang, Jiaheng Liu, Xingwei Qu, Lei Ma, Yiming Liang, Tianyu Zheng, Xinrun Du, Zhenzhu Yang, Jiajun Zhang, Jiawei Guo, Wenhao Huang

- **What's New**: 본 논문은 LLMs(Large Language Models)를 인간과 같은 AGI(Artificial General Intelligence)로 발전시키기 위해 자율적이고 지속적인 자기 정렬을 가능하게 하는 새로운 패러다임인 I-SHEEP를 제안합니다. I-SHEEP는 메타인지(metacognitive) 자기 평가를 통해 모델이 스스로 학습 과정에서 올바른 인지를 유지하고 부정확한 인지를 걸러내어 지속적으로 자기 정렬을 수행할 수 있도록 합니다. 이는 인간 학습의 자동적이고 연속적인 특성을 모방합니다.
- **Technical Details**: I-SHEEP는 시드 데이터로부터 출발하여 LLM의 강력한 이해 및 생성 능력을 활용하여 추가적인 instruction-output 데이터 쌍을 생성합니다. 메타인지 자기 평가를 통해 생성된 데이터의 품질을 검토하고 부정확한 인지는 걸러냅니다. 이 과정을 반복하여 모델은 자율적으로 내부 지식을 기반으로 지속적인 학습을 수행합니다. 데이터 생성은 Self Instruct 방식을 기반으로 하며, Alpaca에서 소개된 표준화된 instruction 형식을 사용합니다. 학습 과정은 In-Context Learning (ICL)과 zero-shot 접근 방식을 포함합니다.
- **Performance Highlights**: 실험 결과, 메타인지 능력의 수준과 모델의 크기에 따라 I-SHEEP의 자기 향상 잠재력이 밀접하게 관련되어 있음을 발견했습니다. 다양한 모델 크기와 메타인지 자기 평가 수준에서 I-SHEEP의 성능을 탐구하여, 모델이 외부 신호에 거의 의존하지 않고도 자율적으로 자체 정렬을 수행할 수 있는 가능성을 확인했습니다. 이는 인간과 같은 AGI로의 발전에 중요한 한 걸음이 될 수 있습니다.

### [Heavy Labels Out! Dataset Distillation with Label Space Lightening](https://arxiv.org/abs/2408.08201)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08201.png)

Vote: 11

Authors: Jingwen Ye, Xinchao Wang, Ruonan Yu, Zigeng Chen, Songhua Liu

- **What's New**: 본 연구는 대규모 데이터셋에서 발생하는 문제를 해결하기 위해 'Dataset Distillation'을 제안한 최신 논문입니다. 이는 원래의 큰 데이터셋을 훨씬 작은 합성 데이터셋으로 압축하여 원래와 동일한 훈련 성능을 유지하면서, 다운스트림 신경망 훈련에 효과적으로 사용할 수 있도록 합니다. 특히, 새로운 'Heavy Labels Out(HeLlO)' 프레임워크를 통해 소프트 라벨(soft labels)의 저장 비용을 크게 줄일 수 있습니다.
- **Technical Details**: 기존 데이터셋 증류 방법들은 bi-level 최적화 문제로 인해 확장성에 한계가 있었으나, HeLlO는 사전 학습된 CLIP 모델을 사용하여 이미지와 라벨 간의 프로젝터를 구축하고 이를 통해 소프트 라벨의 저장 공간을 줄였습니다. 또한 LoRA-like 지식 전이 방법과 이미지 최적화 기법을 도입하여, 원래 라벨 생성기와의 격차를 최소화했습니다. 이를 통해 데이터셋 증류 시 발생하는 소프트 라벨 저장 문제를 혁신적으로 해결했습니다.
- **Performance Highlights**: 실험 결과, 원래 소프트 라벨 저장 공간의 0.003%만을 사용하면서도 최신 데이터셋 증류 방법들과 비슷하거나 더 뛰어난 성능을 보였습니다. 예를 들어, ImageNet-1K 데이터셋에서 10 IPC(이미지 당 클래스)로 훈련된 네트워크가 카테고리 하드 라벨을 사용할 때 15.2%의 정확도를 보이지만, 소프트 라벨을 사용할 때 42.1%의 정확도를 보여주는 등 성능 격차를 효과적으로 해소하였습니다.

### [FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance](https://arxiv.org/abs/2408.08189)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08189.png)

Vote: 7

Authors: Xiaodan Liang, Jing Wang, Jiasong Feng, Dawei Leng, Ao Ma, Yuhui Yin, Bo Cheng

- **What's New**: FancyVideo는 텍스트-비디오 생성 모델(T2V) 중에서도 복잡한 시공간 관계를 이해하는 능력을 갖춘 새로운 접근법입니다. 이 모델은 Cross-frame Textual Guidance Module (CTGM)을 도입하여 각 프레임의 텍스트 조건을 조절하고, 이를 통해 더욱 동적이고 일관성 있는 비디오를 생성합니다.
- **Technical Details**: FancyVideo는 CTGM을 통해 시공간 정보를 더욱 효과적으로 활용합니다. CTGM은 Temporal Information Injector (TII), Temporal Affinity Refiner (TAR), Temporal Feature Booster (TFB)로 구성되어 있으며, 이 모듈들의 협력으로 텍스트와 비디오 간의 모션 논리를 충분히 반영할 수 있게 합니다. 또한 Latent Diffusion Models (LDMs)을 사용하여 픽셀 공간이 아닌 압축된 잠재 공간에서 디노이징 과정을 진행합니다.
- **Performance Highlights**: FancyVideo는 EvalCrafter 벤치마크에서 SOTA 결과를 달성했으며, UCF-101 및 MSR-VTT에서도 경쟁력 있는 성능을 입증했습니다. 실험 결과 Cross-frame Textual Guidance를 통한 접근법이 고품질 비디오 생성에 매우 효과적이라는 것을 보여주었습니다.

### [Towards flexible perception with visual memory](https://arxiv.org/abs/2408.08172)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08172.png)

Vote: 6

Authors: Xi Yi, Priyank Jaini, Austin Stone, Robert Geirhos, Sourabh Medapati, Jonathon Shlens, George Toderici, Abhijit Ogale

- **What's New**: 이번 연구에서는 ImageNet-1K와 iNaturalist 데이터셋에 대한 집계 방법 비교, 하이퍼파라미터 민감도 분석, 추출률 분석, 확장 법칙 세부 사항, NINCO 데이터셋에 대한 OOD(Out-Of-Distribution) 분석, 메모리 가지치기와 관련된 세부 사항, 계층적 라벨 예측을 위한 알고리즘 등을 추가로 제공합니다. 또한, ImageNet-A에서 발생한 추가 오류를 시각화하였습니다.
- **Technical Details**: Section 3.3에서 언급한 대로, log10(memory size)와 log10(error rate) 사이의 로그 형태가 데이터에 잘 맞는다는 것을 발견했습니다. 특히, DinoV2 ViT S14와 DinoV2 ViT L14에 대해 np.polyfit를 사용하여 아래와 같은 함수 형태를 도출했습니다: 
x=log10(memory-size), y=log10(error-rate), memory-size는 [10^3, 10^9] 사이에 있고, error-rate는 [0, 100] 범위입니다.
- **Memory Pruning Details**: Section 3.5의 메모리 가지치기에서는 두 가지 가지치기 방법을 구현했습니다: 신뢰할 수 없는 이웃을 완전히 제거하는 '하드 가지치기'와 가중치를 줄이는 '소프트 가지치기'입니다. ImageNet-train 세트를 메모리에 포함하여 ImageNet validation 세트에 대해 결과를 보고합니다. 하드 가지치기에서는 128번 이상 잘못된 결정을 내린 이미지 26,257장을 메모리에서 제외했습니다. 소프트 가지치기에서는 신뢰도 계수 γ를 곱하여 신뢰할 수 없는 이웃의 가중치를 조정했습니다. 예를 들어, 잘못된 결정을 한 번 내린 이미지에는 γ=0.88, 100번 잘못된 결정을 한 이미지에는 γ=0.02를 부여했습니다.

### [Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability](https://arxiv.org/abs/2408.07852)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.07852.png)

Vote: 5

Authors: Igor Mordatch, Jaehoon Lee, Laura Culp, Alex Rizkowsky, Azade Nova, Maxwell Bileschi, Ben Adlam, +, Izzeddin Gur, Peter J. Liu, Kathleen Kenealy, Jeffrey Pennington, Rosanne Liu, C. Daniel Freeman, Noah Fiedel, Aaron Parisi, Roman Novak, Bernd Bohnet, Gaurav Mishra, Gamaleldin Elsayed, JD Co-Reyes, Jiri Hron, Isabelle Simpson

- **What's New**: 최근 연구들은 대형 언어 모델(Large Language Models, LMs)의 생성 및 예측 능력이 빠르게 발전하고 있음에도 여전히 '환각 현상(hallucinations)'이 큰 과제로 남아있다고 지적합니다. 이 논문에서는 KG(Knowledge Graph)를 활용하여 LMs가 어떤 정보를 학습하는지 완벽하게 제어한 상태에서 훈련을 진행함으로써 기존의 자연 언어 기반 훈련과의 차별성을 강조합니다.
- **Technical Details**: 연구팀은 KG에서 추출한 데이터를 사용해 여러 크기의 LMs를 새롭게 훈련했습니다. KG는 [주어, 술어, 객체]의 삼중쌍(triplet)으로 이루어진 구조화된 데이터로, 생성된 사실이 데이터셋에 존재하는지 직접 질의할 수 있어 환각 현상을 정량적으로 측정하는 데 유용합니다. 훈련 중 각 고유 정보는 한 번의 epoch 동안만 노출되어 여러 번 반복되는 자연 언어와는 차별화됩니다. 또한, auto-regressive log-likelihood를 최적화하는 방식으로 모델을 훈련했으며, 모델 평가 시 주어와 술어를 주고 객체를 예측하게 했습니다.
- **Performance Highlights**: 연구 결과는 세 가지 주요 관찰점을 제시합니다. 첫째, 훈련 손실이 수렴하고 환각률이 감소하기 위해 다중 epoch 훈련이 필요합니다. 둘째, 모델 크기와 훈련 토큰 수 사이의 스케일링이 기존 최적화 방식과 다르며, 훈련 데이터셋의 환각 현상을 제거하기 위해서는 훨씬 큰 모델이 필요합니다. 셋째, 훈련 데이터세트 크기를 증가시키면 고정된 모델 크기와 훈련 길이에서 성능이 저하될 수 있습니다. 또한, 감지기(Detectors)를 통해 환각을 탐지하거나 교정하는 방법을 조사한 결과, 더 큰 감지기가 더 나은 성능을 보이지만, 더 강력한 LMs는 환각을 탐지하기 더 어렵다는 사실도 발견했습니다.

### [MVInpainter: Learning Multi-View Consistent Inpainting to Bridge 2D and 3D Editing](https://arxiv.org/abs/2408.08000)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08000.png)

Vote: 4

Authors: Fan Wang, Chenjie Cao, Xiangyang Xue, Yanwei Fu, Chaohui Yu

- **What's New**: 3D 장면 편집의 중요한 진보를 소개한 이 논문은 몇 개의 조작된 2D 참고 이미지를 다른 관찰된 뷰로 확장하는 방법을 연구합니다. 특히, 확산 기반의 텍스트-이미지 변환(Text-to-Image, T2I) 모델의 발전과 함께 새로운 뷰 생성과 3D 생성, 제어 가능한 생성에서 중요한 성공을 이루었습니다.
- **Technical Details**: 이 논문은 MVInpainter라는 다중 뷰 일관성(inpainting) 모델을 소개하며, 사전 훈련된 StableDiffusion(SD) inpainting 모델 기반으로 구축되었습니다. MVInpainter는 각 뷰 간 일관성이 있는 구조를 위해 도메인 어댑터와 모션 모듈을 통합하며, 외부 광 흐름(optical flow)을 통해 암시적으로 자세를 제어합니다. 또한, 우리의 모델은 명시적인 카메라 포즈 없이 작동하여 2D 및 3D 편집 모두에 새로운 가능성을 열어줍니다.
- **Performance Highlights**: MVInpainter는 새로운 뷰에서 일관성 있는 인페인팅 결과를 달성하고, 명시적인 포즈 조건이나 테스트 시간 최적화 없이 작동합니다. 이를 통해 기존 NVS 방법의 어려움을 크게 완화하며, 다양한 데이터셋에서 높은 성능을 발휘합니다. 예를 들어, MVInpainter는 CO3D, MVImgNet, Omni3D와 같은 객관 중심 데이터셋뿐만 아니라, Scannet++, Real10k 등의 복잡한 전방향 장면에서도 효과적으로 작동함을 실험적으로 확인했습니다.

### [BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts](https://arxiv.org/abs/2408.08274)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08274.png)

Vote: 3

Authors: Qizhen Zhang, Simon Guo, Acyr Locatelli, David Cairuz, Phil Blunsom, Ahmet Ustun, Bharat Venkitesh, Nikolas Gritsch, Sebastian Ruder, Dwaraknath Gnaneshwar, Jakob Foerster

- **What's New**: 이번 연구는 대형 언어 모델(LLMs)의 성능과 효율성을 개선하기 위해 제안된 Branch-Attend-Mix (BAM) 모델을 소개합니다. BAM은 기존의 Branch-Train-MiX (BTX) 모델의 한계를 극복하고자 개발되었으며, 특히 FFN 파라미터와 주의(attention) 파라미터 모두를 활용하여 전문 지식을 최대한 활용합니다.
- **Technical Details**: BAM은 Mixture of Attention (MoA) 모델의 변형을 사용하여 전문화된 주의 전문 모델(attention experts)을 활용합니다. 이를 통해 각 토큰은 각각의 주의 전문 모델에 할당됩니다. BAM은 두 가지 변형을 제안합니다: 1) 모든 주의 파라미터를 포함하는 KV Experts로 최고 성능 달성, 2) 키와 값 파라미터를 공유하여 추론 효율성을 향상시키는 KV sharing. 또한, 병렬 주의 변환기 구조(parallel attention transformer architecture)를 사용하여 주의 전문가와 FFN 전문가를 병렬로 계산하여 처리량을 향상시킵니다.
- **Performance Highlights**: 590백만에서 20억 파라미터에 이르는 시드 모델을 대상으로 한 실험 결과, BAM은 퍼플렉시티(perplexity)와 다양한 도메인에서의 다운스트림 작업 성능 모두에서 BTX 모델을 능가하는 결과를 보였습니다. 이를 통해 제안된 개선 사항의 효과를 입증하였습니다.

### [Accelerating High-Fidelity Waveform Generation via Adversarial Flow Matching Optimization](https://arxiv.org/abs/2408.08019)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08019.png)

Vote: 3

Authors: Ha-Yeong Choi, Seong-Whan Lee, Sang-Hoon Lee

- **What's New**: 이번 논문에서는 최근에 발전된 생성 모델, Conditional Flow Matching (CFM) (Lipman et al. 2022; Tong et al. 2023)과 Generative Adversarial Networks (GANs) (Goodfellow et al. 2014; Lee et al. 2021)을 결합하여 Neural ODE 기반의 Waveform 생성기를 가속화하는 방법을 탐구합니다. PeriodWave-Turbo라는 새로운 ODE 기반의 Waveform 생성기를 제안하여 성능을 개선하고 Generation 속도를 현저히 높였습니다.
- **Technical Details**: CFM과 GAN을 결합한 Adversarial Flow Matching Optimization 방법을 제안하여, Pre-trained CFM 생성 모델을 고정 단계 ODE 생성기로 전환합니다. 이 과정에서 재구성 손실(Mel-spectrogram Reconstruction Loss)과 Adversarial Feedback을 활용하여 생성 속도를 가속화합니다. 고정된 두 단계 또는 네 단계의 ODE 샘플링을 사용하여 노이즈 x0에서 Raw Waveform 신호를 생성합니다. Multi-scale Mel-spectrogram Reconstruction Loss와 Multi-period Discriminator (MPD) 및 Multi-scale Sub-band Constant-Q Transform Discriminator (MS-SB-CQTD)를 통해 성능을 더욱 향상시켰습니다.
- **Performance Highlights**: LibriTTS 벤치마크에서 Perceptual Evaluation of Speech Quality (PESQ) 점수 4.454를 달성하여 기존의 GAN 기반 모델 및 Pre-trained CFM 생성 모델을 능가하는 성과를 보였습니다. 이러한 성능 향상은 최소한의 Fine-tuning으로 구현할 수 있으며, 전체 훈련 시간도 크게 단축되었습니다.

### [The ShareLM Collection and Plugin: Contributing Human-Model Chats for the Benefit of the Community](https://arxiv.org/abs/2408.08291)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08291.png)

Vote: 2

Authors: Shachar Don-Yehiya, Leshem Choshen, Omri Abend

- **What's New**: ShareLM은 인간-모델 대화 데이터셋을 통합하여 연구 및 커뮤니티에서 이용할 수 있도록 하는 새로운 플랫폼입니다. 특히, ShareLM 플러그인은 사용자가 모델과의 대화를 쉽게 공유할 수 있게 해줍니다.
- **Technical Details**: ShareLM 플러그인은 크롬 확장 프로그램으로, 다양한 플랫폼을 지원하며 사용자가 대화를 평가할 수 있는 thumbs up/down 기능을 제공합니다. 사용자는 24시간 이내에 대화를 검토하고 개인 정보를 포함한 대화를 삭제할 수 있습니다. 이는 사용자가 자신의 데이터를 소유하고 관리할 수 있게 합니다. 대화는 PostgreSQL 데이터베이스에 저장되며, 주기적으로 업데이트된 데이터셋이 공개됩니다.
- **Performance Highlights**: ShareLM 컬렉션은 현재 40개 이상의 모델에서 수집한 230만 건 이상의 대화를 포함하고 있습니다. 플러그인의 주요 특징 중 하나는 사용자가 자신의 대화를 철저히 익명화할 수 있도록 돕는 스크립트를 실행한다는 것입니다. 플러그인은 Gradio와 ChatUI를 지원하며, 새로운 웹 플랫폼의 추가 지원도 쉽게 가능합니다.

### [Can Large Language Models Understand Symbolic Graphics Programs?](https://arxiv.org/abs/2408.08313)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08313.png)

Vote: 2

Authors: Haiwen Feng, Joshua B. Tenenbaum, Katherine M. Collins, Adrian Weller, Zeju Qiu, Weiyang Liu, Zhen Liu, Tim Z. Xiao, Bernhard Schölkopf, Michael J. Black

- **What's New**: SGP-Bench는 다양한 프로젝트의 구현을 채택하여 구축되었습니다. 특히, MathVista, vLLM, simple-evals 등의 오픈소스 프로젝트의 구현을 사용했습니다. 이번 벤치마크는 SVG 데이터, CAD(3D) 시퀀스 데이터 및 MNIST SVG 데이터를 포함하여 다양한 데이터 셋을 사용했습니다. 모델 평가에는 OpenAI API와 vLLM을 활용하여 대규모 평가를 수행했습니다.
- **Technical Details**: SGP-Bench는 고효율 모델 추론을 위해 vLLM 엔진을 사용하며, 데이터 소스의 라이센스를 준수합니다. SVG 데이터 처리는 BeautifulSoup과 SvgLib를 사용하여 번역 및 회전 처리를 수행했습니다. CAD 시퀀스는 DeepCAD와 SketchGraphs에서 샘플링되었으며, 모델 추론을 위해 PythonOCC를 사용하여 정규화 및 검증과정을 거쳤습니다. 평가 과정에서는 질문 답변 정확도를 기준으로 하며, GPT API와의 호환성을 위해 vLLM 서버를 사용했습니다.
- **Performance Highlights**: 평가된 모델 가운데 Gemma-1.1-2B/7B 모델은 구글 DeepMind와 협력하여 개발되었으며, Mistral-0.3-7B, Mistral-NeMo 등의 모델도 포함되었습니다. 전반적으로 Gemma 모델은 텍스트 생성에서 우수한 성능을 보였으며, 특정 모델들은 코드 생성 및 이해 중심으로 특화되었습니다. 모든 모델의 성능은 질문 답변 정확도를 통해 검증되었고, 결과는 Table 1에 요약되어 있습니다.

