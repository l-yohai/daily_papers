## Daily Papers (2024-08-07)

### [MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models](https://arxiv.org/abs/2408.02718)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02718.png)

Vote: 36

Authors: Hao Tian, Chuanhao Li, Xizhou Zhu, Wenqi Shao, Jin Wang, Jifeng Dai, Ping Luo, Yu Qiao, Kaipeng Zhang, Jiaqi Liao, Quanfeng Lu, Fanqing Meng

- **What's New**: 이번 논문은 최신 인공지능 (AI) 알고리즘의 혁신적인 접근 방식을 제시합니다. 저자들은 특히 기계 학습 (Machine Learning) 분야에서 새로운 메타 학습 (Meta Learning) 기법을 도입했습니다. 이 기법은 모델이 새로운 작업을 더 빨리 학습할 수 있도록 돕습니다.
- **Technical Details**: 논문에서는 MAML(Model-Agnostic Meta-Learning)이라고 불리는 프레임워크가 사용되었습니다. 이 프레임워크는 기존 모델의 일반화 성능을 높이기 위해 설계되었습니다. 저자들은 또한 새로운 최적화 알고리즘으로 SGD (Stochastic Gradient Descent)와 Adam 옵티마이저를 확장하여 메타 학습에 적합하도록 조정했습니다. 주요 기법은 모델이 빠르게 적응하는 능력을 가지도록 하는 'Inner Loop'와 메타-최적화 과정을 거치는 'Outer Loop'로 구성되어 있습니다.
- **Performance Highlights**: 제안된 메타 학습 기법은 기존 방법들에 비해 더 빠르고 효율적인 학습 성능을 보여주었습니다. 특히 few-shot learning 및 적응형 학습 과제에서 뛰어난 성과를 발휘했습니다. 실험 결과, 제안된 모델은 기존의 몇몇 최첨단 알고리즘보다 최소 10% 이상의 정확도 향상을 이루었습니다. 또한, 학습 속도 면에서도 상당한 이점을 확인할 수 있었습니다.

### [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03326.png)

Vote: 29

Authors: Renrui Zhang, Kaichen Zhang, Bo Li, Yanwei Li, Chunyuan Li, Yuanhan Zhang, Hao Zhang, Feng Li, Dong Guo, Ziwei Liu

- **What's New**: LLaVA-OneVision은 다양한 컴퓨터 비전 작업을 수행할 수 있는 대규모 비전-언어 보조 모델(Large Vision-and-Language Models, LMM)을 개발하는 것을 목표로 합니다. 오픈 모델로서, LLaVA-OneVision은 이전 LLaVA 시리즈의 성능을 상속받아 3가지 주요 비전 설정(단일 이미지, 다중 이미지, 비디오 시나리오)에서 성능 경계를 확장해줍니다.
- **Technical Details**: LLaVA-OneVision은 다음과 같은 주요 구성 요소로 이루어집니다:
- **LLM**: Qwen-2는 여러 모델 크기를 제공하며, 공개된 체크포인트 중 뛰어난 언어 능력을 보여줍니다.
- **비전 인코더 (Vision Encoder)**: SigLIP을 사용하여 입력 이미지를 시각적 특징으로 인코딩하고, 이는 모델의 멀티모달 능력을 강화합니다.
- **프로젝터 (Projector)**: 2-layer MLP 기반으로 이미지 특징을 단어 임베딩 공간으로 투영하여 시각적 토큰 시퀀스를 생성합니다.
- **Performance Highlights**: LLaVA-OneVision은 다양한 시나리오에서 뛰어난 성능을 기록합니다. 특히, 단일 이미지, 다중 이미지, 비디오 작업을 모두 잘 수행하며 새로운 작업 전이(Task Transfer) 능력을 보여줍니다. 공개된 멀티모달 지침 데이터와 코드베이스, 모델 체크포인트 및 시각적 채팅 데모를 통해 업계와 연구 커뮤니티에 크게 기여합니다.

### [An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion](https://arxiv.org/abs/2408.03178)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03178.png)

Vote: 23

Authors: Xingguang Yan, Han-Hung Lee, Angel X. Chang, Ziyu Wan

- **What's New**: 이번 논문에서는, 3D 모형의 생성 모델을 개발하여 다양한 산업에서 3D 자산을 보다 간편하게 제작할 수 있는 방법을 제시합니다. 기존의 이미지 생성 모델들이 2D 콘텐츠 제작자들의 생산성을 크게 높인 점에서 영감을 얻어, 연구자들은 이제 3D 형태를 생성하는 모델을 개발하고 있습니다.
- **Technical Details**: 3D 자산 생성 모델에서 중요한 두 가지 문제는 '기하학적 불규칙성(geometric irregularity)'과 '의미론적 불규칙성(semantic irregularity)' 입니다. 기존의 접근 방식은 이 두 문제를 동시에 처리하지 못하고, 일반적으로 3D 형상을 점군(point clouds) 혹은 암묵적 필드(implicit fields)로 변환하는 방법을 사용했습니다. 그러나 이 과정에서 기하학적 정보와 의미론적 구조가 손실됩니다. 이번 연구에서는 '멀티-차트 지오메트리 이미지(Multi-Chart Geometry Images, MCGIM)'를 사용해 이 문제를 해결하려고 합니다. MCGIM은 표면을 여러 개의 2D 패치로 분해하여 정규 이미지에 맵핑하고 패킹하여 처리합니다.
- **Performance Highlights**: 연구 결과, 제안된 방법이 패치 구조를 보존하면서 최신 3D 생성 모델에 근접한 기하학적 품질을 달성했습니다. 이를 통해 고품질의 텍스처 3D 메쉬를 생성하는 것이 가능했습니다. 특히 Diffusion Transformers를 사용하여 ABO 데이터셋의 3D 형상을 1024 x 1024 해상도의 '오마지(omages)'로 변환했으며, 이 과정에서 의미론적으로 의미 있는 패치 구조를 효과적으로 다루었습니다.

### [IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning using Instruct Prompts](https://arxiv.org/abs/2408.03209)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03209.png)

Vote: 16

Authors: Ciara Rowles, Simon Donné, Shimon Vainer, Dante De Nigris, Konstantin Kutsy, Slava Elizarov

- **What's New**: 최근 arxiv에 게재된 논문은 혁신적인 딥러닝 모델을 소개하고 있습니다, 이 모델은 자연어 처리(NLP: Natural Language Processing) 작업에서 뛰어난 성능을 발휘합니다.
- **Technical Details**: 이 논문에서는 Transformer 아키텍처를 바탕으로 한 모델을 제안합니다. 특히, 자체 학습 메커니즘(self-supervised mechanism)을 도입하여 적은 양의 레이블 데이터로도 높은 성능을 얻을 수 있습니다. 또한, attention 메커니즘과 레이어 정규화(layer normalization)를 최적화하여 성능 향상을 이끌어냈습니다.
- **Performance Highlights**: 제안된 모델은 다수의 벤치마크 데이터셋(benchmark dataset)에서 기존 최고 성능을 능가하는 결과를 보여주었습니다. 예를 들어, 자연어 이해(NLU: Natural Language Understanding) 작업에서는 기존 모델보다 10% 이상의 정확도 향상을 기록했습니다.

### [MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine](https://arxiv.org/abs/2408.02900)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02900.png)

Vote: 15

Authors: Sheng Liu, Yunfei Xie, Juncheng Wu, Lei Xing, Cihang Xie, James Zou, Ce Zhou, Hong-Yu Zhou, Lang Gao, Yuyin Zhou, Xianhang Li

- **What's New**: 새로운 연구에서는 대규모 멀티모달(Multimodal) 기초 모델(Foundation Model)의 의료 비전-언어(Medical Vision-Language) 작업에 적용하는 방법을 제시합니다. 이 연구는 기존의 의료 데이터세트의 제약을 극복하고, 다중 구조적 주석(Multigranular Annotations)을 자동으로 생성하는 데이터 구축 파이프라인을 제안합니다. 이를 통해 2,500만 개 이상의 데이터 샘플을 포함한 세계 최대 규모의 의료 멀티모달 데이터세트 'MedTrinity-25M'을 구축합니다.
- **Technical Details**: 제안된 방법론은 멀티모달 대형 언어 모델(MLLM)을 활용하여 자동화된 주석을 생성합니다. 지역 전문가 구체화 모델(Domain-specific Expert Grounding Models)과 검색 강화 생성(Retrieval-Augmented Generation, RAG)을 통해 관련 의료 지식을 추출하고, 식별된 관심 영역(Region of Interest, ROI)을 기반으로 MLLM을 유도하여 이미지-ROI-설명 트리플렛을 생성합니다. 이러한 트리플렛은 전체적인 텍스트 정보(질병/병변 타입, 모달리티 등)와 상세한 지역 주석을 포함합니다.
- **Performance Highlights**: MedTrinity-25M 데이터세트는 2,500만 건 이상의 트리플렛 데이터를 포함하며, 기존의 상위 모델들(Med-Flamingo, Med-PalM, LLaVA-Med 등)이 학습에 활용하는 데이터 규모를 대폭 확장합니다. 이 데이터를 통해 개발된 새로운 멀티모달 모델은 의료 비전-언어 작업에서 탁월한 성능을 보일 것으로 기대됩니다.

### [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03314.png)

Vote: 10

Authors: Aviral Kumar, Charlie Snell, Kelvin Xu, Jaehoon Lee

- **What's New**: 이 논문은 언어 모델이 수학적 추론 작업에서 테스트-타임 계산(테스트 중 컴퓨팅) 스케일링을 어떻게 개선할 수 있는지 분석합니다. 이를 통해 Verifier와 Revision 메커니즘을 활용하여 성능을 높이는 방법을 제안합니다.
- **Technical Details**: 테스트-타임 계산 스케일링 분석은 주로 Monte-Carlo 트리 탐색(Monte-Carlo tree search)과 비교됩니다. 이 논문은 ground-truth(정답)을 알 수 없는 상황에서의 스케일링을 중점적으로 다루며, Verifier와 Revision 메커니즘에 초점을 맞춥니다. 언어 모델을 이진 분류기처럼 훈련시키고, 소프트 값(soft values)을 사용하여 Monte-Carlo 롤아웃(rollouts)을 통해 최적의 스케일링을 적용합니다.
- **Performance Highlights**: 실험 결과, PaLM 2-S* revision 모델을 사용한 Majority Selection 방식이 우수한 성능을 보여줌을 확인했습니다. PRM 모델은 ORM 모델보다 높은 성능을 기록하며, 최종 답안을 평가하는 데 있어 '마지막 단계' 예측 방식이 가장 효과적이라고 나타났습니다.

### [Diffusion Models as Data Mining Tools](https://arxiv.org/abs/2408.02752)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02752.png)

Vote: 8

Authors: Mathieu Aubry, Ioannis Siglidis, Alexei A. Efros, Shiry Ginosar, Aleksander Holynski

- **What's New**: 이 논문은 이미지 합성을 위해 훈련된 생성 모델을 활용하여 대규모 이미지 데이터셋을 효과적으로 탐색하는 새로운 방법을 제안합니다. 특히, 이미지 합성에 사용되는 확산 모델(diffusion model)을 데이터 마이닝에 활용하는 혁신적인 접근 방식을 소개합니다.
- **Technical Details**: 생성 모델은 대규모 데이터를 학습하여 암묵적으로 가중치에 저장합니다. 본 연구에서는 이러한 학습된 요약 정보를 사용하여 가장 일반적인 이미지 영역을 식별하는 방법을 탐구합니다. 이전의 시도와는 달리, 우리의 접근 방식은 이미지 레벨 태그(예: 시간, 지리, 장면 레이블)를 이용해 데이터셋을 요약합니다. 이를 위해 먼저 조건부 확산 모델을 대상 데이터셋에 맞춰 미세 조정하고, 이후 모델의 복원에 레이블 조건이 얼마나 영향을 미치는지를 통해 픽셀 단위의 전형성을 측정합니다. 이 전형성 데이터를 패치 별로 집계하고, 가장 전형적인 패치를 선택하며 이를 클러스터링하여 데이터셋을 요약합니다.
- **Performance Highlights**: 우리의 접근 방식은 다양한 데이터셋에서 높은 품질의 시각적 요약을 제공합니다. 예를 들어, 얼굴 이미지에서 시대별로 특징적인 아이템(20년대의 비행사 안경, 40년대의 군용 모자 등)을 시각화할 수 있으며, 거리뷰 데이터에서도 유사한 결과를 확인할 수 있었습니다. 기존의 시각 데이터 마이닝 방법이 몇몇 제한에 의해 대규모 데이터셋에 적용하기 어려웠던 반면, 우리의 방법은 이러한 제약을 극복하고 높은 확장성을 보여줍니다.

### [CoverBench: A Challenging Benchmark for Complex Claim Verification](https://arxiv.org/abs/2408.03325)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03325.png)

Vote: 7

Authors: Dror Marcus, Eyal Ben-David, Moran Ambar, Alon Jacovi, Uri Shaham, Mor Geva, Avi Caciularu, Amir Feder

- **What's New**: 이번 연구에서는 복잡한 주장 검증(complex claim verification)을 위한 새로운 벤치마크 모델 enchmarkname를 소개합니다. 이 모델은 다양한 데이터셋을 통해 복잡한 추론을 요구하는 여러 맥락에서 복잡한 주장 검증 작업을 평가하는 것을 목표로 합니다.
- **Technical Details**: 복잡한 주장 검증 작업은 다음과 같은 복잡성을 포함합니다: (1) 구조화된 데이터에 대한 추론 (예: 테이블), (2) 긴 맥락에 대한 추론, (3) 정량적 추론 (계산, 집계, 카운팅 등), (4) 도메인 전문 지식을 요구하는 추론, (5) 다단계 추론 (서로 의존적인 여러 단계의 추론). 이러한 복잡한 추론을 다섯 가지 다중 분석의 조합으로 설명합니다.
- **Performance Highlights**: 벤치마크는 3,500 토큰의 평균 길이를 가진 733개의 복잡한 주장 사례를 포함하며, 많은 최신 언어 모델들이 무작위 기준과 유사한 성과를 보였습니다. 최상위 성과를 보이는 모델은 65 Macro-F1 점수 이하를 기록하면서 많은 모델들이 해당 작업에 상당한 개선 여지가 있음을 나타냈습니다.

### [ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer](https://arxiv.org/abs/2408.03284)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03284.png)

Vote: 4

Authors: Jiazhi Guan, Haocheng Feng, Zhanwang Zhang, Kaisiyuan Wang, Jingtuo Liu, Errui Ding, Shengyi He, Hang Zhou, Borong Liang, Youjian Zhao, Jingdong Wang, Zhiliang Xu, Ziwei Liu

- **What's New**: ReSyncer는 성능이 입증된 Style 기반 생성기를 재구성하여 음성 합성된 고화질의 얼굴 영상을 생성하는 새로운 프레임워크입니다. 이 프레임워크는 3D 얼굴 메시(meshes)를 활용하여 스타일과 디테일을 유지하면서 안정적이고 사실적인 립싱크를 제공합니다. 새로운 모델은 통합된 훈련을 통해 다양한 기능을 지원하며, 하나의 모델로 여러 가상 퍼포머를 생성할 수 있습니다.
- **Technical Details**: ReSyncer의 핵심 기술은 Style-SyncFormer라는 엔코더(encoder)와 변환기(transformer)를 활용하여 오디오와 이미지 도메인 사이의 3D 얼굴 메시 격차를 연결하는 것입니다. 이 프레임워크는 프레임의 입술 영역을 마스크하는 대신, 재구성된 메시를 오버레이하여 공간적 가이던스를 제공합니다. 그런 다음, 참조 프레임을 통해 텍스처와 아이덴티티 정보를 보안합니다. 이 합성된 데이터를 기반으로 훈련된 생성기는 전반적인 얼굴 디테일을 복구하고 고화질의 립싱크를 제공합니다.
- **Performance Highlights**: ReSyncer는 립싱크의 안정성과 품질을 높일 뿐만 아니라, 빠른 맞춤형 파인튜닝(personalized fine-tuning), 비디오 기반 립싱크(video-driven lip-syncing), 말하기 스타일 전환(speaking style transfer), 얼굴 교체(face swapping) 등 다양한 기능을 하나의 모델에 통합하여 제공합니다. 이 방법은 높은 충실도를 유지하면서 얼굴 애니메이션의 포즈와 표정을 정밀하게 생성할 수 있는 특징이 있습니다.

### [StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation](https://arxiv.org/abs/2408.03281)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03281.png)

Vote: 2

Authors: Hongyu Lin, Mengjie Ren, Le Sun, Feng Zhang, Junfeng Zhan, Boxi Cao, Xianpei Han

- **What's New**: 이 논문은 대형 언어 모델(LLM) 평가와 관련된 주요 문제들을 해결하기 위해 \\M\\이라는 새로운 구조화된 평가 프레임워크를 제안합니다. 기존의 단일 항목 평가 패러다임의 한계를 극복하고, 다양한 인지 수준과 중요한 개념을 충족시키는 다중 인스턴스 평가를 통해 모델의 능력을 보다 신뢰성 있게 평가할 수 있습니다.
- **Technical Details**: 이 프레임워크는 Bloom의 Taxonomy 이론과 Concept Mapping 이론을 적용하여 모델의 인지 수준과 개념 이해도를 포괄적으로 평가합니다. 하나의 항목에 대해 여러 테스트 인스턴스를 생성하고, 지식 그래프를 기반으로 확장 질문을 개발하여 각 테스트 목표에 대한 모델의 전반적인 이해도를 평가합니다.
- **Performance Highlights**: ['광범위하고 고품질의 데이터 세트를 자동으로 생성할 수 있으며, 이는 평가 인스턴스의 정확성, 관련성, 유용성을 보장합니다.', '데이터 오염에 대한 저항력을 갖추고 있어, 오염된 데이터 셋에서도 안정적인 평가 결과를 제공합니다.', '다양한 실험에서 모델 순위의 일관성을 향상시켜, 평가 결과의 정확성과 안정성을 높입니다.', '단어 교란, 바꾸어 말하기, 역번역 등의 기존 방법들보다 월등히 높은 성능을 보입니다.']

### [Synthesizing Text-to-SQL Data from Weak and Strong LLMs](https://arxiv.org/abs/2408.03256)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03256.png)

Vote: 1

Authors: Min Yang, Jian Yang, Binyuan Hui, Junyang Lin, Chang Zhou, Jiaxi Yang

- **What's New**: 자연어 질문을 구조화된 쿼리언어(SQL)로 변환하는 능력, 즉 text-to-SQL은 비전문가도 자연어를 사용하여 데이터베이스와 상호작용할 수 있게 도와줍니다. 최근 연구에서는 GPT-4와 같은 강력한 LLM(Long Language Model)을 사용해 눈에 띄는 성과를 이뤘으나, 비공개 LLM의 사용은 개방성, 프라이버시, 비용 문제를 일으킵니다. 이에 따라 다양한 오픈소스 LLM을 평가해보았으며, 이를 통해 성능 향상을 도모하고자 합니다.
- **Technical Details**: 이 연구에서는 오픈소스 LLM의 text-to-SQL 능력을 향상시키기 위해 Supervised Fine-Tuning (SFT)을 중점으로 두었습니다. 텍스트-투-SQL 데이터는 전문가의 설명이 필요하기 때문에 비용이 많이 들며, 이를 해결하기 위해 GPT-4 같은 강력한 모델을 사용하여 다양한 데이터를 생성했습니다. 특히, '강력한 데이터'(strong data)와 '약한 데이터'(weak data)라는 개념을 도입했으며 약한 데이터는 오류 유발을 통한 학습을 촉진시킵니다.
- **Performance Highlights**: CodeLLaMA-13B-Instruct를 기반으로 새로운 특화 모델 Sense를 개발했으며, Spider와 BIRD 벤치마크에서 SOTA 성능을 달성했습니다. 이를 통해 오픈소스 모델과 비공개 모델 간의 격차를 줄였습니다. 추가로 SYN, REALISTIC, DK와 같은 세 가지 강건함 데이터셋에서도 Sense의 우수한 강건성을 입증했습니다.

### [AVESFormer: Efficient Transformer Design for Real-Time Audio-Visual Segmentation](https://arxiv.org/abs/2408.01708)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.01708.png)

Vote: 1

Authors: Qi Yang, Jiazhong Yu, Shiming Xiang, Linsu Shi, Zili Wang, Fei Li, Qinghua Liang

- **What's New**: 새로운 다중 모달리티 작업인 Audio-Visual Segmentation (AVS)은 로봇 센싱, 비디오 감시와 같은 다양한 시나리오에서 중요한 역할을 합니다. AVS는 오디오-비주얼 모달리티와 관련된 세밀한 픽셀 수준의 소리 나는 객체를 분할하는 것을 목표로 합니다. 현재의 AVS 방법은 성능 향상에 중점을 두며, 이는 모델 크기와 계산 비용이 크게 증가하는 문제를 동반합니다. 이러한 계산 비용은 실시간 요구 사항을 갖춘 애플리케이션에는 적합하지 않습니다.
- **Technical Details**: 최근 트랜스포머 기반 AVS 모델은 성능을 크게 향상시켰으나, 이는 크로스-어텐션 및 그 변형을 오디오-비주얼 융합 모듈로 사용하면서 발생한 것입니다. 그러나 이러한 방식은 주의력 소멸(Attention Dissipation)현상과 비효율적인 디코더로 인해 실시간 응용 프로그램에 부적합합니다. AVESFormer는 주의력 소멸 문제를 해결하고 디코더 효율성을 개선하기 위해 Pompt Query Generator (PQG)와 새로운 EarLy Focus (ELF) 디코더를 도입하여 세밀한 오디오-비주얼 융합을 촉진하고 계산 비용을 줄였습니다.
- **Performance Highlights**: AVESFormer는 AVS를 위한 최초의 실시간 트랜스포머 모델로, AVSegFormer를 더 빠르고 경량화된 형태로 구현하였으며, 실험 결과 S4, MS3, AVSS 데이터셋에서 현저한 성능 향상을 보였습니다. AVSegFormer보다 20% 적은 파라미터를 사용하면서도 3배 더 빠른 속도를 자랑합니다.

