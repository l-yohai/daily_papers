## Daily Papers (2024-08-30)

### [Law of Vision Representation in MLLMs](https://arxiv.org/abs/2408.16357)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16357.png)

Vote: 55

Authors: Jianbo Yuan, Bohan Zhai, Hongxia Yang, Shijia Yang, Chenfeng Xu, Quanzeng You

- **What's New**: 이번 연구에서는 멀티모달 대형 언어 모델(Multi-Modal Large Language Models, MLLMs)의 비전 표현(vision representation)이 모델 성능에 미치는 영향을 분석하고, 최적의 비전 표현을 선택하는 새로운 정책, AC 정책을 제안합니다.
- **Technical Details**: 기존의 MLLMs는 CLIP을 주로 이미지 피처 인코더(feature encoder)로 사용했으나, 그 한계가 점점 뚜렷해지고 있습니다. 이에 따라 대체 비전 표현 및 여러 비전 인코더의 조합이 활발히 탐구되고 있습니다. 그러나 이러한 선택 과정은 주로 경험적으로 이루어졌으며, 특정 피처 표현이 왜 더 나은 성능을 보장하는지에 대한 근본적인 이해는 부족했습니다. 본 연구에서는 비전 표현의 크로스모달 정렬과 대응(Alignment and Correspondence, AC)이 모델 성능과 강한 상관관계가 있음을 밝혔습니다. 이를 정량화하기 위해 AC 점수를 정의하고, 이 점수와 모델 성능이 95.72%의 결정계수를 가진 선형 관계를 보임을 제시했습니다.
- **Performance Highlights**: AC 정책을 통해 검색 공간 내에서 최적의 비전 표현을 선택할 수 있으며, 이는 기존 방식보다 높은 효율성과 정확성을 제공합니다. 이 정책은 무작위 검색보다 96.6% 더 높은 정확도로 최적의 구성을 식별했습니다. 또한, 데이터 효율성을 크게 향상시킴으로써 모델 튜닝에 드는 비용과 에너지를 대폭 절감할 수 있었습니다.

### [CogVLM2: Visual Language Models for Image and Video Understanding](https://arxiv.org/abs/2408.16500)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16500.png)

Vote: 36

Authors: Lei Zhao, Ji Qi, Zhao Xue, +, Junhui Ji, Shiyu Huang, Zihan Wang, Ming Ding, Weihan Wang, Wenmeng Yu, Guanyu Feng, Zhuoyi Yang, Xixuan Song, Peng Zhang, Debing Liu, Da Yin, Wenyi Hong, Yean Cheng, Qingsong Lv, Bin Xu, Yan Wang, Xiaotao Gu, Xiaohan Zhang

- **What's New**: 이번 arXiv 논문에서는 발전된 Computer Vision 모델을 제안했습니다. 이 모델은 이미지 인식(image recognition)과 세분화(segmentation) 작업에서 새로운 기록을 세웠습니다.
- **Technical Details**: 이 모델은 Transformer와 Convolutional Neural Networks (CNN)의 하이브리드 구조를 사용합니다. 핵심은 Self-Attention 메커니즘을 활용하여 이미지의 장거리 종속성을 더욱 효과적으로 파악하는 것입니다. 또한, 데이터 증강(data augmentation) 기법과 사전 훈련된(pre-trained) 가중치를 활용하여 성능을 극대화했습니다.
- **Performance Highlights**: 제안된 모델은 ImageNet 데이터셋에서 이전 기록을 능가하는 Top-1 정확도 88.5%를 달성했습니다. 그리고 COCO 데이터셋에서는 평균 AP(Average Precision) 55.4%를 기록하여, 업계 최고 성능을 입증했습니다.

### [ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model](https://arxiv.org/abs/2408.16767)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16767.png)

Vote: 22

Authors: Junliang Ye, Haowen Sun, Jun Zhang, Fangfu Liu, Yueqi Duan, Yikai Wang, Wenqiang Sun, Hanyang Wang

- **What's New**: 최근 포토그래메트리 기술인 NeRF(Neural Radiance Fields)와 3D Gaussian Splatting(3DGS)의 급속한 발전으로 인해 3D 재구성은 다양한 분야에서 주목받고 있습니다. ReconX는 이러한 기술을 한 단계 발전시켜 클래스 선험 지식을 활용하여 고품질 3D 씬을 재구성하도록 돕는 새로운 방법론을 제시합니다.
- **Technical Details**: ReconX는 기존의 불완전한 3D 재구성 문제를 생성 문제로 재구성합니다. 이를 위해 대규모 비디오 확산 모델(Generative Video Diffusion Model)의 강력한 생성 선험 지식을 활용하여 3D 구조 가이던스를 비디오 생성을 통해 확립합니다. 이 과정에서 Pose-Free Stereo Reconstruction 방법을 사용해 글로벌 포인트 클라우드를 생성하고 이를 3D 조건으로 인코딩하여 비디오 확산 모델을 통해 일관된 3D 정보를 가진 프레임을 생성합니다. 이후 Gaussian Splatting과 3D 신뢰도 및 강인한 씬 최적화 기법을 통해 최종 3D 씬을 재구성합니다.
- **Performance Highlights**: 다양한 실험 결과, ReconX는 기존의 방법론을 뛰어넘는 고정밀도 및 범용성을 입증했습니다. 특히, 실제 데이터셋에서의 성능이 매우 우수하며, 비디오 확산 모델을 통해 복잡한 3D 모델을 효과적으로 생성하고 있음이 확인되었습니다.

### [WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling](https://arxiv.org/abs/2408.16532)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16532.png)

Vote: 21

Authors: Ziang Zhang, Rongjie Huang, Yifu Chen, Qian Yang, Qian Chen, Shengpeng Ji, Ruiqi Li, Siqi Zheng, Xize Cheng, Minghui Fang, Jialong Zuo, Wen Wang, Ziyue Jiang, Zhou Zhao, Xiaoda Yang, Yidi Jiang

- **What's New**: 최근, 대규모 언어 모델들이 여러 화자 음성 합성, 음악 생성, 오디오 생성 같은 생성 작업에서 놀라운 성과를 이루어냈습니다. 이러한 성과는 주로 신경 코덱 모델이 생성한 이산 음향 코덱 표현을 활용함으로써 가능해졌습니다. 본 논문에서는 WavTokenizer라는 새로운 이산 음향 코덱 모델을 소개합니다. WavTokenizer은 압축, 재구성 품질, 의미론적 모델링에서 혁신적인 발전을 이루었습니다.
- **Technical Details**: 대부분의 종단간 이산 코덱 모델은 인코더, 잔여 벡터 양자화(RVQ) 모듈, 디코더로 구성된 3단계 구조를 채택합니다. 음성 신호를 시간 도메인에서 다운샘플링하여 압축 오디오 프레임을 얻고, 연속된 양자화기 시리즈를 사용하여 이를 양자화한 후, 디코더를 통해 시간 도메인에서 업샘플링하여 오디오 신호를 재구성합니다. WavTokenizer는 이러한 구조를 따르면서도 독특한 개선점을 가지고 있습니다. K-means 클러스터링 초기화 및 임의 깨우기 전략을 적용하여 단일 양자화기로 압축하였으며, 주파수 변환 모듈과 멀티스케일 디스크리미네이터를 결합해 오디오 재구성 품질을 향상시켰습니다. 또한, 컨텍스트 윈도우 확장 및 디코더에 어텐션 메커니즘을 추가하여 의미론적 정보를 풍부하게 만듭니다.
- **Performance Highlights**: WavTokenizer는 초당 단 75개의 토큰으로 현재 최첨단 모델의 주관적 재구성 성능을 능가했습니다. 초당 40 또는 75개의 토큰만으로도 다양한 메트릭에서 탁월한 성과를 보였으며, 생성 모델들에서 우수한 성능을 발휘했습니다. 엄격한 소거 연구를 통해 각 구성 요소의 필요성을 확인했으며, 추가적인 결론과 실험 결과는 향후 arXiv 버전과 Github에서 확인할 수 있습니다.

### [SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners](https://arxiv.org/abs/2408.16768)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16768.png)

Vote: 17

Authors: Chunyuan Li, Xiangyang Zhu, Peng Gao, Renrui Zhang, Pheng-Ann Heng, Ziyu Guo, Chengzhuo Tong

- **What's New**: 새로운 논문에서는 Segment Anything Model 2 (SAM 2)를 3D 세분화에 효과적으로 적용하기 위한 Sam2Point를 소개합니다. SAM 2의 강력한 비디오 세분화 성능을 활용하여 3D 데이터를 투영 없이 다룰 수 있는 방식으로, 3D 프롬프트를 지원하며 다양한 도메인에 걸쳐 높은 전이 가능성을 보여줍니다.
- **Technical Details**: Sam2Point는 3D 데이터를 비디오 형식으로 모사하기 위해 복셀화를 채택하여 복잡한 투영 과정을 피합니다. 복셀화된 3D 데이터는 SAM 2와 직접 호환되며 추가적인 훈련이나 2D-3D 투영 없이도 공간 정보를 잘 보존합니다. 다양한 3D 프롬프트(점, 경계 상자, 마스크)를 지원하며, 사용자가 제공한 3D 프롬프트를 시작점으로 하여 다방향 비디오를 생성하고, 이를 통합하여 최종 예측 결과를 만듭니다. 이와 같은 간결한 프레임워크를 통해 단일 객체에서 실내, 실외 장면 및 원시 LiDAR 데이터 등 다양한 3D 시나리오에 걸쳐 우수한 전이 능력을 입증합니다.
- **Performance Highlights**: Sam2Point는 특히 복잡한 현실의 동적 상황을 포착하는 데 뛰어난 성능을 보입니다. 3D 지오메트리를 보존하면서도 SAM 2와 호환되도록 복셀화를 통해 비디오는 결과 형태와 비슷한 데이터 형식으로 변환됩니다. 이 과정을 통해 추가 훈련이나 2D-3D 투영 과정 없이도 공간 정보를 유지하면서 zero-shot 3D 세분화를 수행할 수 있습니다.

### [CSGO: Content-Style Composition in Text-to-Image Generation](https://arxiv.org/abs/2408.16766)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16766.png)

Vote: 10

Authors: Haofan Wang, Zechao Li, Xu Bai, Hao Ai, Peng Xing, Qixun Wang, Renyuan Huang, Yanpeng Sun

- **What's New**: 최신 연구들이 텍스트-이미지 생성 분야에서 확산 모델(diffusion models)의 혁신적인 발전을 이루었습니다. 이번 논문에서는 대규모 스타일화된 데이터셋을 구축하고, 이를 통해 고품질 이미지 스타일 변환을 가능하게 하는 새로운 스타일 전환 프레임워크 CSGO를 소개합니다.
- **Technical Details**: 기존의 많은 방법들이 스타일 전환을 위해 훈련이 필요 없는 구조나 DDIM inversion, pre-trained IP-Adapter 등을 사용해왔습니다. 하지만 이러한 방법들은 연산 시간 증가와 정보 손실 문제를 겪습니다. 본 연구에서는 스타일 전환 전용 데이터셋인 IMAGStyle(210K images)을 구축하고, 독립적인 콘텐츠와 스타일 피처 주입 모듈을 사용하는 end-to-end 스타일 전환 프레임워크 CSGO를 제안합니다.
- **Performance Highlights**: CSGO는 스타일 이미지와 콘텐츠 이미지를 동시에 받아들여 피처 주입 블록을 통해 이들 피처를 효과적으로 융합합니다. 훈련이 완료된 후에는 추가적인 fine-tuning 없이 임의의 스타일 전환을 실현할 수 있습니다. 또한, 스타일 전환의 품질을 평가하기 위해 Content Alignment Score (CAS)를 도입하여 콘텐츠 손실 정도를 효과적으로 측정합니다.

### [StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements](https://arxiv.org/abs/2408.15666)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.15666.png)

Vote: 6

Authors: Zaid Harchaoui, Jillian Fisher, Ximing Lu, Yejin Choi, Mitchell Gordon, Skyler Hallinan

- **What's New**: 저자 신원 숨김(authorship obfuscation)은 익명의 토론 포럼, 이중 맹검 리뷰, 건강 서비스 등 민감한 상황에서 저자의 프라이버시를 유지하기 위한 중요한 방법입니다. 새롭게 소개되는 `StyleRemix`는 저자 특정 스타일 정보를 통합하여 대형 언어 모델(LLM)의 유연성과 제어 가능성을 활용한 새로운 방법입니다.
- **Technical Details**: StyleRemix는 사전 학습된 Low-Rank Adaptation 모듈(LoRA)을 사용하여 텍스트 재작성(input rewriting)을 특정 스타일 축(style axes, 예: 길이, 공식성, 학년 수준 등)에서 지정된 방향으로 안내합니다. 이 접근법은 기존의 저자 스타일을 인식하고 이 정보를 이용해 작성 스타일을 숨깁니다. 실험을 위해 `AuthorMix`와 `Distilled Style Components Dataset(DiSC)`라는 두 개의 데이터셋이 공개되었습니다.
- **Performance Highlights**: StyleRemix는 최신의 저자 신원 숨김 기법과 유사한 크기의 모델들보다 우수한 성능을 보여줍니다. 기본적으로 높은 설명 가능성(Explainability)과 유연한 커스터마이징을 제공하며, 다양한 저자 스타일에 맞춤형으로 적용할 수 있습니다.

### [3D Reconstruction with Spatial Memory](https://arxiv.org/abs/2408.16061)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16061.png)

Vote: 6

Authors: Lourdes Agapito, Hengyi Wang

- **What's New**: Spann3R는 DUSt3R 프레임워크를 발전시켜 공간 메모리를 활용한 실시간 3D 재구성을 가능하게 합니다. DUSt3R와 달리 per-scene 최적화 없이 각 이미지의 포인트맵(pointmap)을 공통 좌표계에서 예측할 수 있는 모델입니다.
- **Technical Details**: Spann3R는 transformer 기반 아키텍처를 사용하여 메모리 네트워크 개념을 도입합니다. 이전의 예측 결과를 메모리 값으로 인코딩하고, 현재 프레임의 기하학적 정보를 쿼리(feature)와 메모리 키(key)로 변환합니다. 모델은 다층 퍼셉트론(MLP) 헤드를 사용하여 메모리에서 관련 정보를 검색합니다. 또한 적응형 학습 전략을 통해 짧은 의존성에서 긴 의존성까지 학습합니다.
- **Performance Highlights**: Spann3R는 실시간 온라인 증분 재구성을 50 fps 이상으로 달성하며, 다양한 머신 비전에 대해 경쟁력 있는 재구성 품질과 일반화 능력을 보입니다.

### [Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems](https://arxiv.org/abs/2408.16293)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16293.png)

Vote: 5

Authors: Zeyuan Allen-Zhu, Yuanzhi Li, Zicheng Xu, Tian Ye

- **What's New**: 현재 언어 모델은 다양한 작업에서 인간에 가까운 성능을 발휘하고 있지만, 그들의 문제 해결 능력은 여전히 완벽하지 않으며 종종 논리적 오류를 범하는 경우가 있다. 최근에는 언어 모델의 추론 정확성을 높이기 위한 다양한 시도가 이루어지고 있다. 그 중 하나는 언어 모델이 '자체 검증(self-verify)'을 통해 생성한 내용을 스스로 검증하고 수정하는 방법이다.
- **Technical Details**: 기존 연구에서는 언어 모델이 '분포의 변화(distribution shift)'로 인해 오류를 범한다는 사실을 강조하며, 이는 훈련 데이터셋과 실제 테스트 시 사용되는 프롬프트가 다르기 때문이다. 이런 오류를 수정하기 위해 검증자를 사용하는 방법이나, 언어 모델이 스스로 오류를 검증하도록 유도하는 방법들이 기존 연구들을 통해 제안되었다. 그러나 즉각적으로 오류를 수정하지 못하는 이유에 대한 연구는 부족하다.
- **Performance Highlights**: 이 논문에서는 즉각적인 오류 수정을 통해 언어 모델의 추론 정확성을 개선할 수 있는지 검토하고, iGSM 데이터셋을 활용해 오류와 바로잡기를 포함한 데이터를 통해 학습한 모델이 높은 정확성과 효율을 보임을 초기 실험을 통해 입증하였다. 특히, 'retry upon regret' 을 통한 오류 감지 및 재생성 절차는 기존의 빔 서치(beam search)를 뛰어넘는 성능을 보였다.

### [Scaling Up Diffusion and Flow-based XGBoost Models](https://arxiv.org/abs/2408.16046)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.16046.png)

Vote: 5

Authors: Taewoo Kim, Jesse C. Cresswell

- **What's New**: 이 논문은 새로운 딥러닝 모델을 제안하여 텍스트 분류 문제를 보다 효율적으로 해결하는 방법을 제시합니다. 저자들은 최신 Transformer 기반 아키텍처를 바탕으로 다양한 데이터셋에 대해 높은 성능을 입증하고 있습니다.
- **Technical Details**: 제안된 모델은 기존의 BERT 및 GPT와 같은 모델들의 구조를 개선하여, Attention 메커니즘과 더불어 새로운 학습 방식을 도입하고 있습니다. 또한, 이를 위해 학습률 조절(lr schedule) 및 Dropout 등의 정규화 기법을 사용하여 모델의 과적합(overfitting)을 방지하였습니다.
- **Performance Highlights**: 논문에서 제안된 모델은 다양한 텍스트 분류 데이터셋에서 기존 최고 성능을 능가하는 결과를 보여주었습니다. 특히, GLUE 벤치마크에서 5% 이상의 향상된 성능을 기록하였습니다.

### [Meta Flow Matching: Integrating Vector Fields on the Wasserstein Manifold](https://arxiv.org/abs/2408.14608)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.14608.png)

Vote: 2

Authors: Mathieu Blanchette, Brandon Amos, Leo J. Lee, Alexander Tong, Kirill Neklyudov, Xi Zhang, Yoshua Bengio, Lazar Atanackovic

- **What's New**: 이번 연구에서는 Meta Flow Matching (MFM)이라는 새로운 접근법을 제안합니다. MFM은 기존의 Flow Matching 기법을 입력 측정값에 비례하여 확장하는 방법으로, 입자 간의 상호작용을 고려하고 학습된 모델을 이전에 보지 못한 새로운 입력 인구에 일반화할 수 있습니다.
- **Technical Details**: Meta Flow Matching은 입자들을 독립적으로 모델링하는 대신, 입자 간의 상호작용을 고려하여 학습된 모델을 일반화할 수 있도록 합니다. 이는 초기 분포에서 샘플을 입력으로 받아 최종 분포로 매핑하는 벡터 필드 모델을 학습하는 것으로, 이를 통해 세포 집단의 시간에 따른 발전을 예측할 수 있습니다. 또한, Flow Matching은 밀도 간의 연속적인 보간을 사용하여 생성 모델을 구축하는 접근법으로, 이는 최적의 운송 문제를 공식화하는데 활용될 수 있습니다.
- **Performance Highlights**: 제안된 MFM의 유용성을 두 가지 응용 프로그램에서 입증했습니다. 첫 번째로, MFM을 '문자 소음 제거'라는 합성 작업에 적용했을 때, MFM은 기존의 Flow Matching 접근법과 달리 보지 않은 문자 실루엣에 소음 제거 과정을 일반화할 수 있었습니다. 두 번째로, 최근에 발표된 대규모 단세포 약물 스크리닝 데이터셋에서 MFM을 사용해 환자의 세포가 화학요법 치료에 반응하는 양상을 예측한 결과, MFM이 복제를 통한 실험에서도 성공적으로 세포 집단의 발전을 예측하고, 특히 전에 보지 못한 환자에 대해 환자 특유의 치료 반응을 포착할 수 있음을 확인했습니다.

