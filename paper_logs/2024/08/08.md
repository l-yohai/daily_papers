## Daily Papers (2024-08-08)

### [Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks](https://arxiv.org/abs/2408.03615)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03615.png)

Vote: 21

Authors: Rui Shao, Liqiang Nie, Gongwei Chen, Dongmei Jiang, Yuquan Xie, Zaijing Li

- **What's New**: 새로운 멀티모달 컴포저블 에이전트(Optimus-1)를 개발하여 마인크래프트와 같은 오픈 월드 환경에서 사람처럼 긴 시간에 걸친 작업을 수행할 수 있도록 하였습니다. Hybrid Multimodal Memory 모듈을 도입하여 Hierarchical Directed Knowledge Graph (HDKG)와 Abstracted Multimodal Experience Pool (AMEP)을 결합하여 장기 기억 저장 메커니즘을 제안합니다.
- **Technical Details**: Optimus-1은 Hybrid Multimodal Memory를 중심으로 Knowledge-Guided Planner, Experience-Driven Reflector, Action Controller 세 모듈로 구성됩니다. HDKG는 개체 간의 논리적 관계를 그래프 구조로 매핑하여 지식을 고수준의 의미론적 표현으로 변환하고, AMEP는 에이전트의 작업 수행 과정에서 다양한 방식의 정보를 동적으로 요약하고 저장합니다.
- **Performance Highlights**: 마인크래프트 환경에서 실행된 실험 결과, Optimus-1은 기존의 에이전트들보다 30% 더 나은 성능을 보였으며, 다양한 Multimodal Large Language Models (MLLMs)를 결합하여 2배에서 6배까지의 성능 향상을 달성하였습니다. Plug-and-play 형태의 Hybrid Multimodal Memory는 Optimus-1이 스스로 성능을 점진적으로 향상시키는 것이 가능하게 합니다.

### [EXAONE 3.0 7.8B Instruction Tuned Language Model](https://arxiv.org/abs/2408.03541)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03541.png)

Vote: 15

Authors: Kyunghoon Bae, Hyosang Kim, Sunkyoung Kim, Youchul Kim, Yemuk Choi, Soyeon Kim, +, Soyoung An, Junwon Hwang, Joonkee Kim, Euisoon Kim, Yireun Kim, Eunbi Choi, Gerrard Jeongwon Jo, Hyojin Jeon, Seonghwan Kim, LG AI Research, Yountae Jung, Stanley Jungkyu Choi, Hyunjik Jo, Seokhee Hong, Yeonjung Hong, Jiyeon Jung

- **What's New**: LG AI 연구소는 전문가 수준의 인공지능(EXpert AI for EveryONE)을 대중에게 제공하기 위한 ExaOne 3.0 모델을 2024년 8월에 발표했습니다. 이 모델은 특히 공공에게 비상업적 연구 목적으로 공개될 7.8B 크기의 instruction-tuned 모델을 포함합니다.
- **Technical Details**: ExaOne 3.0은 bilingual(두 언어 지원) 모델로, 영어와 한국어를 지원합니다. 최신 decoder-only transformer architecture를 사용하며, Rotary Position Embeddings (RoPE)과 Grouped Query Attention (GQA)을 채택했습니다. 이 모델의 최대 컨텍스트 길이는 4,096 토큰이며, BBPE (byte-level byte-pair encoding) 토크나이저를 사용하여 102,400 크기의 어휘를 가지고 있습니다.
- **Performance Highlights**: ['7.8B에는 6조(6T)개의 토큰을 포함한 다양한 데이터를 이용하여 학습되었습니다. 성능을 최적화하기 위해 rule-based 필터링, machine learning 기반의 필터링, URL 기반의 필터링 등을 사용해 데이터를 정제했습니다.', '모델의 성능은 영어와 한국어 모두에서 뛰어나며, 공공 벤치마크와 사내 벤치마크 데이터셋을 사용한 평가에서 경쟁력 있는 성능을 보였습니다.', 'Instruction-following 능력 향상을 위해 supervised fine-tuning (SFT)와 direct preference optimization (DPO) 기술을 적용하였습니다.', '모델은 NVIDIA H100 GPUs와 NeMo Framework을 사용하여 Google Cloud Platform에서 학습하였고, NVIDIA TensorRT-LLM으로 최적화되었습니다.']

### [Achieving Human Level Competitive Robot Table Tennis](https://arxiv.org/abs/2408.03906)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03906.png)

Vote: 10

Authors: Sherry Moore, Laura Graesser, Krista Reymann, Heni Ben Amor, Erwin Coumans, David B. D'Ambrosio, Krzysztof Choromanski, Saminda Abeyruwan, +, Reza Mahjourian, Natasha Jaques, Deepali Jain, Nevena Lazic, Barney J. Reed, Yuheng Kuang, Anish Shankar, Navdeep Jaitly, Leila Takayama, Kenneth Oslund, Yuval Tassa, Atil Iscen, Satoshi Kataoka, Alex Bewley

- **What's New**: 최초로 인간 수준의 경쟁적인 탁구 경기를 할 수 있는 로봇을 소개합니다. 이 로봇은 다양한 기술을 이용해 여러 추상화 수준에서 스킬을 획득합니다. 이를 통해 직접적인 신체적 기술뿐만 아니라 고수준의 전략적 결정을 할 수 있습니다.
- **Technical Details**: 이 로봇은 계층적이고 모듈식 정책 아키텍처를 사용합니다. 저수준 스킬 정책은 포핸드 탑스핀, 백핸드 타게팅, 포핸드 서브 등 특정 테이블 테니스 기술에 특화되어 있으며, 고수준 컨트롤러는 현재 경기 통계, 스킬 설명자 및 상대방의 역량을 고려하여 최적의 스킬을 선택합니다. 강화 학습(Reinforcement Learning)과 모방 학습(Imitation Learning)을 결합한 하이브리드 훈련 방법을 도입하여 초기 작업 조건을 인간-인간 플레이 데이터로 수집하고, 시뮬레이션에서 에이전트를 훈련시킨 후 현실 하드웨어로 즉시 배포합니다.
- **Performance Highlights**: 로봇은 게임 중 수집한 데이터를 통해 지속적으로 자신의 기술을 향상시키고, 실시간으로 상대방의 통계 데이터를 추적하여 상황에 맞게 신속히 적응합니다. 사용자 연구 결과, 이 로봇은 새로운 인간 상대방과의 경기에서도 경쟁적인 인간 수준의 성과를 발휘하며, 인간이 실제로 즐길 수 있는 경쟁적인 게임플레이를 제공합니다.

### [WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models](https://arxiv.org/abs/2408.03837)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03837.png)

Vote: 9

Authors: Le Qi Yau, Soujanya Poria, Yu Xin Teoh, Dar Win Liew, I-Shiang Lee, Hugo Maximus Lim, Jia Hng Koh, Rajat Bhardwaj, Prannaya Gupta, Hao Han Low, Rishabh Bhardwaj

- **What's New**: 최근 LLM 기술이 이메일 작성, 정보 접근 및 코드 작성 등 다양한 측면에서 우리의 삶을 단순화하는 도구로 자리 잡고 있습니다. 이러한 기술은 OpenAI의 ChatGPT-3.5 등으로 시작하여 ChatGPT 시리즈와 Claude 시리즈와 같은 닫힌 모델 및 Llamas, Mistrals, Gemmas 같은 열린 모델로 확장되었습니다. 이 종이에서는 새로운 Python 기반의 프레임워크인 WalledEval을 도입하여 이러한 advanced AI 시스템의 안전성을 포괄적으로 평가하는 방법을 제안합니다.
- **Technical Details**: WalledEval은 HuggingFace Transformers 라이브러리를 기반으로 한 다양한 오픈-웨이트 모델을 지원하며, OpenAI, Anthropic, Google 등의 API 기반 모델도 지원합니다. 35개 이상의 AI 안전 벤치마크를 제공하여 다국어 안전성, 과장된 안전성 및 프롬프트 인젝션 등에 대해 포괄적인 테스트를 수행할 수 있습니다. 또한, LlamaGuard, LionGuard와 같은 콘텐츠 모더레이터를 지원하며, 새로운 콘텐츠 모더레이터인 WalledGuard를 출시했습니다.
- **Performance Highlights**: WalledGuard는 최신 LlamaGuard-3 대비 약 16배 작은 사이즈이지만, Aya Red-Teaming (영어) 데이터셋에서 기존 모더레이터를 능가하는 성능을 보여주며, XSTest에서는 LlamaGuard-2 대비 3% 내외의 성능 저하만을 보였습니다. WalledEval은 다양한 텍스트 스타일 변환을 제공하는 mutators를 도입하여, 텍스트의 시제를 변환하거나 문장 구조를 변경하고 노이즈를 추가하는 기능을 지원하여 모델의 안전 감사에 기여합니다.

### [Openstory++: A Large-scale Dataset and Benchmark for Instance-aware Open-domain Visual Storytelling](https://arxiv.org/abs/2408.03695)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03695.png)

Vote: 8

Authors: Xiaoqian Shen, Ruotian Peng, Qi Liu, Zhiyang Chen, Mingyuan Zhou, Jinxiu Liu, Ziwei Xuan, Mohamed Elhoseiny, Jinjin Cao, Zilyu Ye, Yiyang Zhang, Guo-Jun Qi

- **What's New**: Openstory++ 소개: 현재의 이미지-텍스트 결합 생성(image-text interleaved generation) 기술의 한계를 극복하고, 이야기 연속성과 인스턴스레벨(instance-level) 시각적 일관성을 유지하며 고품질의 시각적 이야기를 생성할 수 있는 새로운 데이터셋을 공개하였습니다.
- **Technical Details**: Openstory++ 데이터셋은 비디오 콘텐츠에서 키프레임(keyframes)을 추출하고, 미학적으로 평가하여 BLIP2를 통해 설명 캡션을 생성합니다. 이 캡션은 더 큰 언어 모델(LLM)에 의해 내러티브 일관성을 유지하도록 정제됩니다. 또한, 이미지 내에서 유효한 인스턴스를 식별하고 Segment Anything Model (SAM)을 사용해 이 인스턴스에 대한 마스크를 생성합니다.
- **Performance Highlights**: 실험 결과, Openstory++은 인스턴스 인식 오픈 도메인 시각적 이야기 생성 모델을 개발하는 데 있어 현저한 이점을 보여주었으며, 이를 통해 코히어-벤치(Cohere-Bench)라는 새로운 벤치마크 프레임워크도 설계하였습니다. 이 벤치마크는 이미지-텍스트 생성의 긴 맥락 시나리오에서 주제 일관성을 평가하는 데 중점을 둡니다.

### [RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel View Synthesis](https://arxiv.org/abs/2408.03356)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03356.png)

Vote: 5

Authors: Alexis Paljic, Hugo Blanc, Jean-Emmanuel Deschaud

- **What's New**: 이번 연구는 방사장(radiance fields)의 새로운 정의를 제안하며, 이를 최적화 가능한 위치, 방향 및 규모를 가진 타원 기저 함수(elliptical basis functions)의 가중 합으로 분해합니다. 이는 기존의 Voxel Grid 기반 방식과 달리, 장면의 기하학적 특성에 더욱 세밀히 적응할 수 있게 하며, 해상도를 제한하지 않습니다. 또한, 구면 조화 함수(SH)와 구면 가우시안(SG)을 사용하여 고주파 색 변화를 더 정확히 표현합니다.
- **Technical Details**: 연구는 볼륨 광선 투사(Volume Ray Casting) 알고리즘을 사용하여 고품질 이미지를 생성하며, 이는 3D Gaussian Splatting과 같은 단순화 접근법에서 발생하는 아티팩트(artefacts)를 방지합니다. 광선 투사는 방사장을 타원 기저 함수와 SH/SG 방사 매개변수로 나타내어 장면의 기하학 및 외관에 효율적으로 적응합니다. sparse 기저 함수(sparse basis functions)를 위한 특화된 광선 투사 알고리즘을 도입하여, 여러 샘플을 포함하는 공간 슬랩(slab)을 따라 색 특성을 통합하는 기법을 사용합니다.
- **Performance Highlights**: OptiX 라이브러리가 지원하는 GPU의 경계 부피 계층화(Bounding Volume Hierarchy, BVH)를 사용하여 인터렉티브 렌더링 시간과 합리적인 학습 시간을 구현했습니다. 이를 통해 고품질의 렌더링을 달성했습니다.

### [Fast Sprite Decomposition from Animated Graphics](https://arxiv.org/abs/2408.03923)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03923.png)

Vote: 4

Authors: Kota Yamaguchi, Tomoyuki Suzuki, Kotaro Kikuchi

- **What's New**: 이 논문에서는 애니메이션 그래픽 영상을 스프라이트로 분해하는 방법을 제안합니다. 이는 영상 편집에서 특정 객체를 즉시 수정할 수 있는 가능성을 열어줍니다. 연구자들은 크렐로 애니메이션(Crello Animation) 데이터셋과 벤치마크 메트릭을 구축하여 스프라이트 분해의 품질을 평가합니다.
- **Technical Details**: 스프라이트 분해 방법을 최적화 문제로 공식화하여 주어진 래스터(video raster) 영상에 대한 객체의 스프라이트 파라미터를 맞추는 방식입니다. 정적 스프라이트 가정을 도입하여 고정된 텍스처와 애니메이션 파라미터만 변하는 구조로, 효율적인 최적화를 위해 그래디언트 기반 최적화기(gradient-based optimizer)와 최소한의 사용자인 주석을 이용한 비디오 객체 분할 모델을 사용합니다.
- **Performance Highlights**: 제안된 방법은 기존의 유사한 분해 방식에 비해 품질과 효율성 사이의 균형에서 뛰어난 성능을 보여주었습니다. 특히, 스프라이트 분해의 속도와 품질 측면에서 두드러진 향상을 이루었습니다.

### [CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases](https://arxiv.org/abs/2408.03910)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03910.png)

Vote: 4

Authors: Fei Wang, Michael Shieh, Yang Liu, Zhiyuan Hu, Wenmeng Zhou, Xiangyan Liu, Bo Lan, Zhicheng Zhang

- **What's New**: 연구진들은 소스코드 저장소와 LLMs(Large Language Models)를 연결하기 위해 그래프 데이터베이스 (graph databases)를 활용한 새로운 프레임워크 ramework을 제안했습니다. 이를 통해 LLMs가 복잡한 코드 구조를 이해하고, 정확한 코드 조각을 효율적으로 검색할 수 있습니다.
- **Technical Details**: 연구는 기존의 유사성 기반 검색 방식과 수동 코드 도구/API가 가지는 한계를 극복하기 위해 새로운 그래프 기반 접근 방식을 채택했습니다. ramework는 정적 분석(static analysis)을 통해 소스코드 저장소에서 코드 그래프를 추출하고, 노드와 엣지로 이루어진 태스크에 독립적인 스키마를 사용합니다. 이러한 그래프에서 노드는 모듈(MODULE), 클래스(CLASS), 함수(FUNCTION) 등 소스코드 심볼을 나타내며, 엣지는 이러한 심볼 간의 관계(CONTAINS, INHERITS, USES)를 나타냅니다. 이를 통해 LLM 에이전트가 코드 구조를 더 잘 이해하고, 유연한 질의 (graph query language)를 통해 관련 코드를 검색할 수 있습니다.
- **Performance Highlights**: 제안된 ramework는 CrossCodeEval, SWE-bench, EvoCodeBench와 같은 세 가지 대표적인 저장소 수준 코드 벤치마크에서 뛰어난 성능을 나타냈습니다. 또한, 실제 소프트웨어 개발 환경에서도 코드 디버깅 및 주석 작성과 같은 다섯 가지 현실 세계 응용 시나리오에서 높은 유용성을 보였습니다.

### [Speech-MASSIVE: A Multilingual Speech Dataset for SLU and Beyond](https://arxiv.org/abs/2408.03900)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03900.png)

Vote: 3

Authors: Marco Gaido, Laurent Besacier, Matteo Negri, Beomseok Lee, Ioan Calapodescu

- **What's New**: 다국어 음성 데이터 세트가 음성 관련 작업에 대한 범위가 제한되어 있으며, 주로 자동 음성 인식(ASR) 및 음성 번역(ST)에 집중되고 있는 상황에서, 새로운 다국어 음성 언어 이해(SLU) 데이터 세트를 소개합니다. 본 연구는 MASSIVE NLU(텍스트) 데이터 세트를 기반으로 한 것으로, 여러 언어에서 음성 녹음을 수집하였습니다. Speech-MASSIVE는 12개의 다양한 언어(아랍어, 독일어, 스페인어, 프랑스어, 헝가리어, 한국어, 네덜란드어, 폴란드어, 유럽 포르투갈어, 러시아어, 터키어, 베트남어)를 포함하며, ASR, ST, 언어 식별(LID) 등 다양한 음성 작업을 평가할 수 있도록 합니다.
- **Technical Details**: 텍스트 MASSIVE 데이터를 음성 데이터로 변환하기 위해 Prolific 크라우드소싱 플랫폼을 통해 원어민 화자를 모집하였습니다. 첫 번째 그룹의 작업자들이 MASSIVE 문장의 음성 버전을 녹음했으며, 두 번째 그룹의 원어민 화자가 이를 검증하였습니다. 검증 단계에서는 원본 텍스트를 읽고 녹음을 듣고 이를 유효 또는 무효로 표시했습니다. 무효로 판단된 녹음은 두 번째 단계 반복을 거쳤으며, 최종 데이터의 품질을 보장하기 위해 두 가지 추가 조치를 취했습니다. 녹음 단계에서는 참가자들에게 자신의 녹음을 검토하고 문제가 있으면 다시 녹음하도록 지시했으며, 검증 단계에서는 Common Voice에서 선택된 네 가지 샘플을 삽입하여 검증의 정확성을 높였습니다. 12개의 언어를 선정하여 데이터 수집 및 검증을 진행했으며, 프랑스어와 독일어는 전체 훈련 데이터, 나머지 10개 언어는 제한된 훈련 데이터를 수집했습니다.
- **Performance Highlights**: Speech-MASSIVE를 다국어 ASR에서 평가하기 위해 Whisper 모델(Whisper-large-v3)을 사용했으며, FLEURS 데이터 세트와 비교하여 WER(단어 오류율)과 CER(문자 오류율)을 측정하였습니다. Whisper 모델은 기존 FLEURS 데이터 세트와 비교하여 Speech-MASSIVE에서 더 높은 WER을 보였습니다. 이는 MASSIVE 발화의 어려움 때문일 가능성이 큽니다. 또한 SLU 베이스라인을 설정하고, 다양한 훈련 조건에서 평가하였습니다. 이를 통해 향후 SLU의 발전을 추적하고 NLU와 더 성숙한 분야와 비교할 수 있는 토대를 마련했습니다.

### [Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields](https://arxiv.org/abs/2408.03822)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03822.png)

Vote: 3

Authors: Joo Chan Lee, Jong Hwan Ko, Xiangyu Sun, Daniel Rho, Eunbyung Park

- **What's New**: 본 논문에서는 3D Gaussian splatting (3DGS)의 메모리 및 저장 효율성을 높이면서 고품질의 이미지 및 3D 씬 재구성을 가능하게 하는 새로운 설계를 제안합니다. 이 새로운 프레임워크는 Gaussian 수를 줄이고, Gaussian 속성을 압축하여 메모리와 저장 공간의 사용량을 줄이는 것을 목표로 합니다.
- **Technical Details**: 제안된 프레임워크는 두 가지 주요 영역에서 효율성을 향상시킵니다. 첫 번째는 학습 가능한 마스크를 사용하여 Gaussian 수를 줄이는 것이며, 두 번째는 뷰 종속 색상, 공분산 및 시간 속성과 같은 Gaussian 속성을 압축하는 것입니다. 또한, 해시 기반 그리드 표현 (Instant NGP)을 이용하여 공간 복잡성을 줄였습니다. 이를 통해 각 Gaussian의 color attribute를 별도로 저장할 필요 없이 컴팩트한 그리드 표현에서 추출할 수 있게 되었습니다.
- **Performance Highlights**: 실험 결과, 제안된 방법은 다양한 데이터셋에서 메모리 사용량을 크게 줄이고, 저장 공간을 획기적으로 효율적으로 사용하며, 높은 재구성 품질과 빠른 학습 속도를 유지하면서도 실시간 렌더링을 가능하게 했습니다. 예를 들어, 정적 씬에서는 기존 3DGS 대비 15배, 동적 씬에서는 STG 대비 9배 더 적은 저장 공간을 필요로 했습니다. 또한, 정적 씬에서 25배, 동적 씬에서 12배 이상의 압축률을 달성했습니다.

### [Facing the Music: Tackling Singing Voice Separation in Cinematic Audio Source Separation](https://arxiv.org/abs/2408.03588)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03588.png)

Vote: 2

Authors: Iroro Orife, Chih-Wei Wu, Karn N. Watcharasupat

- **What's New**: 이번 연구에서는 새로운 영화 오디오 소스 분리(CASS, Cinematic Audio Source Separation) 방법을 소개합니다. 이 연구는 발성(스피치)과 악기 음악 중 노래 목소리를 구별하는 것에 중점을 두고 있습니다.
- **Technical Details**: 이번 연구는 Bandit 모델과 Banquet 모델에 추가 stem을 직관적으로 추가하여, Divide and Remaster v3 데이터셋을 수정해 학습했습니다. 특히, Banquet 모델은 전용 디코더를 가지지 않고, 공통 디코더를 사용해 4개의 스템 (다이얼로그, 악기 음악, 노래 음악, 효과음)을 분리합니다.
- **Performance Highlights**: 놀랍게도, query-based인 Banquet 모델(19.7M 파라미터)은 전용 디코더가 있는 Bandit 모델(45.7M 파라미터)을 모든 스템에서 일관되게 뛰어넘는 성능을 보여주었습니다. 전체적인 성능 지표에서는 Banquet 모델이 Bandit 모델을 0.3-0.7 dB 정도 앞섰습니다.

