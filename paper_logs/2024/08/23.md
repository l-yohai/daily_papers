## Daily Papers (2024-08-23)

### [Controllable Text Generation for Large Language Models: A Survey](https://arxiv.org/abs/2408.12599)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12599.png)

Vote: 40

Authors: Xun Liang, Shunyu Yao, Jiawei Yang, Simin Niu, Feiyu Xiong, Zhiyu Li, Hanyu Wang, Jie Hu, Shichao Song, Yezhaohui Wang, Dan Liu

- **What's New**: 현대 대형 언어 모델(LLMs)의 급속한 발전과 널리 퍼진 자연어 처리(NLP) 응용으로 텍스트 생성 품질에서 중요한 돌파구가 달성되었습니다. 특정 조건과 사용자 요구 사항을 정확히 일치시키는 텍스트 생성의 필요성은 제어 가능한 텍스트 생성(CTG) 기술의 개발을 촉진했습니다.
- **Technical Details**: CTG는 미리 정의된 제어 조건(예: 안전성, 감정 등)을 충족하면서도 유창성과 다양성 같은 품질을 유지하며 텍스트 생성을 안내합니다. 이러한 제어 조건은 명시적 또는 암시적일 수 있으며, 특정 스타일 모방이나 비유해 콘텐츠 생성 등 다양한 요구 사항을 충족시킵니다. CTG 연구는 명시적 제어(사용자와의 명확한 상호작용을 통한 지침)와 암시적 제어(비유해 콘텐츠 생성 등)로 나뉩니다.
- **Performance Highlights**: CTG의 주요 목표는 정확하고 유용한 정보를 제공하면서도 미리 정의된 제어 조건을 준수하는 텍스트 생성을 보장하는 것입니다. 성과 지표로는 유창성(텍스트의 논리적 일관성), 유용성(실제 문제 해결 지원), 다양성(반복적이지 않고 혁신적인 텍스트 생성)이 포함됩니다. 연구는 CTG의 고유한 과제를 강조하며, 다양한 응용 시나리오에서 높은 텍스트 품질을 유지하는 동시에 특정 요구 사항을 엄격히 준수할 필요성을 강조합니다.

### [Sapiens: Foundation for Human Vision Models](https://arxiv.org/abs/2408.12569)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12569.png)

Vote: 35

Authors: Julieta Martinez, Peter Selednik, Timur Bagautdinov, Shunsuke Saito, Su Zhaoen, Austin James, Stuart Anderson, Rawal Khirodkar

- **What's New**: 이번 연구에서는 사람 중심의 다양한 작업을 수행할 수 있는 통합된 프레임워크와 모델인 Sapiens를 제안합니다. 이 모델은 큰 규모의 데이터셋과 확장 가능한 모델 아키텍처를 활용하여 일반화를 극대화하였습니다.
- **Technical Details**: 사람 중심의 모델은 세 가지 기준을 만족해야 합니다: 일반화, 광범위한 적용성, 고충실도(high fidelity). 이를 위해 우리는 Humans-300M 데이터셋을 수집하여 다양한 인간 이미지를 포함한 300M 이미지로 이루어져 있습니다. 모델은 mask-autoencoder(MAE) 방식을 통해 사전학습(Pretraining) 되었으며, 각 이미지의 원본 해상도를 1024x1024 픽셀까지 높여 처리했습니다.
- **Performance Highlights**: 사전학습된 비전 트랜스포머(Vision Transformer) 모델들은 제한된 고품질 또는 합성 레이블로 미세 조정(Fine-tuning) 되어 실제 환경에서도 강력한 성능을 보여줍니다. 제안된 모델은 2D 포즈 추정, 신체 부분 세분화, 깊이(depth) 및 표면 노멀 예측에서 전문성과 일관성을 나타냈으며, 기존의 최고 성능을 갱신했습니다.

### [Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://arxiv.org/abs/2408.12528)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12528.png)

Vote: 27

Authors: Zechen Bai, Weijia Mao, Zhijie Chen, Jinheng Xie, David Junhao Zhang, Kevin Qinghong Lin, Weihao Wang, Yuchao Gu, Mike Zheng Shou, Zhenheng Yang

- **What's New**: 우리의 연구는 최초로 단일 트랜스포머 모델로 멀티모달 이해와 생성을 모두 수행할 수 있게 하는 Show-o를 소개합니다. 이는 텍스트와 이미지 생성을 각각의 방식으로 진행하면서도, 동시에 하나의 통합된 네트워크에서 실행됩니다. Show-o는 텍스트 토큰을 자동회귀적으로, 이미지 토큰을 분산 제거(diffusion) 방식으로 모델링합니다. 이 접근법은 텍스트와 이미지 두 가지를 효과적으로 처리하는 데 높은 성능을 보여주며, 기존 개별 모델들과 동등하거나 더 나은 성과를 제공합니다.
- **Technical Details**: Show-o는 사전 훈련된 LLM을 기반으로 하며, 텍스트 기반 추론을 위해 자동회귀적 모델링 능력을 상속합니다. 자연어 처리를 위한 텍스트 토큰화(tokenizer)와 이미지를 위한 이미지 토큰화 도구를 사용하여 모든 종류의 입력 데이터를 처리합니다. 더불어, Show-o는 텍스트 조건 정보를 본질적으로 인코딩하여 추가적인 텍스트 인코더가 필요 없게 합니다. 또한, 분산 제거(diffusion)를 통해 이미지 토큰을 모델링하여 이미지 생성을 수행합니다.
- **Performance Highlights**: Show-o는 서로 다른 모드에서 동등한 또는 더 나은 성능을 보여줍니다. 자동회귀 방식으로 이미지를 생성할 때보다 약 20배 적은 샘플링 스텝만을 요구해 가속화 측면에서 이점을 제공합니다. 또한, 추가적인 미세 조정 없이 텍스트를 기반으로 이미지 복원 및 확장 과제를 효과적으로 수행합니다. 더불어, 텍스트 설명을 동반한 비디오 키프레임 생성을 포함한 혼합 모달리티 생성에도 성공적인 잠재력을 입증하였습니다.

### [xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations](https://arxiv.org/abs/2408.12590)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12590.png)

Vote: 23

Authors: Juan Carlos Niebles, Caiming Xiong, Congying Xia, Michael Ryoo, Senthil Purushwalkam, Manli Shu, Silvio Savarese, Krithika Ramakrishnan, Can Qin, Yingbo Zhou, Le Xue, Yihao Feng, Ran Xu, Jun Wang, Honglu Zhou, Anas Awadalla, Huan Wang, Lifu Tu, Zeyuan Chen

- **What's New**: 최근 텍스트-비디오(T2V) 생성 모델이 주목받고 있는 가운데, Sora 연구 [1]에서는 1분 이상의 현실감 있는 비디오를 생성할 수 있는 방법을 제시했습니다. 이러한 발전에도 불구하고, 가장 능력있는 비디오 생성 모델은 여전히 독점적이며 세부 정보가 공개되지 않았습니다. 이번 연구에서는 경쟁력 있는 T2V 아키텍처를 설계하고, 관련된 모델링 및 훈련 기술을 조사했으며, 데이터 수집 파이프라인을 탐구했습니다.
- **Technical Details**: 본 연구에서는 두 가지 핵심 기술을 사용하여 T2V 모델을 개발했습니다. 첫째, 디퓨전 모델 (Latent Diffusion Model)을 기반으로 비디오 VAE (Variational Autoencoder)를 사용해 공간적 및 시간적 차원을 압축하는 방법을 채택했습니다. 이는 길고 고해상도의 비디오를 효율적으로 생성하는데 주요한 역할을 합니다. 둘째, VDiT (Video Diffusion Transformer) 모델을 도입하여, 공간 및 시간적 셀프 어텐션 층이 포함된 트랜스포머 블록을 활용합니다. ROPE (Relative Positional Encoding)와 사인파 인코딩 (sinusoidal encoding)을 통해 다양한 길이, 종횡비 및 해상도에 걸쳐 효과적으로 일반화할 수 있습니다.
- **Performance Highlights**: xGen-VideoSyn-1 모델은 비디오 VAE와 VDiT 기술을 결합하여 720p 해상도에서 100프레임 이상의 비디오를 생성할 수 있습니다. 모델 훈련에는 40 H100일, DiT 모델은 642 H100일 정도의 시간이 소요되었습니다. 데이터 처리 파이프라인을 통해 1300만 쌍 이상의 고품질 비디오-텍스트 데이터를 수집했으며, 이 파이프라인은 중복 제거, OCR, 모션 및 미학 분석 등을 포함합니다. xGen-VideoSyn-1 프레임워크는 다양한 크기, 길이 및 비율의 비디오를 생성하는 동시에 최첨단 모델과 비교할만한 경쟁력 있는 성능을 보여줍니다.

### [DreamCinema: Cinematic Transfer with Free Camera and 3D Character](https://arxiv.org/abs/2408.12601)

![](/avatars/bbf781594fc8c812316711aa8e2797aa.svg)

Vote: 17

Authors: Diankun Wu, Fangfu Liu, Yueqi Duan, Haixu Song, Weiliang Chen, Haowen Sun

- **What's New**: DreamCinema는 새로운 시네마틱 트랜스퍼 프레임워크로, 사용자 친화적 영화를 제작할 수 있는 새로운 기술을 소개합니다. 이 프레임워크는 매끄러운 시네마틱 요소 추출기와 효율적이고 고품질의 캐릭터 생성기, 그리고 효과적인 시네마틱 전이 최적화 전략을 포함하고 있습니다.
- **Technical Details**: DreamCinema는 선택한 영화에서 인간 및 카메라의 포즈와 같은 시네마틱 요소를 추출하고, 모션 인식 가이드와 물리적 모델링(Bezier curve)을 통해 카메라 경로를 최적화합니다. 생성 모델을 활용하여 구조적 우선순위(SMPL)를 갖춘 3D 캐릭터 생성기를 설계하여 사용자 선호에 맞춘 고품질 캐릭터를 생성합니다. 마지막으로, 생성된 캐릭터를 영화 제작에 매끄럽게 통합하기 위한 효율적인 시네마틱 전이 전략을 개발했습니다.
- **Performance Highlights**: 광범위한 실험을 통해 DreamCinema의 프레임워크가 높은 품질과 사용자 선호도에 맞춘 새로운 영화를 제작하는 데 효과적임을 검증했습니다. DreamCinema는 향후 영화 제작을 위한 잠재적인 시네마틱 전이 프레임워크가 될 것입니다.

### [Jamba-1.5: Hybrid Transformer-Mamba Models at Scale](https://arxiv.org/abs/2408.12570)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12570.png)

Vote: 14

Authors: Haim Rozenblum, Barak Peleg, Erez Safahi, Dor Zimberg, Dan Padnos, Alan Arazi, Amir Bergman, Barak Lenz, Edden M Gerber, Daniel Jannai, Jamba Team, Gal Shachaf, Dor Muhlgay, Avshalom Manevich, Gal Cohen, +, Daniel Gissin, Chen Almagor, Elad Dolev, Erez Schwartz, Eran Krakovsky, Clara Fridman, Ben Aviram

- **What's New**: 이번 논문은 Jamba-1.5의 두 가지 신규 대형 언어 모델을 소개합니다. Jamba-1.5-Mini는 이전 Jamba 버전을 업데이트하고 지시 조정을 한 버전이며, Jamba-1.5-Large는 Transformer와 Mamba 레이어를 결합한 하이브리드 아키텍처로, 256K 토큰의 컨텍스트를 처리할 때 단일 8 80GB GPU 머신에 맞출 수 있는 효율성을 제공합니다.
- **Technical Details**: Jamba-1.5-Large는 Transformer 레이어와 Mamba 레이어를 혼합한 하이브리드 아키텍처를 기반으로 합니다. 모델은 총 398B 파라미터 중 94B 활성 파라미터를 가지고 있으며, 9999 블록이 포함되어 있습니다. 각 블록은 8개의 레이어를 포함하며, 주의 레이어 대비 Mamba 레이어의 비율은 1:7입니다. MoE (mixture-of-experts) 모듈은 매 2 레이어마다 사용되며, 16명의 전문가 중 상위 2명을 선택합니다. 숨김 상태 차원은 8192이고, 주의 쿼리 헤드는 64, KV 헤드는 88입니다.
- **Performance Highlights**: Jamba-1.5 모델들은 특히 긴 컨텍스트 평가에서 두각을 나타내며, RULER 벤치마크에서 유효 길이 256K를 갖춘 유일한 모델입니다. KV 캐시 메모리 사용량은 10배 감소하고, 우수한 처리량과 대기 시간을 제공합니다. 또한 새로운 ExpertsInt8 양자화 기술을 사용하여, 모델의 무게를 INT8로 양자화하고 BF16으로 다시 복원함으로써 지연 시간이 거의 없어집니다. 이 방식은 FP8과 거의 동일한 지연 시간을 제공하면서 품질 손실 없이 GPTQ를 능가합니다.
- **Serving Considerations and Improvements**: 효율적인 서빙을 위해 새로운 양자화 기술인 ExpertsInt8를 개발했습니다. 이 기술은 양자화된 무게를 INT8로 저장하고, 실제 계산 전에 BF16으로 역양자화합니다. 덕분에 지연 시간 없이 더 빠른 속도를 제공합니다. 또한, MoE 모델을 서빙할 때 FP8과 동일한 지연 시간을 제공하면서도 A100 GPU에서도 사용할 수 있어 큰 장점을 가지고 있습니다. 추가적인 'Activation Loss' 용어를 도입하여 특정 활성화 값이 과도하게 커지는 것을 방지합니다.

### [Scalable Autoregressive Image Generation with Mamba](https://arxiv.org/abs/2408.12245)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12245.png)

Vote: 13

Authors: Haopeng Li, Kexin Wang, Jinyue Yang, Yuhong Chou, Guoqi Li, Xuerui Qiu, Xin Li

- **What's New**: 최근 몇 년 동안 Transformer Decoder 아키텍처(Vaswani et al. 2017)를 기반으로 한 autoregressive 모델이 큰 언어 모델(LLMs) 분야에서 혁명을 일으켰습니다. 이 모델들은 'next token prediction' 패러다임을 사용하여 놀라운 성능과 확장성을 보여주었습니다. 이러한 성공에 기반하여 autoregressive 모델의 시각적 생성 작업에 대한 연구가 진행되고 있습니다. 주요 모델인 VQGAN(Esser et al. 2021a)과 DALL-E(Ramesh et al. 2021a)은 이미지를 이산 토큰으로 변환하고 이를 순차적으로 생성하여 고성능을 달성했습니다. 그러나 최근에는 diffusion 모델이 새로운 기준을 설정하면서 autoregressive 모델의 성능을 능가하게 되었습니다.
- **Technical Details**: 하지만 diffusion 모델의 확장성은 제한적이며, autoregressive 모델은 뛰어난 확장성을 제공하여 대규모 응용에 적합합니다. 따라서 연구자들은 autoregressive 시각 생성 모델에 대한 연구를 계속하고 있습니다. 최근 발전된 autoregressive 모델은 next-scale prediction(Tian et al. 2024a)과 같은 기술과, Llama(Touvron et al. 2023; Sun et al. 2024)와 같은 고급 아키텍처를 사용하여 새로운 성과를 보여주고 있습니다. 그러나 시각 데이터의 높은 차원과 복잡성, Transformers의 시퀀스 길이에 따른 이차적인 계산 복잡성과 같은 문제들은 여전히 도전 과제로 남아있습니다. 이를 해결하기 위해 연구자들은 기존의 self-attention 메커니즘을 대체할 수 있는 선형 attention 메커니즘(Lingle 2023; Sun et al. 2023; Peng et al. 2023)을 탐구하고 있습니다. 대표적인 모델 Mamba(Gu and Dao 2023)는 선형 연산 복잡성을 가진 효율적인 시퀀스 모델링을 위해 설계된 상태 공간 모델(SSM)입니다.
- **Performance Highlights**: 이 논문에서 소개된 AiM 모델은 Mamba 아키텍처를 기반으로 한 최초의 autoregressive image generation 모델입니다. AiM은 next-token prediction 패러다임과 시각 도메인에 맞춤화된 새로운 adaptive layer normalization 방법인 adaLN-Group을 통합하여 성능과 파라미터 수의 균형을 최적화하였습니다. AiM은 ImageNet1K 256×256 벤치마크(Deng et al. 2009)에서 기존 Transformer 기반 autoregressive 모델과 diffusion 모델을 뛰어넘는 성능을 보여주었으며, 특히 작은 규모의 AiM 모델은 148M 파라미터만으로 FID 3.5를 달성하여 더 많은 파라미터를 필요로 하는 타 모델들을 능가했습니다. 또한 AiM은 다른 Transformer 기반 AR 모델 및 diffusion 모델보다 훨씬 빠른 추론 속도를 제공합니다.

### [Hermes 3 Technical Report](https://arxiv.org/abs/2408.11857)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11857.png)

Vote: 12

Authors: Jeffrey Quesnelle, Chen Guang, Ryan Teknium

- **What's New**: 이번 주 arxiv에 새롭게 발표된 연구에서, 연구진은 새로운 딥러닝 모델을 소개했습니다. 이 모델은 기존의 한계점을 극복하고 더 나은 정확도를 제공합니다. 특히, 이 모델은 의료 영상 처리 분야에서 혁신적인 발전을 일으킬 것으로 기대됩니다.
- **Technical Details**: 연구진이 제안한 모델은 Transformer 아키텍처를 기반으로 하며, Self-Attention 메커니즘을 활용해 입력 데이터를 효과적으로 처리합니다. 또한, 모델의 학습 과정에서 Data Augmentation 기법을 사용하여 데이터의 다양성을 확보하고, Regularization 기법들을 통해 과적합(overfitting)을 방지했습니다.
- **Performance Highlights**: 실험 결과, 제안된 모델은 기존의 ConvNet(CNN) 대비 평균 5% 이상의 정확도 향상을 보였습니다. 특히, 특정 의료 영상 데이터셋에서는 10% 이상의 성능 개선을 나타내며 주목할 만한 성과를 거두었습니다.

### [Real-Time Video Generation with Pyramid Attention Broadcast](https://arxiv.org/abs/2408.12588)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12588.png)

Vote: 11

Authors: Xuanlei Zhao, Kai Wang, Yang You, Xiaolong Jin

- **What's New**: Sora (Brooks et al., 2024) 논문은 DiT(Diffusion Transform)-기반 비디오 생성의 새로운 접근법을 제시합니다(예: Peebles & Xie, 2023). 최근 연구들은 CNN 기반 방법보다 비디오 품질이 향상된 것을 보여주지만(Zheng et al., 2024), 메모리 사용량, 계산량, 추론 시간 등의 큰 비용을 동반합니다. 따라서 DiT 기반 비디오 생성의 효율적 접근 방법은 매우 중요해졌습니다.
- **Technical Details**: 모델 압축(compression) 방법은 주로 distillation (Crowley et al., 2018; Hsieh et al., 2023), pruning (Han et al., 2015; Ma et al., 2023), 그리고 quantization (Banner et al., 2019; Lin et al., 2024) 같은 기술을 활용하여 딥러닝 모델을 가속화합니다. 그러나 이러한 방법들은 대부분 추가적인 학습(training)을 요구하며, 이는 대규모 사전 학습된 모델에서는 비효율적입니다. 반면, 모델 캐싱(caching) 방법은 추가 학습 없이 네트워크 출력 재사용을 통해 속도를 개선합니다. 특히, DiT 기반 비디오 생성에 대한 훈련 없이 속도를 높이는 방법은 아직 많이 연구되지 않았습니다.
- **Performance Highlights**: Pyramid Attention Broadcast (PAB)를 통해 DiT 기반 비디오 생성에서 실시간 비디오 생성을 구현했습니다. PAB는 각 확산 단계에서 주의(attention) 출력값을 방송(broadcasting)하여 주의 계산을 줄입니다. 이는 10.5배 가속화된 상태에서 20.6 FPS의 실시간 비디오 생성을 가능하게 하며, 품질을 저하시키지 않고도 Open-Sora (Zheng et al., 2024), Open-Sora-Plan (Lab & etc., 2024), Latte (Ma et al., 2024a) 등의 여러 인기 있는 오픈소스 비디오 DiTs에서 우수한 성능을 일관되게 제공합니다.

### [Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search](https://arxiv.org/abs/2408.10635)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10635.png)

Vote: 10

Authors: Min Cai, Guanzhi Wang, Jonathan Light, Xiusi Chen, Weiqin Chen, Yisong Yue, Wei Cheng, Ziniu Hu

- **What's New**: 최근 연구들은 대형 언어 모델(Large Language Models, LLMs)이 상호 작용 환경에서 의사결정을 개선하기 위한 스킬을 학습할 수 있음을 보여주었습니다. 그러나 다중 에이전트가 있는 적대적인 환경에서는 LLM이 스킬을 학습하는 데 큰 어려움이 따릅니다. 이러한 환경에서는 최적의 정책(policy)을 결정하는 것이 어려우며, 상대방이 각기 다른 정책을 사용할 가능성이 있기 때문입니다. 본 논문은 이러한 난점을 해결하고자 새로운 방법 \\method를 제안합니다. 이 방법은 효과적인 정책을 학습하고, 낮은 수준에서 실제 환경 없이 이를 평가하며, 효율적으로 이러한 정책을 찾는 것입니다.
- **Technical Details**: 우리의 접근 방식은 LLM self-improvement와 탐색을 결합합니다. 상위 전략(또는 스킬) 공간과 하위 레벨 동작 공간에서 이를 수행합니다. 상위 레벨에서는 진화 과정에서 전략 트리를 생성하고, 이전에 생성된 전략을 개선합니다. 특히, 게임 상태를 평가하기 위한 가치 휴리스틱(value heuristic)과 대화 생성 프로세스를 안내하는 대화 가이드를 학습합니다. 또한, 우리는 전략 트리를 탐색하고 탐사(exploration)와 활용(exploitation) 방법을 사용하여 좋은 전략을 찾습니다. 검색 과정에서 파라미터 업데이트나 감독 학습 튜닝이 필요하지 않습니다. 하위 레벨에서는 시뮬레이션된 자가 플레이(self-play)를 통해 전략을 평가합니다.
- **Performance Highlights**: 이 방법을 두 가지 게임 - 순수 전략 게임(Game of Pure Strategy, GOPS)과 Resistance: Avalon - 에 적용해 본 결과, 우리의 개선 방법이 대화가 있는 게임과 대화가 없는 게임 모두에서 효과적으로 작용하는 것을 확인했습니다. 우리의 개선 방법은 기존의 여러 자가 개선 방법 및 전통적인 강화 학습(RL) 기반 훈련 접근 방법보다 더 나은 성능을 보였습니다. 또한, 시뮬레이션된 자가 플레이 피드백을 통해 전략을 수집하고 이를 통한 피드백 제공에 우리의 방법이 효과적임을 입증했습니다.

### [SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for Large-scale Vision-Language Models](https://arxiv.org/abs/2408.12114)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12114.png)

Vote: 8

Authors: Yong Man Ro, Sangyun Chung, Byung-Kwan Lee, Youngjoon Yu

- **What's New**: 최근 연구에 따르면 대규모 비전-언어 모델(Large-scale Vision-Language Models, LVLMs)이 시각적 대화, 비디오 분석, 문서 이해 등에서 중요한 돌파구를 마련하였습니다. LVLM은 여러 유형의 멀티모달 입력을 처리할 수 있는 능력을 갖추어 인공지능의 새로운 가능성을 열고 있습니다. 이에 따라 다양한 멀티비전 센서 데이터를 효과적으로 통합하는 연구가 활발히 진행되고 있습니다.
- **Technical Details**: LVLM은 이미지, 비디오, 텍스트 데이터를 동시에 처리할 수 있는 능력을 갖추고 있습니다. 예를 들어, OpenAI의 GPT-4o는 뛰어난 추론 능력을 보여주며, LVLM은 열 센서, 깊이 센서, 의료 영상 데이터 등 다양한 멀티비전 센서 데이터를 입력으로 사용할 수 있습니다. 그러나 LVLM은 아직 물리적 비전 센서의 특성을 충분히 이해하지 못하는 한계가 있습니다. 이에 따라 SPARK 벤치마크가 제안되었으며, 6,248개의 테스트 샘플을 자동 생성하여 LVLM의 멀티비전 인식 및 추론 능력을 평가하였습니다.
- **Performance Highlights**: SPARK 벤치마크를 사용한 평가 결과, 대부분의 최신 LVLM은 멀티비전 센서리 추론에서 다양한 정도의 결함을 보였습니다. 이는 물리적 비전 센서에 대한 기본적인 이해 부족 때문으로 해석됩니다. 이러한 문제를 해결하기 위해 SPARK 벤치마크는 멀티비전 인식 및 추론 능력을 엄격하게 테스트할 수 있는 포괄적인 프레임워크를 제공합니다.

### [SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs](https://arxiv.org/abs/2408.11813)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11813.png)

Vote: 8

Authors: Yaqi Zhao, Yajie Zhang, Xin Tao, Pengfei Wan, Baoqun Yin, Ke Lin, Yuanyang Yin, Wentao Zhang, Jiahao Wang, Di Zhang

- **What's New**: 최근 몇 년간 멀티모달 대형 언어 모델(MLLMs)의 발전은 멀티모달 인지 및 추론 능력에서 인상적인 성과를 보여주었습니다. 이러한 모델은 다양한 모달리티 간의 격차를 줄여 더 종합적이고 상황에 맞는 상호작용을 가능하게 합니다. 이번 논문에서는 멀티모달 언어 모델의 불충분한 정렬 문제를 해결하기 위해 Supervised Embedding Alignment (SEA)라는 새로운 토큰 레벨 정렬 패러다임을 제안합니다.
- **Technical Details**: MLLMs의 전통적인 훈련 패러다임은 주로 두 단계로 구성됩니다: 사전 훈련(pre-training)과 지시 튜닝(instruction-tuning). 그러나 이러한 접근 방식은 텍스트 토큰에 주로 훈련된 언어 모델과 시각적 특징 간의 불일치로 인해 고유한 한계를 지닙니다. 이를 해결하기 위해 SEA는 명시적인 감독을 통해 시각적 토큰을 언어 모델의 임베딩 공간과 정밀하게 일치시키는 토큰 레벨 정렬 방식을 제안합니다. 또한, 대조 학습(contrastive learning) 손실과 LLM 예측 손실을 결합하여 어댑터의 정렬 기능을 개선했습니다.
- **Performance Highlights**: 제안된 SEA 방법론을 사용하여, LLaVA-1.5 모델의 성능을 8개의 벤치마크에서 크게 향상시켰습니다. 이는 추가적인 주석, 데이터 또는 추론 비용 없이 달성되었습니다. SEA를 활용한 이 훈련 전략은 비전-언어 작업에서 훈련된 비전 인코더를 위한 비용 효율적인 보편적 훈련 전략을 제공합니다.

### [Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications](https://arxiv.org/abs/2408.11878)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11878.png)

Vote: 8

Authors: Ruoyu Xiang, Haohang Li, Lihang Shen, Zhengyu Chen, Weiguang Han, Yangyang Yu, Yifei Zhang, Zhiyang Deng, Shunian Chen, Zhiyuan Yao, Zheheng Luo, +, Zihao Jiang, Daniel Kim, Mengxi Xiao, Yupeng Cao, Duanyu Feng, Yuzhe Yang, Yueru He, Dong Li, Xiao Zhang, Zhiwei Liu, Qianqian Xie

- **What's New**: 최근 금융 AI의 발전은 자연어 처리(NLP) 기술의 진보, 특히 대형 언어 모델(LLMs)에 의해 크게 주도되었습니다. 이러한 발전에도 불구하고 금융 도메인에서는 여전히 한계가 존재합니다. 이를 해결하기 위해 Open-FinLLMs라는 금융 분야에 특화된 대형 언어 모델 시리즈를 소개합니다. 이 모델은 복잡한 금융 데이터를 처리하고 분석하는 데 탁월한 성능을 보여줍니다.
- **Technical Details**: Open-FinLLMs는 다음과 같은 세 가지 주요 모델로 구성됩니다: FinLLaMA, FinLLaMA-Instruct, 그리고 FinLLaVA. FinLLaMA는 520억 개의 텍스트, 테이블 데이터, 시계열 데이터를 포함하는 도메인 특화 말뭉치로 사전 학습되었으며, FinLLaMA-Instruct는 573K 개의 다양한 금융 지침으로 미세 조정되었습니다. FinLLaVA는 1,430K 개의 멀티모달 이미지-텍스트 지침 쌍을 통합하여 차트, 테이블 등 복잡한 금융 데이터를 효과적으로 처리합니다.
- **Performance Highlights**: Open-FinLLMs 모델군은 광범위한 금융 및 멀티모달 벤치마크 테스트에서 탁월한 성능을 보여주었습니다. FinLLaMA는 제로샷 및 몇샷 설정에서 최고 성과를 달성했으며, 특히 감성 분석, 분류, 질문 응답(QA)에서 우수한 성과를 보였습니다. FinLLaMA-Instruct는 여섯 가지 금융 특화 다운스트림 작업 중 네 가지에서 다른 금융 LLM을 능가했으며, FinLLaVA는 모든 멀티모달 벤치마크에서 다른 7B 및 13B 모델을 넘어섰습니다.

### [Vintern-1B: An Efficient Multimodal Large Language Model for Vietnamese](https://arxiv.org/abs/2408.12480)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12480.png)

Vote: 6

Authors: Nhat H. Pham, Suong N. Hoang, Bao G. Huynh, Thuc D. Pham, Bang Q. Vo, Khang T. Doan, Quan T. M. Nguyen, Dung T. Hoang

- **What's New**: 최근 자연 언어 처리 분야에서는 대형 언어 모델(LLMs)의 출현이 복잡한 추론 및 심층 언어 이해 작업에서 뛰어난 성능을 입증하며 큰 변화를 가져왔습니다. LLMs의 성공에 힘입어, 연구자들은 텍스트 정보와 시각 데이터를 통합한 멀티모달 대형 언어 모델(Multimodal Large Language Models, MLLMs)을 탐구하기 시작했습니다. 하지만 고품질의 멀티모달 데이터셋이 부족하다는 점이 베트남어 MLLMs 발전의 큰 장벽으로 작용했습니다. 이에 따라, 우리는 베트남어 언어 작업을 위해 특별히 설계된 10억 매개변수 기반의 효율적인 멀티모달 모델 Vintern-1B를 소개합니다.
- **Technical Details**: Vintern-1B는 InternVL2-1B 구조를 기반으로 설계되었으며, Qwen2-0.5B-Instruct 언어 모델과 InternViT-300M-448px 시각 모델을 통합했습니다. 이 모델은 OCR, 문서 추출 및 일반 VQA 작업에 맞도록 최적화되었습니다. 모델의 핵심 구성 요소는 Vision Encoder(InternViT‑300M‑448px), MLP Projector, 그리고 Qwen2‑0.5B‑Instruct 언어 모델입니다. 이 외에도, 모듈 다이나믹 해상도 전략 및 픽셀 셔플 작업 등을 통해 고해상도 이미지 처리 성능을 높였습니다. 베트남어 문맥에서의 성능 향상을 위해, 우리는 다양한 데이터셋을 확장하여 모델을 미세조정했습니다.
- **Performance Highlights**: Vintern-1B 모델은 베트남어 멀티모달 작업에 특화된 다양한 데이터셋을 통해 미세 조정되었습니다. 특히 OCR 및 문서 시각 질문 답변 (DocVQA) 데이터셋을 신설하여, 실제 응용 분야에서 중요한 성능을 보였습니다. 예를 들어, Viet-ViTextVQA-gemini-VQA와 Viet-Vintext-gemini-VQA 데이터셋에서의 성능 향상을 포함한 다중 데이터셋을 기반으로 다양한 베트남어 시각 및 텍스트 데이터를 처리할 수 있도록 하였습니다. 또한, 베트남어 교재에서 추출한 데이터와 수학 및 과학 문제 이해를 위한 데이터셋을 추가하여 모델의 문서 이해 능력을 강화했습니다.

### [Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event Condition For Foley Sound](https://arxiv.org/abs/2408.11915)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.11915.png)

Vote: 6

Authors: Juhan Nam, Dabin Kim, Junwon Lee, Jaekwon Im

- **What's New**: 이번에 공개된 논문에서는 종합적인 작성 가이드를 제공하고 있습니다. 이 가이드는 논문의 폰트, 간격, 및 관련 정보를 포함한 전반적인 포맷팅 규칙에 대해 설명하고 있습니다. 주어진 규칙을 따라 논문을 작성해야 하며, 문의 사항은 제공된 연락처로 문의할 수 있습니다.
- **Technical Details**: 텍스트, 일러스트레이션, 차트 등 모든 인쇄물은 가로 178mm, 세로 229mm의 인쇄 영역 내에 유지되어야 합니다. 모든 텍스트는 두 개의 컬럼으로 나뉘어 있으며, 컬럼 간의 간격은 6mm로 설정됩니다. 페이지의 타이틀 섹션은 페이지 상단에서 약 35mm 떨어져 있어야 하며, Times 14-point boldface type으로 작성됩니다. Times-Roman 폰트 사용이 권장되며, 텍스트는 최소 9포인트로 작성되어야 합니다.
- **Performance Highlights**: 모든 일러스트레이션은 지정된 여백 내에 배치되어야 하며, 컬럼을 넘나들 수 있습니다. 삽화는 컬럼 상단에 배치하는 것이 바람직하며, 모두 캡션과 번호가 있어야 합니다. 발색은 흑백 프린터로 인쇄할 때도 읽기 쉽게 선택되어야 합니다. 각주는 최소한으로 사용하거나 사용하지 않는 것이 권장됩니다.

### [Anim-Director: A Large Multimodal Model Powered Agent for Controllable Animation Video Generation](https://arxiv.org/abs/2408.09787)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.09787.png)

Vote: 5

Authors: Baotian Hu, Haoyuan Shi, Jiashun Zhu, Jinyi Xu, Yunxin Li, Zhen Zhao, Min Zhang, Longyue Wang

- **What's New**: 이 논문은 AI를 활용한 자동 애니메이션 비디오 생성에 관한 새로운 접근을 제안하고 있습니다. Anim-Director라는 새로운 LMM(Large Multimodal Model) 기반 에이전트를 통해, 작은 이야기나 짧은 내러티브로부터 긴 애니메이션 비디오를 제작할 수 있는 방식을 소개합니다. Anim-Director는 애니메이션 제작의 모든 과정을 자동으로 관리하며, 특히 GPT-4V와 같은 최신 LMM 기술을 사용하여 이야기 정교화, 스크립트 생성, 장면 이미지 생성, 비디오 생성 등의 단계를 거칩니다.
- **Technical Details**: Anim-Director는 스토리 확장, 스크립트 생성, 장면 이미지 생성 및 개선, 비디오 제작, 비디오 품질 향상의 여러 단계를 자동화합니다. Text+Image → Image 및 Image+Text → Video와 같은 프레임워크를 사용하며, MidJourney와 같은 이미지 생성 도구와 Pika와 같은 비디오 생성 도구를 통합하여 사용합니다. 또한 LMM 기반 시스템을 통해 이미지와 비디오의 일관성을 유지하고 내용의 정확성을 평가합니다.
- **Performance Highlights**: 실험 결과, Anim-Director는 기존 기법들보다 향상된 스토리라인 및 배경 일관성을 가진 긴 애니메이션 비디오를 생성할 수 있음을 입증했습니다. 다양한 내러티브를 TinyStories 데이터셋에서 수집하여 TaleCraft 및 VBench에서 사용된 평가 메트릭스를 활용해 이미지와 비디오 품질을 평가한 결과, Anim-Director가 훨씬 더 나은 성과를 보였습니다.

### [ConflictBank: A Benchmark for Evaluating the Influence of Knowledge Conflicts in LLM](https://arxiv.org/abs/2408.12076)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12076.png)

Vote: 4

Authors: Xiaoye Qu, Jun Zhang, Yanshu Li, Yu Cheng, Zhaochen Su, Tong Zhu, Min Zhang, Jiashuo Sun, Juntao Li

- **What's New**: 이번 연구에서는 지식 충돌(knowledge conflicts)의 영향을 분석하기 위한 첫 번째 종합 벤치마크인 ConflictBank를 소개합니다. 이는 LLMs(Large Language Models)이 미싱정보(misinformation), 시간적 변화(temporal conflict), 다의어적 성질(semantic conflict) 등과 같은 실세계 시나리오에서 직면하는 다양한 지식 충돌을 분석하기 위한 데이터셋을 포함합니다.
- **Technical Details**: ConflictBank는 Wikipedia 데이터셋을 기반으로 2,863,205개의 주장(claims)을 수집하고 수정된 충돌 주장과 함께 7,453,853개의 주장-증거(claim-evidence) 쌍을 생성합니다. 또한 553,117개의 QA 쌍도 제공합니다. 세 가지 주요 충돌 원인(미싱정보, 시간적 변화, 다의어적 성질)을 다룹니다. 각 주장과 충돌 주장은 데이터를 구조화하여 쿼드러플 형식(quintuplet format)으로 표현됩니다: (s,r,o,sd,od)
- **Performance Highlights**: ConflictBank를 통해 12개의 대형 언어 모델(LLMs)에 대한 초기 실험을 수행하고, 모델의 규모, 충돌 원인, 및 충돌 유형에 대한 종합적인 분석을 제공합니다. 이를 통해 모델의 신뢰성과 신뢰성을 높이기 위한 심층적인 연구를 촉진할 것으로 기대됩니다.

### [Subsurface Scattering for 3D Gaussian Splatting](https://arxiv.org/abs/2408.12282)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12282.png)

Vote: 4

Authors: Hendrik P. A. Lensch, Andreas Engelhardt, Raphael Braun, Jan-Niklas Dihlmann, Arjun Majumdar

- **What's New**: 3D Gaussian Splatting (3D GS) 프레임워크에 서브서피스 관리(Surface Scattering, SSS) 효과를 구현한 새로운 방법을 도입했습니다. 이 방법은 단일 객체의 세부적인 SSS 효과를 캡처하면서 실시간으로 렌더링 및 리라이팅을 가능하게 합니다.
- **Technical Details**: 본 연구에서는 3D GS에 PBR(Material Parameters)와 연기된 쉐이딩(Deferred Shading)을 결합한 하이브리드 표현 방식을 제안했습니다. 경량의 잔차 예측 네트워크(Residual Prediction Network)를 통해 표면 셰이더로 모델링되지 않은 SSS 셰이딩 컴포넌트를 학습합니다. 신경 네트워크를 활용해 국부 및 전역 광원의 전송을 모델링하면서 PBR 렌더링 단계에서 사용되는 입사광을 예측하게끔 제약을 줬습니다. 특히, 이미지를 통해 셰이딩을 수행하여 3D Gaussian의 해상도 한계를 극복하고, 명확한 반사면 표현을 개선했습니다.
- **Performance Highlights**: 신규 OLAT(One-Light-at-a-Time) 데이터셋을 사용해 본 메소드는 NeRFs 기반의 이전 SSS 접근법에 비해 보다 향상된 학습시간과 빠른 렌더링 속도를 제공합니다. PBR 컴포넌트와 SSS 효과를 통해 종합적인 객체 기반 분해를 제공하여, 편집 및 새로운 소재 합성이 가능하게 했습니다. 실시간으로 의료 영상 및 시각화를 포함한 다양한 어플리케이션에서 활용될 수 있습니다.

### [The Russian-focused embedders' exploration: ruMTEB benchmark and Russian embedding model design](https://arxiv.org/abs/2408.12503)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12503.png)

Vote: 3

Authors: Alena Fenogenova, Alexander Abramov, Artem Snegirev, Maria Tikhonova, Anna Maksimova

- **What's New**: 이 논문은 러시아어에 특화된 새로운 텍스트 임베딩 모델과 이를 평가하기 위한 새로운 벤치마크, ruMTEB를 소개합니다. 이 모델은 영어와 러시아어 모두를 지원하여 고자원 언어인 영어로부터 지식을 전이할 수 있는 특성을 가집니다. 뿐만 아니라, ruMTEB는 MTEB 형식의 23개 텍스트 임베딩 작업을 포함하고 있으며, 그 중 17개는 새로운 러시아어 데이터셋이고 나머지 6개는 다국어 MTEB 데이터셋에서 형성된 작업입니다.
- **Technical Details**: ruMTEB는 분류(Classification), 군집화(Clustering), 다중 라벨 분류(MultiLabel Classification), 쌍 분류(Pair Classification), 재정렬(Reranking), 검색(Retrieval), 의미적 텍스트 유사도(STS) 등 7개 작업 범주로 구성된 23개의 데이터셋을 통합하여, 원래 MTEB 벤치마크의 유사한 범주와 일치시켰습니다. 평가 프로세스는 10번의 연속 실험(부트스트랩 평가)으로 수행됩니다.
- **Performance Highlights**: 새롭게 제안된 러시아어 중심 텍스트 임베딩 모델인 ru-en-RoSBERTa는 영어와 러시아어 양쪽에서 효과적인 성능을 보였습니다. 이 모델은 ruMTEB에서 성능을 평가받았으며, 오픈 소스 베이스라인 솔루션과 비교하여 우수한 성능을 입증했습니다.

### [Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning with LLMs](https://arxiv.org/abs/2408.12060)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12060.png)

Vote: 2

Authors: Pransh Patwa, Amitava Das, Parth Patwa, Ronit Singhal, Aman Chadha

- **What's New**: 소셜 미디어 플랫폼에서 가짜 뉴스와 허위 정보의 확산이 중요한 현대적 문제로 부상하고 있습니다. 이에 대응하기 위해 자동화된 사실 검증 시스템의 개발이 필수적입니다. 본 논문에서는 최신 Large Language Models(LLMs) 기술인 Retrieval-Augmented Generation (RAG)과 In-Context Learning (ICL)을 활용하여 증거 기반의 사실 검증 시스템을 제안합니다. 이 시스템은 '지원됨', '반박됨', '상충 증거/체리픽', '증거 불충분' 중 하나로 주장을 분류하고, 해당 분류에 대한 증거를 제공합니다.
- **Technical Details**: 제안된 시스템은 주어진 주장과 문서 집합을 가지고 RAG 파이프라인을 사용하여 가장 관련성이 높은 문서 3개를 검색하고, 그 문서에서 증거를 추출합니다. 이후 ICL을 활용해 추출된 증거를 바탕으로 주장에 대한 진위를 판단합니다. 실험은 Averitec 데이터셋을 사용하여 진행되었으며, 이는 Schlichtkrull et al.(2023)이 제공하는 데이터셋입니다. 이 데이터셋은 주장이 포함된 기사들을 모아놓은 지식 저장소와 함께 제공되며, 각 주장은 증거와 진위 라벨, 라벨에 대한 정당성을 포함합니다.
- **Performance Highlights**: 본 시스템은 Averitec 데이터셋에서 기존 베이스라인 대비 성능이 크게 향상된 결과를 보였습니다. 특히, 소량의 훈련 샘플만으로 우수한 성능을 달성하여 수작업으로 주석을 달아야 하는 대규모 데이터셋의 필요성을 없앴습니다. 다양한 최신 LLM들을 사용한 실험과 결과 분석이 포함되어 있습니다.

