## Daily Papers (2024-08-01)

### [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21783.png)

Vote: 54

Authors: Alan Schelten, Abhinav Pandey, Archi Mitra, Aiesha Letman, Aobo Yang, Abhimanyu Dubey, Aston Zhang, Angela Fan, Arun Rao, Abhishek Kadian, Austen Gregerson, Ava Spataru, Arthur Hinsvark, +, Ahmad Al-Dahle, Akhil Mathur, Abhinav Jauhri, Anirudh Goyal, Aurelien Rodriguez, Archie Sravankumar, Amy Yang, Anthony Hartshorn, Artem Korenev

- **What's New**: 이 논문은 AI 모델의 학습 효율성을 극대화하기 위한 새로운 알고리즘을 제안합니다. 특히, 데이터의 양이 한정된 환경에서 높은 성능을 발휘할 수 있는 방법론을 소개하고 있습니다.
- **Technical Details**: 제안된 알고리즘은 기존의 Gradient Descent(GD) 방식에서 변형된 Adaptive Learning Rate(ALR)를 사용합니다. 이 방법은 학습 과정에서 자동으로 학습률을 조정하여 최적화 속도를 높입니다. 또한, Regularization Technique(정규화 기법)를 통해 Overfitting(과적합)을 방지합니다.
- **Performance Highlights**: 실험 결과, 제안된 알고리즘은 기존 알고리즘 대비 평균 15% 이상의 성능 향상을 보였습니다. 특히, 소규모 데이터셋에서 높은 일반화 능력을 자랑하며, 다양한 분야의 테스트에서도 우수한 성능을 기록했습니다.

### [Tora: Trajectory-oriented Diffusion Transformer for Video Generation](https://arxiv.org/abs/2407.21705)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21705.png)

Vote: 11

Authors: Long Qin, Junchao Liao, Menghao Li, Zhenghao Zhang, Weizhi Wang

- **What's New**: Diffusion 모델(Dhariwal and Nichol 2021; Ramesh et al. 2022)은 고품질의 다양한 이미지나 영상을 생성할 수 있는 능력을 보여주었습니다. 최근 텍스트-영상 생성 모델인 Sora(Brooks et al. 2024)는 Diffusion Transformer(DiT)(Peebles and Xie 2023)을 활용해 기존의 최신 기술을 크게 능가하는 영상 생성 능력을 시연했습니다. Sora는 10초에서 60초까지 다양한 해상도의 영상을 생성할 수 있으며, 실제 물리 법칙을 준수하는 능력도 갖추고 있습니다.
- **Technical Details**: 영상 생성에서 중요한 요소는 시퀀스에서 일관된 모션을 생성하는 것입니다. 이전 연구들은 일반적으로 모션 벡터와 옵티컬 플로우 데이터를 추출해 영상 생성 모델을 지시하는 방식으로 모션 조작 기술을 구현했습니다. MotionCtrl(Wang et al. 2023b)은 카메라 이동과 객체 움직임을 별도로 관리하여 모션 범위와 패턴을 확장시켰습니다. 그러나 이러한 방법들은 최대 16프레임의 저해상도 영상 생성에 제한이 있으며, 이는 모션 블러, 외형 왜곡 등의 문제를 초래할 수 있습니다. 이러한 문제를 해결하기 위해 Tora라는 DiT 모델을 소개합니다. 이 모델은 텍스트, 이미지, 경로 데이터를 동시에 통합하여 영상 콘텐츠에 대한 세밀하고 다재다능한 제어를 보장합니다. Tora는 제공된 경로를 여러 패치로 내장하여 모션 유동성을 높이고, OpenSora(Zheng et al. 2024)를 기초로 설계되었습니다. TE(Trajectory Extractor)와 MGF(Motion-guidance Fuser)라는 두 가지 새로운 모듈이 설계되어 모션 패치를 DiT 블록에 통합합니다.
- **Performance Highlights**: 실험 결과, Tora는 다양한 종횡비의 720p 해상도 영상을 최대 204 프레임까지 생성할 수 있으며, 지정된 경로에 따라 움직임을 시뮬레이션하는 데 우수함을 입증했습니다. 또한, 물리 세계의 움직임을 더 효율적으로 시뮬레이션합니다.

### [Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent](https://arxiv.org/abs/2407.21646)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21646.png)

Vote: 9

Authors: Shanbo Cheng, Ningxin Peng, Qini Zhang, Hang Li, Zhichao Huang, Tom Ko, Lu Xu

- **What's New**: 본 논문은 대규모 언어 모델(LLM)을 기반으로 한 실시간 음성 번역(SiST) 시스템 'CLASI'를 제안합니다. CLASI는 순차적인 번역 작업을 수행하며, 인간 번역자의 작업 방식을 모방해 의미 단위로 텍스트를 분할하여 번역합니다.
- **Technical Details**: CLASI는 'Cross-Lingual Agent'로, 음성 인식(ASR), 구두점(Punctuation) 모델, 기계 번역(MT) 모델을 포함한 전통적인 연속 시스템의 한계를 극복합니다. CLASI는 데이터 기반 정책 학습 과정과 인간 번역자의 주석을 통해 의미 단위로 텍스트를 분할하는 법을 학습합니다. 또한, 외부 지식 데이터베이스와 메모리 모듈을 사용해 전문 용어와 문맥 정보를 실시간으로 반영합니다.
- **Performance Highlights**: 중국어-영어 번역에서 CLASI는 'Valid Information Proportion (VIP)' 평가 기준에서 81.3%라는 높은 점수를 기록하며, 현재 이용 가능한 다른 시스템보다 월등히 우수한 성능을 보여줍니다. 인간 번역자가 평균 70% 이상의 정보 전달 비율을 보이는 것과 비교했을 때, 매우 유사한 성능을 발휘합니다.

### [MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](https://arxiv.org/abs/2407.21770)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21770.png)

Vote: 7

Authors: Akshat Shrivastava, Armen Aghajanyan, Gargi Gosh, Srinivasan Iyer, Xi Victoria Lin, Luke Zettlemoyer, Mike Lewis, Liang Luo

- **What's New**: 이번 연구에서는 Chameleon 팀이 제안한 초기 융합 아키텍처를 기반으로 혼합 모드 (mixed-modal) 모델의 효율성과 성능을 향상시키기 위해 모델리티 인지 희소성 (modality-aware sparsity) 기법을 도입하였습니다. 이러한 기법을 통해 더 강력한 크로스 모드 (cross-modal) 추론 능력을 유지하면서 모델의 계산 효율성을 최적화하는 가능성을 탐구하였습니다.
- **Technical Details**: Chameleon 모델은 이미지와 텍스트 토큰을 단일 트랜스포머 (transformer) 아키텍처에서 처리하여 복잡한 모드 간의 관계를 캡처합니다. 이는 다음 토큰 예측 목표 (next-token prediction objective)를 사용하여 텍스트와 이미지 토큰을 자동 회귀적으로 생성하도록 학습됩니다. 본 연구에서는 혼합 모드 초기 융합 모델을 위해 모드 인지 전문가 혼합 (Mixture of Modality-aware Experts, MoMa) 기법을 제안하였으며, 이는 피드 포워드 모듈에 모드 인지 블록 희소성을 도입하여 새로운 차원의 스케일링을 가능하게 합니다.
- **Performance Highlights**: 모드 인지 전문가 혼합(MoMa)을 통한 모델은 기본 모델 대비 3.7배의 FLOPs 절감을 달성했으며, 모드 인지 깊이 혼합(MoD)을 추가하면 전체 FLOPs 절감이 4.2배로 증가했습니다. 또한 믹스드 모드의 긴 형태 응답 생성 면에서 Chameleon-MoMa는 상용 모델을 능가하는 성능을 보였습니다. 단, MoD 모델은 라우팅 정확도에 민감하여 자동 회귀적 추론 성능이 Chameleon-MoMa에 비해 다소 저하되는 것을 확인하였습니다.

### [TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization Methods](https://arxiv.org/abs/2407.21630)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21630.png)

Vote: 5

Authors: Damien Riquet, Gabriel Loiseau, Marc Tommasi, Maxime Meyer, Damien Sileo

- **What's New**: 이 논문에서는 텍스트 익명화(AO)를 재구성하여 두 가지 적대적 상황을 제시합니다. 하나는 작성자의 정체를 드러내려는 공격자 모델이고, 다른 하나는 주어진 작업을 수행하려는 유틸리티 모델입니다. 이러한 접근 방식은 저작물의 저자 신호를 제거하면서도 텍스트의 유용성을 최대한 보존하는 것을 목표로 합니다. 이를 위해 정책 최적화(Policy Optimization, PO)와 지도 학습(Supervised Fine-Tuning, SFT)을 결합한 새로운 모델 TAROT을 도입합니다.
- **Technical Details**: 이 연구에서는 TAROT라는 모델을 제안합니다. TAROT는 지도 학습(SFT)과 정책 최적화(PO)를 통해 텍스트 재작성 목표를 달성하고, 유사한 문장 임베딩을 유지하면서도 저자 신호를 제거합니다. 이 모델은 다양한 데이터셋과 작업에 대해 작동합니다. 특히, TAROT는 TAROT-PPO와 TAROT-DPO라는 두 가지 버전으로 구성되어 있으며, 각각의 PO 알고리즘을 통해 미세 조정되었습니다.
- **Performance Highlights**: TAROT 모델은 영화 리뷰, 블로그 기사, 학술 문서 등 세 가지 데이터셋에서 평가되었습니다. 여러 저작물 공격자와 다운스트림 사용 사례를 통해 TAROT는 다양한 데이터셋에서 저작물 보호와 작업 유용성 사이의 균형을 효과적으로 유지함을 입증했습니다.

### [ShieldGemma: Generative AI Content Moderation Based on Gemma](https://arxiv.org/abs/2407.21772)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21772.png)

Vote: 5

Authors: Bhaktipriya Radharapu, Yuchi Liu, Olivia Sturman, Karthik Narasimhan, Piyush Kumar, Ludovic Peran, Wenjun Zeng, Oscar Wahltinez, Hamza Harkous, Drew Proud, Ryan Mullins, Joe Fernandez

- **What's New**: 최근 논문에서 다양한 애플리케이션에 사용되는 범용 인공지능 모델인 대형 언어 모델(Large Language Models, LLMs)의 컨텐츠 안전성을 강화하기 위해 새로운 프레임워크를 제안했습니다. 이 프레임워크는 기존 LLM 콘텐츠 중재 솔루션의 한계를 극복하고 고품질 데이터 생성 방법론을 도입합니다.
- **Technical Details**: 이 논문은 범위별로 2B에서 27B 파라미터를 갖춘 최첨단 컨텐츠 중재 모델을 제안합니다. 이러한 다양한 모델 크기는 다양한 사용 사례에 최적화된 성능을 제공합니다. 제안된 모델은 사용자 입력과 모델 출력 모두에서 주요 해악 유형을 필터링하는데 적용될 수 있습니다. 또한 인간 주석 작업을 줄이고, 안전 관련 데이터를 생성하는데 도움이 되는 고품질, 적대적, 다양한, 공정한 데이터를 생성하는 새로운 방법론을 제시합니다.
- **Performance Highlights**: 제안된 모델은 성능 최적화된 다양한 크기의 모델을 제공하여 다양한 애플리케이션 요구사항을 충족시킵니다. 추가적으로, 새로운 데이터 생성 방법론은 공정하고, 적대적 데이터를 자동으로 생성하여 모델의 안전성을 향상시킵니다.

### [Data Contamination Report from the 2024 CONDA Shared Task](https://arxiv.org/abs/2407.21530)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21530.png)

Vote: 4

Authors: Iker García-Ferrero, Alon Jacovi, Pengfei Liu, Jenny Chim, Luca D'Amico-Wong, Emily Silcock, Yoav Goldberg, Wei-Lin Chen, Yanai Elazar, Suryansh Sharma, Run-Ze Fan, Eneko Agirre, Oscar Sainz, Yucheng Li, Leshem Choshen, Melissa Dell, +, Jon Ander Campos, Kateryna Solonko, Bhavish Pahwa, Ameya Prabhu, Shahriar Golchin, David Stap

- **What's New**: 최근 언어 모델(large language models, LLMs)과 데이터셋의 규모가 커짐에 따라 평가 데이터가 사전 훈련 데이터에 무단으로 포함되는 데이터 오염 이슈가 부각되고 있습니다. 이러한 오염은 LLMs의 특정 과제나 벤치마크에서 성능을 인위적으로 높이는 결과를 초래하며, 공정하고 편향되지 않은 평가를 어렵게 만듭니다. 이번 보고서는 데이터 오염 데이터베이스(https://huggingface.co/spaces/CONDA-Workshop/Data-Contamination-Database)에 제출된 오염 증거를 수집 및 정리하고, 이를 통해 NLP(자연어 처리) 분야에서 데이터 오염 상태를 전체적으로 조망합니다.
- **Technical Details**: 오염 증거는 데이터를 기반으로 한 방법(data-based)과 모델을 기반으로 한 방법(model-based)으로 나뉘어 수집되었습니다. 데이터 기반 접근법은 사전 훈련 코퍼스를 검사하여 오염 증거를 찾는 방식으로, 문자열 매칭 기술(13-gram overlap, 50-character overlap 등)을 주로 사용합니다. 반면, 모델 기반 접근법은 모델의 출력 또는 출력을 분석하여 오염 정도를 추정하는 방법으로, Membership Inference Attacks(MIA)를 활용합니다. 각 증거는 공개된 풀 리퀘스트를 통해 논의된 후 데이터베이스에 포함되었습니다.
- **Performance Highlights**: 오염된 소스(훈련 코퍼스 또는 모델) 42개, 데이터셋 91개, 오염 항목 566개가 보고되었습니다. 오염 항목 중 432개는 오염 이벤트(훈련 세트 20개, 개발 세트 95개, 테스트 세트 317개)이고, 144개는 비오염 이벤트입니다. 데이터베이스는 각 평가 데이터셋의 분할(훈련, 개발, 테스트)별로 오염 비율을 제공합니다. 보고서에는 주로 CommonCrawl 데이터 기반의 사전 훈련 코퍼스에서 발견된 테스트 세트의 수가 포함되어 있습니다.

### [Open-Vocabulary Audio-Visual Semantic Segmentation](https://arxiv.org/abs/2407.21721)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21721.png)

Vote: 3

Authors: Wenzhen Yue, Ji Shi, Bowei Xing, Dantong Niu, Xianghua Ying, Yanyu Qi, Ruohao Guo, Liao Qu

- **What's New**: 이 논문에서는 새로운 멀티모달 작업인 'open-vocabulary audio-visual semantic segmentation'을 제안합니다. 이 작업은 비디오에서 소리를 내는 객체들을 열려있는 범주(seen and unseen)에 따라 분류하고 세그멘테이션을 수행하는 것을 목표로 합니다. 이는 기존의 이진 마스크 생성과 달리 각 픽셀에 의미적 레이블을 할당하는 더 도전적인 작업입니다.
- **Technical Details**: 제안된 모델 'OV-AVSS'는 두 주요 구성 요소로 이루어져 있습니다: 범용 음원 위치 추적 모듈(Universal Sound Source Localization Module, USSLM)과 열린 어휘 분류 모듈(Open-Vocabulary Classification Module, OVCM). USSLM은 오디오-비주얼 조기 융합 모듈(audio-visual early fusion module)과 오디오 조건 변환기 디코더(audio-conditioned Transformer decoder)를 사용해 공간 및 시간 도메인에서 오디오 및 비주얼 특징을 통합합니다. OVCM은 사전 학습된 CLIP 모델을 활용하여 소리나는 객체의 범주를 예측합니다. 문제 해결을 위해 다양한 기술 요소들이 포함되었습니다: bi-directional cross-attention, Mask2Former 기반 디코딩, 및 cross-attention 레이어를 통한 오디오 정보 추출 등이 있습니다.
- **Performance Highlights**: 모델의 일반화 성능을 평가하기 위해 새로운 open-vocabulary 데이터셋이 구축되었으며, 실험 결과 OV-AVSS 모델이 다양한 unseen 객체와 데이터셋에서 우수한 세그멘테이션 성능을 발휘하는 것으로 나타났습니다.

### [Expressive Whole-Body 3D Gaussian Avatar](https://arxiv.org/abs/2407.21686)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21686.png)

Vote: 3

Authors: Takaaki Shiratori, Gyeongsik Moon, Shunsuke Saito

- **What's New**: 저자들은 ExAvatar라는 새로운 전신 3D 인간 아바타 생성 방식을 소개합니다. 이 방식은 짧은 단안 비디오(mono video)로부터 만들어지며, 3D 관측 없이 얼굴 표정과 손 동작을 포함해 전체 신체의 3D 동작을 모델링할 수 있습니다.
- **Technical Details**: ExAvatar는 전신 3D 파라메트릭 모델(SMPL-X)과 3D Gaussian Splatting(3DGS)을 결합하여 설계되었습니다. 이 하이브리드 표현 방식은 표면 메시와 3D Gaussian을 이용하며, 각 3D Gaussian을 표면의 vertex로 취급하여 메시 토폴로지를 따릅니다. 이를 통해 SMPL-X의 얼굴 표정 공간과 완전하게 호환됩니다. 또한, 연결성 기반 규제자(regularizer)를 사용하여 새로운 얼굴 표정과 포즈에서 발생할 수 있는 아티팩트를 크게 줄였습니다.
- **Performance Highlights**: 실험을 통해 ExAvatar가 기존의 3D 인간 아바타 모델들보다 뛰어난 성능을 보임을 입증했습니다. 특히, 짧은 단안 비디오 데이터로만 훈련했음에도 불구하고 높은 정확도의 얼굴 표정과 몸 동작 애니메이션이 가능했습니다.

### [Fine-gained Zero-shot Video Sampling](https://arxiv.org/abs/2407.21475)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21475.png)

Vote: 3

Authors: Jie Hu, Dengsheng Chen, Xiaoming Wei, Enhua Wu

- **What's New**: 새로운 제로-샷(Zero-shot) 비디오 샘플링 알고리즘인 𝒵⁢𝒮2𝒵(Zero-Shot video Sampling)을 소개합니다. 이 알고리즘은 사전 학습된 이미지 디퓨전 모델(pre-trained image diffusion model)로부터 고품질의 비디오 세그먼트를 직접 샘플링하는 것을 가능하게 합니다.
- **Technical Details**: 본 연구는 연속적인 비디오 시퀀스가 노이즈(라티언트) 공간에서 상당한 상관관계를 보인다는 점을 관찰하여, 전통적인 랜덤 노이즈 초기화를 대체할 의존 노이즈 모델(dependency noise model)을 제안합니다. 또한, 긴 세그먼트 동안 샘플링된 비디오 콘텐츠의 연속성을 강화하기 위해 자기 주의 기능(self-attention function) 내에 시간적 모멘텀 메커니즘(temporal momentum mechanism)을 도입합니다.
- **Performance Highlights**: ['새로운 제로-샷 비디오 샘플링 알고리즘이 고품질 비디오 세그먼트를 생성할 수 있음을 입증합니다.', '의존 노이즈 모델과 시간적 모멘텀 주의가 처음으로 생성된 비디오의 시간적 변화를 유연하게 제어할 수 있게 합니다.', '조건부 및 특수 비디오 생성, 텍스트 지시에 따른 비디오 편집 등 다양한 응용 분야에서 본 기법의 효과를 입증합니다.']

### [Berkeley Humanoid: A Research Platform for Learning-based Control](https://arxiv.org/abs/2407.21781)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21781.png)

Vote: 2

Authors: Xuanyu Huang, Qiayuan Liao, Xiaoyu Huang, Koushil Sreenath, Bike Zhang, Zhongyu Li

- **What's New**: 이 논문은 최신 인공지능(AI) 기술을 적용한 새로운 알고리즘을 소개하며, 이는 기존 방법보다 더 높은 성능을 보입니다. 특히, 이 알고리즘은 이미지 분류(image classification) 분야에서 혁신적인 접근 방식을 제시합니다.
- **Technical Details**: 논문에서 제안된 모델은 Transformer 아키텍처를 기반으로 하며, 이를 통해 이미지의 특징(feature)을 더 효율적으로 추출할 수 있습니다. 또한, Self-Attention 메커니즘을 활용하여 이미지의 중요한 부분을 강조합니다. 데이터셋으로는 CIFAR-10과 ImageNet을 사용하였으며, 각 데이터셋에서의 성능을 비교하였습니다.
- **Performance Highlights**: 실험 결과, 제안된 알고리즘은 기존의 ResNet과 VGG 등 유명한 모델들보다 더 높은 정확도(accuracy)를 기록하였습니다. 특히 CIFAR-10 데이터셋에서는 95% 이상의 정확도를 달성하였으며, 이는 현재 최고 성능 중 하나로 평가됩니다. ImageNet에서도 탁월한 성능을 보이며, 다양한 분야에서의 활용 가능성을 보여줍니다.

### [NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields](https://arxiv.org/abs/2404.01300)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.01300.png)

Vote: -

Authors: Rares Ambrus, Zsolt Kira, Muhammad Zubair Irshad, Sergey Zakharov, Vitor Guizilini, Adrien Gaidon

- **What's New**: 이 논문은 Neural Radiance Fields(NeRFs)을 이용하여 셀프-슈퍼바이즈드(Self-Supervised) 학습을 수행하는 새로운 프레임워크, NeRF-MAE(NeRF Masked Auto Encoders)를 제안합니다. NeRF의 Radiance와 Density Grid를 입력 모달리티로 활용하여, Masked Autoencoders를 적용함으로써 다양한 다운스트림(Downstream) 작업에서 뛰어난 성능을 보여줍니다.
- **Technical Details**: NeRF-MAE는 표준 3D Transformer를 활용하여 Masked Autoencoding 목표를 수행합니다. 이는 NeRF의 Radiance와 Density Grid를 volumetric masking 및 embedding 모듈과 Autoencoder 객체로 분할하여 학습합니다. 제안된 모델은 4D Voxel Grid를 작은 패치로 나누고, 무작위로 일부를 가려 데이터 중복을 최소화한 다음, 남은 패치로부터 방사 및 밀도 패치를 재구성합니다. 훈련과 평가를 위해서는 Front3D, ScanNet, ARKitScenes, Hypersim 등의 다양한 소스로부터 180만 개 이상의 이미지와 3600개 이상의 실내 장면을 포함하는 대규모 데이터셋을 준비했습니다.
- **Performance Highlights**: NeRF-MAE는 다른 셀프-슈퍼바이즈드 3D 전이학습 기법과 비교했을 때, 성능 면에서 뛰어나며, 특히 Front3D와 ScanNet 데이터셋에서 3D 객체 경계상자(OBB) 예측에서 21.5%의 AP50과 8%의 AP25 향상을 보였습니다. 또한, 제안된 접근법은 동일한 성능을 달성하기 위해 기존 최첨단 기법보다 절반 이하의 데이터만 필요로 합니다. 추가로, Front3D 데이터셋에서 3D 객체 탐지와 시멘틱 Voxel 레이블링에서 각각 20% 이상의 AP50과 12%의 mAcc을 달성했습니다.

