## Daily Papers (2024-08-19)

### [xGen-MM (BLIP-3): A Family of Open Large Multimodal Models](https://arxiv.org/abs/2408.08872)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08872.png)

Vote: 61

Authors: An Yan, Jieyu Zhang, Shu Zhang, Shrikant Kendre, Ning Yu, Huan Wang, Senthil Purushwalkam, Yejin Choi, Juntao Tan, Honglu Zhou, Jun Wang, +, Le Xue, Ludwig Schmidt, Tulika Manoj Awalgaonkar, Chia-Chih Chen, Yutong Dai, Manli Shu, Michael S Ryoo, Viraj Prabhu, Shelby Heinecke, Anas Awadalla, Can Qin

- **What's New**: 새로운 대규모 다중모달 모델 프레임워크인 xGen-MM (BLIP-3)을 소개합니다. 이 프레임워크는 다중모달 데이터셋과 정렬된 캡션 데이터셋 등 공공 데이터셋을 활용하여 LMM(대형 다중모달 모델) 훈련을 확장합니다. 또한 모델의 가중치와 코드도 공개하여 오픈소스 커뮤니티의 연구와 개발을 장려합니다.
- **Technical Details**: xGen-MM (BLIP-3)은 비전 토큰 샘플러(Perceiver Resampler)를 사용하여 이미지 임베딩을 다운샘플링하고, 사전 훈련된 대형 언어 모델(phi3-mini)을 통합하여 다중모달 데이터를 처리합니다. 이미지의 고해상도 이해를 위하여 이미지의 패치를 개별적으로 코딩하며, 다운샘플된 비전 토큰을 사용하여 시퀀스 길이를 줄입니다. 모델은 오토레그레시브 방식의 텍스트 토큰 손실를 중심으로 훈련됩니다.
- **Performance Highlights**: BLIP-3 모델은 약 1000억 개의 다중모달 토큰을 사용하여 사전 훈련되었으며, 고해상도의 다양한 이미지와 텍스트 데이터에서 우수한 성능을 보였습니다. 두 단계의 포스트-트레이닝을 통해 모델의 유용성과 안전성을 향상시켰습니다. 또한, 대규모 OCR(annotation)과 시각적 그라운딩 데이터셋을 추가하여 특정 작업에서의 성능을 더욱 강화했습니다.

### [JPEG-LM: LLMs as Image Generators with Canonical Codec Representations](https://arxiv.org/abs/2408.08459)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08459.png)

Vote: 24

Authors: Xiaochuang Han, Pang Wei Koh, Yulia Tsvetkov, Marjan Ghazvininejad

- **What's New**: 이번 연구에서는 최소한의 적응으로 다양한 작업을 수행할 수 있는 대형 언어 모델(LLM)을 이미지 및 비디오 생성 작업에 활용하는 방법을 제시합니다. 기존의 복잡한 벡터 양자화(Vector Quantization, VQ) 방법 대신 일반적인 코덱(JPEG, AVC/H.264)을 사용하여 데이터를 분리(discretize)하는 간단한 접근법을 도입했습니다.
- **Technical Details**: 우리는 이미지 및 비디오 생성을 위한 LLM의 주요 장애물인 연속 데이터를 이산 토큰(discrete tokens)으로 변환하는 문제를 해결했습니다. 이를 위해 JPEG 및 AVC/H.264 코덱을 비신경 네트워크(non-neural) 예비 처리기로 사용하여 데이터의 이산화 과정에서 복잡함을 줄였습니다. 두 가지 7B 모델(Jpeg-LM 및 Avc-LM)을 Llama-2 아키텍처로 사전 학습하여 각각 256x256 이미지 및 256x144 픽셀, 15프레임 비디오를 생성할 수 있음을 시연했습니다.
- **Performance Highlights**: 우리 모델은 기존의 VQ 기반 모델을 뛰어넘는 성능을 보였습니다. Jpeg-LM은 생성 품질에서 평균 31%의 FID 감소를 기록했고, Avc-LM은 현실적인 움직임을 가진 비디오를 생성했습니다. 특히, Jpeg-LM은 인간의 얼굴/눈과 같은 세밀한 요소를 더 잘 포착하는 것으로 나타났습니다. 이러한 결과는 비전 특화 요소를 필요로 하지 않는 간단한 트레이닝 방식으로도 높은 성능을 낼 수 있음을 보여줍니다.

### [Automated Design of Agentic Systems](https://arxiv.org/abs/2408.08435)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08435.png)

Vote: 13

Authors: Jeff Clune, Shengran Hu, Cong Lu

- **What's New**: 최근 Arxiv에 게시된 논문은 혁신적인 인공지능(AI) 모델 또는 알고리즘에 대한 최신 연구를 다루고 있습니다. 이번 논문은 특정 문제 영역에서 기존 방법보다 향상된 성능을 보여주는 새로운 접근법을 제안하고 있습니다.
- **Technical Details**: 이 논문에서는 딥러닝(deep learning) 기술을 활용한 새로운 네트워크 아키텍처(architecture)를 제안하고 있습니다. 주요 기법으로는 Transformer, Convolutional Neural Network(CNN), Recurrent Neural Network(RNN) 등의 새로운 변형이 포함됩니다. 또한, 효율적인 학습을 위한 데이터 증강(data augmentation)과 정규화(regularization) 기법도 사용하였습니다.
- **Performance Highlights**: 제안된 모델은 다양한 벤치마크(benchmark) 데이터셋에서 기존 최첨단 모델들보다 뛰어난 성능을 보여주었습니다. 특히 정확도(accuracy), 정밀도(precision), 재현율(recall)과 같은 주요 평가 지표(metrics)에서 높은 점수를 기록하였습니다. 이러한 성과는 제안된 방법론이 실제 문제 해결에 유용할 수 있음을 입증합니다.

### [Surgical SAM 2: Real-time Segment Anything in Surgical Video by Efficient Frame Pruning](https://arxiv.org/abs/2408.07931)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.07931.png)

Vote: 12

Authors: Mingxuan Hong, Junde Wu, Haofeng Liu, Yueming Jin, Erli Zhang

- **What's New**: SurgSAM-2는 SAM2 모델을 수술 비디오 분할(surgical video segmentation)에 최적화하여 실시간 적용 가능성을 높였습니다. 이는 수술 환경의 제약을 고려하여 효율성과 정확성을 동시에 달성하는 최초의 시도 중 하나입니다.
- **Technical Details**: SurgSAM-2는 ViT 아키텍처를 토대로, 동적 메모리 관리 메커니즘을 추가하여 프레임의 중요도를 기반으로 불필요한 프레임을 제거하고, 중요한 프레임만 유지하는 방식으로 최적화되었습니다. 이를 통해 정교한 분할 작업을 구현하며 연산 비용을 줄였습니다. 메모리 관리 모듈은 코사인 유사도(cosine similarity)를 이용해 각 프레임의 중요성을 평가합니다.
- **Performance Highlights**: SurgSAM-2는 EndoVis17 및 EndoVis18 데이터셋 테스트에서 우수한 성능을 보였습니다. 이는 자원 제약이 있는 환경에서도 높은 정확도를 유지하면서도 프레임 당 처리 속도를 향상시켰습니다. 이 모델은 수술 중 즉각적이고 정확한 의사결정을 지원하여 수술 시간을 단축하고 환자 결과를 개선할 수 있습니다.

### [TurboEdit: Instant text-based image editing](https://arxiv.org/abs/2408.08332)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08332.png)

Vote: 11

Authors: Eli Shechtman, Zongze Wu, Nicholas Kolkin, Richard Zhang, Jonathan Brandt

- **What's New**: 이번 연구에서는 텍스트-이미지 디퓨전 모델을 빠르고 효율적으로 사용할 수 있는 TurboEdit를 제안합니다. TurboEdit는 실시간 이미지 편집을 가능하게 하며, 고품질의 텍스트 기반 이미지 편집을 빠르게 수행할 수 있습니다.
- **Technical Details**: TurboEdit는 두 가지 주요 기여를 합니다. 첫째, 입력 이미지를 재구성하기 위해 노이즈를 예측하는 인버젼 네트워크를 도입했습니다. 이 네트워크는 이전 단계의 재구성 이미지를 조건으로 반복적으로 수정하는 방식으로 훈련됩니다. 둘째, 디퓨전 디스틸레이션(distillation) 과정을 분석하여 긴 텍스트 프롬프트(prompt)의 속성을 분리해 정밀하게 조작할 수 있음을 보였습니다. 이 방법은 별도의 구현 노력이 필요 없으며 실용적입니다.
- **Performance Highlights**: TurboEdit는 기존의 멀티 스텝 디퓨전 모델보다 훨씬 빠르며 (<0.5초), 텍스트-이미지 정렬 및 배경 보존 면에서 더 나은 성능을 보여줍니다. 인버젼(inversion)은 8번의 기능 평가(NFEs)만 필요하며, 편집은 4번의 NFEs만을 요구합니다. 이는 기존 방법들이 인버젼에 50번 NFEs, 편집에 30-50번 NFEs를 요구하는 것과 비교해 매우 효율적입니다.

### [Fine-tuning Large Language Models with Human-inspired Learning Strategies in Medical Question Answering](https://arxiv.org/abs/2408.07888)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.07888.png)

Vote: 3

Authors: Andrew M. Bean, Adam Mahdi, Yushi Yang, Robert McCraith

- **What's New**: 이번 연구는 의료 질문 응답 분야에서 대규모 언어 모델(Large Language Models, LLMs)을 효율적으로 훈련시키기 위한 인간 학습 기반 전략(human-inspired learning strategies)을 평가합니다. 기존 연구들은 주로 커리큘럼 학습(curriculum learning)을 단일 모델에 적용한 반면, 이번 연구는 네 가지 크기와 아키텍처가 다른 LLM과 다양한 데이터 레이블링 시나리오를 통해 범용성을 조사합니다.
- **Technical Details**: 연구는 다섯 가지 인간 학습 기반 전략을 랜덤 셔플드(Random Shuffled) 기준과 비교하여 네 가지 LLM(예: 모델)을 24개의 세밀한 모델로 훈련하고, 세 가지 다른 의료 데이터세트에 대해 평가했습니다. 데이터 레이블링은 기계 생성 레이블과 인간 생성 레이블 두 가지 방식으로 이루어졌습니다. 연구 결과 모델 생성 데이터 레이블이 커리큘럼 학습 성능을 높일 수 있음을 확인했습니다.
- **Performance Highlights**: 기존 연구와 달리 다양한 학습 전략과 모델 조합에 따른 성능 변화를 종합적으로 평가했습니다. 특히, 모델이 정의한 질문 난이도가 인간 정의 난이도보다 커리큘럼 학습 성능을 향상시켰습니다. 블록드 커리큘럼(Blocked Curriculum)과 스파이럴 커리큘럼(Spiral Curriculum) 전략이 더 나은 일반화를 보여주었습니다.

### [D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning](https://arxiv.org/abs/2408.08441)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.08441.png)

Vote: 2

Authors: Chelsea Finn, Philippe Hansen-Estruch, Jiajun Wu, Sergey Levine, Aviral Kumar, Kyle Hatch, Ilya Kostrikov, Victor Kolev, Philip Ball, Anikait Singh, Rafael Rafailov, Laura Smith

- **What's New**: 이 논문에서는 현실적인 로봇 조작 및 이동 환경을 시뮬레이션한 새로운 오프라인 강화학습(Offline RL) 벤치마크를 제안합니다. 이 벤치마크는 Franka 로봇 팔과 A1 로봇을 포함한 실제 로봇 시스템 모델을 기반으로 하며, 인간 운영자에 의해 수집된 여러 데이터 소스 및 스크립트 데이터를 포함합니다. 오프라인 RL의 성능 평가 및 온라인 미세조정(online finetuning)에 필요한 다양한 과제를 다룹니다.
- **Technical Details**: 새로운 벤치마크는 상태 기반(state-based) 및 이미지 기반(image-based) 도메인을 모두 포함하며, 사전 교육(pre-training)과 미세조정(finetuning)을 모두 필요로 하는 작업을 포함합니다. A1 퀘드러펫(Quadruped)과 Franka 로봇 팔과 같은 실제 로봇 플랫폼의 시뮬레이션 모델을 사용하여 벤치마크를 설계하였으며, 다양한 요소를 결합하여 과제를 디자인했습니다. 예를 들어, 장기간의 수평선과 보상의 희소성(sparity), 데이터의 탐색 범위 등을 포함합니다.
- **Performance Highlights**: 현재 RL 알고리즘은 낮은 차원의 작업(예: 이동성)에서는 성능이 좋지만, 더 복잡한 로봇 시나리오에서는 신뢰할 수 없다는 것을 실험적으로 보여줍니다. 특히 복잡한 모션을 포함하거나 여러 목표를 연결하는 작업에서는 단순한 모방 학습(Behavior Cloning, BC) 접근 방식보다 성능이 떨어지는 것으로 나타났습니다. 모든 실험은 단일 GPU에서 12시간 이내에 완료될 수 있도록 설계되어 접근성과 속도를 유지합니다.

