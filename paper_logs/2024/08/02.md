## Daily Papers (2024-08-02)

### [SAM 2: Segment Anything in Images and Videos](https://arxiv.org/abs/2408.00714)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00714.png)

Vote: 39

Authors: Tengyu Ma, Junting Pan, Chaitanya Ryali, Haitham Khedr, Ross Girshick, Valentin Gabeur, Ronghang Hu, Christoph Feichtenhofer, Yuan-Ting Hu, Nikhila Ravi, Laura Gustafson, Kalyan Vasudev Alwala, Nicolas Carion, Piotr Dollár, Roman Rädle, Chao-Yuan Wu, Chloe Rolland, Eric Mintun

- **What's New**: Segment Anything Model 2 (SAM 2)는 이미지와 비디오를 모두 처리할 수 있는 통합된 segmentation 모델로 도입되었습니다. SAM 2는 비디오 내 객체의 시공간적 경계를 예측하기 위해 Promptable Visual Segmentation (PVS) 작업에 초점을 맞추고 있으며, 이는 이미지 segmentation 작업을 비디오 도메인으로 일반화한 것입니다. 새로운 Segment Anything Video (SA-V) 데이터셋은 사용자를 통해 상호작용적으로 새로운 데이터를 주석하는 데이터 엔진을 사용해 생성되었습니다.
- **Technical Details**: SAM 2는 객체와 이전 상호작용에 대한 정보를 메모리에 저장하여 비디오 전반에 걸쳐 'masklet' 예측을 생성합니다. 스트리밍 아키텍처는 단일 이미지와 비디오 프레임을 하나씩 처리하는데, 메모리 주의 모듈(memory attention module)을 통해 대상 객체의 이전 메모리를 참조합니다. 데이터 엔진은 주석자와 상호작용하여 비디오 데이터에 대해 새로운 도전적인 데이터를 생성합니다. SA-V 데이터셋은 총 5,090개의 비디오와 3,550만 개의 마스크를 포함하고 있습니다.
- **Performance Highlights**: SAM 2는 반복적 상호작용을 통해 이전 접근법보다 3배 적은 상호작용으로 더 나은 segmentation 정확도를 달성합니다. 또한 여러 비디오 객체 segmentation 벤치마크 및 이미지 segmentation 벤치마크에서도 우수한 성능을 보입니다. SAM 2는 이미지 세그멘테이션에서는 기존 SAM보다 6배 빠른 속도를 자랑합니다. 여러 제로샷 테스트에서도 뛰어난 성능을 보였으며, SAM 2 및 SA-V 데이터셋은 오픈 라이센스 하에 공개됩니다.

### [Coarse Correspondence Elicit 3D Spacetime Understanding in Multimodal Language Model](https://arxiv.org/abs/2408.00754)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00754.png)

Vote: 17

Authors: Yansong Tang, Wei-Chiu Ma, Yongming Rao, Yiqin Wang, Yuhao Dong, Benlin Liu, Ranjay Krishna

- **What's New**: 새로운 연구로, 우리는 'Coarse Correspondences'라는 시각적 프롬팅 방법을 소개했습니다. 이 방법은 다중모달 큰 언어 모델(Multimodal LLMs, MLLMs)의 3D와 시간적 이해 능력을 강화하기 위해 설계되었습니다.
- **Technical Details**: Coarse Correspondences는 처음에 가벼운 트래킹 모델을 사용하여 비디오의 고해상도 프레임에서 동일 인스턴스의 물체를 추적합니다. 그 후, 비디오를 희소화하여 몇 개의 프레임만 MLLM에 전달합니다. 마지막으로, 가장 많이 나타난 최상위 k개의 물체를 시각화하고, 이를 독특한 ID로 강조 표시합니다. 수정된 이미지들과 질문을 함께 MLLM에 전달합니다.
- **Performance Highlights**: 우리의 프레임워크는 ScanQA와 OpenEQA 같은 3D 이해 벤치마크에서 주목할 만한 성능 향상을 보였으며, 긴 비디오 이해 벤치마크인 EgoSchema에서도 우수한 성과를 기록했습니다. 특히, GPT-4V를 사용하여 적은 수의 뷰로도 제로 샷 방식에서 특별히 설계되고 미세 조정된 방법을 능가했습니다. 또한, GPT-4O와 함께 사용하면 매우 희소한 뷰로부터 3D 공간 이해를 가능하게 하여 계산 비용과 MLMM의 비용을 줄이는 데 기여합니다.

### [Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/abs/2408.00118)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00118.png)

Vote: 15

Authors: Nino Vieillard, Piotr Stanczyk, Cassidy Hardin, Charline Le Lan, Michelle Casbon, Surya Bhupatiraju, Thomas Mesnard, Shreya Pathak, Anton Tsitsulin, Abe Friesen, Alexandre Ramé, Morgane Riviere, Pouya Tafti, Sammy Jerome, Johan Ferret, Pier Giuseppe Sessa, Sabela Ramos, +, Bobak Shahriari, Léonard Hussenot, Ravin Kumar, Gemma Team, Peter Liu

- **What's New**: 이번 연구에서는 작은 모델의 성능을 높이기 위한 대안으로 앙상블 지식 증류(knowledge distillation) 기법을 사용하여 훈련 길이를 단순히 증가시키는 대신 각 훈련 단계에서 네트워크가 받는 정보의 품질을 높였습니다.
- **Technical Details**: Gemma 2 모델은 Transformer 아키텍처(Vaswani et al., 2017)에 기반하며, 감마 모델의 이전 버전과 유사한 요소들(콘텍스트 길이 8192 토큰, Rotary Position Embeddings(Su et al., 2021), approximated GeGLU non-linearity(Shazeer, 2020))을 채택했습니다. 지역 슬라이딩 윈도우 및 글로벌 어텐션 방식을 교대로 사용하며, logit soft-capping과 RMSNorm(Zhang and Sennrich, 2019)을 적용했습니다. Pre-training 과정에서는 13조 토큰을 사용하여 27B 모델을 훈련하였고, TPUv4, v5e 및 v5p 하드웨어를 사용했습니다.
- **Performance Highlights**: Gemma 2는 다양한 자동화된 벤치마크와 인간 평가에서 기존 동급 모델을 능가하며, 질문 응답(Clark et al., 2019), 상식적 사고(Sakaguchi et al., 2019), 수학 및 과학(Cobbe et al., 2021), 코딩(Austin et al., 2021)과 같은 도메인에서 경쟁력 있는 성과를 보였습니다.

### [SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement](https://arxiv.org/abs/2408.00653)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00653.png)

Vote: 10

Authors: Zixuan Huang, Varun Jampani, Aaryaman Vasishta, Mark Boss

- **What's New**: 이번 연구에서는 aliasing artifact를 줄이기 위해 트랜스포머 백본(backbone)을 업그레이드하여 해상도가 384×384인 트리플레인(triplane)을 생성할 수 있게 되었습니다.
- **Technical Details**: 기존의 TripoSR에서 단순히 트리플레인 토큰을 증가시키는 것은 self-attention의 이차 복잡성 때문에 계산적으로 너무 부담스럽습니다. 이를 해결하기 위해 PointInfinity의 개념을 차용하여 트리플레인 스트림과 잠재 스트림으로 구성된 두 개의 스트림 트랜스포머를 활용했습니다. 잠재 스트림은 cross attention을 통해 트리플레인 스트림에서 정보를 가져오고, 일정 크기의 잠재 토큰에 대해 주요 계산을 수행한 후 트리플레인 토큰을 업데이트합니다. 본 아키텍처는 이러한 두 개의 스트림 유닛이 네 개로 구성되어 있습니다.
- **Performance Highlights**: 이런 분리된 설계 덕분에 트랜스포머는 96×96 해상도에 1024 채널의 트리플레인을 생성할 수 있습니다. 또한, 해상도를 더 높이고 aliasing을 줄이기 위해 픽셀 셔플(pixel shuffling) 작업을 통합하여 트리플레인 해상도를 384×384로 향상시켰으며, 피처 차원은 40으로 설정했습니다.

### [OmniParser for Pure Vision Based GUI Agent](https://arxiv.org/abs/2408.00203)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00203.png)

Vote: 9

Authors: Yelong Shen, Ahmed Awadallah, Yadong Lu, Jianwei Yang

- **What's New**: 대형 언어 모델(large language models)이 그들의 이해 및 추론 기능에서 큰 성공을 거두었습니다. 최근 연구들은 사용자인터페이스(UI)에서 복잡한 작업을 수행하기 위해 대형 비전-언어 모델(vision-language models, VLMs)을 에이전트로 사용하는 것을 탐구하고 있습니다. 이 논문에서는 다양한 운영 체제 및 응용 프로그램에서 작동할 수 있는 일반적인 접근 방식을 목표로 하는 OmniParser를 제안합니다. OmniParser는 UI 스크린샷에서 정보를 구조적인 바운딩 박스와 레이블로 추출하여 GPT-4V의 작업 예측 성능을 향상시킵니다.
- **Technical Details**: OmniParser는 인터랙티브 아이콘 검출 모델, 설명 모델(icon description model) 및 OCR 모듈의 출력을 통합하여 UI의 구조적 DOM-유사 표현과 상호작용할 수 있는 요소들의 스크린샷을 생성합니다. 이 접근 방식은 GPT-4V가 특정 xy 좌표를 예측하는 대신, 바운딩 박스를 사용해 작업을 지시할 수 있도록 합니다. 또한, 최신 디자인의 아이콘과 버튼이 포함된 새로운 데이터셋을 큐레이션하였습니다. 이는 DOM 트리에서 바운딩 박스를 가져와 생성된 것입니다.
- **Performance Highlights**: OmniParser는 ScreenSpot, Mind2Web, AITW 벤치마크에서 평가되었으며, 추가적인 입력 없이 스크린샷만으로 GPT-4V 원본 기준보다 성능이 크게 향상되었습니다. OmniParser의 비전 기반 화면 파싱 기술은 화면 이해와 작업 예측의 정확성을 향상시키는 것으로 나타났습니다.

### [Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning](https://arxiv.org/abs/2408.00690)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00690.png)

Vote: 7

Authors: Zhicheng Lee, Trapoom Ukarapol, Amy Xin

- **What's New**: 이 연구는 소규모 언어 모델의 텍스트 임베딩 성능을 개선하는 것을 목표로 합니다. 특히, MiniCPM 모델을 중심으로 향상된 임베딩 품질을 달성하여 자원이 제한된 환경에서도 효율적으로 활용될 수 있도록 합니다.
- **Technical Details**: 텍스트 임베딩은 텍스트 데이터를 벡터 형태로 표현하여 머신이 자연어를 이해하고 처리할 수 있도록 합니다. 대규모 언어 모델(LLMs)은 자연어 이해에서 높은 성능을 보이지만, 소규모 모델은 자원 소모가 적으면서도 성능이 낮은 경향이 있습니다. 이를 개선하기 위해 Contrastive Fine-Tuning과 LoRA(Low-Rank Adaptation)를 사용하여 소규모 모델의 임베딩 성능을 높였습니다. 실험에는 Gemma, Phi-2, MiniCPM 모델이 포함되었습니다.
- **Performance Highlights**: 실험 결과, MiniCPM을 포함한 소규모 모델이 Contrastive Fine-Tuning과 LoRA를 통해 텍스트 임베딩 품질에서 유의미한 개선을 보였습니다. 또한, MTEB 벤치마크를 사용하여 이러한 향상을 다양한 도메인에서 일반화할 수 있음을 확인했습니다.

### [Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual Inversion](https://arxiv.org/abs/2408.00458)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00458.png)

Vote: 6

Authors: Manuel Kansy, Markus Gross, Jacek Naruniec, Romann M. Weber, Christopher Schroers

- **What's New**: 최근 arXiv에 게재된 논문에서는 새로운 AI 모델을 제안하고 있습니다. 이 모델은 인간처럼 학습하고 추론하는 능력을 갖추고자 하였습니다. 특히, 이 모델은 다양한 데이터 소스에서 유의미한 패턴을 학습할 수 있도록 설계되었습니다.
- **Technical Details**: 이 모델은 Transformer Architecture를 기반으로 하였으며, Multi-Head Attention 메커니즘을 통해 다양한 데이터 포인트(Data points) 간의 관계를 효과적으로 학습할 수 있습니다. 또한, Position Encoding, Dropout, 및 Layer Normalization과 같은 최적화 기법들이 포함되었습니다. 모델의 학습은 대규모 데이터셋(dataset)을 활용하여 이루어졌습니다.
- **Performance Highlights**: 이 새로운 모델은 기존의 state-of-the-art 모델들에 비해 우수한 성능을 보였습니다. 여러 벤치마크 테스트(benchmark tests)에서 높은 정확도(accuracy)와 빠른 처리 속도(inference speed)를 입증하였습니다. 특히, 텍스트 분류(text classification), 이미지 인식(image recognition) 및 자연어 처리(Natural Language Processing) 분야에서 뛰어난 결과를 보여주었습니다.

### [MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities](https://arxiv.org/abs/2408.00765)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00765.png)

Vote: 6

Authors: Chung-Ching Lin, Lijuan Wang, Zhengyuan Yang, Zicheng Liu, Weihao Yu, Xinchao Wang, Linfeng Ren, Kevin Lin, Linjie Li, Jianfeng Wang

- **What's New**: MM-Vet v2가 도입되었습니다. 이 새로운 벤치마크는 '이미지-텍스트 시퀀스 이해(Understanding)'라는 신규 핵심 능력을 추가하여, 크고 복잡한 다중모달 모델(LMM, Large Multimodal Models)의 평가를 더욱 정교하게 수행할 수 있게 되었습니다.
- **Technical Details**: 기존의 MM-Vet는 인식(Recognition), 지식(Knowledge), OCR, 공간 인식(Spatial Awareness), 언어 생성(Language Generation), 수학(Math) 등 6가지 핵심 능력을 평가했으나, MM-Vet v2는 이미지와 텍스트가 혼합된 시퀀스를 처리하는 능력을 포함하는 7번째 핵심 능력을 추가합니다. 이를 위해 517개의 고품질 질문을 설계하였으며, 이 중 218개는 기존 MM-Vet에서 가져온 것입니다.
- **Performance Highlights**: MM-Vet v2 벤치마크에서 Claude 3.5 Sonnet 모델이 71.8점으로 가장 높은 점수를 기록했습니다. GPT-4o는 71.0점으로 근소한 차이로 2위를 차지했고, 오픈 소스 모델 중에서는 InternVL2-Llama3-76B가 68.4점으로 우수한 성과를 보였습니다.

### [Finch: Prompt-guided Key-Value Cache Compression](https://arxiv.org/abs/2408.00167)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00167.png)

Vote: 5

Authors: Paolo Papotti, Giulio Corallo

- **What's New**: Finch는 LLMs의 KV 캐시 메모리 소비를 줄이기 위한 혁신적인 접근 방식을 제안하고 있습니다. 이는 리트레이닝이나 파인튜닝 없이 효율적인 텍스트 생성을 가능하게 하는 새로운 방법입니다.
- **Technical Details**: Finch는 기존의 토큰 재계산을 우회하는 KV 캐시 메커니즘을 사용하지만, 압축된 KV 벡터를 사용함으로써 메모리 소비를 효과적으로 줄입니다. 또한, Prefill 단계에서 어텐션 정보를 이용하여 가장 관련성 높은 KV 쌍을 식별하고, 이들을 KV 캐시에 저장합니다. 압축률은 모델 컨텍스트 크기에 따라 조정이 가능하며, 학습이나 요약 모듈 없이 플러그 앤 플레이 방식으로 사용할 수 있습니다.
- **Performance Highlights**: Finch는 SQuAD v2 벤치마크에서 원본 LLM과 비교하여 2.35배 압축시 유사한 생성 품질을 유지하며, 3.76배 압축시에도 90%의 정확도를 유지합니다. LongBench 테스트에서 Finch는 2배에서 93배 압축범위를 달성하며, 대부분의 작업에서 첨단 압축 방법보다 우수한 품질 점수를 기록했습니다. 특히, 일부 작업에서는 풀 컨텍스트를 사용하는 LLM보다도 뛰어난 성능을 보였습니다.

### [UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified Model](https://arxiv.org/abs/2408.00762)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00762.png)

Vote: 5

Authors: Zhiqian Lin, Jiaqi Li, Xiangyu Fan, Lei Yang, Weiye Xiao

- **What's New**: 이 논문은 여러 데이터셋의 불일치한 데이터 주석을 통합하여 학습할 수 있는 다중-헤드 모델 'UniTalker'를 제안했습니다. UniTalker는 다양한 데이터셋과 주석 유형으로부터 학습해 보다 강력한 3D 얼굴 애니메이션을 구현할 수 있습니다.
- **Technical Details**: UniTalker 모델은 Principal Component Analysis(PCA)를 사용하여 Vertex 기반 주석의 차원을 줄여 여러 모션 디코더 헤드 간의 학습 매개변수를 균형 있게 합니다. 또한, 클래스-프리 가이던스(classifier-free guidance)를 참고하여 피벗 아이덴티티 임베딩(pivot identity embedding)을 도입해 서로 다른 모션 디코더 헤드 간의 편향을 줄입니다. 모델 학습에는 5개의 기존 데이터셋과 새로 조성한 3개의 데이터셋을 결합하여 총 18.53시간 분량의 8,654 시퀀스를 포함한 A2F-Bench를 사용했습니다.
- **Performance Highlights**: UniTalker는 BIWI와 Vocaset 데이터셋에서 이전 최첨단 모델들보다 낮은 립 버텍스 에러(LVE)를 기록했습니다. BIWI에서 4.25e-4에서 3.86e-4로, Vocaset에서 9.63e-6에서 8.30e-6으로 성능이 향상되었습니다. 데이터셋-특정 파인튜닝을 통해 A2F-Bench에서 평균 6.3%의 에러 감소를 달성했습니다.

### [TurboEdit: Text-Based Image Editing Using Few-Step Diffusion Models](https://arxiv.org/abs/2408.00735)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00735.png)

Vote: 5

Authors: Rinon Gal, Gilad Deutch, Daniel Garibi, Daniel Cohen-Or, Or Patashnik

- **What's New**: 대규모 텍스트-이미지 확산 모델(Diffusion Models)의 뛰어난 표현력 덕분에 사용자가 자연 언어 명령을 통해 기존 이미지를 수정할 수 있는 텍스트 기반 편집 프레임워크가 늘어나고 있습니다. 그러나 기존의 방법들은 여러 단계에 걸친 확산 과정(Diffusion Process)에 의존하며, 여기서 발생하는 통계적 변동으로 인해 편집 작업에 부정적인 영향이 발생할 수 있습니다. 최신 모형 증류 방법(Model Distillation Methods)을 통해 이러한 문제를 해결하기 위한 몇 가지 새로운 기법이 개발되었습니다.
- **Technical Details**: 본 연구에서는 DDPM 노이즈 반전 프레임워크(DDPM Noise-Inversion Framework)를 사용하여 이미지를 편집하는 방법을 제안합니다. 노이즈 맵(Noise Maps)을 사전에 계산하여 이를 확산 과정의 역방향 단계에서 사용함으로써 원래 이미지를 재구성할 수 있습니다. 이는 간단한 텍스트 설명 변경만으로 편집이 가능하며 동일한 노이즈 맵을 사용할 수 있다는 장점이 있습니다. 더 나아가, 'Shifted Denoising Schedule'를 도입하여 노이즈 제거 과정에서 발생하는 시프트(shifts)를 보정할 수 있습니다.
- **Performance Highlights**: 텍스트 기반 이미지 편집을 통해 기존 작업 대비 최대 500배 속도를 높이면서도, 결과물의 품질을 보존하고 심지어 개선할 수 있는 방법을 소개합니다. 본 연구는 다양한 실험을 통해 이러한 성능 향상을 입증하였으며, 코드 또한 공개될 예정입니다.

### [Tails Tell Tales: Chapter-Wide Manga Transcriptions with Character Names](https://arxiv.org/abs/2408.00298)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00298.png)

Vote: 4

Authors: Gyungin Shin, Ragav Sachdeva, Andrew Zisserman

- **What's New**: 최근 컴퓨터 비전과 머신러닝의 진보를 통해 시각 장애인을 위한 만화 접근성을 향상시키는 연구가 진행되고 있습니다. 신형 모델 Magiv2는 기존 모델 Magi의 한계를 극복하여 장별 일관된 캐릭터 이름과 대화 속성을 제공할 수 있도록 개선되었습니다.
- **Technical Details**: Magiv2는 고해상도 만화 페이지를 처리하여 캐릭터, 텍스트, 패널을 탐지하고 캐릭터 클러스터를 예측하는 모델입니다. 주요 기술적 특징으로는 캐릭터 이름과 이미지를 포함한 캐릭터 뱅크(character bank)를 사용하여 일관된 이름 제공, 대화 속성 개선을 위한 speech-bubble tail 인식, 텍스트 분류를 위한 가벼운 텍스트-분류 헤드(head)를 포함합니다.
- **Performance Highlights**: Magiv2는 읽기 향상을 위해 챕터 전체의 일관된 캐릭터 이름을 생성하며, 이미지와 이름을 포함한 새로운 캐릭터 뱅크 데이터를 통해 76개 만화 시리즈, 11K 이상의 주 캐릭터 및 11.5K 예시 이미지로 구성된 데이터셋을 제공합니다. 이를 통해 10,000개 이상의 만화 챕터를 직접 전사할 수 있습니다.

### [Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention](https://arxiv.org/abs/2408.00760)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00760.png)

Vote: 2

Authors: Susung Hong

- **What's New**: 이 논문에서는 Smoothed Energy Guidance (SEG)라 불리는 새로운 무조건 이미지 생성 방식이 제안되었습니다. SEG는 기존 방법들과 달리 헤어리스틱에 의존하지 않고, 에너지 기반의 접근법을 통해 자기 주의 메커니즘의 에너지를 직접적으로 조정하여 이미지 품질을 향상시킵니다.
- **Technical Details**: 특히, SEG는 자기 주의 메커니즘(self-attention mechanism)의 에너지 정의에서 출발해 Gaussian 커널의 파라미터를 조정하여 에너지 커브쳐를 줄이는 방법을 채택합니다. 이러한 접근 방식은 기존 방법에서 발생하는 부작용, 예를 들면 이미지 세부사항의 평활화, 색상 변화 등을 줄입니다.
- **Performance Highlights**: SEG는 다양한 실험에서 텍스트 조건부 및 무조건부 이미지 생성 모두에서 뛰어난 성능을 보였으며, ControlNet을 사용한 실험에서도 높은 품질의 샘플을 생성함을 확인하였습니다. 이전 방법들에 비해 구조적 변화가 덜 발생하면서도 더 나은 샘플 품질을 달성하였습니다.

### [Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian Rebuses](https://arxiv.org/abs/2408.00584)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00584.png)

Vote: 2

Authors: Malvina Nissim, Arianna Bisazza, Tommaso Caselli, Gabriele Sarti

- **What's New**: 이 연구는 최초로 거대한 언어 모델(LLMs)의 이미지 기반 언어 게임인 '이탈리아어 Rebuses'를 해결하는 능력을 평가했습니다. 이는 복잡한 멀티스텝 추론 작업을 위한 새로운 평가 기준을 제시합니다. 연구진은 8만 개 이상의 텍스트-only rebuses를 생성하고, 이를 통해 LLM의 성능을 평가하였습니다.
- **Technical Details**: 이 연구에서는 이탈리아어 퍼즐 저장소인 'Eureka5'에서 모든 rebus의 첫 번째 구절과 해결책을 추출하여, 이를 기반으로 대규모의 텍스트-only rebuses 데이터셋을 만들었습니다. 이 과정에서 피덕팅 알고리즘과 체인 오브 소트(Chain-of-Thought) 추론을 포함한 기술을 사용하여 모델의 추론 능력을 강화했습니다. 또한, Phi-3 Mini 3.8B 모델을 미세 조정(QLoRA)하여 성능을 개선했습니다.
- **Performance Highlights**: 평가 결과, GPT-4o, Claude-3.5 Sonnet 등의 최신 상태의 독점 LLM 시스템은 이 과제에서 상대적으로 낮은 성능을 보였으나, 미세 조정된 Phi-3 Mini 모델은 이탈리아어 rebus 해결에서 51%의 정확도를 기록하며 뛰어난 성능을 보여주었습니다. 이는 기존 시스템 대비 크게 향상된 결과입니다.

### [Sentence-wise Speech Summarization: Task, Datasets, and End-to-End Modeling with LM Knowledge Distillation](https://arxiv.org/abs/2408.00205)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.00205.png)

Vote: 2

Authors: Takafumi Moriya, Marc Delcroix, Masato Mimura, Kohei Matsuura, Takatomo Kano, Atsunori Ogawa, Takanori Ashihara

- **What's New**: 최근 수십 년간 자동 음성 인식(ASR)은 상당한 발전을 이루어왔으며, 주로 단순한 텍스트 전사에 초점을 맞추고 있습니다. 하지만 ASR의 전사 결과는 구어체와 중복 표현 때문에 사람들에게 읽기 어려울 수 있습니다. 반면, 음성 요약(SSum)은 구어 문서를 간결하고 읽기 쉬운 서면 스타일로 요약하여 정보를 쉽게 소화할 수 있게 합니다. 그러나 SSum은 일반적으로 전체 음성 문서를 한 번에 처리해야 하기 때문에 실시간 애플리케이션에 적합하지 않습니다. 이에 대응하기 위해, 우리는 문장 단위 음성 요약(Sen-SSum) 방법을 제안합니다.
- **Technical Details**: Sen-SSum은 ASR과 SSum의 간극을 메우기 위한 기술로, 관련 기술인 비유창성 감지 및 제거(disfluency detection and removal)를 넘어 더 간결하고 명확한 출력을 제공합니다. Sen-SSum은 각 음성 문장이 끝날 때마다 요약을 생성하므로, 사용자는 회의나 강의가 끝날 때까지 기다릴 필요 없이 즉시 요약된 정보를 접할 수 있습니다. 우리는 Mega-SSum이라는 새 데이터를 소개하며, 이 데이터는 Gigaword 데이터셋을 기반으로 하여 3.8백만 개의 영문 음성, 전사, 요약 트리플릿을 포함합니다. 또한, 내부 일본어 Sen-SSum 코퍼스인 CSJ-SSum도 사용하여 실험의 타당성을 높였습니다. 우리는 Sen-SSum을 위해 두 가지 접근법을 조사했습니다: 1) 캐스케이드 모델과 2) 엔드 투 엔드(E2E) 모델.
- **Performance Highlights**: 캐스케이드 모델은 ASR과 텍스트 요약모델(TSum)을 결합하여 높은 품질의 요약을 생성할 수 있습니다. 반면, E2E 모델은 입력된 음성으로부터 직접 텍스트 요약을 생성하는 단일 인코더-디코더 모델을 사용하며, 이는 매개변수 효율성과 잠재적으로 빠른 디코딩 능력이 있습니다. 하지만 E2E 모델은 대규모 학습 데이터가 필요하며, 이러한 데이터를 확보하기 어려운 점이 있습니다. 이를 해결하기 위해, 우리는 엔드 투 엔드(E2E) 모델을 위한 지식 증류(knowledge distillation)를 제안합니다. 실험 결과, 이 방법은 두 데이터셋 모두에서 E2E 모델의 성능을 크게 개선하였으며, 특정 조건에서는 인공 요약이 수동 요약보다 더 나은 요약 정확도를 제공하는 것으로 나타났습니다.

### [Enhancing Semantic Similarity Understanding in Arabic NLP with Nested Embedding Learning](https://arxiv.org/abs/2407.21139)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21139.png)

Vote: 1

Authors: Anis Koubaa, Omer Nacar

- **What's New**: Nested Embedding Models 기술을 활용한 아랍어 자연어 처리(NLP)를 위한 최첨단 텍스트 임베딩 모델을 개발했습니다. Matryoshka Representation Learning(MRL)은 입력 텍스트를 다양한 차원으로 표현하면서 정보 손실을 최소화하는 새로운 접근법입니다. 이는 특히 웹 스케일에서 이식성과 효율성을 높이는 데 중점을 둡니다.
- **Technical Details**: Matryoshka Representation Learning(MRL)은 '마트료시카 인형'의 구조를 차용해, 초기 차원에 중요한 정보를 더 많이 저장하고 나중 차원에 덜 중요한 정보를 저장합니다. 이를 통해 원래의 큰 임베딩을 잘라내도 충분한 정보를 유지할 수 있으며, 다양한 차원에서 유효한 임베딩을 생성할 수 있습니다. 아랍어를 위한 Matryoshka 임베딩 모델을 개발하고, 영어와 아랍어의 대표 모델을 Matryoshka 버전으로 변환하여 이들의 적응성과 성능을 향상시켰습니다.
- **Performance Highlights**: 이 연구는 아랍어 자연어 추론(NLI)을 위한 중요한 자원으로서 영어 Stanford Natural Language Inference(SNLI)와 MultiNLI 데이터셋을 Neural Machine Translation(NMT)을 통해 아랍어로 번역해 제공했습니다. 이 과정에서 생성된 데이터셋과 트레이닝된 모델은 더욱 광범위한 연구와 응용을 촉진하기 위해 Hugging Face에서 공개되었습니다. Matryoshka 임베딩 모델은 대규모 분류 및 검색 작업에서 계산 효율성과 정확성을 유지하면서 뛰어난 성능을 보였습니다.

### [Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey](https://arxiv.org/abs/2407.21794)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21794.png)

Vote: 1

Authors: Jingkang Yang, Kiyoharu Aizawa, Ziwei Liu, Yixuan Li, Qing Yu, Jingyang Zhang, Yifei Ming, Toshihiko Yamasaki, Go Irie, Shafiq Joty, Hai Li, Atsuyuki Miyai, Yueqian Lin

- **What's New**: 이번 연구는 Vision Language Models (VLMs)의 등장으로 인해 Out-of-Distribution (OOD) 검출 및 관련 문제의 발전을 체계적으로 조사하고 이를 통합된 프레임워크로 제시합니다. 이전의 일반화된 OOD 검출 프레임워크를 발전시킨 generalized OOD detection v2를 소개합니다. 이 프레임워크는 Vision Language Model 시대의 다섯 가지 문제(Outlier Detection, Anomaly Detection, Novelty Detection, Open Set Recognition, 그리고 OOD Detection)의 진화를 요약합니다.
- **Technical Details**: 일반화된 OOD 검출 프레임워크 v2는 다음 네 가지 기준에 따라 다섯 가지 문제를 분류합니다: (1) 분포 변화 탐지(공변량 변이 또는 의미적 변이), (2) in-distribution (ID) 데이터 유형(단일 클래스 또는 다중 클래스), (3) ID 데이터의 분류 필요 여부, (4) transductive vs inductive 학습. CLIP을 주요 VLM으로 사용하여 OOD 검출을 조사하며, 이를 CLIP-based OOD detection이라 부릅니다. 나머지 서브 태스크들도 'CLIP-based' 접두사를 붙여 설명합니다.
- **Performance Highlights**: 연구는 VLM 시대에서 각 문제의 진화 과정을 체계적으로 탐구하며, 특히 CLIP을 사용한 OOD 검출이 주된 관심사임을 밝히고 있습니다. 또한, VLM 시대의 패러다임 전환이 AD 및 OOD 검출 분야의 활성 상태와 통합 과정을 강조하며, 이를 통해 각 연구 커뮤니티가 협력할 수 있는 기회를 제시합니다.

