## Daily Papers (2024-08-09)

### [Transformer Explainer: Interactive Learning of Text-Generative Models](https://arxiv.org/abs/2408.04619)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.04619.png)

Vote: 56

Authors: Benjamin Hoover, Alec Helbling, Seongmin Lee, Aeree Cho, Grace C. Kim, Duen Horng Chau, Alexander Karpekov, Zijie J. Wang

- **What's New**: 이번 연구에서는 Transformer Explainer라는 오픈 소스 웹 기반 인터랙티브 시각화 도구를 소개합니다. 이 도구는 비전문가들이 Transformer 모델의 고수준 구조와 저수준 수학 연산을 쉽게 학습할 수 있도록 설계되었습니다. 특히 텍스트 생성 응용을 통해 Transformer의 작동 방식을 설명하며, Sankey 다이어그램을 활용하여 입력 데이터가 모델의 구성 요소를 통해 '흐르는' 방식을 시각적으로 강조합니다.
- **Technical Details**: Transformer Explainer는 Svelte와 D3를 사용하여 프론트엔드에서 인터랙티브한 시각화를 구현하고, 백엔드에서는 ONNX runtime과 HuggingFace’s Transformers 라이브러리를 사용하여 GPT-2 모델을 브라우저에서 실행합니다. 사용자 제공 텍스트 입력을 받아 임베딩하고, 다층 Transformer 블록을 통해 처리한 후 가장 가능성 높은 다음 토큰 예측 결과를 순위화하는 전체 파이프라인을 시각화합니다. 주요 설계 원칙으로는 '멀티레벨 추상화를 통한 복잡성 감소'와 '인터랙티브 요소를 통한 이해 및 참여 증진'이 있습니다.
- **Performance Highlights**: Transformer Explainer는 실시간 추론을 통합하여 사용자가 자신의 입력 텍스트를 인터랙티브하게 실험해 볼 수 있도록 합니다. 이 도구는 고급 컴퓨팅 자원이나 설치 없이도 학생들이 현대 생성 AI 기술을 교육적 목적으로 접근할 수 있게 합니다. 또한, 온도(Temperature) 파라미터를 조정하여 모델의 예측 디터미니즘(determinism)을 시각적으로 제어할 수 있어 학습자들의 이해도를 높입니다. 도구는 웹 브라우저에서 직접 실행되므로 소프트웨어 설치나 특수 하드웨어 관리에 대한 걱정 없이 사용할 수 있습니다.

### [GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI](https://arxiv.org/abs/2408.03361)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.03361.png)

Vote: 55

Authors: Pengcheng Chen, Jianfei Cai, Yu Qiao, Shaoting Zhang, Jin Ye, Tianbin Li, Guoan Wang, Zhongying Deng, Ziyan Huang, Junjun He, Bin Fu, Bohan Zhuang, Yanjun Li, Wei Li, Haodong Duan, Eric J Seibel, Benyou Wang, Yanzhou Su

- **What's New**: 최근 의료기관 내 질병 진단과 치료에 대한 다양한 요구를 충족시키기 위해, 범용 의료 AI가 다양한 의료 과제를 해결하기 위한 일반 목적의 의료 모델을 제공하고 있습니다. 이러한 모델은 주로 다양한 데이터 유형(예: 영상 및 임상 텍스트)에 대한 Large Vision-Language Models (LVLMs)를 사용하여 훈련되며, 질병 진단 및 중증도 등급 매기기 같은 다양한 과제를 해결합니다. 하지만 현재까지 이러한 LVLMs가 실제 임상 시나리오에서 얼마나 다양한 요구를 수용할 수 있는지는 명확하지 않습니다.
- **Technical Details**: 이를 해결하기 위해 GMAI-MMBench라는 새로운 종합 멀티모달 벤치마크를 도입했습니다. GMAI-MMBench는 전 세계에서 수집된 285개의 고품질 데이터셋을 바탕으로 구축되었으며, 39가지의 다른 이미지 모달리티(모달리티)를 포함한 약 26,000개의 케이스로 구성되었습니다. 이 벤치마크는 다양한 모달리티, 임상 VQA(Visual Question Answering) 과제, 임상 부서, 그리고 다양한 지각적 세분성을 평가하는 데 사용됩니다.
- **Performance Highlights**: ['GMAI-MMBench는 임상 실습에서 상당한 도전과제를 제시합니다. 최고 성능을 보이는 GPT-4o조차도 52.24%의 정확도만을 달성했습니다.', '오픈소스 LVLMs, 예를 들어 MedDr와 DeepSeek-VL-7B는 약 41%의 정확도를 달성하여 상업용 모델과 비교해 경쟁력이 높습니다.', '대부분의 의료 전용 모델은 일반 LVLMs가 달성한 중간 성능 수준(약 30% 정확도)에 도달하는 데 어려움을 겪고 있으며, 가장 성능이 좋은 모델은 MedDr입니다.', '대부분의 LVLMs는 다양한 임상 VQA 과제, 부서 및 지각적 세분성에서 불균형한 성능을 보입니다.', '성능 병목의 주요 요인은 지각 오류, 의료 도메인 지식의 부족, 관련 없는 응답, 안전 프로토콜로 인한 응답 거부 등이 있습니다.']

### [Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from User's Casual Sketches](https://arxiv.org/abs/2408.04567)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.04567.png)

Vote: 13

Authors: Hongdong Li, Yongzhi Xu, Inkyu Sa, Pan Ji, Yifu Wang, Yunfei Duan, Yonhon Ng, Yang Li

- **What's New**: Sketch2Scene는 사용자가 그린 스케치와 텍스트 프롬프트를 입력받아 현실감 있고 상호작용 가능한 3D 장면을 생성하는 새로운 파이프라인입니다. 이는 기존 방법들이 단일 객체 수준의 3D 자산 생성에 주로 집중했던 것과 대조됩니다.
- **Technical Details**: Sketch2Scene은 사전 훈련된 2D denoising diffusion 모델을 사용해 등각 투영 방식의 2D 이미지를 생성한 뒤, 이를 통해 3D 장면의 개념적 이미지를 생성합니다. 생성된 2D 이미지는 시각적 장면 이해 모듈을 통해 해석되어 지형과 객체 레이아웃 맵을 형성합니다. 이 레이아웃 맵은 절차적 콘텐츠 생성 파이프라인이 3D 장면을 생성하는 데 사용됩니다. ControlNet을 트레이닝하여 스케치 기반의 정확한 레이아웃 조정이 가능하며, 새로운 베이스맵 복원 모델을 사용해 장면의 베이스맵을 생성합니다.
- **Performance Highlights**: Sketch2Scene는 고해상도 텍스처 타일 및 BEV(Bird’s Eye View) 이미지에서 생성된 스플랫 맵을 활용해 게임 준비가 완료된 품질을 달성합니다. 이 방법은 형태의 품질, 다양성, 제어 가능성 면에서 기존 장면 생성 기법을 능가합니다.
- **Dataset**: Sketch2Scene의 트레이닝을 위해 게임 등각 투영 데이터셋을 큐레이션하였으며, 이 데이터셋은 ControlNet과 베이스맵 복원 네트워크를 트레이닝하는 데 사용됩니다.

### [Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models](https://arxiv.org/abs/2408.04594)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.04594.png)

Vote: 9

Authors: Ying Shen, Yaliang Li, Yilun Huang, Daoyuan Chen, Qirui Jiao

- **What's New**: 새로운 Img-Diff 데이터셋이 도입되었습니다. 이 데이터셋은 기존의 비주얼 인스트럭션 튜닝 데이터셋과 차별화되며, 거의 비슷한 이미지를 쌍으로 생성하여 세부적인 객체 차이를 포착하게 합니다. 이 데이터셋은 모델이 짝을 이룬 이미지를 분석하고 특정 영역 내에서 차이점을 설명하도록 도전합니다.
- **Technical Details**: Img-Diff 데이터셋은 객체 교체라는 방식으로 매우 유사한 이미지 쌍을 생성하여 제작되었습니다. 이를 통해 이미지의 미세한 영역을 분석하는 능력을 향상시키고, 세밀한 이미지 인식 능력을 키우고자 합니다. LLaVA-1.5와 MGM 모델에 이 데이터셋을 통합하여 파인튜닝을 수행하고 다양한 이미지 차이 벤치마크와 MLLM 벤치마크에서 성능을 평가했습니다.
- **Performance Highlights**: Img-Diff 데이터셋을 활용한 파인튜닝 결과, LLaVA-1.5-7B와 MGM-7B 모델이 MMVP, Spot-the-Diff, Image-Edit-Request 등 기존의 SOTA 모델인 GPT-4V와 Gemini를 뛰어넘는 성능을 보였습니다. 여러 MLLM 벤치마크에서도 평균 3.06%의 성능 향상을 달성하였습니다. 데이터셋의 다양성과 품질도 meticulous manual labeling을 통해 검증되었습니다.

### [Task-oriented Sequential Grounding in 3D Scenes](https://arxiv.org/abs/2408.04034)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.04034.png)

Vote: 7

Authors: Baoxiong Jia, Xiaojian Ma, Qing Li, Siyuan Huang, Ziyu Zhu, Tengyu Liu, Yixin Chen, Pengxiang Li, Zhuofan Zhang

- **What's New**: 이번 연구에서는 3D 장면 내에서의 Task-oriented Sequential Grounding을 제안합니다. 이 문제는 Embodied AI 에이전트가 실내 장면에서 일상적인 활동을 수행하며 각 단계를 위한 목표 객체를 찾아야 하는 새로운 과제입니다. 이를 위해 현실적인 실내 장면을 포함한 대규모 데이터셋 SG3D를 구축했습니다.
- **Technical Details**: SG3D는 ScanNet, ARKitScenes, 3RScan 등 다양한 3D 장면 데이터셋에서 수집한 RGB-D 스캔을 포함합니다. 각 장면은 SceneVerse의 3D scene graph를 사용하여 객체의 카테고리, 속성 및 공간 관계를 설명합니다. GPT-4를 사용해 일상적인 활동을 생성하고, 사람 검증 과정을 통해 과제의 적절성과 계획의 충분성, 각 단계에서 목표 객체가 올바르게 식별되었는지 확인했습니다.
- **Performance Highlights**: 실험에서는 3D-VisTA, PQ3D, LEO 등 최신 3D Visual Grounding 모델을 순차적 Grounding 작업에 적용하고 SG3D에서 평가했습니다. 실험 결과, 기존 벤치마크에서 우수한 성능을 보였던 모델들이 SG3D 벤치마크에서는 복잡하고 현실적인 Grounding 작업에서 어려움을 겪고 있음을 확인했습니다. 이는 향후 연구와 개발의 필요성을 강조합니다.

### [LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection](https://arxiv.org/abs/2408.04284)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.04284.png)

Vote: 6

Authors: Kareem Elozeiri, Raj Vardhan Tomar, Alexander Aziz, Minh Ngoc Ta, Rui Xing, Saad El Dine Ahmed, +, Jonibek Mansurov, Zain Muhammad Mujahid, Artem Shelmanov, Alham Fikri Aji, Akim Tsvigun, Mervat Abassy, Ekaterina Artemova, Tarek Mahmoud, Osama Mohammed Afzal, Vladislav Mikhailov, Yuxia Wang, Nizar Habash, Bimarsha Adhikari, Zhuohan Xie, Hasan Iqbal, Jiahui Geng

- **What's New**: 최근 GPT-4, Claude-3.5, Gemini-1.5, Llama-70b와 같은 대형 언어 모델(large language models, LLM)의 발전으로 기계가 생성한 콘텐츠의 비율과 일관성이 크게 증가했습니다. 이로 인해 인간이 작성한 텍스트와 기계가 작성한 텍스트를 구분하는 것이 점점 어려워지고 있습니다. 기존에는 텍스트가 전적으로 기계에 의해 작성되었는지, 인간에 의해 작성되었는지 여부만을 판단하는 이분법적(binary) 탐지가 주로 연구되었습니다. 하지만 보다 세밀한 텍스트 분류의 필요성이 대두되고 있습니다.
- **Technical Details**: 새로운 문제를 해결하기 위해, MGT(machine-generated text) 탐지 작업의 새로운 공식화 방식을 제안했습니다. 우리의 LLM-DetectAIve 시스템은 다음과 같은 네 가지 레이블로 구성된 다중 분류 작업을 수행합니다. 1) Human-Written: 인간이 작성한 텍스트, 2) Machine-Generated: 기계가 전적으로 생성한 텍스트, 3) Machine-Written Machine-Humanized: 기계가 생성한 텍스트를 약간 수정하여 더 인간적인 느낌을 준 텍스트, 4) Human-Written Machine-Polished: 인간이 작성한 텍스트를 기계가 다듬어 완성한 텍스트.
- **Performance Highlights**: 우리는 다양한 LLM을 사용하여 데이터 세트를 확장하고 다양한 도메인에서의 탐지 정확성을 향상시키기 위해 노력했습니다. LLaMA3-8b, LLaMA3-70b, Mixtral 8x7b, Gemma-7b, GPT-4o, 등 여러 LLM을 사용하여 데이터를 생성했으며, 총 303,110개의 텍스트를 수집했습니다. 탐지 모델로는 RoBERTa, DeBERTa, DistilBERT를 사용하여 성능을 평가했으며, 이 중 DeBERTa가 가장 우수한 성능을 보였습니다.

### [Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics](https://arxiv.org/abs/2408.04631)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.04631.png)

Vote: 6

Authors: Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi, Ruining Li

- **What's New**: 본 논문은 자연 객체의 동작을 이해할 수 있는 개방형 모델을 학습하는 방법을 다룹니다. 이 모델은 객체의 내부 동역학을 이해할 수 있는 유연한 구조를 갖춰야 하며, 방대한 양의 학습 데이터를 효율적으로 활용하는 방법을 제시합니다. 새로운 제안은 단일 이미지와 드래그(drag) 정보를 입력받아 객체의 전체 동작을 예측할 수 있는 조건부 비디오 생성기를 사용하는 것입니다.
- **Technical Details**: 이 연구는 기존의 비디오 생성기, 특히 Stable Video Diffusion (SVD)을 활용해 동작 모델을 학습하고자 합니다. 드래그 컨트롤을 비디오 생성 파이프라인에 효과적으로 주입하기 위해 adaptive layer normalization을 제안하며, 크로스-어텐션 모듈에 드래그 토큰을 추가하여 공간 인식성을 개선합니다. 또한, out-of-distribution 데이터셋에서 비디오 생성기를 미세 조정할 때 품질 저하 문제를 해결하기 위해 모든 생성된 프레임이 첫 프레임에 주의를 기울이는 all-to-first attention 메커니즘을 도입합니다.
- **Performance Highlights**: 이 연구에서 제안된 모델은 다양한 객체 카테고리에 대해 동작 예측을 높은 정확도로 수행할 수 있으며, 여러 벤치마크에서 이전 작업을 능가합니다. 특히, 이 모델은 합성 데이터로만 미세 조정되었음에도 불구하고 실제 데이터에 잘 일반화되며, 추가 조정 없이 zero-shot 방식으로 고성능을 보입니다.

### [Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP](https://arxiv.org/abs/2408.04303)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.04303.png)

Vote: 5

Authors: François Remy, Thomas Demeester, Miryam de Lhoneux, Alfiya Khabibullina, Hayastan Avetisyan, Pieter Delobelle

- **What's New**: 이 논문은 다국어 서브워드 토크나이제이션(multilingual subword tokenization)과 관련된 언어 편향 문제를 해결하기 위해 고유한 토크나이저를 각 언어에 맞춤화 하는 접근 방안을 제공합니다. 또한, 고용량 언어 모델(LLMs)을 다양한 언어로 적응시키는 데 필요한 자원이 부족한 상황을 고려하여, 모델 변환을 통해 제한된 자원만으로 새로운 언어에 대해 적응할 수 있는 방법을 제안합니다.
- **Technical Details**: 이 논문은 'Trans-Tokenization'이라는 새로운 크로스링얼 어휘 전이(cross-lingual vocabulary transfer) 전략을 소개합니다. 이 전략은 병렬 코퍼스와 임베딩 맵핑을 사용하여 토큰들을 배열 및 매핑합니다. 특히, 병렬 코퍼스를 통한 통계적 기계 번역(SMT) 모델 FastAlign을 활용하여 높은 품질의 토큰 맵핑을 생성합니다. 이는 병렬 코퍼스를 사용한 단어 수준의 SMT 정렬을 통해 매핑 과정을 수행하며, 정확하지 않은 정렬을 필터링하여 노이즈를 최소화합니다.
- **Performance Highlights**: Trans-Tokenization 접근 방식은 GPT 스타일 모델에서 특히 효과적임을 보였습니다. 기존의 BERT 스타일 모델에 비해 GPT 스타일 모델에서 더 나은 성능을 보여주며, 다양한 스크립트와 언어 계열에 대한 언어 모델 적응 가능성을 확장합니다. 또한, 대부분의 원래 모델 계층을 유지하면서, 다른 언어로 작성된 쿼리를 일괄 처리할 수 있어 경제적으로도 실용적입니다.

### [Better Alignment with Instruction Back-and-Forth Translation](https://arxiv.org/abs/2408.04614)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.04614.png)

Vote: 5

Authors: Jeffrey Li, Sewoong Oh, Luke Zettlemoyer, Ludwig Schmidt, Xian Li, Jason Weston, Thao Nguyen

- **What's New**: 최근 몇 년간, 대규모 언어 모델(LLMs)을 채팅 인터페이스를 통해 사용자 질의와 상호작용하게 하는 것이 일반화되고 있습니다. 이러한 기능은 웹에서 크롤링한 대량의 텍스트로 사전 학습된 모델을 소규모의 지시-응답 쌍 데이터 또는 사용자 선호 데이터로 미세 조정하여 구현됩니다. 이번 연구에서는 지시-응답 데이터 생성의 새로운 접근법을 제안하며, 돌마(Dolma)와 같은 대규모 오픈소스 코퍼스를 이용한 백트랜슬레이션을 통해 데이터를 생성합니다.
- **Technical Details**: 이 연구는 Li et al. (2023a)의 백트랜슬레이션 방법을 기반으로 하고 있습니다. 기존의 방법에서는 ClueWeb과 같은 비싼 접근 권한이 필요한 코퍼스를 사용하였으나, 이번 연구에서는 공개된 Dolma 코퍼스를 사용하여 데이터를 얻고, LLM을 통해 이를 AI 어시스턴트와 유사한 형태로 재작성하는 과정을 추가합니다. 파이프라인의 주요 단계는 백트랜슬레이션(Backtranslation), 필터링(Filtering) 및 재작성(Rewriting)입니다. 이 과정을 통해 ClueWeb과 유사한 품질의 지시-응답 데이터를 생성합니다.
- **Performance Highlights**: 본 연구의 데이터 생성 파이프라인을 통해 얻어진 지시-응답 데이터로 라마(Llama)-2-70B 모델을 미세 조정한 결과, 이전 연구의 백트랜슬레이션 데이터와 비교하여 AlpacaEval 승률이 3.6% 향상되었고, 다른 기존의 증류 데이터셋(OpenOrca, ShareGPT, Alpaca-GPT4, Self-instruct 등)과 비교해 최소 3.2% 향상되었습니다. 이는 백트랜슬레이션과 재작성 과정을 통해 AI 어시스턴트 유사 데이터의 품질과 다양성을 효과적으로 확보할 수 있음을 시사합니다.

### [Advancing Molecular Machine (Learned) Representations with Stereoelectronics-Infused Molecular Graphs](https://arxiv.org/abs/2408.04520)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.04520.png)

Vote: 3

Authors: Gabe Gomes, Samuel M. Blau, Thiago Reschützegger, Benjamin Sanchez-Lengeling, Daniil A. Boiko

- **What's New**: 이 논문에서는 Transformer 기반 모델의 효율성을 높이고자 새로운 메커니즘을 도입했습니다. 기존 모델의 연산 복잡성을 줄이면서도 성능을 유지 또는 향상시키는 것이 주요 목표입니다.
- **Technical Details**: 제안된 모델은 Self-Attention 메커니즘과 Feed-Forward Network(FFN)에서의 최적화를 통해 연산량을 크게 줄였습니다. 특히 Sparse Attention과 Dynamic Programming 기법을 이용해 메모리 사용량을 효율적으로 관리합니다. 이렇게 함으로써 모델의 학습과 추론 속도를 크게 향상시켰습니다.
- **Performance Highlights**: 새로운 모델은 다양한 자연어 처리(NLP) 태스크에서 기존 모델 대비 뛰어난 성능을 보였습니다. 특히, 계산 효율성 면에서 Transformer 모델의 한계를 극복하였다는 점이 큰 장점입니다. 실험 결과, 기존의 SOTA(State Of The Art) 모델과 비교하여 파라미터 수를 줄이면서도 유사한 정확도를 유지했습니다.

### [Learning to Predict Program Execution by Modeling Dynamic Dependency on Code Graphs](https://arxiv.org/abs/2408.02816)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.02816.png)

Vote: 1

Authors: Hoang Nhat Phan, Cuong Chi Le, Nghi D. Q. Bui, Tien N. Nguyen, Huy Nhat Phan

- **What's New**: 이번 연구에서는 CodeFlow라는 새로운 코드 커버리지 예측 모델을 소개합니다. 이 모델은 source code와 주어진 입력으로부터 코드 커버리지를 예측하는데, 이를 통해 동적 프로그램의 행동을 보다 정확하게 예측할 수 있습니다.
- **Technical Details**: CodeFlow는 제어 흐름 그래프(Control Flow Graph, CFG)를 활용해 코드의 동적 의존성을 효과적으로 학습합니다. 이를 통해 코드 내 루프와 조건문 등의 다양한 분기와 반복문들을 더 정확하게 이해하고 예측할 수 있습니다. CFG는 프로그램의 실행 흐름을 그래프 형태로 나타내며, 각 노드는 실행 가능한 코드 블록을, 엣지는 이들 간의 제어 흐름을 나타냅니다. 또한, CodeFlow는 기존 모델들이 가진 정적 코드 표현의 한계를 극복하여, 변수 상태와 제어 흐름을 다수의 반복을 거쳐 정확히 시뮬레이션 할 수 있습니다.
- **Performance Highlights**: CodeFlow는 코드 커버리지 일치율에서 75.24%의 정확도를 기록해 GPT-4의 68.13%를 넘어섰습니다. 브랜치 커버리지(Branch coverage)에서도 CodeFlow는 87.88%로 GPT-4의 78.75%를 능가했습니다. 런타임 오류 탐지(Run-time error detection)에서는 97.51%의 높은 정확도를 달성해 기존 모델들을 크게 앞섰습니다. 또한, 불완전한 코드 스니펫에서도 높은 정확도를 유지해 일반화 능력을 입증했습니다.

### [VGGHeads: A Large-Scale Synthetic Dataset for 3D Human Heads](https://arxiv.org/abs/2407.18245)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.18245.png)

Vote: -

Authors: Orest Kupyn, Eugene Khvedchenia, Christian Rupprecht

- **What's New**: 이번 발표에서는 완전히 합성된 이미지로 구동되는 새로운 데이터셋을 소개합니다. 이 데이터셋은 편견(bias), 프라이버시 및 윤리적 문제를 줄이기 위해 만들어졌으며, 사람의 머리를 일반적이고 포괄적으로 표현하는 것을 목표로 합니다.
- **Technical Details**: 각 인스턴스는 합성 이미지와 3D 머리 메쉬(mesh) 한 쌍으로 구성됩니다. 이미지는 모두 합성된 것이며 실제 사람을 나타내지 않습니다. 각 머리는 바운딩 박스(bounding box)와 3D 머리의 자세, 형태, 표정을 인코딩하는 3D Morphable Model 매개변수로 라벨링되어 있습니다. 총 1,022,944개의 이미지로 구성된 이 데이터셋은 다양한 현실 세계의 샘플을 포함하는 것을 목표로 합니다. 빠른 테스트를 위해 10,000개의 이미지를 포함한 소규모 버전도 제공됩니다.
- **Performance Highlights**: 데이터셋은 큰 규모이지만 합성되어 생성된 것이므로 생성 모델 또는 탐지/3DMM 추정 모델의 실패 사례에서 발생하는 노이즈가 존재할 수 있습니다. 수동 검사 결과, 이러한 샘플의 수는 1% 미만으로 예측되었습니다. 데이터셋은 안정적인 디퓨전 모델(stable diffusion model)에서 생성되었으며 이는 고품질의 합성 인물 이미지를 만드는 데 사용되었습니다.

### [Learning Task Decomposition to Assist Humans in Competitive Programming](https://arxiv.org/abs/2406.04604)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04604.png)

Vote: -

Authors: Zhihong Shao, Pei Ke, Hongning Wang, Minlie Huang, Jiaxin Wen, Ruiqi Zhong

- **What's New**: 이번 연구에서는 새로운 목표 Assistive Value (AssistV)를 통해 작업 분해(Task Decomposition)를 활용하여 인간의 수정을 돕고자 합니다. 이를 위해, 복잡한 문제를 여러 하위 작업으로 나누어 이해하고 수정하기 쉽게 만드는 방법을 제안합니다.
- **Technical Details**: 기술적 상세 사항으로는 세 단계 과정으로 이루어진 신경망 모델을 사용하여 매우 높은 AssistV를 가진 분해를 생성합니다. 1단계에서는 인간 비평(Critique) 모델 π_critiquesubscript로 초기 분해의 개선 방법을 예측하고, 2단계에서는 비평을 통합해 초기 분해를 정제하는 Refine 모델 π_refinesubscript를 사용하며, 3단계에서는 높은 AssistV를 가진 분해를 선택하는 Rank 모델 π_ranksubscript를 사용합니다.
- **Performance Highlights**: 경쟁 프로그래밍 문제 해결에서 인간의 감독 성능을 33.3% 향상시키고, 비전문가와 전문가의 속도를 각각 3.3배, 2.4배로 가속화하며, 비전문가가 전문가 수준의 성과를 내도록 도왔습니다. 또한, LMs가 인간보다 더 높은 정확도로 도움 값을 예측할 수 있음을 확인했습니다. 예를 들어 GPT-3.5-Turbo는 62.5%의 정확도를, GPT-4는 15.6% 더 나은 성과를 보였습니다.

