## Daily Papers (2024-08-26)

### [Building and better understanding vision-language models: insights and future directions](https://arxiv.org/abs/2408.12637)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12637.png)

Vote: 34

Authors: Hugo Laurençon, Andrés Marafioti, Victor Sanh, Léo Tronchon

- **What's New**: 이번 논문은 최신 비전-언어 모델(VLMs) 개발의 주요 연구 질문과 최신 접근 방식의 강점과 약점을 상세히 검토합니다. 또한, 오픈 데이터셋과 간단한 파이프라인을 이용하여 효율적으로 훈련된 강력한 VLM, Idefics3-8B의 구축 방법을 소개하며, 해당 모델과 함께 사용된 데이터셋도 공개합니다.
- **Technical Details**: 논문은 VLM 훈련에 사용되는 다양한 아키텍처, 데이터의 유형과 유용성, 훈련 방법, 모델 평가에서의 문제점 등을 집중적으로 논의합니다. 특히, 교차 주의(cross-attention)와 자기 주의(self-attention) 아키텍처 간의 성능 비교와 이에 따른 모델 성능에 대한 분석을 제공합니다.
- **Performance Highlights**: 새로 제안된 Idefics3-8B는 전작인 Idefics2-8B보다 문서 이해 작업에서 13.7점 더 높은 성능을 보이며 DocVQA 데이터셋에서 뛰어난 성과를 달성했습니다. 이를 위해 2.4백만 장의 이미지와 9.5백만 쌍의 QA 쌍을 포함한 대규모 Docmatix 데이터셋을 생성하였습니다.

### [LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation](https://arxiv.org/abs/2408.13252)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.13252.png)

Vote: 18

Authors: Shuai Yang, Gordon Wetzstein, Jing Tan, Yixuan Li, Ziwei Liu, Dahua Lin, Mengchen Zhang, Tong Wu

- **What's New**: 새로운 연구 LayerPano3D는 텍스트 프롬프트로부터 고해상도 및 일관성있는 3D 파노라마 장면을 생성하는 혁신적인 프레임워크를 제안합니다. 기존의 방법들이 가진 문제점, 예를 들어 장면 확장을 통한 일관성 저하나 제한된 탐험 가능성 등을 해결하고자 합니다.
- **Technical Details**: LayerPano3D는 다중 레이어 3D 파노라마(Multi-Layered 3D Panorama) 구조를 도입하여 복잡한 장면 계층을 효율적으로 생성합니다. 먼저 기준 파노라마를 생성하고, 이를 깊이 레이어로 나눕니다. 각 레이어는 특정 깊이 수준에서 장면 내용을 설명하며, 숨겨진 자산들을 배치하여 복잡한 장면 계층을 완전한 형태로 구성합니다. 이 과정에서 텍스트 안내 앵커 뷰 합성을 사용하여 파노라마를 생성하여 의미의 일관성을 유지하고, K-Means 클러스터링과 사전 학습된 파놉틱 세그먼테이션(prior)을 사용하여 자동 레이어 생성 파이프라인을 제공합니다.
- **Performance Highlights**: LayerPano3D는 단일 텍스트 프롬프트로 하이퍼 몰입형의 레이어드 파노라마 장면을 효과적으로 생성할 수 있음을 입증했습니다. 다양한 실험 결과에서 텍스트 정렬된 2D 파노라마 및 완전한 시야각('Full-view') 일관성을 지닌 탐험 가능한 3D 파노라마 환경에서 최첨단 방법들을 능가하였습니다. 또한 특정 장면 네비게이션 경로가 필요하지 않아 비전문가에게 사용자 친화적인 인터페이스를 제공한다는 점에서 진보를 보였습니다.

### [MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?](https://arxiv.org/abs/2408.13257)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.13257.png)

Vote: 15

Authors: Feng Li, Tieniu Tan, Yi-Fan Zhang, Qingsong Wen, Zhang Zhang, Chaoyou Fu, Liang Wang, Huanyu Zhang, Haochen Tian, Junfei Wu, Rong Jin, Shuangqing Zhang, Kun Wang

- **What's New**: 새로운 벤치마크 MME-RealWorld가 도입되었습니다. 이 벤치마크는 자율주행, 원격 감지, 비디오 감시, 신문, 거리 뷰 및 금융 차트 등 여러 이미지 소스를 포함한 현실적인 시나리오를 통해 Multimodal Large Language Models (MLLMs) 성능을 평가합니다. 3232명의 자원봉사자가 총 29,429개의 QA 쌍을 수작업으로 주석하여 최고의 데이터 품질을 자랑합니다.
- **Technical Details**: MME-RealWorld는 13,366개의 고해상도 이미지를 300K 이상의 공공 및 인터넷 소스에서 수집했습니다. 평균 해상도는 2000x1500으로, 2525명의 전문 주석자와 77명의 MLLM 전문가가 데이터 품질을 보장하기 위해 참여했습니다. 주요 목표는 인간이 해결하기 어려운 질문을 생성하는 것입니다. 그리고 MME-RealWorld-CN 버전은 중국 시나리오에 중점을 두고 5,917개의 QA 쌍을 추가로 포함했습니다.
- **Performance Highlights**: 기존 벤치마크의 최고 성능이 80%에서 90% 정확도에 이르렀지만, MME-RealWorld에서는 대부분의 고급 모델들이 60% 정확도를 넘지 못했습니다. 이는 실제 세계의 문제들이 기존 벤치마크보다 훨씬 더 어렵다는 것을 보여줍니다. 예를 들어, 비디오 감시에서 133개의 차량의 존재를 세거나 원격 감지에서 작은 물체를 식별하는 것과 같은 과제가 포함됩니다.

### [Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time](https://arxiv.org/abs/2408.13233)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.13233.png)

Vote: 10

Authors: Yufa Zhou, Zhao Song, Zhenmei Shi, Zhizhou Sha, Yingyu Liang

- **What's New**: 본 논문은 대형 언어 모델(LLMs)의 긴 문맥 처리 능력을 개선하기 위해 새로운 접근 방식을 제안합니다. 특히, 현재의 자기 주의 메커니즘(self-attention mechanism)이 가지고 있는 계산 복잡도를 줄이기 위해 주목할 만한 방법을 소개합니다.
- **Technical Details**: LLMs는 대형 문서나 레포트 등 긴 문맥 정보를 처리하는 데 있어 뛰어난 성능을 보여주고 있습니다. 그러나, 현재 LLMs의 Transformer 아키텍처는 자기 주의 계산 시 쿼드러틱(quadratic) 복잡성을 가지며, 이는 긴 문맥 길이(n)에 대해 O(n²d)만큼의 계산 시간을 필요로 합니다. 이를 해결하기 위해 본 논문은 자기 주의 역전파(backpropagation) 계산을 거의 선형 시간 내에 근사할 수 있는 방법을 제안합니다.
- **Performance Highlights**: 제안된 방법은 LLaMA 3.1 405B와 같이 긴 문맥 길이를 지원하는 모델에서도 효과적으로 작용하며, 모델의 훈련 효율성을 크게 향상시킬 수 있습니다. 이는 훈련 시간과 메모리 소모를 크게 줄이고, 에너지 사용량을 감소시켜 탄소 배출을 줄이는 데 기여할 수 있음을 의미합니다.

### [CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities](https://arxiv.org/abs/2408.13239)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.13239.png)

Vote: 7

Authors: Yong Zhang, Xi Li, Ying Shan, Xintao Wang, Xianpan Zhou, Zhongang Qi, Tao Wu, Guangcong Zheng

- **What's New**: 본 논문에서는 Text-to-Video 생성 분야에서 새로운 딥러닝 모델인 CustomCrafter를 소개합니다. CustomCrafter는 확산 모델(Diffusion Model)을 기반으로 하며, 학습된 주제의 일관된 외관, 자유로운 개념 결합, 그리고 부드러운 동작 생성을 동시에 만족시키는 맞춤형 비디오 생성을 목표로 합니다.
- **Technical Details**: CustomCrafter는 주체 학습 동안 모델의 개념 결합 능력과 동작 생성 능력을 보존하기 위해 다양한 모듈을 사용합니다. 공간적 주체 학습 모듈(Spatial Subject Learning Module)은 공간적 교차 주의(spatial cross-attention) 및 자기 주의(self-attention) 레이어의 가중치(weight)를 업데이트함으로써 새로운 주제의 외관을 캡처할 수 있습니다. 동작 생성 능력의 감소 문제를 해결하기 위해, 동적 가중치 비디오 샘플링 전략(dynamic weighted video sampling strategy)을 제안하여 모델의 추론 과정을 개선합니다.
- **Performance Highlights**: CustomCrafter는 추가 비디오나 반복적인 미세 조정을 필요로 하지 않으면서도 지정된 주제에 대해 고품질의 비디오를 생성할 수 있습니다. 실험 결과, 이 방법은 개념 결합과 동작 생성 능력을 효과적으로 보존하면서도 주제 학습만으로 비디오 생성을 가능하게 합니다. 다양한 정성적, 정량적 실험과 사용자 연구 결과를 통해 CustomCrafter의 우수성을 입증하였습니다.

### [T3M: Text Guided 3D Human Motion Synthesis from Speech](https://arxiv.org/abs/2408.12885)

![](/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg)

Vote: 6

Authors: Sai Qian Zhang, Wenshuo Peng, Kaipeng Zhang

- **What's New**: 이번 연구에서는 새로운 텍스트 기반 3D 인간 모션 합성 방법인 T3M을 소개합니다. T3M 프레임워크는 텍스트 프롬프트를 통해 신체 및 손 동작 생성을 정밀하게 제어할 수 있습니다. 이는 기존의 오디오만 입력으로 사용하는 방식에서 발생하는 비정확하고 원치 않는 동작 생성을 극복할 수 있는 개선점입니다.
- **Technical Details**: T3M 프레임워크는 VQ-VAE 네트워크, 오디오 특징 추출 네트워크, 그리고 멀티모달 융합 블록의 세 가지 주요 구성 요소로 이루어져 있습니다. VQ-VAE 네트워크는 액션-투-액션 맵핑을 위한 중간 코드북을 생성하며, 오디오 특징 추출 네트워크는 오디오의 음향 정보를 추출합니다. 텍스트 정보와 오디오 정보를 융합하기 위해 멀티모달 융합 인코더 구조를 제안하며, 이는 트랜스포머 디코더에 크로스 어텐션 레이어를 삽입하여 구현됩니다.
- **Performance Highlights**: 제안된 T3M 프레임워크는 정량적 및 정성적 평가에서 기존 방법보다 현저히 우수한 성능을 보입니다. 비디오와 텍스트를 조인트 임베딩에 정렬하고 이를 이용하여 학습하며, 다양한 텍스트 입력을 이용한 모션 생성 성능을 크게 개선했습니다.

### [Memory-Efficient LLM Training with Online Subspace Descent](https://arxiv.org/abs/2408.12857)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12857.png)

Vote: 5

Authors: Bo Liu, Lizhang Chen, Kaizhao Liang, Qiang Liu

- **What's New**: 큰 언어 모델(LLMs) 학습의 지속적인 발전은 계산 효율성과 모델 성능의 균형을 맞추는 데 있어서 도전 과제를 제시합니다. 본 연구에서는 투사 행렬의 임의 업데이트 규칙에 대한 최초의 수렴 보장을 제공합니다. 이는 다양한 최적화 도구에 널리 적용될 수 있는 중요한 보장입니다.
- **Technical Details**: 투사 행렬 업데이트 규칙에 대한 수렴 보장을 제공함으로써, 본 접근 방식이 특정한 업데이트 규칙에 제한되지 않고, Hamiltonian descent 프레임워크에서 분석 가능한 다양한 최적화 도구에 확장될 수 있음을 보입니다. 이러한 최적화 도구에는 LION과 Adam과 같은 널리 사용되는 알고리즘도 포함됩니다. 또한, 본 연구에서는 온라인 PCA를 이용해 변화하는 기울기 지형에 대응하여 투사 행렬을 동적으로 변경하는 메모리 효율적인 최적화 도구인 Online Subspace Descent를 소개합니다.
- **Performance Highlights**: LLaMA 모델(60M부터 1B 파라미터)에 대한 사전 학습 단계에서 낮은 perplexity를 보여주며, 기존의 저순위 학습 방법들과 비교하여 더욱 뛰어난 성능을 보였습니다. 이는 언어 모델 사전 학습 시 풀랭크 베이스라인과의 perplexity 격차를 줄이는 데 기여합니다.

### [A Web-Based Solution for Federated Learning with LLM-Based Automation](https://arxiv.org/abs/2408.13010)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.13010.png)

Vote: 2

Authors: Chamith Mawela, Mehdi Bennis, Chaouki Ben Issaid

- **What's New**: 최근 머신러닝(ML)의 발전으로 인해 사용자 생성 데이터의 풍부함을 활용하여 여러 조직이 이러한 기술을 비즈니스 솔루션에 채택하고 있습니다. 그러나 전통적인 중앙 집중식 모델 학습 방식은 사용자 개인정보 보호에 위협이 되며 원시 클라이언트 데이터를 외부에 공유해야 하기 때문입니다. 이를 해결하기 위해 분산 모델 학습을 가능하게 하는 Federated Learning(FL)이 등장했습니다. FL은 모델 가중치(weight)만을 엣지 노드와 파라미터 서버(PS) 간에 전달하여 사용자 데이터 유출의 위험을 최소화합니다.
- **Technical Details**: 기존 FL 프레임워크는 프로그래밍 및 네트워크 전문가들의 전문 지식을 요구하며, 모델 아키텍처와 하이퍼파라미터가 PS와 클라이언트 프로그램 내에 고정되어 있어 반복적인 실험에 불편함이 있었습니다. 이를 해결하기 위해 웹 기반 솔루션을 개발하여 클라이언트와 PS가 API(endpoint)로 작동하게 하였고, 모델 압축 기법을 통해 통신 병목 현상을 줄였습니다. 또한, 큰 언어 모델(LLM)을 활용해 사용자 프롬프트에 기반한 FL 자동화를 시도하였습니다.
- **Performance Highlights**: FL과 관련된 이차 기능, 예를 들어 모델 압축, 클라이언트 참여 스케줄링 알고리즘의 구현을 탐구했습니다. 특히, LLM을 활용하여 NAS(Neural Architecture Search)와 HPO(Hyperparameter Optimization)를 통한 자동화된 솔루션의 성능 향상을 목표로 하였으며, NAS의 탐색 공간을 생성하기 위해 LLM을 사용했습니다. 이를 통해 사용자가 직접 디바이스에 접근하지 않아도 쉽게 FL 작업을 수행할 수 있는 웹 애플리케이션을 개발했습니다.

### [CODE: Confident Ordinary Differential Editing](https://arxiv.org/abs/2408.12418)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12418.png)

Vote: 2

Authors: Alexandre Alahi, Tommaso Martorella, Bastien van Delft

- **What's New**: 최근 연구는 기존의 부패된 이미지를 복원하는 방식에서 벗어나 새로운 방법으로 이 문제를 접근합니다. Confident Ordinary Differential Editing (CODE)라는 새로운 방법을 제안하여, 사전 학습된 diffusion model을 활용하여 단일 부패 이미지에서 실제적이고 신뢰성 있는 이미지를 생성합니다. 추가적인 데이터 증강 또는 부패된 데이터에 대한 미세 조정 없이, 어떤 유형의 부패에 대한 가정도 필요 없이 작동합니다.
- **Technical Details**: CODE 방법론은 사전 학습된 diffusion model을 활용하며, 이는 어떠한 사전 학습된 diffusion model과 데이터셋에도 호환됩니다. 이 방법론은 생성된 이미지의 가능성을 최적화하는 동시에 입력 이미지와의 거리를 제약하는 방식으로 복원을 최적화 문제로 프레이밍합니다. 이는 GAN-inversion 방식과 유사합니다만, GAN 대신 diffusion model을 생성적 프라이어로 활용하여 더욱 정교한 제어를 가능케 합니다. CODE는 Stochastic Differential Equation (SDE)을 대신해 확률유동 ODE를 사용하여 노이즈 주입, 보정 수준 및 잠재 공간을 분리하며, 이미지 정보의 확률적 분포를 활용한 confidence-based clipping 방법을 도입합니다.
- **Performance Highlights**: CODE는 특히 어려운 상황에서 SDEdit보다 더 현실적이고 신뢰성 있는 이미지를 생성합니다. 실험 결과는 CODE가 복잡하고 다양한 부패 상황에서도 SDEdit보다 우수한 성능을 입증하였습니다. 이는 특히 입력 이미지의 변형을 최소화 하면서도 높은 수준의 리얼리즘을 보장할 수 있는 능력을 강조합니다.

### [FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting for Customizable Rendering](https://arxiv.org/abs/2408.12894)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.12894.png)

Vote: 2

Authors: Young Sun Choi, Youngjung Uh, Yunji Seo, Hyun Seung Son

- **What's New**: 최신 3D 재구성 기술은 새로운 뷰 합성(novel view synthesis)의 정확성과 렌더링 속도를 크게 개선했습니다. 특히 3D Gaussian Splatting (3DGS, Kerbl et al. 2023)은 포토리얼리틱 품질을 매우 빠른 렌더링 속도로 달성했습니다. 그러나 3DGS는 높은 메모리 요구 사항이 단점으로 작용하고 있습니다. 이에 따라, 다양한 장치에서 유연하게 렌더링할 수 있도록 레벨 오브 디테일(Level of Detail, LoD)을 3DGS 프레임워크에 통합한 새로운 방법론인 Flexible Level of Detail(FLoD)을 제안합니다.
- **Technical Details**: FLoD는 3DGS의 메모리 사용량을 최적화하기 위해 여러 레벨의 Gaussians를 사용하여 장치의 메모리 용량에 맞는 렌더링 옵션을 제공합니다. 각 레벨은 그 자체의 디테일 정도를 갖추고 있으며, 이를 통해 메모리 효율적이면서도 이미지 품질의 손실을 최소화할 수 있습니다. 특히, '선택적 렌더링'을 도입하여 이미지의 일부는 고해상도로, 다른 일부는 저해상도로 렌더링하여 전체적인 성능을 향상시킵니다.
- **Performance Highlights**: FLoD의 효과를 검증하기 위해 Tanks and Temples, Mip-Nerf360, DL3DV-10K 등의 데이터셋에서 실험을 진행하였으며, 그 결과 FLoD는 3DGS 기반 모델과 쉽게 통합되어 추가적인 계산 비용 없이도 렌더링 품질을 향상시켰음을 확인하였습니다.

### [HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments](https://arxiv.org/abs/2408.10945)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.10945.png)

Vote: -

Authors: Deepu John, Kazi Hasan Ibn Arif, JinYi Yoon, Hans Vandierendonck, Dimitrios S. Nikolopoulos, Bo Ji

- **What's New**: 최근 비전-언어 모델(Vision-Language Models, VLMs) 분야에서 새로운 고해상도 VLM들이 등장했습니다. 이들 모델은 기존 저해상도 이미지 인코딩의 한계를 극복하고자 하며, 고해상도 이미지의 세부 정보를 보다 효과적으로 처리할 수 있습니다. 
본 연구는 High-Resolution Early Dropping (	extit{HR-ED})라는 새로운 토큰 드롭핑 프레임워크를 제안하여 이러한 고해상도 VLM inferencing을 더욱 효율적으로 만듭니다.
- **Technical Details**: 고해상도 VLM들은 여러 이미지 파티셔닝 방식을 통해 이미지를 다수의 저해상도 서브 이미지로 나누고, 이들을 비전 인코더(예: Vision Transformers, ViTs)가 인코딩하여 텍스트 임베딩 공간으로 변환시킵니다. 	extit{HR-ED}는 주의(attention) 기반의 초기 토큰 드롭핑을 통해 이러한 고해상도 이미지에서 중요한 비주얼 토큰만을 선택하는 방법을 제안합니다. 구체적으로, CLS 토큰의 주의 맵을 이용해 이미지의 주요 객체와 배경을 구분하고, 파티션별로 시각적 콘텐츠 점수를 정산하여 토큰을 할당합니다. 이후 마지막 레이어의 CLS 주의 맵을 기반으로 중요한 특징 점수를 매기고, 이를 통해 시각적 토큰을 선택합니다.
- **Performance Highlights**: 본 연구의 실험 결과에 따르면, 	extit{HR-ED}는 LLaVA-Next-7B 모델에서 20% 토큰 할당 시 생산된 토큰 비율을 4.7배 향상시키고, 응답 지연 시간을 15초 줄이며, NVIDIA TESLA P40 GPU에서 2.3GB의 GPU 메모리 사용량을 절감할 수 있습니다. 또한, PruMerge 및 PruMerge+ 등 기존 베이스라인 방법보다 높은 정확도를 보입니다.

