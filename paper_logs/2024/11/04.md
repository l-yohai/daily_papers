## Daily Papers (2024-11-04)

### [Constant Acceleration Flow](https://arxiv.org/abs/2411.00322)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00322.png)

Vote: 14

Authors: Taehoon Lee, Dogyun Park, Hyunwoo J. Kim, Sojin Lee, Youngjoon Hong, Sihyeon Kim

- ***What's New***: 이 연구에서는 일정한 가속 흐름(Constant Acceleration Flow; CAF)이라는 새로운 ODE 프레임워크를 제안하여, 상수 가속 방정식을 도입함으로써 정확한 ODE 흐름 추정을 지원합니다. 기존의 상수 속도 모델과 달리, CAF는 가속을 학습 가능한 변수로 추가하여 더 표현력 있는 모델링을 가능하게 했습니다.
- ***Technical Details***: CAF는 초기 속도 조건(Initial Velocity Conditioning)과 재흐름 절차(Reflow Process)를 통해 흐름 교차 문제를 해결합니다. 초기 속도는 학습 가능한 신경망으로 모델링되며, 가속도 필드는 다른 신경망을 통해 학습됩니다. 이로써 복잡한 관계를 학습할 때 발생하는 곡선을 줄이고 데이터 커플링의 보존성을 높였습니다.
- ***Performance Highlights***: 다양한 데이터셋(CIFAR-10, ImageNet 64x64)에서 CAF는 최신 기법 대비 우수한 성능을 보였으며, 특히 CIFAR-10에서 FID 조건부 생성에서 1.39, 무조건부 생성에서 1.48을 기록하며 탁월한 결과를 달성했습니다. 이는 짧은 단계에서의 높은 정확도와 효율성을 보여주며, 실제 응용에서도 효과적임을 입증했습니다.

### [WikiNER-fr-gold: A Gold-Standard NER Corpus](https://arxiv.org/abs/2411.00030)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00030.png)

Vote: 2

Authors: Nicolas Béchet, Pierre-François Marteau, Danrun Cao

- ***What's New***: WikiNER-fr-gold는 프랑스어 명명된 개체 인식(Named Entity Recognition; NER)을 위한 금 표준 코퍼스(gold-standard corpus)를 제공합니다. 이는 원래 WikiNER-fr의 20% 샘플을 수작업으로 수정하여 제작된 것으로, 보다 일관된 토큰 경계를 가지며 명명된 개체의 정의를 명확하게 하고 있습니다.
- ***Technical Details***: WikiNER-fr-gold의 주된 목적은 기존 WikiNER의 오류와 불일치를 수작업으로 교정하여 명명된 개체와 일관된 레이블을 부여하는 것이었습니다. 이 작업은 부분적으로는 원본 코퍼스의 레이블링 오류를 식별하기 위한 심층 분석을 포함하고 있으며, 이를 통해 새로운 주석 지침(annotation guidelines)을 정의하고 있습니다. 이 코퍼스는 각 카테고리에 대한 명확한 정의를 제공하며, BIOES 형식으로 주석을 추가하여 정보 표현의 명확성을 높였습니다.
- ***Performance Highlights***: 현재는 성능 측면보다는 corpus의 표준화 및 교정 작업 자체에 중점을 두고 있으며, 향후 WikiNER 문화의 다른 언어 세그먼트로 교정 작업을 확장 할 계획입니다. 엔티티 표준화를 통해 외부 NER 시스템에 대한 학습 데이터를 향상시키고 일관된 성과 평가를 가능하게 할 것으로 기대됩니다.

### [In-Context LoRA for Diffusion Transformers](https://arxiv.org/abs/2410.23775)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.23775.png)

Vote: 5

Authors: Huanzhang Dou, Wei Wang, Chen Liang, Yu Liu, Yutong Feng, Yupeng Shi, Lianghua Huang, Jingren Zhou, Zhi-Fan Wu

- ***What's New***: In-Context LoRA는 기존의 Text-to-Image DiTs를 활용하여 최소한의 데이터만으로도 다양한 이미지 생성 작업을 수행할 수 있는 새로운 파이프라인을 제안합니다. 이 방법은 모델의 아키텍처를 수정하지 않고도 텍스트를 기반으로 여러 이미지를 생성하는데 적합한 기능을 활성화하여 심플하면서도 효율적인 방법을 제공합니다.
- ***Technical Details***: 제안된 방법은 이미지 대신 토큰을 이어 붙이며 여러 이미지를 합친 캡션을 사용합니다. 대규모 학습 대신, 로라(LoRA) 튜닝을 소규모 데이터셋(20~100 샘플)으로 수행함으로써 학습 시간을 줄이고 효율성을 높였습니다. 또한, 서로 다른 이미지 세트를 취합하여 통합 프롬프트를 통해 한 번에 이미지를 생성할 수 있도록 설계되었습니다.
- ***Performance Highlights***: 제안된 In-Context LoRA 모델은 다양한 생성 작업에 적용할 수 있으며, 텍스트 기반 세트 설명을 통해 고품질의 일관된 이미지 생성을 수행할 수 있습니다. 샘플링 동안 20단계의 가이던스 스케일을 사용하고 SDEdit를 도입하여 이미지 기반 생성 시 일부 이미지를 마스킹하여 활용합니다. 해당 방법은 이미지 간의 시각적 일관성이 텍스트 기반 생성보다 낮을 수 있다는 점에서 개선 여지가 있습니다.

### [Fashion-VDM: Video Diffusion Model for Virtual Try-On](https://arxiv.org/abs/2411.00225)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00225.png)

Vote: 2

Authors: Innfarn Yoo, Yingwei Li, Andreas Lugmayr, Johanna Karras, Luyang Zhu, Nan Liu, Ira Kemelmacher-Shlizerman, Chris Lee

- ***What's New***: Fashion-VDM은 가상 착용 시뮬레이션(Video Virtual Try-On)을 위한 비디오 디퓨전 모델(Video Diffusion Model)을 제안하였습니다. 이는 가상 시착 방법의 영상 생성 품질과 시간 일관성을 크게 향상시킵니다. 이 모델은 한 번의 추론으로 최대 64프레임의 길이를 일관되게 생성할 수 있는 최초의 통합 비디오 디퓨전 모델입니다.
- ***Technical Details***: Fashion-VDM의 핵심 기술로는 3D 합성곱(3D Convolution)과 시간 주의 블록(Temporal Attention Block)을 활용한 VTO-UDiT 아키텍처의 확장, 각 입력 신호에 대한 개별 제어를 가능하게 하는 Split Classifier-Free Guidance(Split-CFG)를 도입하여 의류 충실도를 높이는 것, 그리고 점진적인 시간별 훈련을 통해 높은 격자의 긴 비디오 생성을 가능하게 하는 기술이 포함됩니다. 이러한 접근 방식은 복잡한 옷의 디테일과 현실감을 유지하는 데 도움을 줍니다.
- ***Performance Highlights***: Fashion-VDM은 이전의 최첨단 방법들보다 가상 시착 비디오의 현실성과 일관성 측면에서 대규모 벤치마크를 통해 성능을 상회하는 결과를 기록하였습니다. 특히 사용자 조사에서는 비디오의 부드러움, 사람의 충실도, 의상 충실도 모두에서 90% 이상의 선호도를 보였습니다.

### [OS-ATLAS: A Foundation Action Model for Generalist GUI Agents](https://arxiv.org/abs/2410.23218)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.23218.png)

Vote: 31

Authors: Yian Wang, Zhiyong Wu, Chengyou Jia, Liheng Chen, Paul Pu Liang, Yu Qiao, Zichen Ding, Zhenyu Wu, Kanzhi Cheng, Qiushi Sun, Fangzhi Xu

- ***What's New***: OS-ATLAS는 GUI 에이전트를 위한 기초 행동 모델로, GUI 그라운딩과 OOD 에이전트 작업에서 뛰어난 성능을 발휘하는 혁신적인 데이터와 모델링 기술을 도입하였습니다. Windows, Linux, MacOS, Android, 그리고 웹을 포함한 다양한 플랫폼에서 GUI 그라운딩 데이터를 합성할 수 있는 오픈 소스 툴킷을 개발하여, 1,300만 개 이상의 GUI 요소를 포함한 대규모 오픈 소스 교차 플랫폼 GUI 그라운딩 코퍼스를 공개하였습니다.
- ***Technical Details***: OS-ATLAS는 GUI 화면을 이해하고 보지 못한 인터페이스로 일반화 할 수 있는 기초를 제공합니다. 모델은 GUI 인식과 실행 가능한 GUI 동작으로 명령어를 변환하는 데 중점을 둔 프리 트레이닝과 행동 미세 조정의 두 단계로 훈련됩니다. 이 과정에서 통일된 행동 공간(unified action space)을 제안하여 다양한 플랫폼 간의 행동 명칭 충돌 문제를 해결하고, 다플랫폼 GUI 참조 코퍼스를 사용하여 대규모 프리 트레이닝을 진행했습니다.
- ***Performance Highlights***: OS-ATLAS는 모바일, 데스크톱, 웹 플랫폼을 아우르는 6개의 벤치마크에서 이전 최첨단 모델보다 우수한 성능을 보여주었습니다. 모바일, 데스크톱, 그리고 웹의 각기 다른 플랫폼에서 차별화된 성능을 발휘하며, 특히 OS World 벤치마크에서 눈에 띄는 성공률을 기록하였습니다. 이로써 오픈 소스 VLMs의 에이전트 기능 강화를 위한 발전 방향을 제시합니다.

### [Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation](https://arxiv.org/abs/2411.00412)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00412.png)

Vote: 8

Authors: Taylor Berg-Kirkpatrick, Duncan Watson-Parris, Yadi Cao, Bohan Lyu, Leon Bergen, Rose Yu

- ***What's New***: 본 논문은 대형 언어 모델(LLMs)이 과학적 문제 해결 과정에서 문제의 난이도에 따라 적절한 도구 사용을 결정하게 하는 새로운 두 단계의 훈련 패러다임을 제안합니다. 이는 기존의단순 추론을 넘어서 LLM이 도구를 사용할 경우와 기본적인 추론만으로 문제를 해결할 수 있는 경우를 구별하여, 인간 전문가의 문제 해결 방식을 모델 내에 구현하고자 합니다.
- ***Technical Details***: 제안된 방법론은 두 컴포넌트로 구성됩니다. 첫 번째 컴포넌트인 World Knowledge Distillation (WKD)에서는 외부 도구를 이용해 생성된 정확한 솔루션을 기반으로 모델을 미세 조정하고, 과학적 지식을 내재화 합니다. 두 번째 컴포넌트인 Tool Usage Adaptation (TUA)에서는 모델의 문제별 정답률을 평가하고 문제를 쉬운 문제와 어려운 문제로 구분하여 쉬운 문제는 WKD와 동일한 목표로 정렬하고, 어려운 문제는 도구 사용 흔적을 따르도록 훈련합니다.
- ***Performance Highlights***: 제안된 방법은 수학, 기후 과학, 역학을 포함한 다양한 과학적 벤치마크 데이터셋에서 검증되었으며, 응답 정확도에서 평균 28.18% 향상과 도구 사용 정확도에서 13.89% 증가를 보여줍니다. 이는 GPT-4o, Claude-3.5 등의 최신 모델을 능가하는 성능입니다.

### [HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models](https://arxiv.org/abs/2410.22901)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22901.png)

Vote: 5

Authors: Jun Gao, Chaojie Yang, Nianhong Jiao, Shengkai Zhang, Chenhui Xue, Boya Niu, Tian Li

- ***What's New***: HelloMeme는 텍스트-이미지 모델(Text-to-Image Models)에 어댑터를 효과적으로 삽입하여 복잡한 다운스트림 작업을 수행하면서도 모델의 일반화 능력을 유지할 수 있는 새로운 방법론을 제안합니다. 이 방법론은 주로 2D 피처 맵(feature maps) 관련 주의력 메커니즘을 최적화하는 것으로, 밈 비디오 생성을 통해 성능을 검증하였습니다.
- ***Technical Details***: HelloMeme 구조는 HMReferenceNet, HMControlNet, HMDenoisingNet의 세 모듈로 구성됩니다. 이들 각각의 모듈은 참고 이미지에서 고충실도 피처를 추출하거나, 헤드 포즈 및 얼굴 표정과 같은 고수준 피처를 UNet에 전달함으로써 최종 이미지를 생성합니다. 특히, Spatial Knitting Attention(SK Attentions) 메커니즘을 도입하여 2D 피처 맵과 선형 피처를 효율적으로 융합하였습니다.
- ***Performance Highlights***: HelloMeme는 자가 재연성(self-reenactment) 및 교차 재연성(cross-reenactment) 설정에서 기존의 오픈 소스 SOTA(State Of The Art) 방법에 비해 뛰어난 성능을 보였습니다. 특히 자가 재연성 설정에서, PSNR, SSIM, LPIPS 등에서 우수한 점수를 기록하며, 주관적인 품질 측면에서도 긍정적인 평가를 받았습니다.

### [Physics in Next-token Prediction](https://arxiv.org/abs/2411.00660)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00660.png)

Vote: 1

Authors: Xuelong Li, Hongjun An, Yiliang Song

- ***What's New***: 이 연구는 다음 토큰 예측(Next-token Prediction; NTP)에서 정보 보존법칙을 발견하고, 정보 용량의 첫 번째 법칙(IC-1)을 제안하여 오토레그레시브 모델에서 지능의 출현 본질이 정보 전송 과정임을 보여줍니다. 또한 Landauer의 원리를 NTP에 도입하여 훈련 과정과 에너지 소비 간의 관계를 밝히는 정보 용량의 두 번째 법칙(IC-2)을 수립하였습니다. 이를 통해 실질적 생산 활동에 활용 가능한 여러 후속 이론을 제공합니다.
- ***Technical Details***: 첫 번째 정보 용량 법칙(IC-1)은 오토레그레시브 모델에서 정보가 보존되고 압축되어 모델이 학습하는 과정을 나타냅니다. 모델의 정보 용량 η는 모델에 저장된 유효 정보와 데이터 크기의 비율로 정의됩니다. 두 번째 정보 용량 법칙(IC-2)은 Landauer의 원리를 기반으로 하여 정보 전송 과정에 필요한 최소 에너지를 제시합니다. 이는 실제 세계의 물리적 에너지 소비와 관련 있습니다.
- ***Performance Highlights***: 이번 연구는 기존 이론과의 높은 호환성을 검증하였으며, IC-1이 OpenAI의 Scaling Law와 일관됨을 보였습니다. 또한 대규모 언어 모델의 정보 용량이 0.115 ~ 0.268 범위에 있음을 확인하여, 이는 기존 지식 용량의 척도인 0.125 ~ 0.25와 일관성을 이루었습니다. 이를 통해 오토레그레시브 모델 훈련이 효율적이고 지속 가능하게 발전할 수 있는 토대를 마련합니다.

### [CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes](https://arxiv.org/abs/2411.00771)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00771.png)

Vote: 4

Authors: Yang Liu, Chuanchen Luo, Zhaoxiang Zhang, Zhongkai Mao, Junran Peng

- ***What's New***: CityGaussianV2는 2D Gaussian Splatting (2DGS)을 기반으로 대규모 장면 재구성의 도전 과제를 해결하기 위해 제안된 새로운 방법입니다. 이 방법은 기하학적 정확성과 효율성을 모두 향상시키며, 병렬 학습을 통해 최대 10배의 압축과 최소 25%의 학습 시간, 메모리 사용량의 50% 감소를 달성합니다.
- ***Technical Details***: CityGaussianV2는 Depth-Anything V2와 분해된-그라디언트-기반 밀도화(decomposed-gradient-based densification) 같은 기법을 활용하여 빠른 수렴을 유도합니다. 또한, Elongation Filter를 통해 2DGS의 퇴화 시 발생하는 Gaussian count 폭발 문제를 완화하고, CityGaussian의 블록 분할 전략을 기반으로 병렬 학습을 실행합니다. 수학적 기법을 통한 벡터리 양자화를 사용하여 대규모 2DGS의 저장 요구를 감소시킵니다.
- ***Performance Highlights***: CityGaussianV2는 모든 벤치마크에서 기하학적 품질과 효율성에서 최상의 성능을 보여줍니다. SSIM 0.742와 PSNR 23.57을 기록하고, F1 점수에서는 0.477로 다른 최신 기법들과 비교하여 우수한 성능을 입증했습니다. 특히, 3시간의 학습 시간과 14GB 메모리 사용으로 Gaussians의 수를 8백만개로 유지하면서 성능을 최적화했습니다.

### [Randomized Autoregressive Visual Generation](https://arxiv.org/abs/2411.00776)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00776.png)

Vote: 10

Authors: Xiaohui Shen, Ju He, Qihang Yu, Xueqing Deng, Liang-Chieh Chen

- ***What's New***: 새로운 Randomized AutoRegressive 모델(RAR)이 시각적 생성(visual generation) 작업에서 최신 성능을 달성하면서도 언어 모델링 프레임워크와 완전한 호환성을 유지합니다. 이 방법은 시퀀스의 입력 순서를 무작위로 변형하여 양방향 문맥을 효과적으로 학습할 수 있도록 하며, ImageNet-256 벤치마크에서 FID 1.48을 기록하여 이전의 상태-of-the-art 생성기들을 능가합니다.
- ***Technical Details***: RAR 모델은 다음-토큰 예측 목표를 가진 표준 자기회귀 학습 과정에서, 입력 시퀀스를 무작위로 섞음으로써 다양한 순서로 변형하여 학습합니다. 이 방법은 모델이 가능한 모든 순서의 우도(likelihood)를 최대화하도록 하여 양방향 문맥을 학습합니다. 또한 이 과정은 훈련 과정동안 r이라는 확률로 최대 무작위 순서를 표시하며 점점 줄어들어 일반적인 스캔 순서로 되돌아옵니다.
- ***Performance Highlights***: RAR 모델은 ImageNet-256 벤치마크에서 FID 1.48을 달성했으며, 이는 이전의 자기회귀 이미지 생성기를 넘어서는 결과입니다. 그리고 양방향 문맥 학습의 제약을 극복함으로써, RAR은 시각적 생성에서 중요한 변화를 이끌어냈습니다.

### [Face Anonymization Made Simple](https://arxiv.org/abs/2411.00762)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00762.png)

Vote: 3

Authors: Han-Wei Kung, Tuomas Varanka, Nicu Sebe, Sanjay Saha, Terence Sim

- ***What's New***: 이 논문에서는 새로운 얼굴 익명화(Anonymization)를 제안하였습니다. 이는 얼굴 인식 모델에 의존하지 않고 얼굴 랜드마크나 마스크와 같은 추가 데이터를 요구하지 않는 확산 모델(Diffusion Model) 방식을 사용합니다. 이 접근법은 다양한 실제 응용에서 개인의 얼굴을 보호하면서도 표정, 자세, 시선, 배경 등을 보존합니다.
- ***Technical Details***: 이 방법은 소스 이미지와 드라이빙 이미지를 가지고 현실적이고 매끄럽게 얼굴을 교환하는 것을 학습하는 프레임워크를 설계하였습니다. 중심에는 텍스트-이미지 확산 모델에서 사용하는 것과 유사한 디노이징 UNet 아키텍처가 있으며, 이미지의 세부 사항을 입력 이미지에서 합성 출력으로 전송하는 이미지 특징 추출 메커니즘으로 강화되었습니다. 모델은 소스 이미지가 주어졌을 때와 주어지지 않았을 때 모두 작업할 수 있도록 이중 설정으로 훈련됩니다.
- ***Performance Highlights***: 모델은 기존 기술들과 비교하여 식별 손실을 줄이며, 얼굴 속성 보존과 이미지 품질 측면에서 뛰어난 성능을 보여주었습니다. 특히 d = 1.4로 설정했을 때 원래의 자세와 시선을 유지하면서도 독특한 얼굴 형태를 생산하는데 탁월한 성능을 발휘했습니다. 이미지 품질 측면에서 FALCO에 이어 두 번째로 높은 점수를 기록했습니다.

### [M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation](https://arxiv.org/abs/2410.21157)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.21157.png)

Vote: 4

Authors: Linzheng Chai, Ge Zhang, Peng Zhao, Zekun Wang, Shukai Liu, Jiaheng Liu, Bangyu Xiang, Jian Yang, Ken Deng, Yanan Wu, He Zhu, Bo Zheng, Wenbo Su, Ke Jin, Congnan Liu, Guoan Zhang

- ***What's New***: M2RC-EVAL은 18개의 프로그래밍 언어를 포괄하는 최초의 대규모 다국어 저장소 수준 코드 완성 벤치마크로, 코드 대형 언어 모델(Code Large Language Models, LLMs)을 다국어 시나리오에서 연구할 수 있도록 설계되었습니다. 이 벤치마크는 추상 구문 트리(Abstract Syntax Tree, AST)에 기반한 세부 주석을 제공하며, 다국어 코드 완성 능력을 평가합니다.
- ***Technical Details***: M2RC-EVAL은 18개의 프로그래밍 언어에 대해 100개의 검증 및 500개의 테스트 샘플을 포함하고 있으며, 버킷 수준과 의미적 수준 두 가지 유형의 세부 주석을 제공합니다. 각 언어는 고유의 구문 구조에 맞춰 설계된 11개의 주요 의미적 레이블을 갖추었습니다. 또한, 저장소 수준 코드 완성을 향상시키기 위해 18개 언어의 다국어 교육 데이터셋인 M2RC-INSTRUCT를 수집하였습니다.
- ***Performance Highlights***: Code Llama-7B, StarCoder-7B 및 DeepSeekCoder-6.7B 모델을 M2RC-EVAL에서 평가한 결과, 저장소 수준 코드 완성 성능은 다양한 프로그래밍 언어에서 상이하게 나타났습니다. 모델들은 교차 파일(context) 정보를 사용하면 성능이 크게 개선되었으며, M2RC-INSTRUCT에 기반한 조정 후에는 더욱 향상되었습니다. 특히 M2RC-INSTRUCT에서 조정된 Code Llama-7B 모델은 초기 성능이 낮았으나 조정 후 주요 모델들을 능가하는 성능을 보여주었습니다.

### [SambaMixer: State of Health Prediction of Li-ion Batteries using Mamba State Space Models](https://arxiv.org/abs/2411.00233)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00233.png)

Vote: 4

Authors: Clara Pérez-Molina, José Ignacio Olalde-Verano, Sergio Martin, Sascha Kirch

- ***What's New***: SambaMixer는 Mamba State Space Model을 활용하여 Li-ion 배터리의 State of Health(SOH)을 예측하는 새로운 구조적 상태 공간 모델(Structured State Space Model; SSM)입니다. 이번 연구에서는 최초로 앵커 기반 재샘플링(Anchor-based Resampling) 기법을 소개하며, 이는 시간 신호의 일관된 길이를 보장하고 데이터 증강(Data Augmentation) 방법으로 활용됩니다. 또한, 샘플 시간 정보와 사이클 시간 차이를 활용하여 배터리 수복 효과를 학습하는 위치 인코딩(Positional Encoding) 방법이 적용됩니다.
- ***Technical Details***: SambaMixer 모델은 다양한 배터리 신호 데이터를 처리하며, Mamba Mixer 아키텍처를 기반으로 긴 시간 대역의 의존성을 다루고, 다변량(多變量) 시간 시계열 데이터(Channel-wise Multi-variate Time Signals) 간의 정보 전달을 가능하게 설계되었습니다. NASA의 배터리 방전 데이터셋을 사용하여 평가했으며, 기존 최첨단 딥러닝 모델보다 우수한 성능을 보여줍니다. 또한, 주목할만한 점은 신호 사이의 원래 앵커에 노이즈를 추가하는 앵커 기반 재샘플링 기법을 사용하여, 재샘플링된 샘플 시간에 선형으로 보간하는 것입니다.
- ***Performance Highlights***: SambaMixer는 각 대회의 배터리 SOH 예측에서 양호한 성능을 보였으며, 특히 평가 배터리 #06, #07, #47의 전체 수명 동안 실제 SOH 곡선을 정확하게 예측하였습니다. SambaMixer는 MAE(Mean Absolute Error), RMSE(Root Mean Square Error), MAPE(Mean Absolute Percentage Error)에서 기존 모델 대비 우수한 성능을 보였으며, 특히 큰 데이터셋과 더 큰 모델 환경에서 더 나은 성능을 발휘하였습니다.

### [Survey of User Interface Design and Interaction Techniques in Generative AI Applications](https://arxiv.org/abs/2410.22370)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.22370.png)

Vote: 5

Authors: Nedim Lipka, Tong Yu, Reuben Luera, Puneet Mathur, Franck Dernoncourt, Samyadeep Basu, Alexa Siu, Ryan A. Rossi, Hanieh Salehy, Ruiyi Zhang, Jian Zhao, Sungchul Kim, Xiang Chen

- ***What's New***: 이 연구는 생성형 AI 응용 프로그램에서 사용자 인터페이스 디자인과 상호작용 기술의 최신 상태를 종합적으로 조사하였습니다. 사용자 주도 인터랙션(user-guided interaction) 패턴과 다양한 용례를 고려한 상호작용 기술을 탐색하였습니다.
- ***Technical Details***: 이 서베이는 사용자와 생성형 AI 시스템 간의 인터페이스에서 중요한 정의와 명확화를 제공하며, 사용자 주도 인터랙션의 새로운 개념을 제안합니다. 다양한 접근 방식으로 사용자 주도 인터랙션을 분류하여 제시하였습니다. 또한 디자인 라이브러리로 활용할 수 있는 사용자 인터페이스 레이아웃과 상호작용 기술들을 소개합니다.
- ***Performance Highlights***: 이 연구는 100개 이상의 관련 논문을 통해 사용자와 생성형 AI 시스템 간의 인터랙션 유형을 조사하고 분류하였습니다. 이러한 데이터는 생성형 AI 응용 프로그램 설계에 있어 디자이너와 개발자에게 유용한 참조 정보를 제공합니다.

### [Zipfian Whitening](https://arxiv.org/abs/2411.00680)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00680.png)

Vote: 3

Authors: Sho Yokoi, Hidetoshi Shimodaira, Han Bao, Hiroto Kurita

- ***What's New***: 이 연구는 Zipf 법칙을 활용하여 단어 임베딩 공간을 조정하는 새로운 방법인 Zipfian Whitening을 제안합니다. 이는 기존의 PCA whitening 방법을 개선하여 단어의 실제 빈도를 반영함으로써 NLP 태스크 성능을 획기적으로 향상시킵니다.
- ***Technical Details***: Zipfian Whitening은 주어진 단어 벡터 세트를 단어 빈도를 반영하여 중심화하고 정규화하는 방법입니다. 알고리즘은 PCA whitening의 각 단계에서 기대값을 계산할 때 단어의 경험적 빈도로 가중치를 추가합니다. 이 과정을 통해 Zipfian 법칙에 근거한 임베딩을 만들며, 이는 저빈도 단어의 정보를 더 강조합니다.
- ***Performance Highlights***: Zipfian Whitening을 통해 STS-B 데이터셋에서 GloVe 및 Word2Vec와 같은 모델의 성능이 70점 이상으로 상승하여 기존 센터링 및 정규화 방법보다 우수한 성능을 나타냈습니다. 이는 특히 저빈도 단어에 대한 정보력이 강조되어 더 좋은 임베딩을 생성함으로써 가능해집니다.

### [GRS-QA -- Graph Reasoning-Structured Question Answering Dataset](https://arxiv.org/abs/2411.00369)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00369.png)

Vote: 4

Authors: Yu Wang, Samyak Rajesh Jain, Franck Dernoncourt, Namyong Park, Anish Pahilajani, Devasha Trivedi, Ryan A. Rossi, Jincen Shuai, Khin S. Yone, Nesreen K. Ahmed

- ***What's New***: GRS-QA 데이터셋은 질문 응답(QA) 쌍에 대한 명시적인 추론 구조를 포함하는 최초의 데이터셋으로, 논리적 흐름을 나타내는 그래프 추론 구조(reasoning graphs)로 Large Language Models(LLMs)의 다중 단계 추론 능력을 평가할 수 있는 기회를 제공합니다.
- ***Technical Details***: GRS-QA는 기존의 다중 단계 질문 응답(M-QA) 데이터셋인 HotpotQA, MuSiQue, 2WikiMultiHopQA에서 질문 응답 쌍을 활용하여, 각 문장을 노드로, 논리적 흐름을 엣지로 구성하여 추론 그래프를 생성합니다. 데이터셋은 모든 QA 쌍에 대해 명시적인 추론 구조를 제공하며, 더욱 복잡한 질문에 대한 LLM의 성능을 평가할 수 있는 미세한 분석이 가능하도록 합니다.
- ***Performance Highlights***: BM25, TF-IDF, DPR의 성능을 사용해 적용한 실험 결과, BM25는 다중 단계 질문에 높은 성능을 보였으나, 복잡한 질문에서는 재현의 차이를 보였습니다. LLM의 경우, GPT-3.5가 가장 높은 성능을 기록하며, 추론 과정이 명시된 긍정적 추론 그래프 제공 시 성능 개선이 관찰되었습니다.

### [GPT or BERT: why not both?](https://arxiv.org/abs/2410.24159)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.24159.png)

Vote: 4

Authors: Lucas Georges Gabriel Charpentier, David Samuel

- ***What's New***: GPT-BERT라는 혼합 언어 모델이 출시되었습니다. 이는 마스킹 언어 모델링(MLM)과 인과 언어 모델링(CLM)을 결합하여 단일 트랜스포머 아키텍처 내에서 두 가지의 장점을 모두 통합하였습니다. 실험을 통해 혼합 모델링 접근법이 단일 목적 모델을 상회하는 성능을 보여주었습니다.
- ***Technical Details***: GPT-BERT는 수정된 마스킹 언어 모델링, 즉 마스크된 다음-토큰 예측(MNTP: Masked Next-Token Prediction)을 사용하여 마스킹 토큰의 위치를 클럼(CLiam)과 일치하도록 하나의 위치 오른쪽으로 이동시킵니다. 모든 학습 파라미터는 공유되며, 트랜스포머 인코더/디코더 모듈을 사용하였습니다. 데이터셋은 원본과 마스킹 목적을 위한 데이터셋을 중복 사용하여 균형을 맞춥니다.
- ***Performance Highlights***: BabyLM Challenge 2024의 여러 벤치마크에서 GPT-BERT는 순위에 상관없이 MLM과 CLM 접근법 모두에서 더 나은 결과를 보여주었습니다. 본 모델은 제약된 상황에서도 더 복잡하고 능력이 뛰어난 언어 모델을 구현할 가능성을 열어줍니다.

### [Personalization of Large Language Models: A Survey](https://arxiv.org/abs/2411.00027)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.00027.png)

Vote: 13

Authors: Jiuxiang Gu, Zichao Wang, Ryan A. Rossi, Branislav Kveton, Yijia Shao, Yu Wang, Subrata Mitra, Hamed Zamani, Tong Yu, Franck Dernoncourt, Junda Wu, Ruiyi Zhang, Diyi Yang, Sungchul Kim, Nesreen Ahmed, Nedim Lipka, Xiang Chen, Zhehao Zhang, Hongjie Chen, Tyler Derr, Joe Barrow

- ***What's New***: 이 논문은 대형 언어 모델(Large Language Models; LLMs)의 개인화에 대한 포괄적인 설문을 제공하며, 개인화된 LLMs의 사용을 위한 분류 체계(Taxonomy)를 도입하고 앙상블된 개념들을 확장하여 새로운 개인화 측면의 정의와 논의를 포함합니다.
- ***Technical Details***: 개인화된 LLMs에 대한 사용 사례를 분류하는 체계를 제시하며, LLMs의 개인화를 데이터의 세분화, 개인화 기법, 데이터셋, 평가 방법 및 응용 분야로 체계적으로 점검하고 있습니다. 개인화 기법은 검색-증강 생성(Retrieval-Augmented Generation; RAG), 프롬프트 엔지니어링(Prompt Engineering), 임베딩 학습(Embedding Learning), 인적 피드백을 기반으로 한 강화 학습(Reward Learning from Human Feedback; RLHF) 등을 포함합니다.
- ***Performance Highlights***: 논문은 강화 학습을 통한 개인화 프레임워크(Personalized-RLHF)의 초기 결과를 제시하며, LLMs의 응답을 개별 사용자의 선호도에 맞춰 조정함으로써 사용자 만족도를 향상시킬 수 있음을 나타냅니다. 다양한 도메인에서 LLMs가 개인화된 결과를 생성하는 데 있어 상당한 잠재력을 지니고 있음을 보여줍니다.

### [TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models](https://arxiv.org/abs/2410.23266)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.23266.png)

Vote: 12

Authors: Yuxuan Ding, Chuhan Li, Arman Cohan, Yanan Zheng, Tesca Fitzgerald, Ziyao Shangguan, Yilun Zhao

- ***What's New***: TOMATO는 멀티모달 기초 모델(Multimodal Foundation Models; MFMs)의 영상 이해에서 시각적 시간 추론 능력을 엄격하게 평가하기 위해 설계된 새로운 벤치마크입니다. 이 벤치마크는 여섯 가지 태스크(action count, direction, rotation, shape & trend, velocity & frequency, visual cues)에 대한 1,484개의 다지선다 질문으로 구성되어 있습니다.
- ***Technical Details***: TOMATO는 6가지 시각적 시간 추론 태스크로 구성된 벤치마크로, 각 태스크는 다중 프레임 기반의 시각적 시간 추론을 요구합니다. 예를 들어, 'Rotation' 태스크에서는 주제의 회전 방향 판단이 요구되며, 'Velocity & Frequency' 태스크에서는 객체의 속도 변화 탐지가 필요합니다. 이러한 태스크들은 인간 중심, 실제 시나리오, 시뮬레이션된 시나리오를 포괄하는 다양한 동영상에 적용됩니다.
- ***Performance Highlights***: 최고의 성과를 보인 오픈 소스 모델 Qwen2-VL-72B는 37.9%의 정확도를 기록했으며, 이는 독점 모델인 GPT-4o의 37.7%를 약간 상회합니다. 그러나 이는 95.2%를 기록한 인간 성능과 비교할 때 여전히 57.3%의 큰 격차를 드러내고 있습니다. 이러한 결과는 현재의 대부분의 모델들이 시간 추론 능력에서 여전히 한계를 가지고 있음을 보여줍니다.

