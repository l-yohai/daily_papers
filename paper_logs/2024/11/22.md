## Daily Papers (2024-11-22)

### [Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation](https://arxiv.org/abs/2411.14384)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14384.png)

Vote: 2

Authors: Jianming Zhang, Soo Ye Kim, Yuqian Zhou, Yuanhao Cai, Zhe Lin, Kai Zhang, Fujun Luan, Alan Yuille, Qing Liu, He Zhang, Yixun Liang, Mengwei Ren, Zhifei Zhang

- ***What's New***: 이 논문에서는 단일 이미지를 통해 3D 객체와 장면을 생성할 수 있는 새로운 3DGS 기반의 확산 모델인 DiffusionGS를 소개합니다. 이 모델은 예고된 방향의 다양한 프롬프트 뷰뿐만 아니라 객체 중심의 입력을 넘어서는 강력한 생성 능력을 발휘합니다.
- ***Technical Details***: DiffusionGS는 3D 일관성을 강제하기 위해 각 타임스텝마다 3D Gaussian 포인트 클라우드를 직접 생성하며, Scene-Object 혼합 학습 전략을 통해 기존의 3D 장면과 객체 데이터를 최대한 활용합니다. 또한 Relational Plücker Coordinate (RPPC)라는 새로운 카메라 조건 방법을 사용하여 깊이와 3D 기하학을 더 잘 인식하도록 설계되었습니다.
- ***Performance Highlights***: DiffusionGS는 기존 SOTA(최첨단) 기술 대비 품질면에서 큰 향상을 보여주었으며, PSNR에서는 2.2/2.91dB, FID 점수에서는 23.25/75.68 만큼의 개선을 이루었습니다. 뿐만 아니라, A100 GPU에서의 실행 속도 또한 약 6초로 상당히 빠르다는 장점을 가지고 있습니다.

### [MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control](https://arxiv.org/abs/2411.13807)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.13807.png)

Vote: 3

Authors: Bo Xiao, Kai Chen, Zhenguo Li, Qiang Xu, Ruiyuan Gao, Lanqing Hong

- ***What's New***: MagicDriveDiT는 자율 주행을 위한 고해상도 장시간 비디오 생성에 적합한 새로운 비디오 생성 프레임워크로 소개됩니다. DiT 아키텍처 기반의 이 접근 방식은 확장성과 기하학적 제어 문제를 해결하며, 자율 주행 기술에 필수적인 다중 시공간 제어를 지원합니다.
- ***Technical Details***: MagicDriveDiT는 DiT 아키텍처와 Flow Matching 기법을 활용하여 고해상도, 장시간 비디오 생성을 증대시키며, 시공간 조건 인코딩(spatial-temporal conditional encoding)을 통해 정밀한 제어를 구현합니다. 프로그레시브 트레이닝 전략(progressive training strategy)을 사용하여 간단한 시나리오부터 복잡한 시나리오까지 점진적으로 학습합니다.
- ***Performance Highlights***: MagicDriveDiT는 기존의 여러 모델보다 높은 해상도와 더 많은 프레임을 생성할 수 있으며, 6개의 시점과 제어 지원을 통해 뛰어난 현실감을 갖춘 거리 장면 비디오를 효과적으로 생성합니다. 이 모델의 유연성은 다양한 해상도와 프레임을 처리할 수 있도록 하며, 새로운 거리 장면 시뮬레이션의 잠재적 응용 범위를 확장시킵니다.

### [Ultra-Sparse Memory Network](https://arxiv.org/abs/2411.12364)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12364.png)

Vote: 14

Authors: Defa Zhu, Hongzhi Huang, Yutao Zeng, Ran Guo, Qiyang Min, Xun Zhou, Zihao Huang

- ***What's New***: UltraMem은 대규모, 초저밀도 메모리 계층을 도입하여 추론 지연 시간을 크게 줄이면서 모델 성능을 유지하도록 설계된 최신 아키텍처입니다. 특히, PKM(Product Key Memory) 및 MoE(Mixture of Experts) 모델의 한계를 극복하여 메모리 접근 비용을 절감함으로써, 리소스가 제한된 환경에서도 고성능 언어 모델 배포를 지원합니다.
- ***Technical Details***: UltraMem은 MLP(MultiLayer Perceptron) 기반의 PKM을 확장하여 대규모 초저밀도 메모리 계층을 포함합니다. 이 메모리 계층은 Query-Value 매칭 과정에서 필요한 계산을 최소화하며, Tucker Decomposed Query-Key Retrieval(TDQKR) 및 Implicit Value Expansion(IVE)을 통해 메모리 접근을 더욱 효율적으로 합니다. 즉, 높은 차원의 행렬 계산을 피하고 다양한 매개변수 조정을 통해 모델의 성능을 극대화합니다.
- ***Performance Highlights***: UltraMem은 동일한 파라미터 크기에서 MoE보다 최대 6배 빠른 추론 속도를 달성했습니다. 이를 통해 초당 메모리 접근 횟수가 크게 감소하여, 더 큰 배치 크기에서도 MoE와 비슷한 수준의 성능을 유지하거나 그 이상을 보여주었습니다. 또한, UltraMem은 모델의 크기가 증가함에 따라 성능이 지속적으로 향상되는 강력한 스케일링 능력을 보여주었습니다.

### [Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2411.14432)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14432.png)

Vote: 10

Authors: Hai-Long Sun, Yuhao Dong, Yongming Rao, Ziwei Liu, Winston Hu, Zuyan Liu, Jingkang Yang

- ***What's New***: Insight-V는 복잡한 멀티모달 과제를 위한 장기적이고 견고한 추론 데이터를 대규모로 생성하고, 멀티모달 대형 언어 모델(MLLMs)의 추론 능력을 향상시키기 위한 효과적인 학습 파이프라인을 소개합니다. 이 시스템은 인간의 노동 없이도 긴 체인 추론 데이터를 생성하기 위한 진보적 전략을 포함합니다. 또한 데이터 품질을 보장하기 위해 다단계 평가 방법을 사용합니다.
- ***Technical Details***: Insight-V는 두 단계의 데이터 생성 파이프라인을 통해 다양한 추론 경로를 가진 구조화된 긴 체인 추론 데이터를 생성합니다. 이 파이프라인은 자동 생성, 평가 및 순위 결정 전략을 사용하여 모델의 추론 능력을 높이는 데 필요합니다. MLLMs의 추론을 향상시키기 위해, 추론 에이전트와 요약 에이전트를 갖춘 다중 에이전트 시스템을 설계하였습니다. 이러한 에이전트들은 문제 해결 과정을 두 개의 다른 단계로 나누어 협력하여 추론 품질을 향상시킵니다.
- ***Performance Highlights***: LLaVA-NeXT 모델에 Insight-V를 통합함으로써 7개의 어려운 시각적 추론 벤치마크에서 평균 7.0%의 성능 향상을 달성했습니다. 또한, 강력한 기본 모델에서는 2.9%의 성능 향상이 관찰되었습니다. 이는 Insight-V의 유효성과 일반화를 강조하며, 다양한 시각적 추론 벤치마크에서 상당한 성능 향상을 보여줍니다.

### [Patience Is The Key to Large Language Model Reasoning](https://arxiv.org/abs/2411.13082)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.13082.png)

Vote: 3

Authors: Yijiong Yu

- ***What's New***: 이 논문은 대형 언어 모델(Large Language Model; LLM)의 복잡한 문제 해결 능력을 향상시키기 위해 새로운 방법을 제안합니다. 저자들은 모델이 인내심을 가지고 더 자세한 추론을 하도록 장려함으로써, 추가적인 지식이나 기술을 도입하지 않고도 성능을 개선할 수 있음을 보여주었습니다. 특히, 이는 철저한 추론 과정을 긍정적 예시로, 간단한 답변을 부정적 예시로 사용하여 모델을 훈련시키는 선호 최적화(Preference Optimization) 접근법을 통해 구현되었습니다.
- ***Technical Details***: 본 연구는 약 5000개의 수학 문제로 이루어진 경량 데이터셋을 기반으로 하여, 간결한 추론 단계를 거쳐 문제를 해결한 후, 이를 더욱 직관적이고 이해하기 쉽도록 재작성한 솔루션을 제공합니다. 훈련은 Qwen2-7B-Instruct 모델을 기반으로 진행되며, DPO(Direct Preference Optimization) 기법을 사용하여 모델이 스스로 인내심을 가지고 추론하는 과정을 자연스럽게 채택하도록 합니다.
- ***Performance Highlights***: 제안된 방법을 사용하여 GSM8k 벤치마크에서 성능이 6.7% 향상되었으며 MATH 벤치마크에서도 0.2%의 향상을 이루었습니다. 실험 환경에서 LLM의 문제 해결 시 소요 시간이 증가했지만, 성능 향상을 고려할 때 이러한 비용은 수용할 만한 수준으로 평가됩니다.

### [Natural Language Reinforcement Learning](https://arxiv.org/abs/2411.14251)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14251.png)

Vote: 15

Authors: Zhiyuan Hu, Haotian Fu, Mengyue Yang, Bo Liu, Ying Wen, Jun Wang, Ziyu Wan, Xidong Feng, Girish A. Koushik

- ***What's New***: 이 논문은 강화 학습(Reinforcement Learning; RL)을 자연어 기반 표현 공간으로 확장한 자연어 강화 학습(Natural Language Reinforcement Learning; NLRL)을 소개합니다. NLRL은 기존의 마코프 결정 과정(Markov Decision Process; MDP)의 원칙들을 언어 기반의 대응 요소들로 재정의하였습니다. 이를 통해 자연어를 통해 더 많은 정보를 활용하여 RL 시스템의 해석 가능성 및 효율성을 향상시킬 수 있습니다.
- ***Technical Details***: NLRL은 최근 대형 언어 모델(Large Language Models; LLMs)의 발전과 결합하여, 순수한 프롬프팅이나 그레이디언트 기반 학습을 통해 강화 학습과 유사한 정책 및 가치 개선을 실현할 수 있도록 설계되었습니다. 실험에서는 미로 게임과 같은 다양한 환경에서 NLRL의 효과성과 해석 가능성을 입증하였습니다.
- ***Performance Highlights***: NLRL의 적용 결과, 현재 LLM 기반의 영향력을 통한 다양한 게임 환경에서 효율성을 입증했습니다. 특히, 이질적이고 다양성이 높은 데이터를 활용할 수 있는 능력을 통해 RL 구현에서 해석 가능성과 학습 안정성을 유지할 수 있는 잠재력을 보여주었습니다.

### [Generating Compositional Scenes via Text-to-image RGBA Instance Generation](https://arxiv.org/abs/2411.10913)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10913.png)

Vote: 2

Authors: Yongxin Yang, Shifeng Zhang, Alessandro Fontanella, Petru-Daniel Tudosiu, Sarah Parisot

- ***What's New***: 이 연구는 텍스트-이미지 변환을 통해 합성 장면을 생성하는 새로운 다단계 생성 패러다임을 도입하였습니다. 이는 장면의 구성 요소를 RGBA 이미지로 생성하여 개별 객체 속성 및 위치에 대한 미세 조절을 가능하게 합니다.
- ***Technical Details***: 제안된 방법은 RGBA 인스턴스 생성 및 다층 합성 과정을 포함하며, VAE와 확산 모델을 투명성을 고려하는 방식으로 훈련해 개별 장면 구성 요소를 투명도 정보를 포함한 RGBA 이미지로 생성하도록 합니다. 이 과정을 통해 각 객체의 속성(예: 색상, 패턴, 자세)을 미세하게 제어할 수 있으며, 객체를 여러 레이어에 배치하여 위치 및 비율을 쉽게 조절할 수 있습니다.
- ***Performance Highlights***: RGBA 확산 모델은 다양한 고품질 인스턴스를 정확하게 생성하였으며, 제시된 실험 결과에 따르면 최신 레이아웃 제어 방법을 능가하여 복잡한 장면에서도 강력한 객체 속성 및 위치 제어 능력을 보여주었습니다.

### [Stable Flow: Vital Layers for Training-Free Image Editing](https://arxiv.org/abs/2411.14430)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14430.png)

Vote: 7

Authors: Or Patashnik, Daniel Cohen-Or, Egor Nemchinov, Kfir Aberman, Omri Avrahami, Ohad Fried, Dani Lischinski

- ***What's New***: 본 논문은 훈련 없이 이미지 편집을 가능하게 하는 Stable Flow라는 접근법을 소개하고 있습니다. 기존의 UNet 아키텍처를 대체해 Diffusion Transformer (DiT)을 이용하며, 이미지 형성에 중요한 'vital layers'를 자동으로 식별해 이들에 주의(attention) 기능을 선택적으로 주입함으로써 일관된 이미지 편집을 수행합니다.
- ***Technical Details***: DiT 모델에서 각 층의 중요성을 분석하여 이미지 형성에 필수적인 'vital layers'를 찾아냅니다. 이를 통해 비휘발성(non-rigid) 편집, 객체 추가, 장면 변경 등 다양한 편집 작업을 동일한 메커니즘으로 수행할 수 있습니다. 실 이미지 편집을 위해서는 FLUX 모델을 이용하여 이미지와 관련된 공간을 역변환(inverting)하는 것도 포함됩니다.
- ***Performance Highlights***: 우리의 방법은 기존 방법들(SDEdit, Instruct-P2P, MagicBrush 등)과 비교했을 때 이미지와 텍스트 지시 사항 간의 유사성을 더 잘 유지했습니다. 사용자 연구 결과, 제안된 방법이 목표 프롬프트 준수, 입력 이미지 보존, 현실성, 전반적인 편집 품질 측면에서 기존 방법들보다 우수한 것으로 나타났습니다.

### [DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding](https://arxiv.org/abs/2411.14347)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14347.png)

Vote: 0

Authors: Xiaoke Jiang, Shilong Liu, Xingyu Chen, Yuda Xiong, Feng Li, Han Gao, Tianhe Ren, Zhaoyang Zeng, Hongjie Huang, Wenlong Liu, Hao Zhang, Junyi Shen, Kent Yu, Lei Zhang, Zhuheng Song, Yuhong Zhang, Yihao Chen, Yuan Gao, Zhengyu Ma, Qing Jiang

- ***What's New***: 이 논문에서는 DINO-X라는 새로운 객체 중심의 통합 비전 모델을 소개하고 있습니다. 이 모델은 개방 세계 객체 탐지 및 이해에 있어서 최상의 성능을 보이며, 텍스트, 시각, 그리고 사용자 맞춤형 프롬프트를 지원하여 사용자가 제공하는 프롬프트 없이도 객체를 탐지할 수 있습니다. 또한, DINO-X는 1억 개 이상의 고품질 데이터셋(Grounding-100M)으로 사전 학습을 통해 개방 세계에서의 언어 이해를 강화합니다.
- ***Technical Details***: DINO-X는 Grounding DINO 1.5와 동일한 Transformer 기반의 인코더-디코더 구조를 사용하며, 개체 수준의 표현을 추구합니다.긴 꼬리 객체 탐지를 용이하게 하기 위해, 텍스트, 시각, 그리고 사용자 맞춤형 프롬프트를 인풋으로 확장하였습니다. 모델은 프로(Pro)와 엣지(Edge) 두 가지 버전으로 나뉘며, 프로 모델은 다양한 시나리오에서 강화된 인식 능력을 제공하고, 엣지 모델은 빠른 추론 속도를 위해 최적화되어 엣지 장치에 적합합니다.
- ***Performance Highlights***: DINO-X Pro 모델은 COCO, LVIS-minival, 및 LVIS-val 제로샷 객체 탐지 벤치마크에서 각각 56.0 AP, 59.8 AP, 52.4 AP를 기록했습니다. 특히 LVIS 드문 클래스에서 63.3 AP와 56.5 AP를 달성하여 이전의 SOTA 성능을 각각 5.8 AP와 5.0 AP 개선했습니다. 엣지 모델인 DINO-X Edge는 COCO와 LVIS 벤치마크에서 눈에 띄는 제로샷 탐지 성능을 기록했으며, 뛰어난 실시간 성능을 자랑합니다.

### [UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages](https://arxiv.org/abs/2411.14343)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14343.png)

Vote: 4

Authors: Tae-Sun Chung, Akhil Kedia, Bethel Melesse Tessema

- ***What's New***: UnifiedCrawl은 저자원이 사용가능한 언어에서 LLMs의 성능을 개선할 수 있는 합리적인 방법을 제시합니다. Common Crawl 전체 코퍼스를 효과적으로 활용하여, 저자원 언어를 위한 대규모 모노링구얼 데이터셋을 생성했습니다. 이 과정을 통해 저자원 언어의 LLMs를 설정하는 비용을 크게 절감할 수 있습니다.
- ***Technical Details***: UnifiedCrawl은 Common Crawl의 인덱스를 필터링하고 DuckDB를 사용하여 메모리 내 데이터베이스 관리의 효율성을 극대화합니다. Data Deduplication 기법을 사용하여 고품질의 텍스트 데이터를 확보하고, QLoRA와 같은 경량의 어댑터를 사용하여 멀티링구얼 LLMs를 미세조정(fine-tuning)합니다. 이 모든 과정이 10GB 이하의 RAM과 저장 공간을 사용하여 개인 소비자 하드웨어에서 수행될 수 있음을 보였습니다.
- ***Performance Highlights***: UnifiedCrawl로 수집한 데이터로 미세조정한 XGLM 모델은 Amharic 데이터셋에서 perplexity를 35.6에서 19.6으로 45% 개선시켰으며, down-stream Task로 few-shot Question-Answering에서는 F1 점수가 8.0에서 9.9로 증가했습니다. 이는 LLMs가 저자원 언어에서의 성능을 크게 향상시킬 수 있음을 시사합니다.

### [Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions](https://arxiv.org/abs/2411.14405)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14405.png)

Vote: 21

Authors: Yu Zhao, Chenyang Lyu, Weihua Luo, Longyue Wang, Huifeng Yin, Tianqi Shi, Kaifu Zhang, Bo Zeng, Hao Wang

- ***What's New***: Marco-o1는 개방형 정답을 가진 문제에 대처하기 위한 오픈 리즈닝 모델(Open Reasoning Models)을 목표로 하며, 과학 및 코딩과 같은 표준적 답이 있는 분야뿐 아니라 정답을 구체화하기 어려운 분야로 모델의 일반화 능력을 확장하려는 점에서 차별화됩니다. 체인 오브 톳(Chain-of-Thought; CoT) 파인튜닝과 몬테카를로 트리 탐색(Monte Carlo Tree Search; MCTS) 및 반영 메커니즘을 활용하여 복잡한 문제 해결을 향상시키는 것이 특징입니다.
- ***Technical Details***: Marco-o1는 Qwen2-7B-Instruct를 기반으로 CoT, MCTS 기술을 통해 전면적인 파라미터 파인튜닝을 거쳤습니다. MCTS는 소프트맥스를 적용한 로그 확률을 통해 여러 리즈닝 경로를 탐색하고, 리워드(score) 계산을 통해 최적 경로를 찾아냅니다. 또한, 자기 반영 메커니즘을 추가하여 모델이 스스로 생각하는 방식의 개선을 유도하며, 보다 미세한 그레이뉴러티 (granularity)로 인해 문제 해결 능력을 향상시킵니다.
- ***Performance Highlights***: MGSM(English) 데이터셋에서 Marco-o1는 6.17%, MGSM(Chinese) 데이터셋에서 5.60%의 정확도 향상을 이뤘습니다. 특히, 슬랭 번역 작업에서는 Marco-o1가 구글 번역보다 더 자연스러운 번역을 제공하며, 언어의 미세한 뉘앙스를 이해하는 능력을 보였습니다. 이는 정량적 리워드 신호를 정의하여 강화학습을 통해 모델의 성능을 더 향상시키려는 향후 연구 방향을 제공합니다.

### [Hymba: A Hybrid-head Architecture for Small Language Models](https://arxiv.org/abs/2411.13676)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.13676.png)

Vote: 16

Authors: Yonggan Fu, Pavlo Molchanov, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Jan Kautz, Yoshi Suhara, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Wonmin Byeon, Yingyan Lin, Shizhe Diao, Xin Dong

- ***What's New***: Hymba는 소형 언어 모델(Small Language Models)에 대한 새로운 하이브리드 헤드 아키텍처(Hybrid-head Architecture)를 제안합니다. 이 아키텍처는 고해상도 회상(high-resolution recall)을 제공하는 어텐션 메커니즘과 효율적인 문맥 요약(context summarization)을 돕는 State Space Models(SSMs)를 통합하여 효율성을 향상시킵니다. 또한, 프롬프트에 메타 토큰(meta tokens)을 추가하여 필수 정보를 저장하고 어텐션 메커니즘의 부담을 줄입니다.
- ***Technical Details***: Hymba는 어텐션 헤드와 SSM 헤드를 같은 레이어에 통합하여 입력을 병렬적으로 처리합니다. 또한, 각 레이어 간에 키-값(KV) 공유 및 슬라이딩 윈도우 어텐션을 통해 캐시 크기를 최소화합니다. 제안된 아키텍처는 다양한 설정에서 다른 아키텍처와 비교하여 우수한 성능을 보였습니다.
- ***Performance Highlights***: Hymba-1.5B-Base 모델은 하위 2B 모델들 중에서 최고의 성능을 나타내며, Llama-3.2-3B보다 평균 정확도가 1.32% 높습니다. 또한, 캐시 크기는 11.67배 줄었고, 처리량은 3.49배 향상되었습니다. 이는 Hymba의 하이브리드 아키텍처가 제시한 효율성과 정확성 모두에서 새로운 최고 성능(State of the Art)을 설정했음을 보여줍니다.

### [OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs](https://arxiv.org/abs/2411.14199)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14199.png)

Vote: 14

Authors: Weijia Shi, Kyle Lo, Minyang Tian, Matt Latzke, Wen-tau Yih, Jacqueline He, Graham Neubig, Pan Ji, Hannaneh Hajishirzi, Bohao Wu, Sergey Feldman, Pang Wei Koh, Luke Zettlemoyer, Rulin Shao, Yanyu Xiong, Akari Asai, Shengyan Liu, Joseph Chee Chang, Hao Tong, Doug Downey, Mike D'arcy, Amanpreet Singh, Dan Weld, Luca Soldaini, David Wadden

- ***What's New***: OPENSCHOLAR는 retrieval-augmented LMs과 함께 과학 질문에 대한 답변을 제공하는 시스템으로, 4,500만 개의 오픈 액세스 논문에서 관련 구절을 식별하고 인용 기반 응답을 생성합니다. 또한, SCHOLARQABENCH라는 다중 도메인 벤치마크를 도입하여 컴퓨터 과학, 물리학, 신경과학 및 생물의학 분야에서 2,967개의 전문가 작성 질의와 208개의 장문 답변으로 구성된 최초의 대규모 벤치마크를 제안합니다.
- ***Technical Details***: OPENSCHOLAR는 새로운 OPENSCHOLAR 데이터 저장소(OSDS)와 과학 문헌에 훈련된 검색기 및 재정렬기를 사용하여 관련 구절을 검색하고 인용을 통해 응답을 생성합니다. 이 시스템은 자가 피드백 추론 방법을 사용하여 성능을 지속적으로 개선합니다. 또 SCHOLARQABENCH 벤치마크를 통해 개방형 과학 질문에 대한 현실적이고 재현 가능한 평가를 가능하게 합니다. 4,500만 개의 논문과 2억 3천 7백만 개의 구절 임베딩을 포함한 데이터 저장소는 현재까지 공개된 가장 큰 과학 도메인의 공개 데이터 저장고입니다.
- ***Performance Highlights***: OPENSCHOLAR-8B는 정확도 면에서 GPT-4o보다 5%, PaperQA2보다 7% 더 우수하며, 특히 인용 정확도에서는 인간 전문가와 동등한 성능을 보였습니다. HUMANS 평가에서는 전문가들이 OPENSCHOLAR-8B와 OPENSCHOLAR-GPT4o의 응답을 각각 51% 및 70%의 경우에 전문가 작성 응답보다 선호했습니다. 이러한 결과는 OPENSCHOLAR의 응답이 보다 포괄적이고 잘 조직되어 있으며, 문헌 종합에서 특히 유용함을 나타냅니다.

### [Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models](https://arxiv.org/abs/2411.14257)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14257.png)

Vote: 5

Authors: Neel Nanda, Oscar Obeso, Senthooran Rajamanoharan, Javier Ferrando

- ***What's New***: 이 연구에서는 대형 언어 모델(Large Language Models; LLMs)에서 발생하는 환각(Hallucinations)에 대한 새로운 이해를 발견했습니다. 자주 발생하는 문제지만, 이에 대한 정확한 메커니즘이 잘 알려져 있지 않아서 이를 해결하는 데 한계가 있었습니다. 이 논문은 희소 오토인코더(Sparse Autoencoders; SAEs)를 사용하여 모델이 특정 엔티티(Entity)를 인지할 수 있는 형태로 모델의 내부 지식을 측정하며, 이를 통해 모델의 자발적 지식 거부(Refusal)와 잘난 체 하는 경향을 조절할 수 있음을 입증했습니다.
- ***Technical Details***: 희소 오토인코더를 사용하여 모델의 표현 공간에서 방향성을 발견하였고, 이 방향성은 모델이 엔티티에 대한 사실을 상기할 수 있는지 여부를 감지합니다. Gemma Scope SAEs는 다양한 엔티티 유형에 대한 일반화된 인지 방향을 보여주었고, 모델 훈련 후 채팅 모델에서 특히 지식 기반 거부들이 미세 조정 단계에서 다시 활용된다는 것 또한 주장했습니다.
- ***Performance Highlights***: Gemma 2 모델에 대한 실험에서 엔티티 인식 방향은 채팅 모델의 지식 거부 반응에 인과적 영향을 미쳤습니다. 모델의 거부를 유도하거나, 알지 못하는 엔티티에 대해 환각하게 만듦으로써 모델의 반응을 조절할 수 있었습니다. 이러한 결과는 LLMs의 환각 및 지식 재현 메커니즘에 대한 더 깊은 이해와 잠재적 해법을 제공합니다.

### [Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization](https://arxiv.org/abs/2411.10442)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10442.png)

Vote: 42

Authors: Lewei Lu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Yue Cao, Weiyun Wang, Jifeng Dai, Yu Qiao, Wenhai Wang, Yangzhou Liu, Zhe Chen

- ***What's New***: 본 연구에서는 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 추론 능력을 향상시키기 위해 혼합 선호 최적화(Mixed Preference Optimization; MPO) 방법을 도입하는 새로운 접근법을 제안합니다. 구체적으로, 고품질의 대규모 멀티모달 추론 선호 데이터셋인 MMPR를 자동화된 데이터 구축 파이프라인을 통해 생성하고, MLLMs에 MPO를 결합하여 멀티모달 Chain-of-Thought (CoT) 성능을 향상시켰습니다.
- ***Technical Details***: MPO는 선호 손실(Preference Loss), 품질 손실(Quality Loss), 생성 손실(Generation Loss)의 조합으로 정의되며, 각각의 손실 요소에 가중치를 할당하여 모델이 응답의 상대적 선호, 개별 응답의 절대 품질, 선호하는 응답 생성 과정을 학습합니다. DPO(Direct Preference Optimization)와 BCO(Binary Classifier Optimization)가 각각 선호 손실과 품질 손실로 선택되었습니다. 모델은 주어진 데이터에 대한 SFT(Supervised Fine-Tuning) 손실을 포함하여 학습되었습니다.
- ***Performance Highlights***: 본 연구의 InternVL2-8B-MPO 모델은 MathVista 벤치마크에서 67.0%의 정확도를 기록하며, InternVL2-8B에 비해 8.7 포인트의 성능 향상을 보여주었습니다. 이는 10배 더 큰 InternVL2-76B와 유사한 성능을 달성한 것으로, MPO 방법이 멀티모달 추론능력 향상에 효과적임을 시사합니다. 또한, POPE 및 복잡한 VQA 벤치마크에서도 성능이 향상되어, 일반적인 모델 역량도 개선되었습니다.

### [Multimodal Autoregressive Pre-training of Large Vision Encoders](https://arxiv.org/abs/2411.14402)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14402.png)

Vote: 22

Authors: Philipp Dufter, Mustafa Shukor, Joshua M. Susskind, Moin Nabi, Sai Aitharaju, Marcin Eichner, David Haldimann, Louis Béthune, Xiujun Li, Alexander T Toshev, Yinfei Yang, Alaaeldin El-Nouby, Michal Klein, Victor Guilherme Turrisi da Costa, Enrico Fini, Zhe Gan

- ***What's New***: 새로운 멀티모달 오토레그레시브 사전학습 방법 AIMV2가 소개되었습니다. 이는 이미지 패치와 텍스트 토큰을 자동회귀적으로 생성하여 대규모 비전 인코더를 효율적으로 사전학습할 수 있게 해 줍니다.
- ***Technical Details***: AIMV2는 Vision Encoder와 멀티모달 덧셈기를 조합하여 이미지 패치와 텍스트 토큰을 생성합니다. 이 과정에서 Causal Multimodal Decoder를 활용하여 이미지 패치를 먼저 레그레스(regress)한 후 텍스트 토큰을 디코드합니다. 이를 통해 LLM(대형 언어 모델) 기반의 멀티모달 응용 프로그램과도 원활히 통합될 수 있는 점이 특징입니다.
- ***Performance Highlights***: AIMV2-3B 인코더는 ImageNet-1k 데이터셋에서 동결된 트렁크로 89.5%의 정확도를 기록했습니다. 뿐만 아니라 다양한 멀티모달 이해 벤치마크에서 CLIP, SigLIP 등의 최첨단 대조 모델들을 능가하는 성능을 보여줍니다.

