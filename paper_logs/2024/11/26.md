## Daily Papers (2024-11-26)

### [Material Anything: Generating Materials for Any 3D Object via Diffusion](https://arxiv.org/abs/2411.15138)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15138.png)

Vote: 32

Authors: Xin Huang, Ziwei Liu, Tengfei Wang, Qing Wang

- ***What's New***: Material Anything은 3D 객체에 대해 물리 기반 소재 (Physically-Based Materials)를 생성할 수 있는 통합 확산 프레임워크 (Unified Diffusion Framework)입니다. 기존의 복잡한 파이프라인이나 케이스별 최적화를 필요로 하는 방법과 달리, 이 모델은 다양한 조명 조건에서 적용 가능한 강력한 엔드투엔드 솔루션을 제공합니다.
- ***Technical Details***: Material Anything은 사전 훈련된 이미지 확산 모델 (Pre-trained Image Diffusion Model)을 활용하여 트리플-헤드 아키텍처 (Triple-Head Architecture)와 렌더링 손실 (Rendering Loss)을 통해 안정성과 물질의 질을 개선합니다. 확신 마스크 (Confidence Masks)를 통해 조명의 불확실성을 동적으로 처리하며, 점진적 소재 생성 전략과 UV 공간 소재 보정기로 일관된 실시간 UV 준비 소재 출력을 보장합니다.
- ***Performance Highlights***: Material Anything은 다양한 객체 카테고리와 조명 조건에서 기존 방법보다 우수한 성능을 보였습니다. 이 모델은 두 가지 프로세스를 통해 고품질의 소재 맵을 생성하여 최첨단 성능을 달성했으며, 다양한 조명 조건을 하나의 모델로 처리할 수 있는 능력을 입증했습니다.

### [LLMs Do Not Think Step-by-step In Implicit Reasoning](https://arxiv.org/abs/2411.15862)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15862.png)

Vote: 3

Authors: Yijiong Yu

- ***What's New***: 이번 연구에서는 내재적 사슬 사고(Implicit Chain-of-Thought; CoT)의 실제 메커니즘을 실험적으로 밝혀내려 합니다. 연구 결과는 LLMs가 내재적 추론 과정에서 중간 단계를 단계별로 생각하지 않아, 경험에 의존할 뿐이라는 것을 발견하였으며, 이는 기존의 내재적 CoT가 명시적 CoT와 기능적으로 대등하지 않음을 보여줍니다.
- ***Technical Details***: 연구는 Qwen2.5-72B-Instruct 모델을 활용하여 다단계 산술 문제를 다루는 실험을 설계했습니다. 모델이 직접 중간 단계를 출력하지 않도록 유도하고, 문제의 중간 결과를 숨은 신경망 상태에서 추출하기 위해 선형 탐사(Linear Probing)를 적용하여 각 단계를 학습시키는 21-클래스 분류기를 사용하였습니다.
- ***Performance Highlights***: 실험 결과, LLMs는 다단계 문제의 중간 결과를 정확히 계산하지 않으며, 직접적으로 직관과 기억을 통해 정답을 도출합니다. 더욱이 문제를 약간 수정했을 때 모델의 내재적 추론 성능은 불안정해졌으며, 이는 내재적 추론이 명시적 CoT 추론과 달리 덜 신뢰할 수 있음을 제시합니다. 명시적 CoT를 통한 명확한 단계별 추론이 필요함을 강조합니다.

### [Cautious Optimizers: Improving Training with One Line of Code](https://arxiv.org/abs/2411.16085)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16085.png)

Vote: 6

Authors: Bo Liu, Lizhang Chen, Qiang Liu, Kaizhao Liang

- ***What's New***: Cautious Optimizers는 모멘텀 기반 옵티마이저에 대해 단일 라인 코드를 추가하여 성능을 향상시키는 새로운 접근 방식을 제안합니다. 이를 통해 부정적인 업데이트 방향이 현재의 기울기와 정렬되지 않는 경우, 업데이트를 수행하지 않도록 하여 AdamW와 같은 기존의 옵티마이저에 비해 학습 속도를 크게 증가시킬 수 있습니다.
- ***Technical Details***: Cautious Optimizer는 기존 옵티마이저와의 가벼운 결합으로, 주어진 업데이트 방향과 기울기의 부호가 일치하는 경우에만 업데이트를 수행하도록 합니다. 이 수정은 원래 옵티마이저의 수렴 보장을 손상시키지 않으며, Lyapunov 분석을 통해 일관성을 유지합니다. 주목할만한 점은, 가벼운 마스크 함수의 사용을 통해 업데이트가 이루어질 수 있는 신호를 효과적으로 관리할 수 있다는 것입니다.
- ***Performance Highlights***: LLaMA와 MAE 사전 학습에서 Cautious Optimizer를 사용하면 각각 1.47배 및 1.28배까지 훈련 속도가 증가하는 것을 확인했습니다. 이러한 성능 향상은 기존 방법에 비해 계산 비용이 늘지 않고, zusätzlichen 하이퍼파라미터 튜닝 없이 순수한 효율성 개선을 제공합니다.

### [Edge Weight Prediction For Category-Agnostic Pose Estimation](https://arxiv.org/abs/2411.16665)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16665.png)

Vote: 2

Authors: Shai Avidan, Or Hirschorn

- ***What's New***: EdgeCape는 카테고리 불가지론적 포즈 추정(Category-Agnostic Pose Estimation)에서 국부화 개선을 위해 그래프의 엣지 가중치를 예측하는 새로운 프레임워크로, 기존 정적 포즈 그래프의 제한을 극복합니다. 이를 통해 1-샷 및 5-샷 설정에서 주목할 만한 위치 정확도 향상을 달성하였습니다.
- ***Technical Details***: EdgeCape는 입력 포즈 그래프를 개선하고 엣지 가중치를 할당하여 구조에 민감한 스켈레톤을 제작합니다. 또한, 트랜스포머의 자가 집중(Self-Attention) 메커니즘에 그래프 구조를 통합하여 복잡한 공간적 의존성을 포착합니다. 학습은 사용자 주석 그래프를 기반으로 하고, 시험 시에는 사용자가 제공한 선험적 그래프를 기반으로 국부화를 개선합니다.
- ***Performance Highlights***: EdgeCape는 MP-100 벤치마크에서 100개의 다양한 객체 카테고리 및 20,000개 이상의 이미지에 대해 평가된 결과, 1-샷 환경에서 전례 없는 최첨단 결과를 달성하였으며, 5-샷 환경에서도 유사한 크기의 다른 방법론을 앞서는 성능을 보였습니다. 또한 다양한 개체의 기하학적 구조 처리 능력이 입증되었습니다.

### [Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator](https://arxiv.org/abs/2411.15466)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15466.png)

Vote: 28

Authors: Jooyoung Choi, Sungroh Yoon, Heeseung Kim, Chaehun Shin

- ***What's New***: 이 논문은 대규모 텍스트-이미지 모델이탑재된 새로운 제로-샷 주제 기반 이미지 생성 방법론인 Diptych Prompting을 제안합니다. 이를 통해 주제 정렬을 정확히 유지하는 동시에 텍스트 프롬프트와 주제를 맞춤화한 이미지 생성을 가능하게 합니다.
- ***Technical Details***: 기존 대규모 텍스트-이미지 모델들의 Diptych 생성 기능을 활용하여 이를 보완하는 인페이팅(Inpainting) 모듈을 이용합니다. 이 방법은 참조 이미지와 텍스트 프롬프트를 구성한 Diptych 이미지를 생성 부분으로 나누어 인페이팅을 진행합니다. 특히 참조 이미지에서 배경을 제거하여 불필요한 내용 누출을 방지하고, 좌우 패널 간 주의 가중치를 강화하여 세부 사항을 개선합니다.
- ***Performance Highlights***: 실험 결과, 제안된 Diptych Prompting은 기존 제로-샷 이미지 프롬프트 방법들보다 높은 선호도를 보이는 이미지를 생성하며, 사용자의 시각적으로 선호되는 결과를 만듭니다. 이 방법은 또한 스타일 기반 이미지 생성 및 주제 중심 이미지 편집을 지원하여 다양한 이미지 생성 응용 분야에의 적응성을 증명합니다.

### [From CISC to RISC: language-model guided assembly transpilation](https://arxiv.org/abs/2411.16341)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16341.png)

Vote: 6

Authors: Rania Hossam, Chaimaa Abi, Abdulrahman Mahmoud, Ahmed Heakl

- ***What's New***: 이 논문은 CRT라는 경량 LLM 기반의 transpiler를 소개합니다. 이 도구는 x86 어셈블리를 ARM 어셈블리로 자동 변환하여, 무거운 가상화 계층 없이 직접적인 ISA 변환을 가능하게 합니다. 이는 특히 x86의 CISC와 ARM의 RISC 간의 근본적 아키텍처 차이를 극복하면서 프로그램의 의미를 보존하고 성능을 최적화하는 하드웨어-소프트웨어 통합 솔루션을 제공합니다.
- ***Technical Details***: CRT는 500k AnghaBench 데이터세트를 활용해 x86과 ARM 어셈블리의 매핑을 학습하도록 설계되었습니다. LLM이 x86 코드와 ARM 코드 간의 조건부 분포를 학습하여, 문맥적으로 정확한 번역을 생성합니다. DeepSeekCoder, Yi-Coder와 같은 다양한 오픈 소스 모델을 조정하고 실험하여 최적의 하이퍼파라미터 세팅을 찾았습니다. 실행 시 LLM의 효율성을 높이기 위해 확장된 토크나이저를 사용하여 어셈블리 코드에 대한 인식을 향상시켰습니다.
- ***Performance Highlights***: CRT는 x86에서 ARMv5로 79.25%의 번역 정확도를 달성했으며, ARMv8 기반 Apple M2 하드웨어에서 Rosetta 2 가상화 엔진보다 1.73배 빠른 실행 속도를 보였습니다. 또한, 에너지 소비는 1.47배, 메모리 효율성은 2.41배 향상되었습니다. 이는 CRT가 CISC와 RISC 간의 장벽을 성공적으로 넘었음을 나타내며, 향후 다양한 ISA 간의 효율적인 코드를 생성하는 가능성을 엽니다.

### [One Diffusion to Generate Them All](https://arxiv.org/abs/2411.16318)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16318.png)

Vote: 10

Authors: Ranjay Krishna, Stephan Mandt, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Jiasen Lu, Tuan Pham, Duong H. Le

- ***What's New***: OneDiffusion은 다양한 작업에서 이미지 합성과 이해를 동시에 지원하는 대형 통합 확산 모델(Diffusion Model)입니다. 텍스트, 깊이, 자세, 레이아웃, 시맨틱 맵 등 다양한 입력을 통한 조건부 생성과 이미지 디블러링(deblurring), 업스케일링(upscaling)과 같은 작업을 수행하며, 멀티뷰 생성(multi-view generation), 카메라 자세 추정(camera pose estimation) 등도 지원합니다.
- ***Technical Details***: OneDiffusion은 모든 작업을 프레임 시퀀스의 형태로 훈련하는 단순하면서도 효과적인 접근 방식을 채택했습니다. 다양한 노이즈 스케일을 통해 프레임을 조건부 이미지로 사용하며, 이는 특히 특정 아키텍처를 필요로 하지 않으면서 다양한 작업을 스케일에 맞춰 처리할 수 있도록 합니다. 이 모델은 Next-DiT 아키텍처를 활용하여 다양한 수의 뷰를 지원합니다.
- ***Performance Highlights***: OneDiffusion은 텍스트-이미지 생성(Text-to-Image Generation), 멀티뷰 생성, 깊이 추정(depth estimation), 카메라 자세 추정 등 다양한 작업에서 경쟁력 있는 성능을 발휘합니다. 특히, Zero123와 Zero123-XL과 같은 최첨단 방법과 비교하여 비슷한 성능을 보여주며, 동일 크기의 다른 모델에 비해 다양한 작업을 소화할 수 있는 능력을 가지고 있습니다. 또한, 생성을 위한 조건 설정의 유연성을 보여주었습니다.

### [Predicting Emergent Capabilities by Finetuning](https://arxiv.org/abs/2411.16035)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16035.png)

Vote: 5

Authors: Sergey Levine, Dan Klein, Eric Wallace, Charlie Snell

- **What's New**: 이번 논문에서는 대형 언어 모델(LLM; Large Language Models)의 확장 과정에서 발생하는 '출현 능력(emergent capabilities)'을 예측하는 새로운 방법을 제안합니다. 연구자들은 기존의 LLMs를 소형 모델로 미세 조정(finetuning)하여 이후에 발생할 출현 능력을 예측하는 방법을 개발했습니다.
- **Technical Details**: 이 연구에서는 출현 지점을 예측하기 위해 '출현 법칙(emergence laws)'이라는 함수적 형식을 개발했습니다. 이 함수는 데이터 양에 따라 출현 지점이 어떻게 변화하는지를 모델링합니다. 출현 법칙을 사용하여 몇 가지 표준 NLP 벤치마크(MMLU, GSM8K, CommonsenseQA, CoLA)에서 검증을 수행하였습니다.
- **Performance Highlights**: 실험 결과, 소형 LLMs만으로도 최대 4배의 계산(1/4 FLOPS)의 증가가 필요로 하는 더 강력한 모델의 출현을 정확히 예측할 수 있었습니다. 또한, 이 접근법은 향후 더 복잡한 기능을 가진 LLMs의 능력 예측에도 활용될 수 있음을 보여주었습니다.

### [MH-MoE:Multi-Head Mixture-of-Experts](https://arxiv.org/abs/2411.16205)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16205.png)

Vote: 15

Authors: Shuming Ma, Shaohan Huang, Xun Wu, Furu Wei

- ***What's New***: MH-MoE(Multi-Head Mixture-of-Experts)는 여러 표현 공간으로부터 정보를 집합적으로 수집하도록 다중 헤드 메커니즘을 개선하여 성능을 향상시키는 새로운 접근 방식입니다. 본 연구에서는 희소(MoE) 모델과의 FLOPs 동등성을 유지하는 새로운 MH-MoE 구현을 제안합니다. 실험 결과, 이 새로운 구현은 기존의 MoE 및 세분화된 MoE 모델보다 품질이 개선된다는 것을 보여줍니다.
- ***Technical Details***: MH-MoE는 토큰 차원에 '헤드' 차원을 추가하고 MoE 레이어의 시작과 끝에 두 개의 선형 투영층을 통합하여 기존의 Sparse Mixture-of-Experts와 비교해 두 가지 주요 수정 사항을 도입합니다. 입력은 먼저 선형 레이어에 의해 투영되어 서브-토큰으로 나뉘며, MoE 레이어에 의해 E개의 전문가와 게이트 함수에 의해 처리되어 최종 출력을 생성합니다. 이 모델은 두 가지 변형으로 실험되었으며, FLOPs와 모델 파라미터가 표준 SMoE 모델과 비교 가능하게 유지합니다.
- ***Performance Highlights***: MH-MoE는 1-bit LLM(BitNet)과의 호환성이 확인되었으며, RedPajama, Wiki, C4 데이터셋에서 낮은 perplexity를 기록하여 성능 향상을 보였습니다. 특히, 헤드가 3일 때 MH-MoE는 다른 모델 구성보다 더 우수한 성능을 발휘했습니다. 실험 결과는 MH-MoE가 BitNet과 효과적으로 통합되어 모델 성능을 저하시키지 않으면서도 경량 배포를 가능케 함을 보여주었습니다.

### [GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI](https://arxiv.org/abs/2411.14522)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14522.png)

Vote: 11

Authors: Yanzhou Su, Zhe Chen, Bin Fu, Ming Hu, Junjun He, Chenglong Ma, Xiaowei Hu, Guoan Wang, Yuanfeng Ji, Pengcheng Chen, Zhongying Deng, Yu Qiao, Jin Ye, Tianbin Li, Yanjun Li, Wei Li, Ying Chen, Ziyan Huang

- ***What's New***: GMAI-VL 및 GMAI-VL-5.5M은 일반 의료 AI(General Medical AI)를 목표로 하는 대형 시각언어 모델(Vision-Language Model)과 포괄적인 멀티모달 의료 데이터셋으로 소개되었습니다. GMAI-VL-5.5M은 수백 가지의 전문 의료 데이터셋을 정교하게 변환하여 고품질의 이미지-텍스트 쌍으로 새롭게 구성한 데이터셋입니다. 이를 바탕으로 GMAI-VL은 시각 및 텍스트 정보를 통합하여 정확한 진단 및 임상 결정 지원에 기여합니다.
- ***Technical Details***: GMAI-VL-5.5M은 다양한 의료 이미지 형태와 텍스트 데이터를 포함, 13가지 의료 이미지 모달리티와 18개의 전문 분야를 다루는 5.5M 샘플을 포함한 종합적인 멀티모달 의료 데이터셋입니다. GMAI-VL 모델은 세 가지 단계의 훈련 전략을 통해 시각적 및 텍스트적 특징을 점진적으로 통합하여 모델의 멀티모달 데이터를 처리하는 능력을 크게 향상시킵니다. 데이터 품질을 보장하기 위해 세심한 주석 기반 데이터 생성 방법론이 사용되었습니다.
- ***Performance Highlights***: GMAI-VL은 다양한 멀티모달 의료 과제에서 최첨단 결과를 달성했으며, 특히 OmniMedVQA와 같은 벤치마크에서 평균 88.48%의 정확도를 기록하며 기존 모델을 능가했습니다. 다양한 진단 및 생물학적 속성 인식 과제에서 유연성을 보여줍니다. 이 결과들은 GMAI-VL이 의료 이미지 이해와 질의 응답에서 탁월한 성능을 발휘함을 시사하며, 의료 분야에서의 사용자 가능성을 넓힙니다.

### [O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?](https://arxiv.org/abs/2411.16489)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16489.png)

Vote: 13

Authors: Weizhe Yuan, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Yiwei Qin, Pengfei Liu, Zhen Huang, Shijie Xia, Haoyang Zou

- ***What's New***: 본 논문은 OpenAI의 O1 모델의 기능을 복제하는 현재 접근법에 대한 비판적 검토를 제공합니다. 특히 단순한 Distillation이 어떻게 복잡한 수학적 추론 작업에서 우수한 성능을 달성할 수 있는지를 설명합니다. 연구는 O1의 API를 이용한 Distillation과 감독된 Fine-Tuning이 결합되어 놀라운 성능 향상을 가능하게 한다고 밝히고 있습니다. 이를 통해 O1-preview를 초과하는 성과를 이끌어내는 것을 목표로 합니다.
- ***Technical Details***: O1 모델의 복제를 위한 핵심 기술 스택(Core Technical Stack)으로서 단순한 Distillation을 강조하며, O1의 복잡한 문제를 통해 생성된 long-thought chains을 활용한 감독된 Fine-Tuning 과정을 설명합니다. Tree 검색 알고리즘, 거시적 노드 선택을 통해 다양한 솔루션 경로를 탐색하는 'journey learning' 기법을 소개하며 LLMs 분석 및 교정을 통해 보다 정확한 답변을 이끌어냅니다. 또 다른 일반화 방법들은 인간의 사고 과정과 멀티 에이전트 접근법 등을 포함합니다.
- ***Performance Highlights***: 수학적 추론 능력을 평가할 때, Distillation된 모델은 AIME2024에서 O1-preview를 초과하는 성과를 보여주었습니다. 그러나 다른 작업에서는 여전히 많은 발전 여지가 있으며 안전성 및 사실성 개선이 필요합니다. 특히 WildSafety 데이터셋에서 성능 저하가 발생하며, 이는 훈련 데이터의 안전성 정렬 부족으로 인한 결과로 해석됩니다.

### [Factorized Visual Tokenization and Generation](https://arxiv.org/abs/2411.16681)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16681.png)

Vote: 10

Authors: Jianxiong Gao, Ziteng Gao, Zechen Bai, Zheng Zhang, Pichao Wang, Tong He, Mike Zheng Shou

- ***What's New***: 이 논문에서는 Factorized Quantization (FQ)라는 새로운 접근법을 통해 VQ기반의 비주얼 토크나이징(Visual Tokenization)을 개선하였습니다. 이는 큰 코드북을 여러 독립적인 서브 코드북(sub-codebook)으로 분해하여 코드북의 크기에 따른 불안정성을 획기적으로 줄이고, 보다 효율적이고 확장 가능하게 합니다.
- ***Technical Details***: 제안된 FQGAN 모델은 VQ기반 토크나이저들이 자주 직면하는 대규모 코드북 관리의 단점을 해결하기 위해 토크나이징 공정을 간소화합니다. 특히, 각 서브 코드북이 상호 보완적이고 독립적인 정보를 캡처할 수 있도록 구조적 비슷류화 규제가 도입되었습니다. 또한, 사전 학습된 비전 모델(CLIP, DINO)에서 표현 학습을 통합하여 의미론적 풍부함을 학습된 표현에 주입합니다.
- ***Performance Highlights***: FQGAN 모델은 시각적 토크나이저의 재구성 품질을 크게 향상시켜 최첨단 성능을 달성하였으며, ImageNet 벤치마크에서 강력한 성능을 보였습니다. 제안된 토크나이저는 오토리그레시브(AR) 이미지 생성에 효과적으로 적응 가능하며, 이는 이미지 생성 품질을 크게 개선합니다.

### [Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline](https://arxiv.org/abs/2411.12814)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12814.png)

Vote: 6

Authors: Yanzhou Su, Bin Fu, Ruoyu Li, JingWen Li, Junjun He, Junren Chen, He Yao, Guoan Wang, Min Zhu, Jin Ye, Tianbin Li, Junlong Cheng, Haoyu Wang

- ***What's New***: IMed-361M은 인터랙티브 메디컬 이미지 분할(Interactive Medical Image Segmentation; IMIS)을 위한 새로운 벤치마크 데이터셋입니다. 기존의 제약을 극복하며, 6.4백만 개의 의료 이미지를 포함하고 14개의 이미지 모달리티와 204개의 분할 타겟으로 구성되어 총 3억 6천 1백만 개의 마스크를 제공합니다.
- ***Technical Details***: IMed-361M 데이터셋은 여러 데이터 소스에서 수집된 의료 이미지와 그라운드 트루스 마스크를 활용하여 자동으로 인터랙티브 마스크를 생성합니다. IMIS-Net은 ViT-base를 이미지 인코더로 사용하며, 포인트, 박스, 텍스트의 다양한 상호작용 입력을 지원하는 프롬프트 인코더로 구성됩니다.
- ***Performance Highlights***: IMIS-Net은 기존의 기초 모델들과의 비교 실험에서 뛰어난 성능을 보여주었습니다. 특히, 다양한 상호작용 전략을 통한 분할 성능을 평가한 결과, 평균 Dice 점수가 다른 모델보다 높았습니다. 외부 데이터셋 평가에서도 다양한 의료 시나리오와 상호작용 방법에 대한 강력한 일반화 능력을 입증했습니다.

### [From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge](https://arxiv.org/abs/2411.16594)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16594.png)

Vote: 17

Authors: Alimohammad Beigi, Zhen Tan, Amrita Bhattacharjee, Canyu Chen, Bohan Jiang, Dawei Li, Chengshuai Zhao, Kai Shu, Yuxuan Jiang, Tianhao Wu, Lu Cheng, Huan Liu, Liangjie Huang

- ***What's New***: LLM-as-a-judge는 대형 언어 모델(LLM)을 활용하여 다양한 평가 작업에서 점수 매기기, 순위 평가 및 선택을 수행하는 새로운 패러다임으로 주목받고 있습니다. 이 연구는 LLM을 평가자로 사용하는 방법을 체계적으로 조사하고, 평가 기준의 정의, 평가 방법론, 응용 분야 등을 다차원적으로 분석하여 이 분야의 발전을 도모하고 있습니다.
- ***Technical Details***: LLM-as-a-judge 연구는 평가 시스템에서 LLM이 어떤 속성을 평가해야 하는지를 정의하고(속성), 평가 방법론을 튜닝 방법과 프로팅 기술로 분류하여 설명합니다. 튜닝에는 인간이 레이블 표시한 데이터를 사용한 방법과 합성 피드백 데이터를 통한 학습 방법이 포함되며, 프로팅 기술은 스와핑 오퍼레이션, 규칙 증대, 다중 에이전트 협력 등을 활용합니다.
- ***Performance Highlights***: 이 논문의 평가 벤치마크는 LLM-as-a-judge의 성능을 다각도로 측정하는 다양한 벤치마크를 제공합니다. 이러한 벤치마크는 일반 성능, 편향성 평가, 특정 도메인 성능, 멀티모달 평가 등을 다루며, LLM이 다양한 작업에서 인간의 평가와 얼마나 일치하는지를 측정합니다.

### [The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz](https://arxiv.org/abs/2411.14486)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14486.png)

Vote: 6

Authors: Forrest McKee, David Noever

- ***What's New***: 실현 불가능한 문제 675개로 구성된 '불가능한 테스트(The Impossible Test)'가 소개되었습니다. 이 데이터셋은 대형 언어 모델(LLMs)이 불확실성을 인식할 수 있는 능력을 평가하기 위해 설계되었으며, 문제가 해결되지 못함을 인정하는 것을 관찰합니다. 이는 AGI 평가에 대한 새로운 방향을 제시합니다.
- ***Technical Details***: 데이터셋은 생물학부터 수학까지 다양한 분야의 고급 과제들로 구성되어 있으며, 모든 문제의 정답은 '인간이 이를 모른다'거나 '현재로선 해결 불가능하다'입니다. 12개의 최신 LLM들이 테스트에 사용되었으며, 이 모델들은 49개의 카테고리로 조직화된 문제를 해결했습니다. 'I don't know' 옵션을 포함한 다지선다(multiple-choice) 형식을 채택하여 모델이 불확실성을 얼마나 잘 인정할 수 있는지를 측정했습니다.
- ***Performance Highlights***: GPT-4는 더 어려운 문제(35.8%)에서 더 높은 불확실성 인식률을 보이는 반면, 더 쉬운 문제(20.0%)에서는 낮은 인식률을 보였습니다. 모델들은 발명과 NP-하드 문제를 해결하는 데 어려움을 겪었고 철학 및 심리학 문제에서는 상대적으로 좋은 성과를 보였습니다. 이 결과는 모델의 현재 한계를 나타내며 AGI 평가 및 훈련 방법 개선의 필요성을 시사합니다.

### [Reflections from the 2024 Large Language Model (LLM) Hackathon for Applications in Materials Science and Chemistry](https://arxiv.org/abs/2411.15221)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15221.png)

Vote: 4

Authors: Nathan Daelman, Xuefeng Liu, Ian Foster, Vraj Patel, Alessandro Canalicchio, Julia Schumann, Dieter Plessers, Katharina Ueltzen, Tehseen Rug, Sebastian Pagel, Aritra Roy, Jaehee Park, Seyed Mohamad Moosavi, Hasan M Sayeed, Archit Vasan, Mara Schilling-Wilhelmi, Ankur K. Gupta, Kevin Shen, Trupti Mohanty, Giuseppe Fisicaro, Pradip Si, Alexander Moßhammer, Sheng-Lun Mark Liao, Piyush Ranjan Maharana, Aleyna Beste Ozhan, Abdelrahman Ibrahim, Teslim Olayiwola, Shagun Maheshwari, Rob Mills, Matthew L. Evans, Dana O'Connor, Suneel Kuman, Dmytro Antypov, Adib Bazgir, Jennifer D'Souza, Yoel Zimmermann, Soroush Mahjoubi, Amirhossein D. Naghdi, Alvin Noe Ladines, L. Catherine Brinson, Jose D. Cojal Gonzalez, Anastasiia Tsymbal, Sylvester Zhang, Jiale Shi, Jan Janssen, Ramsey Issa, Sarom Leang, Martin Hoffmann Petersen, Anna Borisova, Philippe Schwaller, Martiño Ríos-García, Oleksandr Narykov, Suraj Sudhakar, Amro Aswad, Maryam Ghazizade Fard, Janine George, Weijie Zhang, Trung Vo, Luis Pinto, Tyler R. Josephson, Sascha Klawohn, Xuan Vu Nguyen, Andrew Qin, Chi Zhang, Shang Zhu, Faradawn Yang, Carla Terboven, Ghazal Khalighinejad, Magdalena Lederbauer, Samiha Sharlin, Kedar Dabhadkar, Markus Scheidgen, Tirtha Vinchurkar, Greg Juhasz, Vikrant Chaudhary, Gabriel Vogel, Federico Ottomano, Judith Clymo, Mehrad Ansari, Hassan Harb, Qianxiang Ai, Ruijie Zhu, Ben Blaiszik, Hampus Näsström, Pengyu Hong, Rongda Kang, Dandan Tang, Utkarsh Pratiush, Zartashia Afzal, Victor Chen, Fabian Schöppach, Sarthak Kapoor, Jiaru Bai, Francesco Ricci, Kevin Maik Jablonka, Kevin Ishimwe, Leopold Talirz, Devi Dutta Biswajeet, Jan Weinreich, Charishma Puli, Christoph Völker, Alishba Imran, Erik Bitzek, Andres M Bran, Elena Patyukova, Abhijeet Sadashiv Gangan, Marcus Schwarting, Joshua D. Bocarsly, Benjamin Charmes, Mohd Zaki, Michael Götte, Marcel Moran Calderon, Sartaaj Khan, José A. Márquez, Min-Hsueh Chiu, Chiku Parida, Nawaf Alampara, Stanley Lo, Aakash Naik, Mark Tropin, Ahmed Ilyas, Tapashree Pradhan, Pablo Andres Unzueta, Zizhang Chen, Sandeep Madireddy, Alexander Al-Feghali, Elliot Risch, Colin Jones, Hao Liu, Taylor Sparks, Viktoriia Baibakova, Fariha Agbere, Yuan Chiang, Xinyi Ni, Marcel Schloz, Mahyar Rajabi, Olga Taran, José M. Pizarro, Archit Datar, Defne Circi, Bernadette Mohr

- **What's New**: 2024년의 대형 언어 모델(LLM) 해커톤에서 재료과학과 화학 응용을 위한 새로운 접근 방식과 아이디어가 논의되었습니다. 참가자들은 LLM이 재료과학 및 화학 분야의 다양한 응용에 어떻게 사용될 수 있는지를 탐구하고 혁신적인 프로토타입을 개발했습니다.
- **Technical Details**: 해커톤은 34개의 팀 제출물을 받았고, 분자 및 물질 속성 예측, 분자 및 물질 설계, 과학적 커뮤니케이션 및 교육 강화와 같은 주요 응용 분야로 분류되었습니다. LLM 활용을 통한 실험 자동화와 새로운 인터페이스 개발도 이루어졌습니다. RAG(Retrieval-Augmented Generation) 기술을 통해 LLM의 성능을 향상시킨 사례도 있습니다.
- **Performance Highlights**: MLLLM 기반 데이터 관리 도구인 yeLLowhaMmer는 전자 실험실 노트북(ELN)과 실험실 정보 관리 시스템(LIMS)과 통합하여 데이터 처리 및 관리 작업을 자동화하고 있습니다. 최신 MLLM을 활용해 사용자 입력에 따라 실험 데이터 정리, 구조화, 요약을 자동으로 수행할 수 있습니다.

### [SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image Segmentation](https://arxiv.org/abs/2411.14525)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14525.png)

Vote: 6

Authors: Yanzhou Su, Junjun He, Chenglong Ma, Yuanfeng Ji, Zhongying Deng, Jin Ye, Yanjun Li, Ying Chen, Ziyan Huang, Haoyu Wang

- ***What's New***: SegBook은 의료 영상 분할을 위한 새로운 대규모 평가 벤치마크이며, 이는 전신 CT(Computed Tomography) 사전 학습 모델의 다양한 후속 의료 분할 작업에의 전이 가능성을 평가하기 위한 것입니다. 이 연구는 모델이 다른 이미지 모달리티(모달리티)와 다양한 목표 타겟으로 얼마나 잘 전이할 수 있는지를 조사합니다.
- ***Technical Details***: SegBook에서는 87개의 공개 데이터 세트를 수집하여 다양한 모달리티(CT, MRI, PET 등)와 목표(구조, 병변 등)를 바탕으로 STU-Net라는 모델을 사용하여 전이 학습을 수행합니다. STU-Net은 확장 가능한 아키텍처로 다양한 모델 크기(베이스, 라지, 휴지)와의 전이 학습을 평가합니다. 전이 학습 평가를 위해 모델은 사전 학습한 가중치를 기반으로 작은 케이스와 큰 케이스에 대한 미세 조정을 통해 학습됩니다.
- ***Performance Highlights***: STU-Net 모델은 다양한 데이터셋 규모에서 일관된 성능 향상을 보이며, 특히 모든 데이터 크기에서 미세 조정을 통해 성능이 향상됩니다. 특히, 큰 모델(STU-Net-large 및 STU-Net-huge)이 심층 학습 모델에서 경쟁력 있는 성능을 나타냈으며, 모든 데이터 범위에서 더 높은 평균 성능을 보였습니다. 전이 학습 결과, CT 구조 데이터에 대한 사전 학습이 다양한 의료 타겟 탐지에 효과적임을 증명했습니다.

### [Find Any Part in 3D](https://arxiv.org/abs/2411.13550)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.13550.png)

Vote: 2

Authors: Georgia Gkioxari, Ziqi Ma, Yisong Yue

- ***What's New***: FIND3D는 텍스트 쿼리에 따라 어떤 3D 객체의 부품이든 식별 가능하게 하는 새로운 오픈월드 파트 세분화 모델입니다. 이것은 기존의 객체 카테고리나 부품 어휘에 제한되지 않아, 모든 객체와 부품을 직접 예측할 수 있는 능력을 갖추고 있습니다. 우리는 온라인의 대규모 3D 자산을 활용하여, 사람의 주석 없이 FIND3D를 훈련시켰습니다.
- ***Technical Details***: FIND3D는 2D 기초 모델인 SAM과 Gemini에 의해 온라인 3D 자산에 자동 주석을 붙이고, 대조적 학습 방법을 사용해 훈련되었습니다. 이 모델은 점군 데이터(Point Cloud)를 입력받아 텍스트 쿼리에 반응하는 의미론적 피처를 각 점마다 예측하는 데 사용됩니다. 주어진 텍스트 쿼리에 대한 답변은 각 점의 피처와 쿼리 임베딩 사이의 코사인 유사도를 계산하여 이루어집니다.
- ***Performance Highlights***: FIND3D는 기존 방법보다 최대 3배 높은 mIoU 성능과 최대 300배 더 빠른 추론 속도를 보여주었습니다. 일반 객체나 다양한 자세에도 강력한 일반화를 보여 주었으며, 실질적인 애플리케이션에서의 활용 가능성을 입증했습니다. 특히, FIND3D는 완전 새로운 객체에서도 뛰어난 성능을 발휘하며, 고유의 챌린지에 대하여 현저하게 잘 대처합니다.

### [VisualLens: Personalization through Visual History](https://arxiv.org/abs/2411.16034)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16034.png)

Vote: 10

Authors: Deqing Fu, Kanika Narang, Kai Sun, Yue Liu, Anuj Kumar, Xin Luna Dong, Wang Bill Zhu, Seungwhan Moon, Zhaojiang Lin, Mustafa Canim, Yi Lu

- ***What's New***: VisualLens는 사용자의 시각적 역사를 활용하여 개인화된 추천을 제공하는 혁신적인 접근 방식입니다. 이는 기존 추천 시스템이 주로 텍스트 신호에 의존했던 것에 비해, 이미지 데이터를 필터링하고 정제하여 사용자 관심사를 추론하는 방향으로 발전한 것입니다.
- ***Technical Details***: VisualLens는 사용자 촬영 사진을 기반으로 기존의 대규모 시각 로그를 효과적으로 대체하여, 필요 저장 공간을 대폭 줄이면서도 사용자의 관심사와 선호도를 반영하는 중요한 신호를 제공합니다. 사용자의 시각적 히스토리를 기반으로 다양한 이미지를 추출하고, 이를 토대로 측면 단어(aspect words)를 생성하여 보다 정밀한 개인화 추천을 진행합니다. LLaVA-v1.6 모델을 수용하여 이미지 캡션을 생성하고, MiniCPM-v2.5 등을 통한 다중 이미지 캡션 프리트레이닝 과정을 거칩니다.
- ***Performance Highlights***: VisualLens는 두 가지 새롭게 생성한 벤치마크인 Google Review-V 및 Yelp-V에서 기존 최신 연구보다 5-10% 높은 성능을 기록했습니다. GPT-4o와 비교해서도 Hit@3에서 1.6%에서 4.6%까지 개선되었습니다. 특히, PaliGemma와 MiniCPM-V2.5 모델을 사용한 실험에서 각각 Google Review-V 기준으로 Hit@10에서 82-91% 성능을, Yelp-V 기준으로 90-91% 성능을 보여 우수한 개인화 추천 품질을 나타냈습니다.

### [Knowledge Transfer Across Modalities with Natural Language Supervision](https://arxiv.org/abs/2411.15611)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15611.png)

Vote: 13

Authors: Emanuele Aiello, Marco Grangetto, Carlo Alberto Barbano, Luca Molinaro

- ***What's New***: 이 논문에서는 새로운 개념을 텍스트 설명만으로 학습하는 방법인 Knowledge Transfer를 제안합니다. 이는 인간의 지각과 유사하게 크로스 모달(Cross-Modal) 상호작용을 활용하여 새로운 개념을 소개합니다. 사전 학습된 비주얼 인코더(Visual Encoder)에 이미 학습된 저수준 특징들이 있어, 이를 고수준 텍스트 설명과 정렬하여 새로운 개념을 효율적으로 도입하는 방법이 개발되었습니다.
- ***Technical Details***: Knowledge Transfer는 사용자에게 제공된 텍스트 설명을 바탕으로 비주얼 인코더의 저수준 특징들을 고수준 텍스트 설명에 맞게 조정합니다. 이러한 방식은 모델 아키텍처에 대한 엄격한 요구사항이 적기 때문에 다양한 구조에 폭넓게 적용될 수 있으며, 잘 알려진 예로 CLIP 같은 모델에서 사용 가능합니다. 모델 반전을 통해 이미 구체화된 이미지를 사용하여 간접적인 형태로도 Knowledge Transfer를 적용할 수 있습니다.
- ***Performance Highlights***: Knowledge Transfer는 사전 학습된 멀티모달 모델에서 새로운 개념을 소개하는 데 성공적이며, 기존의 모델 성능을 저하시키지 않고도 새로운 개념을 도입할 수 있습니다. 또한, 분류, 세분화, 이미지-텍스트 검색 및 캡셔닝과 같은 제로샷 다운스트림 작업에서 성능을 향상시키며, 도메인 외의 일반화 가능성도 보입니다.

### [Best of Both Worlds: Advantages of Hybrid Graph Sequence Models](https://arxiv.org/abs/2411.15671)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15671.png)

Vote: 4

Authors: Bryan Perozzi, Ali Parviz, Mahdi Karami, Ali Behrouz, Vahab Mirrokni, Clayton Sanford

- ***What's New***: 하이브리드 그래프 시퀀스 모델(Hybrid Graph Sequence Models)이 다른 시퀀스 모델을 통합하여 그래프 데이터 학습에 대한 새로운 접근 방식을 제안합니다. 이 모델은 시퀀스 기반 백본의 장단점을 그래프 작업의 맥락에서 분석하고 평가할 수 있는 통일된 프레임워크를 제공합니다.
- ***Technical Details***: 그래프 시퀀스 모델(Graph Sequence Model; GSM)은 세 가지 주요 단계로 구성됩니다: (1) 토크나이제이션(Tokenization), (2) 로컬 인코딩(Local Encoding), (3) 글로벌 인코딩(Global Encoding)입니다. GSM++는 계층적 친화 클러스터링(Hierarchical Affinity Clustering; HAC)을 사용하여 그래프를 계층적 시퀀스로 변환하며, 하이브리드 구조를 통해 시퀀스를 인코딩합니다. 이는 이론적 및 실험적 결과를 통해 기존 모델보다 다양한 벤치마크 평가에서 우수하다고 입증되었습니다.
- ***Performance Highlights***: GSM++ 모델은 대부분의 벤치마크 평가에서 엄청난 성능 향상을 보여주었습니다. 이는 특히 분자 특성 예측과 같은 복잡한 그래프 작업에서 선도적인 결과를 나타내었으며, 기존의 메시지 패싱 신경망(Message Passing Neural Networks; MPNNs) 보다 글로벌 및 로컬 구조를 더욱 효과적으로 캡처하였습니다.

### [DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation](https://arxiv.org/abs/2411.16657)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16657.png)

Vote: 13

Authors: Jaehong Yoon, Mohit Bansal, Zun Wang, Jialu Li, Han Lin

- ***What's New***: DreamRunner는 스토리텔링 영상 생성(Storytelling Video Generation; SVG)에서 모션 맞춤화(Motion Customization)를 위한 검색 기반의 적응기법(Retrieval-Augmented Adaptation)을 새롭게 도입하여, 여러 캐릭터가 등장하는 복잡하고 유동적인 장면의 영상을 생성합니다. 이로써 스토리 서술과 일치하도록 장면을 유지하는 강화된 능력을 제공합니다.
- ***Technical Details***: DreamRunner는 대형 언어 모델(LLM)을 통해 사용자 제공 내러티브로부터 계층적인 비디오 계획을 구축하고, 테스트 시점에서 검색된 영상들을 통해 모션 우선순위(Motion Priors)를 학습하여 다양한 모션의 사용자 정의를 지원합니다. SR3AI(Space-Temporal Region-based 3D Attention and Prior Injection)는 영상 생성 시 오브젝트의 미세한 모션 바인딩(Object-Motion Binding)과 프레임별 의미 제어를 가능하게 합니다.
- ***Performance Highlights***: DreamRunner는 DreamStorySet 데이터셋에서 기존의 SOTA 접근 방식인 VideoDirectorGPT 및 VLogger와 비교하여 캐릭터 일관성(CLIP 점수 기준)에서 13.1%의 향상을 나타내었으며, 텍스트 따름 능력(ViCLIP 점수 기준)에서 8.56%의 상대적 증가를 기록했습니다. 또한, T2V-CompBench에서의 평가에서 모든 메트릭에서 기존의 베이스라인들을 능가하는 성능을 보여주었습니다.

### [SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis](https://arxiv.org/abs/2411.16443)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16443.png)

Vote: 5

Authors: Changick Kim, Hyojun Go, Soonwoo Kwon, Byeongjun Park, Jiho Jang, Jin-Young Kim

- ***What's New***: SplatFlow는 3D Gaussian Splatting (3DGS)를 활용하여 고품질의 실시간 3D 생성 및 편집을 가능하게 하는 통합 프레임워크입니다. 이 프레임워크는 직접적인 3DGS 생성과 편집을 지원하며, 다양한 장면 규모와 복잡한 카메라 궤적을 처리할 수 있습니다.
- ***Technical Details***: SplatFlow는 멀티뷰 Rectified Flow (RF) 모델과 Gaussian Splatting Decoder (GSDecoder)로 구성되어 있습니다. 멀티뷰 RF 모델은 잠재 공간에서 작동하며, 텍스트 프롬프트를 통해 멀티뷰 이미지, 깊이, 카메라 포즈를 동시에 생성합니다. GSDecoder는 이러한 잠재 출력을 효율적인 피드-포워드 3DGS 방법을 통해 3DGS 표현으로 변환합니다. SplatFlow는 학습이 필요 없는 역변환 및 인페인팅 기법을 활용하여 다양한 3D 작업을 지원합니다.
- ***Performance Highlights***: MVImgNet과 DL3DV-7K 데이터셋에서의 실험 결과, SplatFlow는 고품질 이미지 생성과 텍스트 프롬프트와의 강력한 정렬을 유지하며, 다양한 3D 생성, 편집 및 인페인팅 기반 작업에서 높은 성능을 입증했습니다. 특히, SDS++ 적용 후 텍스트 정렬이 더욱 개선되었습니다.

