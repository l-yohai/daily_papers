## Daily Papers (2024-11-13)

### [Stronger Models are NOT Stronger Teachers for Instruction Tuning](https://arxiv.org/abs/2411.07133)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.07133.png)

Vote: 11

Authors: Bill Yuchen Lin, Radha Poovendran, Zhangchen Xu, Luyao Niu, Fengqing Jiang

- ***What's New***: 이 논문은 기존의 대형 모델이 꼭 더 강력한 교사가 아니라는 이전의 가정을 도전하며, 큰 모델이 작은 모델을 지도 조정(instruction tuning)하는 데 있어서 더 강력한 교사가 아닐 수 있음을 밝힙니다. 이를 '더 큰 모델의 역설(Larger Models’ Paradox)'이라고 명명했습니다.
- ***Technical Details***: 연구는 다섯 개의 기본 모델과 스무 개의 응답 생성기(response generator)를 통해 광범위한 실험을 진행했으며, 질문에 대한 응답을 생성하는 응답 생성기의 효과를 측정하기 위한 새로운 지표, 호환성 조정 보상(Compatibility-Adjusted Reward; CAR)을 개발했습니다. CAR은 응답 생성기의 효과를 측정할 수 있으며, 기본 모델과의 호환성을 위험 요소로 개념화합니다.
- ***Performance Highlights***: Gemma-2와 Qwen2 가족의 오픈 소스 모델들(Gemma-2-9b-it와 Qwen2.5-72B-Instruct)은 대체로 상용 모델인 GPT-4보다 뛰어난 성능을 보였습니다. CAR 지표는 반면, 호환성을 고려하지 않는 기존의 메트릭스보다 더 우수한 성능을 보였습니다.

### [BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions](https://arxiv.org/abs/2411.07461)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.07461.png)

Vote: 5

Authors: Le Xue, Sheng Shen, Senthil Purushwalkam, Oscar Lo, Manli Shu, Jae Sung Park, Caiming Xiong, Hannah Lee, Anas Awadalla, Ran Xu, Etash Guha, Jun Wang, Silvio Savarese, An Yan, Yejin Choi, Ludwig Schmidt

- ***What's New***: KALE는 웹 기반 사실적 서술과 합성된 이미지 캡션을 결합한 새로운 대규모 데이터셋입니다. 총 2억 1,800만 개의 이미지-텍스트 쌍으로 구성된 이 데이터셋은 웹에서 가져온 알트텍스트와 합성 캡션을 결합하여 지식-증강(Knowledge-Augmented)된 이미지 캡션을 생성합니다.
- ***Technical Details***: KALE 데이터셋은 두 단계로 생성됩니다. 첫 단계에서는 CogVLM-17B와 Mistral을 활용해 데이터컴프-1B 이미지에 대한 지식-증강 캡션을 생성하고, 두 번째 단계에서는 이 정보를 바탕으로 특화된 VLM을 훈련시켜 2억 1,800만 쌍의 이미지-텍스트 데이터를 생성합니다. 이 절차는 기존 모델보다 효율적이며, 높은 품질의 캡션 생성을 가능하게 합니다.
- ***Performance Highlights***: KALE에 기반한 기계 학습 모델은 다양한 비전-언어 과제에서 평균 51.96%의 성능을 기록하며, 기존 데이터셋보다 더 나은 성능을 발휘합니다. 특히 TextVQA에서는 59.92%, VQAv2에서는 70.10%, ScienceQA에서는 72.68%의 높은 성과를 보였습니다. 기존 합성 데이터셋보다 높은 정확도를 기록하며, 전반적으로 향상된 성능을 보여줍니다.

### [Scaling Properties of Diffusion Models for Perceptual Tasks](https://arxiv.org/abs/2411.08034)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08034.png)

Vote: 6

Authors: Rahul Ravishankar, Jathushan Rajasegaran, Zeeshan Patel, Jitendra Malik

- ***What's New***: 이 논문은 시각적 지각 작업을 위한 확산 모델(Diffusion Models)의 스케일링 특성을 탐구하여 새로운 통합 프레임워크를 제공합니다. 깊이 추정, 옵티컬 플로우, 세그멘테이션 등의 작업을 이미지 간 변환으로 통일하고, 이를 통해 확산 모델이 적은 데이터 및 연산으로 동급 최강의 성능을 달성할 수 있음을 보여줍니다.
- ***Technical Details***: 이 연구는 미리 학습된 확산 모델(Pre-trained Diffusion Model)을 다양한 시각적 지각 작업에 맞춰 세밀하게 조정하는 과정에서의 스케일링 법칙을 연구합니다. 특히, 모델 크기, 데이터 해상도, 그리고 전처리 학습 연산을 변화시키면서, 고밀도(Dense) 및 전문가 혼합 모델(Mixture-of-Experts)을 사전 학습합니다. 또한, 테스트 시 연산 스케일링 기법으로는 디퓨전 단계 증가, 테스트 시 앙상블, 노이즈 분산 스케줄 조정 등을 사용합니다.
- ***Performance Highlights***: 모델은 다양한 벤치마크에서 최신 기술과 유사하거나 더 나은 성능을 발휘하며, 특히 ETH3D 데이터셋에서 절대 상대 오차(AbsRel) 및 델타1(Delta1) 지표에서 높은 수준의 예측 정확도를 나타냅니다. 이는 높은 품질의 시각적 지각을 위해 대규모 인터넷 데이터셋을 사용할 필요성을 제거하며, 주어진 자원에서 효율적으로 학습을 수행할 수 있음을 증명합니다.

### [Hardware and Software Platform Inference](https://arxiv.org/abs/2411.05197)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.05197.png)

Vote: 3

Authors: Hanna Foerster, Cheng Zhang, Robert D. Mullins, Yiren Zhao, Ilia Shumailov

- ***What's New***: 이 논문에서는 하드웨어 및 소프트웨어 플랫폼 추론(Hardware and Software Platform Inference; HSPI)을 제안하며, 이는 입력-출력 행위를 통해 머신러닝 모델의 하드웨어 및 소프트웨어 스택을 식별하는 방법입니다. HSPI는 다양한 GPU 아키텍처와 소프트웨어 스택 간의 미세한 차이를 활용하여 하드웨어 유형과 소프트웨어 구성을 구별할 수 있습니다.
- ***Technical Details***: HSPI는 두 가지 방법을 제안합니다: 경계 입력을 활용한 HSPI-BI(Border Inputs)와 로짓 분포를 활용한 HSPI-LD(Logit Distributions)입니다. 이 두 방법은 각각 화이트 박스와 블랙 박스 설정 하에서 효율성을 입증하며, 비전 및 언어 작업에 걸쳐 시스템 최적화와 소프트웨어 및 하드웨어 공급망 내 변동의 영향을 탐구합니다.
- ***Performance Highlights***: 화이트 박스 설정에서 다른 GPU 간의 구별은 83.9%에서 100%의 정확도로 이루어졌으며, 블랙 박스 설정에서도 임의 추측 정확도보다 최대 세 배 높은 결과를 기록했습니다. 실험은 실시간 하드웨어에서 진행되었으며, GPU 아키텍처 및 소프트웨어 스택을 성공적으로 구별할 수 있음을 보여줍니다.

### [Acoustic Volume Rendering for Neural Impulse Response Fields](https://arxiv.org/abs/2411.06307)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.06307.png)

Vote: 2

Authors: Mingmin Zhao, Zhiwei Zheng, Zitong Lan, Chenhao Zheng

- ***What's New***: 이 논문에서는 Neural Impulse Response Fields를 위해 Acoustic Volume Rendering (AVR)을 도입하여 가상 및 증강 현실에서 몰입감을 주는 오디오 합성을 가능하게 합니다. AVR은 시간-시리즈 신호로 나타나는 Impulse Response (IR)의 정확한 특성을 모델링하기 위해 주파수 도메인에서의 볼륨 렌더링 방법과 구형 통합 기법을 활용하여 기존 방법보다 뛰어난 성능을 보입니다.
- ***Technical Details***: AVR은 Neural Radiance Fields에서 영감을 얻어 음파의 전파 원리를 암시적으로 인코드하는 Impulse Response Field를 구축합니다. 이 방법은 시간 지연과 에너지 감쇠를 고려하여 주파수 도메인에서의 볼륨 렌더링을 적용하며, 모든 가능한 방향으로 균일하게 광선을 발사하고 구상 통합을 통해 IR 측정값을 합성합니다. 이를 통해 시간 도메인 샘플링의 한계를 극복하고 낮은 공간 변화를 유지하여 네트워크 최적화를 용이하게 합니다.
- ***Performance Highlights***: AVR은 시뮬레이션 및 실제 데이터셋 모두에서 현존하는 방법들을 큰 차이로 능가하며, 제로샷(Zero-shot)으로 바이노럴 오디오(Binaural Audio)도 렌더링할 수 있습니다. 이는 다양한 테스트에서 다른 방법들과 비교했을 때 위상 오류, 진폭 오류, 외피 오류 등에서 모두 더 낮은 값을 기록했습니다. 또한, AVR은 AcoustiX라는 오픈소스 음향 시뮬레이션 플랫폼과 함께 제공되어 실제 음향 특성을 더 잘 반영합니다.

### [JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2411.07975)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.07975.png)

Vote: 12

Authors: Jiaying Liu, Yisong Wang, Chong Ruan, Haowei Zhang, Yiyang Ma, Xiaokang Chen, Liang Zhao, Wen Liu, Zhenda Xie, Chengyue Wu, Zizheng Pan, Xingchao Liu, Xingkai yu, Zhiyu Wu

- ***What's New***: JanusFlow는 자율회귀(Auto regression) 모델과 수정된 흐름(Rectified Flow)을 통합하여 통합 멀티모달 이해와 생성 업무를 수행하는 강력한 프레임워크를 소개합니다. 이 프레임워크는 간소한 아키텍처로 대형 언어 모델(LLM) 내에서의 수정된 흐름 메커니즘 교육을 용이하게 합니다.
- ***Technical Details***: JanusFlow는 이해와 생성 인코더를 분리하고, 훈련 과정에서 각각의 표현을 정렬하여 성능을 향상시킵니다. 시각적 이해를 위한 인코더로 SigLIP을 사용하고, 생성 작업에는 처음부터 초기화된 ConvNeXt 블록을 사용하여 디코플(Decouple)된 인코더를 구성합니다.
- ***Performance Highlights***: JanusFlow는 MJHQ FID-30k, GenEval, DPG-Bench 등의 벤치마크에서 교체 가능한 특화 모델들을 능가하는 성능을 보여주며, 가장 작은 모델(1.3B 매개변수)로 우수한 능력을 발휘합니다. 시각적 이해 벤치마크에서도 LLaVA나 Qwen-VL-Chat과 같은 대형 모델을 넘어서는 성과를 보였습니다.

### [Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings](https://arxiv.org/abs/2411.08017)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08017.png)

Vote: 4

Authors: Hooman Shayani, Aditya Sanghi, Derek Cheung, Pradyumna Reddy, Aliasghar Khani, Kamal Rahimi Malekshan, Arianna Rampini, Kanika Madan

- ***What's New***: Wavelet Latent Diffusion (WaLa)는 3D 형상 생성을 위한 새로운 방법으로, 3D 데이터를 파형 기반의 콤팩트한 잠재 공간에 인코딩하여 고효율의 압축을 실현하면서도 세부적인 정보를 유지하는 것이 특징입니다. 약 10억 개의 파라미터를 가진 거대 모델을 통해 3D 형상을 2~4초 내에 고품질로 생성하여 기존 최첨단 방법들을 능가합니다.
- ***Technical Details***: WaLa는 파동 기법을 사용하여 3D 모양을 보다 작은 잠재 공간으로 압축하는 Wavelet VQ-VAE를 활용하여 높은 압축 비율을 얻습니다. 이로 인해 대규모 3D 생성 모델의 학습을 효율적으로 수행 할 수 있습니다. 또한 다양한 입력 모달리티(단/다각도 이미지, 부피 요소, 포인트 클라우드, 깊이 맵, 스케치, 텍스트 설명)를 지원하며, 다단계 학습 프로세스를 통해 압축된 잠재 공간에서 네트워크 학습을 진행합니다.
- ***Performance Highlights***: WaLa 모델은 다양한 벤치마크 데이터셋에 대해 세부적이고 다양한 형태의 3D 모양을 생성하며, 요약된 LFD 및 IoU 메트릭에서 기존 방법들을 크게 능가합니다. 특히 단일 이미지 및 다가기 이미지 기반 3D 생성 작업에서 뛰어난 성능을 보이며, 각 조건에 대해 10 이하의 타임스텝만으로도 고품질 생성을 달성할 수 있음을 보여줍니다.

### [SAMPart3D: Segment Any Part in 3D Objects](https://arxiv.org/abs/2411.07184)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.07184.png)

Vote: 16

Authors: Xiaoyang Wu, Xihui Liu, Yukun Huang, Yan-Pei Cao, Yunhan Yang, Yuan-Chen Guo, Edmund Y. Lam, Liangjun Lu

- ***What's New***: SAMPart3D는 3D 객체를 다중 수준의 의미론적 파트로 분할하는 확장 가능한 제로샷(Zero-Shot) 3D 파트 세분화 프레임워크로, 미리 정해진 부분 라벨 세트나 텍스트 프롬프트 없이도 다양한 수준의 세분화를 지원합니다. 또한, 복잡한 비일반적 객체를 효과적으로 처리할 수 있어, 새로운 3D 파트 세분화 벤치마크 PartObjaverse-Tiny를 제공합니다.
- ***Technical Details***: SAMPart3D는 텍스트 비의존적인 비전 모델(DINOv2)을 사용하여 2D에서 3D로 특징 추출을 진행하며, Granularity 조정 가능한 MLP를 통해 다중 세분화 수준에서 3D 파트를 분할하는 능력을 가집니다. 이 프레임워크는 두 단계의 학습 과정으로 나뉘며, 이는 효율성과 성능 간의 균형을 맞춥니다. 최종적으로 다중 뷰 렌더링을 통해 각 부분에 의미론적 라벨을 할당하여 결과를 도출합니다.
- ***Performance Highlights***: SAMPart3D는 실험 결과, 기존의 제로샷 3D 파트 세분화 방법들보다 복잡하고 다양한 3D 객체에서 뛰어난 세분화 결과를 보여주었으며, 부품 수준 편집 및 상호작용적 세분화 등 다양한 응용에 기여할 수 있음을 입증했습니다. 또한, 클래스 비의존 평균 교차합(mIoU)과 같은 성능 지표에서 탁월함을 보였습니다.

