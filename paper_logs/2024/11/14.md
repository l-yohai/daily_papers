## Daily Papers (2024-11-14)

### [PerceiverS: A Multi-Scale Perceiver with Effective Segmentation for Long-Term Expressive Symbolic Music Generation](https://arxiv.org/abs/2411.08307)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08307.png)

Vote: 2

Authors: Yungang Yi, Quan Bai, Weihua Li, Matthew Kuo

- ***What's New***: PerceiverS는 상징적 음악 생성(Symbolic Music Generation)의 새로운 아키텍처로, 긴 구조적 종속성과 짧은 표현적 세부사항을 동시에 학습하여 보다 일관되고 다양한 음악을 생성하도록 설계되었습니다. 이 모델은 Maestro 등의 데이터셋을 기반으로 평가되어 구조적 일관성과 표현적 변화를 모두 갖춘 음악 생성에 있어 개선된 결과를 보여줍니다.
- ***Technical Details***: PerceiverS는 효과적인 세분화(Effective Segmentation)와 다중 스케일 주의 메커니즘(Multi-Scale Attention)을 활용하여 상징적 음악 생성을 향상시킵니다. 본 아키텍처는 교차 주의(Cross-Attention)와 자가 주의(Self-Attention)를 결합하여 멀티 스케일 환경에서 긴 거리의 음악적 구조를 포착함과 동시에 공연의 뉘앙스를 보존합니다. 제안된 모델은 Maestro와 같은 데이터셋으로 평가되어 일관되고 다양한 음악을 생성하는 데 있어 개선된 결과를 나타냅니다.
- ***Performance Highlights***: PerceiverS는 원래 학습 데이터셋과 비교할 때 Overlap Area에서 평균 40% 개선을 보여, Perceiver AR에 비해 더 높은 품질의 상징적 음악을 생성하는 데 있어 상당한 우위를 기록하였습니다.

### [Motion Control for Enhanced Complex Action Video Generation](https://arxiv.org/abs/2411.08328)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08328.png)

Vote: 0

Authors: Ye Qian, Shaofeng Zhang, Qiang Zhou, Hao Li, Nianzu Yang

- **What's New**: MVideo는 복잡한 동작 비디오 생성을 위한 새로운 프레임워크로, 기존의 텍스트 기반 비디오 생성 모델들이 가진 문제점을 극복하기 위해 설계되었습니다. 이 프레임워크는 텍스트 프롬프트(T2V) 외에 마스크 시퀀스(mask sequence)를 조건으로 추가하여 복잡한 동작을 보다 명확하게 표현할 수 있도록 하였습니다.
- **Technical Details**: MVideo는 GroundingDINO와 SAM2 같은 혁신적인 비전 모델을 활용하여 자동으로 마스크 시퀀스를 생성함으로써 효율성과 견고성을 높였습니다. 기존의 CogvideoX 모델을 파인튜닝(finetuning)하여 개발되었으며, 마스크 시퀀스를 추가함으로써 동영상의 동작 일치도를 향상시키는 동시에, 원래의 텍스트 정렬 능력을 보존하기 위한 새로운 컨시스턴시 손실(consistency loss)을 도입하였습니다.
- **Performance Highlights**: MVideo는 VBench 시험 세트에서 전반적인 일관성과 이미지 품질, 동작 부드러움 측면에서 기존 모델과 유사한 성능을 보였으며, 미공개 개체에 대한 일반화 능력을 증명했습니다. 또한 ComplexMotion 테스트 세트에서 복잡한 동작에도 효과적인 일치도를 보여주었습니다.

### [Can sparse autoencoders be used to decompose and interpret steering vectors?](https://arxiv.org/abs/2411.08790)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08790.png)

Vote: 4

Authors: Yushi Yang, Harry Mayne, Adam Mahdi

- ***What's New***: 이 논문은 희소 오토인코더(Sparse Autoencoders; SAEs)가 스티어링 벡터(Steering Vectors)를 해석하고 분해하는 데 직접적으로 사용되기에는 한계가 있음을 지적합니다. 특히, 스티어링 벡터가 SAEs의 입력 분포에서 벗어나고, SAEs가 의미 있는 음의 투영을 포착하지 못한다는 두 가지 이유가 강조됩니다.
- ***Technical Details***: 스티어링 벡터는 Contrastive Activation Addition 기법으로부터 추출되어, 모델의 내부 활성화를 조절하여 원하는 행동을 유도할 수 있는 메커니즘입니다. 그러나 이러한 벡터는 자주 사용되는 SAEs의 설계 범위를 벗어나며, 이는 SAE 복원 코사인 유사성 등에서의 큰 편차로 이어집니다. SAEs는 모델의 활성화를 복원하기 위해 훈련되었으며, 이는 스티어링 벡터와는 체계적인 차이가 존재합니다.
- ***Performance Highlights***: 스티어링 벡터를 사용하여 양수와 음수 프롬프트 쌍에 대한 SAE 활성화를 비교한 결과, 상당수의 부정적인 투영이 관찰되었습니다. 이는 SAEs가 부정적인 재구성 계수를 허용하지 않기 때문에 스티어링 벡터의 해석이 제한적이라는 점을 시사합니다. 따라서, 이러한 문제를 해결하기 위해 SAEs의 입력 분포 내에서 학습되는 새로운 방법이 필요함을 제시합니다.

### [Large Language Models Can Self-Improve in Long-context Reasoning](https://arxiv.org/abs/2411.08147)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08147.png)

Vote: 39

Authors: Mo Yu, Siheng Li, Lemao Liu, Zesen Cheng, Yujiu Yang, Wai Lam, Cheng Yang

- ***What's New***: 이 연구는 대형 언어 모델(LLM)이 자체적으로 긴 문맥에서의 추론 능력을 향상시킬 수 있는 가능성을 탐구하고, 이를 위해 SEALONG이라는 새로운 접근법을 제안합니다. 기존 방법이 사람이나 고급 모델에 의존한다면, SEALONG은 모델 자체 내에서 자율적인 개선을 이끌어내는 데 초점을 맞추고 있습니다.
- ***Technical Details***: SEALONG은 주어진 질문에 대해 여러 출력을 샘플링한 후, Minimum Bayes Risk로 점수를 매기고, 이 결과를 바탕으로 지도 학습(finetuning) 또는 선호도 최적화(preference optimization)를 수행합니다. 이 접근법은 Llama-3.1-8B와 같은 여러 대형 언어 모델에서 실험되어 그 효과가 입증되었습니다.
- ***Performance Highlights***: SEALONG은 Llama-3.1-8B-Instruct 모델에 대해 점수를 50.8에서 55.0으로 4.2 포인트 향상시켰습니다. 또한, 인간 전문가나 고급 모델의 데이터에 의존하지 않고도 우수한 성능을 입증하였으며, 이는 LLM이 긴 문맥에서의 추론 능력을 자체적으로 개선할 수 있음을 보여줍니다.

### [CamemBERT 2.0: A Smarter French Language Model Aged to Perfection](https://arxiv.org/abs/2411.08868)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08868.png)

Vote: 8

Authors: Djamé Seddah, Rian Touchent, Wissam Antoun, Francis Kulumba, Benoît Sagot, Éric de la Clergerie

- ***What's New***: CamemBERT 2.0 모델은 프랑스어 모델의 최신 버전으로 CamemBERTav2와 CamemBERTv2로 구성되어 있습니다. 이 모델들은 최신 데이터를 기반으로 업데이트하여 더 나은 시맨틱 이해를 제공하며, 각각 DeBERTaV3 구조와 RoBERTa 구조를 활용합니다. 또한, Replaced Token Detection (RTD)과 Masked Language Modeling (MLM) 목표를 사용하여 컨텍스트 이해를 강화했습니다.
- ***Technical Details***: CamemBERTav2는 DeBERTaV3 아키텍처 기반으로 RTD 목표를 사용하며, CamemBERTv2는 RoBERTa 아키텍처를 기반으로 MLM 목표를 사용합니다. 두 모델 모두 확장된 최신 데이터셋과 개선된 토크나이저를 바탕으로 하며, 프랑스어의 복잡성을 보다 잘 포착하도록 설계되었습니다. 이 모델들은 기본 NLP 작업과 의료 분야와 같은 도메인 특화 응용에서도 테스트되었습니다.
- ***Performance Highlights***: CamemBERTv2와 CamemBERTav2는 기존 모델들과 비교하여 전반적인 성능 향상을 보였습니다. Named Entity Recognition (NER)에서는 CamemBERTav2가 93.4%의 F1 점수를 기록하여 큰 향상을 보여줬습니다. 질문 응답(Question Answering)에서는 FQuAD 1.0 데이터셋에서 가장 높은 F1 점수 83.04%를 기록했으며, 이는 강력한 맥락 이해를 입증합니다. 이러한 새 모델들은 프랑스어 NLP 작업 전반에 걸쳐 우수한 성능을 보여주며, 특히 도메인 특화된 영역에서도 뛰어난 결과를 보였습니다.

### [Direct Preference Optimization Using Sparse Feature-Level Constraints](https://arxiv.org/abs/2411.07618)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.07618.png)

Vote: 10

Authors: Minjun Zhu, Jun Wang, Yulan He, Hongbo Zhang, Qingyu Yin, Qiang Zhang, Wenjie Li, Linyi Yang, Hanqi Yan, Yue Zhang, Chak Tou Leong

- ***What's New***: 본 연구는 희소한 특징 수준의 제약(Sparse Feature-Level Constraints)을 사용하여 대형 언어 모델(LLM)을 직접적으로 인적 선호와 일치시키는 새로운 방법인 FPO(Feature-level constrained Preference Optimization)를 제안합니다. FPO는 희소 오토인코더(Sparse Autoencoder; SAE)를 활용하여 효율적이고 안정적인 정렬을 가능하게 하며, 기존의 Direct Preference Optimization(DPO)나 강화학습을 통한 인적 피드백(RLHF) 보다 적은 계산 비용으로 인적 선호에 대한 향상을 이끌어냅니다.
- ***Technical Details***: FPO는 SimPO와 DPO를 바탕으로 제약 조건을 개선한 구조로, SAE를 이용하여 모델의 특징 레벨에서 제약을 가하며, 이로 인해 토큰 수준의 발산(KL divergence)을 대체합니다. SAE는 희소한 활성화 특징을 생산하여 계산 효율성을 높입니다. 또한, SAE는 모델의 내부 표현을 해석 가능한 구성요소로 분리하여 정렬의 복잡성을 줄이고, 사전 계산된 참조 모델 출력의 마진을 사용하여 학습 중에는 메모리 사용과 계산 요구량을 줄입니다.
- ***Performance Highlights***: FPO는 AlpacaEval-2와 Arena-Hard 등 벤치마크를 기반으로 최고 5%의 절대적 승률 향상을 달성하며, 기존의 SFT(Supervised Fine-Tuning), DPO, 및 TDPO-2와 같은 방법에 비해 더 나은 성능을 보입니다. 또한, 17.6%의 계산 비용 감소와 현저한 메모리 최적화를 이룩하여, 높은 효율성과 정렬 품질을 함께 제공합니다.

### [EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation](https://arxiv.org/abs/2411.08380)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08380.png)

Vote: 15

Authors: Kang Zhao, Xiaoyi Bao, Jiayu Wang, Xingang Wang, Zheng Zhu, Yingya Zhang, Feng Liu, Xiaofeng Wang, Guosheng Zhao

- ***What's New***: EgoVid-5M은 세계 최초로 주관적 비디오 생성(egocentric video generation)을 위한 고품질 데이터셋으로 꼼꼼하게 큐레이션되었습니다. 이 데이터셋은 500만 개의 비디오 클립과 정밀한 행동 주석을 포함하고 있으며, 다양한 장면과 행동의 복잡성을 효과적으로 다루기 위해 설계되었습니다. 또한, EgoVid-5M은 데이터 셈세(cleaning) 프로세스를 통해 프레임의 일관성, 행동의 일관성, 운동의 매끄러움을 보장합니다.
- ***Technical Details***: EgoVid-5M은 파티클 SfM(ParticleSfM)과 IMU와 같은 센서를 사용한 시각적 관성 측정기법(Visual Inertial Odometry; VIO)을 이용하여 세밀한 운동 제어 주석을 제공합니다. 이 데이터셋은 텍스트 주석을 위해 멀티모달 대형 언어모델(Multimodal Large Language Model; MLLM)을 사용하며, 동작 설명과 운동 제어 신호를 결합하여 주관적인 비디오 생성을 촉진하는 EgoDreamer를 제안합니다.
- ***Performance Highlights***: 실험 결과, EgoVid-5M은 여러 비디오 생성 모델의 훈련을 크게 강화하여 고품질의 주관적 비디오 생성 능력을 제공함을 보여주었습니다. EgoDreamer는 텍스트 설명과 운동 신호 모두를 활용하여 복잡하고 사실적인 주관적 비디오를 생성할 수 있습니다. 이 데이터셋과 이를 사용한 모델은 VR, AR, 그리고 게임 분야에서의 새로운 응용의 가능성을 제시합니다.

