## Daily Papers (2024-11-27)

### [SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis](https://arxiv.org/abs/2411.16173)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16173.png)

Vote: 3

Authors: Hosu Lee, Hyunjun Kim, Junho Kim, Yong Man Ro

- ***What's New***: SALOVA는 긴 영상 콘텐츠의 이해를 향상시키기 위해 설계된 새로운 비디오-LLM(비디오-대형 언어 모델; Video-LLM) 프레임워크입니다. 이 프레임워크는 대상 검색 프로세스를 통해 긴 영상 내용을 보다 효과적으로 이해할 수 있도록 돕습니다.
- ***Technical Details***: SALOVA는 87,800개의 길이가 긴 비디오로 구성된 SceneWalk 데이터셋을 사용합니다. 각 비디오는 세그먼트 수준에서 밀도 높게 캡션화되어 있어 장면의 연속성을 유지하고 풍부한 설명 컨텍스트를 제공합니다. 우리는 동적 라우팅 메커니즘과 시공간 프로젝터를 통합하여 사용자의 질의에 기반하여 관련 비디오 세그먼트를 효율적으로 검색하고 처리할 수 있는 아키텍처를 개발했습니다.
- ***Performance Highlights***: SALOVA는 긴 형식의 영상을 처리하는 데 뛰어난 성능을 보여주며, 확장된 시퀀스를 통해 맥락적 완전한 이해 능력을 유지하는 능력을 입증했습니다. 이를 통해 현재 비디오-LMMs의 한계를 완화하고 개선된 컨텍스트적 관련성을 보여주는 반응을 생성합니다.

### [Controllable Human Image Generation with Personalized Multi-Garments](https://arxiv.org/abs/2411.16801)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16801.png)

Vote: 2

Authors: Sihyun Yu, Jinwoo Shin, Hyungwon Choi, Yisol Choi, Sangkyung Kwak

- ***What's New***: BootComp은 텍스트-이미지 확산 모델을 기반으로 한 새로운 프레임워크로, 여러 참조 의류를 사용하여 제어 가능한 인간 이미지 생성을 가능하게 하는 새로운 방법을 제시합니다. 이 방법은 인간 이미지와 여러 의류 이미지 쌍을 포함한 대용량 합성 데이터셋을 구성하여 데이터 획득의 문제를 해결합니다.
- ***Technical Details***: BootComp은 두 단계로 구성됩니다. 첫 번째 단계는 합성 데이터 생성으로, 분해 모듈을 사용하여 고품질의 참조 의류 이미지를 추출하여 합성 쌍 데이터를 생성합니다. 두 번째 단계는 T2I 확산 모듈을 미세 조정하여 의류 이미지를 조건으로 인간 이미지를 생성합니다. 이 절차는 다양한 스타일을 지원하며, 다양한 응용 프로그램에 적용될 수 있습니다.
- ***Performance Highlights***: BootComp은 MP-LPIPS 측정에서 기존 방법보다 30% 향상된 결과를 보여주며, 참조 의류의 세부 사항을 보존하는 데 효과적임을 입증했습니다. 또한, 다양한 스타일로 인간 이미지를 생성할 수 있는 일반화 능력을 보여줌으로써 패션 도메인에서의 광범위한 적용 가능성을 강조합니다.

### [AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation](https://arxiv.org/abs/2411.17383)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17383.png)

Vote: 1

Authors: Yuchen Wang, Ziyao Huang, Juan Cao, Fan Tang, Linchao Bao, Qing Shuai, Xiaodong Cun, Ziyi Xu, Yong Zhang, Jintao Li

- ***What's New***: AnchorCrafter는 인간-객체 상호작용(Human-Object Interaction, HOI)을 활용하여 맞춤형 객체가 포함된 고화질 앵커 스타일 제품 홍보 영상을 자동 생성하기 위한 새로운 디퓨전 기반 시스템입니다. HOI-외형 인식(HOI-Appearance Perception)과 HOI-모션 주입(HOI-Motion Injection)을 통해 객체의 외형 보존 및 상호작용 인식을 향상시켰다는 점에서 혁신적입니다.
- ***Technical Details***: AnchorCrafter는 디퓨전 기반 비디오 생성 프레임워크로, 대상 인간과 맞춤형 객체를 포함한 2D 비디오를 생성합니다. HOI-외형 인식을 이용하여 다중 보기 관점에서 객체의 외형을 인식하고, 인간과 객체의 외형을 분리하여 보존합니다. HOI-모션 주입을 통해 객체의 궤적 조절 및 상호차폐 문제를 해결하며, HOI-지역 재가중치 손실을 도입하여 객체의 세부사항 학습을 강화합니다.
- ***Performance Highlights***: 광범위한 실험 결과 AnchorCrafter는 기존 방법들과 비교해 객체 외형 보존과 형태 인식 측면에서 우수한 성능을 보였습니다. 주관적 및 배경 일관성 평가에서 높은 점수를 얻었고, 이미지 및 비디오 품질 평가에서도 최첨단 성능을 기록했습니다.

### [Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens](https://arxiv.org/abs/2411.17691)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17691.png)

Vote: 4

Authors: Dong Yu, Xu Ouyang, Tao Ge, Haitao Mi, Thomas Hartvigsen, Zhisong Zhang

- ***What's New***: 이 연구는 저학습된 대형 언어 모델(LLM)에 대해 저비트 양자화(Low-Bit Quantization)가 더 적합함을 밝혀냈습니다. 특히, 모델 크기가 크거나 훈련 토큰이 적은 경우 저비트 양자화로 인한 성능 저하(QiD)가 적게 발생한다는 점을 관찰했습니다. 이는 기존의 저비트 양자화 연구에서 간과되었던 부분으로, 모델의 훈련 수준을 평가할 때 이러한 점을 고려해야 함을 시사합니다.
- ***Technical Details***: 1500개 이상의 다양한 크기와 훈련 단계에 있는 양자화된 LLM 체크포인트를 분석하여, 훈련 토큰 수, 모델 크기 및 비트 너비와 같은 요소들이 QiD에 미치는 영향을 이해하기 위한 스케일링 법칙(Scaling Laws)을 도출했습니다. 또한, 우리는 저비트 양자화를 통해 다양한 크기의 LLM을 100조(Trillion)개의 훈련 토큰으로 훈련했을 때의 양자화 성능을 예측했습니다. 이 결과는 미래의 모델이 더 많은 훈련 토큰으로 완전히 훈련된다면 저비트 양자화의 성능이 바람직하지 않을 가능성을 나타냅니다.
- ***Performance Highlights***: 모델 크기가 작은 경우 또는 더 많은 훈련 토큰이 있을 경우, 저비트 양자화로 인해 성능 저하가 더욱 심화되는 경향을 보였습니다. 특히, 2비트와 3비트 양자화를 사용할 때, 100조 토큰으로 예상되는 훈련 규모에서는 성능 저하가 심각할 것으로 예측되며 이는 현재의 훈련 규모에 비해 큰 도전 과제를 제시합니다.

### [Learning 3D Representations from Procedural 3D Programs](https://arxiv.org/abs/2411.17467)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17467.png)

Vote: 7

Authors: Zezhou Cheng, Xuweiyi Chen

- ***What's New***: 이 연구는 프로세듀럴 3D 프로그램(Procedural 3D Programs)을 활용하여 합성된 3D 데이터로부터의 3D 표현(3D Representations) 학습을 제안합니다. 이는 의미가 없는 합성 데이터임에도, 기존의 3D 데이터셋처럼 효과적으로 작동할 수 있음을 보여줍니다.
- ***Technical Details***: 본 연구는 Point-MAE 방식을 기반으로 하여 학습을 진행하였으며, 이는 3D 포인트 클라우드(Point Cloud)로부터의 셀프 슈퍼바이즈드 학습(Self-supervised Learning)을 위한 최신 방법입니다. 합성 3D 데이터셋은 주사위, 구, 실린더 등 기본 형태에서 시작하여 다양하게 변형하여 생성됩니다. 생성된 합성 포인트 클라우드는 150K개로 구성되어 있으며, 이는 저작권 문제 없이 무한히 생성할 수 있습니다. Point-MAE-Zero는 이렇게 생성된 데이터로만 학습됩니다.
- ***Performance Highlights***: 모델 시험 결과, Point-MAE-Zero는 기존 Point-MAE-SN과 비교하여 다양한 다운스트림 3D 작업에서 유사한 성능을 보였습니다. ShapeNet과 같은 데이터셋과는 도메인 차이가 있었음에도 불구하고, 복잡한 지형적 다양성을 포함한 학습 데이터의 효과를 증명합니다. 따라서 모델은 채워지지 않은 포인트를 효과적으로 예측하며, 이는 모형의 구조적 유사성을 강조합니다.

### [TEXGen: a Generative Diffusion Model for Mesh Textures](https://arxiv.org/abs/2411.14740)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14740.png)

Vote: 12

Authors: Yan-Pei Cao, Xin Yu, Xiaojuan Qi, Yuan-Chen Guo, Yangguang Li, Ying-Tian Liu, Ze Yuan, JianHui Liu, Ding Liang

- ***What's New***: TEXGen은 3D 메쉬 텍스처를 생성하기 위해 UV 텍스처 공간에서 고해상도 텍스처 맵을 직접 생성할 수 있는 대형 확산 모델입니다. 이는 텍스트 프롬프트와 단일 뷰 이미지를 이용한 조건부 텍스처 생성을 가능하게 하며, 텍스처 인페인팅(texture inpainting), 드문 뷰 텍스처 완성(sparse-view texture completion)과 같은 다양한 확장 응용 프로그램을 자연스럽게 지원합니다.
- ***Technical Details***: TEXGen 모델은 UV 텍스처 맵을 통해 고해상도 텍스처를 효과적으로 학습하고, 텍스처 맵에서 바로 고해상도 텍스처 맵을 생성할 수 있는 700만 파라미터 확산 모델을 제안합니다. 이는 UV 맵의 컨볼루션(convolutions)과 포인트 클라우드의 주의(attention) 레이어를 교차 배열한 확장 가능한 하이브리드 2D-3D 네트워크 아키텍처로 구성됩니다.
- ***Performance Highlights***: TEXGen은 텍스처 생성 품질과 일관성 면에서 TEXTure 및 Paint3D와 같은 초기 방법들보다 뛰어난 성과를 보였으며, FID 및 KID 측정 결과에서도 더 낮은 수치를 기록하였습니다. 또한, 테스트타임 최적화 없이 텍스처 맵을 생성할 수 있어, 기타 방법보다 10초 이내에 빠르게 작업을 완료할 수 있습니다.

### [SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE](https://arxiv.org/abs/2411.16856)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16856.png)

Vote: 5

Authors: Yushi Lan, XIngang Pan, Shangchen Zhou, Yongwei Chen, Tengfei Wang

- ***What's New***: SAR3D는 AutoRegressive 모델을 활용하여 3D 객체를 빠르게 생성하고 깊이 있는 이해를 가능하게 하는 새로운 프레임워크입니다. Multi-scale 3D VQVAE(Vector-Quantized Variational AutoEncoder)를 활용하여 3D 객체를 효율적으로 토크나이즈하고, AutoRegressive 기법을 통해 다음 Scale을 예측함으로써 빠른 3D 객체 생성을 실현합니다.
- ***Technical Details***: SAR3D의 중심은 Multi-scale 3D VQVAE입니다. 이 VQVAE는 멀티뷰 RGB-D 이미지를 인코딩하여 3D 객체를 Discrete한 Multi-scale Latent 공간에 매핑합니다. 3D 생성 시, AutoRegressive 모델을 통해 Latent Triplane의 다음 Scale을 예측하며, 각 Scale의 종속관계를 바탕으로 3D 객체 생성 속도를 가속화합니다. 3D 이해를 위해서는 Truncated Scale Token을 활용하여 미리 학습된 대형 언어 모델(LLM)을 파인튜닝(finetune)하여 텍스트와 3D 데이터를 동시에 이해할 수 있도록 합니다.
- ***Performance Highlights***: SAR3D는 기존 3D 생성 방법에 비해 빠른 속도와 높은 품질을 자랑합니다. A6000 GPU를 이용할 때 0.82초 만에 3D 객체 생성을 완료하며, 생성된 3D 토큰을 통해 3D 모델의 세부 사항까지 포함된 캡션을 생성할 수 있습니다. 이러한 성능은 특히 Multi-scale VQVAE와 다음 Scale 예측 전략을 기반으로 한 AutoRegressive 모델 덕분입니다.

### [VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models](https://arxiv.org/abs/2411.17451)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17451.png)

Vote: 7

Authors: Lei Li, Yifan Song, Peiyi Wang, Chenxin An, Zhihui Xie, Tianyu Liu, Bill Yuchen Lin, Lingpeng Kong, Yuancheng Wei, Xuqing Yang, Qi Liu, Sujian Li

- ***What's New***: VL-RewardBench는 비전-언어 생성 보상 모델(Vision-Language Generative Reward Models; VL-GenRMs)의 신뢰성과 효과성을 평가하기 위한 포괄적 벤치마크입니다. 기존의 AI 주석 선호도에 의존하는 평가 방식의 한계를 극복하고, 다중 모델 쿼리, 시각적 환각 검출, 복잡한 추론 작업을 포함한 다양한 테스트를 제공합니다.
- ***Technical Details***: VL-RewardBench는 1,250개의 고품질 예제를 제공하며, 범위는 일반 멀티모덜 쿼리, 환각 검출, 다중 모델 수학적 추론 작업으로 확장됩니다. 사람 검증 단계가 포함된 AI 지원 주석 파이프라인을 통해 데이터 세트를 수집하며, 모든 선호 레이블은 명확성을 보증하기 위해 인간 검증을 거칩니다. 이러한 엄격한 데이터셋 구축 전략을 통해 다양한 LVLM의 성능을 철저히 평가합니다.
- ***Performance Highlights***: GPT-4o 같은 선도적인 상업 모델은 VL-RewardBench에서 65.4%의 정확도를 기록하며, 최첨단 오픈 소스 모델 Qwen2-VL-72B조차 무작위 추측을 넘어서지 못합니다. VL-RewardBench의 성능은 MMMU-Pro 정확도와 강한 상관관계가 있으며, VL-GenRMs 개선에 필수적인 통찰력을 제공합니다. 특히, 시각적 지각 작업에 모델들이 더 많은 오류를 발생시킴을 확인했습니다.

### [Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration](https://arxiv.org/abs/2411.17686)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17686.png)

Vote: 14

Authors: Honggang Chen, Yuhang Han, Siteng Huang, Donglin Wang, Qingsen Yan, Pengxiang Ding, Xuyang Liu

- ***What's New***: 이 논문은 대규모 멀티모달 언어 모델(Multimodal Large Language Models; MLLMs)의 추론을 가속화하기 위한 새로운 '필터-관련-압축' 패러다임을 제안합니다. 이 패러다임은 학습 없이도 사용할 수 있는 토큰 축소 방법론으로, 기존의 복잡한 방법들을 세 가지 주요 단계로 나누어 체계적으로 설명합니다.
- ***Technical Details***: '필터-관련-압축' 패러다임은 각 단계마다 명확한 설계 목표와 요소들을 유지하면서 독창적인 구현이 가능하도록 설계되었습니다. 이 패러다임을 기반으로 FiCoCo라는 새로운 메서드 시리즈를 개발했으며, 이는 다양한 MLLM 추론 단계에서 토큰을 효과적으로 줄이는 데 중점을 두고 있습니다. FiCoCo는 시각적 인코더(Visual Encoder)와 LLM 디코더 내에서 토큰을 줄여주는 FiCoCo-V와 FiCoCo-L을 포함합니다.
- ***Performance Highlights***: 10개의 멀티모달 벤치마크 실험에서 FiCoCo는 최대 82.4%의 플롭스(FLOPs)를 줄이면서도 성능에는 최소한의 영향을 미칩니다. 이는 최신 기술의 학습 없는 방법을 능가하며, 특히 효율성과 정확성 사이의 최적의 균형을 달성합니다.

### [MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts](https://arxiv.org/abs/2411.14721)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14721.png)

Vote: 2

Authors: Wei Liu, Jingdi Le, Yuqiang Li, Di Zhang, Qing Li, Yunqing Liu, Wenqi Fan, Jiatong Li, Dongzhan Zhou

- ***What's New***: MolReFlect는 분자와 텍스트 간의 세밀한 정렬(fine-grained alignments)을 수행하는 새로운 교사-학생(teaching-student) 프레임워크를 소개합니다. 이 모델은 자연어 설명과 분자 구조 간의 정렬을 개선하여 LLMs의 생성 능력을 향상시키고, 기존 기초 모델 대비 ChEBI-20 데이터셋에서 최첨단(SOTA) 성능을 달성합니다.
- ***Technical Details***: MolReFlect는 세 가지 주요 단계로 구성됩니다. '제로샷 정렬 추출(Zero-shot Alignment Extraction)' 단계에서는 대형 교사 LLM이 화학적 지식을 활용하여 SMILES 문자열이나 분자 캡션에서 중요한 구문을 추출합니다. '인컨텍스트 선택적 반사(In-Context Selective Reflection)' 단계에서는 유사 케이스를 예로 사용하여 교사가 반사(reflect)하고 학생 LLM이 선택할 수 있도록 합니다. 마지막으로 '사고 과정 내에서의 문맥 조정(Chain-of-Thought In-Context Molecule Tuning)'을 통해 고급 정렬을 학습 과정에 통합하여 LLM의 추론 능력을 개선합니다.
- ***Performance Highlights***: MolReFlect는 ChEBI-20 데이터셋에서 기존 기초 모델보다 월등히 뛰어난 성능을 보여줍니다. 특히 Mol2Cap와 Cap2Mol 작업에서 블루(BLEU) 지표 및 기타 관련 성능 수치에서 최고치를 기록하며, 화합물 캡션 번역 작업의 설명력을 높이는 데 기여합니다.

### [ShowUI: One Vision-Language-Action Model for GUI Visual Agent](https://arxiv.org/abs/2411.17465)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17465.png)

Vote: 45

Authors: Zhengyuan Yang, Lijuan Wang, Kevin Qinghong Lin, Zechen Bai, Difei Gao, Shiwei Wu, Linjie Li, Mike Zheng Shou, Weixian Lei

- ***What's New***: ShowUI는 GUI 비주얼 에이전트를 위한 비전-언어-액션 모델(Vision-Language-Action Model)을 개발하여, 인간이 시각적으로 상호작용하는 방식에 가까운 사용자 인터페이스(UI) 인식을 가능하게 합니다. 본 연구는 GUI 과제 내에서 다양한 필요를 유연하게 통합하는 'Interleaved Vision-Language-Action Streaming'과 고품질의 GUI 지시-이행 데이터셋을 통해, GUI 시각 에이전트를 효율적으로 발전시키고자 합니다.
- ***Technical Details***: ShowUI는 UI-Guided Visual Token Selection을 통해 고해상도 스크린샷을 UI 연결 그래프로 변환하여 불필요한 관계를 식별하고, 시각적 토큰을 줄임으로써 컴퓨팅 비용을 감소시킵니다. 또한, 다양한 GUI 과제 내에서의 효과적인 학습을 위해 Interleaved Vision-Language-Action Streaming을 통해 시각-액션 히스토리 관리가 가능하며, 데이터의 불균형을 해소하기 위한 재샘플링 전략과 신중한 데이터 큐레이션을 통해 작은 규모의 고품질 데이터셋을 구축했습니다.
- ***Performance Highlights***: ShowUI는 2B 크기의 경량 모델로, 256K 데이터만을 사용하여 zero-shot screenshot grounding에서 75.1%의 정확도를 달성했습니다. UI가이드 토큰 선택은 학습 중 불필요한 시각적 토큰을 33% 줄이고, 성능을 1.4배 향상시켰습니다. 웹, 모바일, 온라인 환경에서의 내비게이션 실험은 ShowUI의 효과성과 잠재력을 강조하며, GUI 비주얼 에이전트의 발전을 촉진합니다.

### [Visual Counter Turing Test (VCT^2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (V_AI)](https://arxiv.org/abs/2411.16754)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16754.png)

Vote: 1

Authors: Shreyas Dixit, Abhilekh Borah, Amitava Das, Nasrin Imanpour, Aman Chadha, Hasnat Md Abdullah, Ashhar Aziz, Vinija Jain, Subhankar Ghosh, Shashwat Bajpai, Sainath Reddy Sankepally, Amit Sheth, Nishoak Kosaraju, Shwetangshu Biswas

- ***What's New***: 이 논문에서는 AI 생성 이미지 검출법의 비효율성을 지적하고, 이에 대한 재평가가 필요함을 주장합니다. 새롭게 Visual Counter Turing Test (VCT2) 벤치마크를 도입하여 최신 텍스트-이미지 생성 모델 (Stable Diffusion, DALL-E, Midjourney)의 약 13만 개 이미지를 평가하였으며, Visual AI Index (VAI)를 제안하여 AI 생성 이미지의 시각적 품질을 정량적으로 평가하는 새로운 기준을 제시합니다.
- ***Technical Details***: VCT2는 뉴욕타임스 트위터 계정과 MS COCO 데이터셋에서 수집한 프롬프트를 사용하여 생성된 이미지로 구성된 벤치마크입니다. 이로써 다양한 AGID 기법들의 성능을 평가하였습니다. VAI는 텍스처 복잡성, 색상 분포, 객체 일관성, 문맥적 관련성을 포함한 7가지 주요 지표를 평가하여 AI 생성 이미지의 품질을 측정합니다. VAI는 또한 각 지표의 측정값을 기반으로 계산되어 AI 생성 이미지가 실제보다 더 실제적인지 평가합니다.
- ***Performance Highlights***: VAI를 기반으로 한 평가는 Midjourney 6이 가장 높은 점수로 우수한 시각적 일관성과 품질을 보여줌을 나타냈습니다. 다른 모델들, 특히 SDXL 및 DALL-E 3은 중간 수준의 성능을 기록했습니다. 이는 고품질의 AI 생성 이미지의 검출이 여전히 도전 과제임을 시사합니다. 드페이크(De-Fake)와 DRCT 방법은 이러한 검출 문제에 효과적이지만, 모델에 따라 성능 차이를 보입니다. 이는 AI 생성 이미지의 질이 검출의 어려움에 영향을 미치는 중요한 요인 중 하나임을 드러냅니다.

### [MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs](https://arxiv.org/abs/2411.15296)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15296.png)

Vote: 14

Authors: Caifeng Shan, Sirui Zhao, Bo Li, Yi-Fan Zhang, Xing Sun, Shukang Yin, Ran He, Ziwei Liu, Haodong Duan, Liang Wang, Xinyu Fang, Chaoyou Fu

- ***What's New***: 이 논문은 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 평가에 관한 종합적인 설문 조사를 제공합니다. 최근 MLLMs의 급속한 발전에 맞춰 다양한 벤치마크와 평가 방법들이 제안되고 있으며, 이 논문은 그 중 4가지 주요 측면을 중심으로 MLLM 평가를 체계적으로 조사합니다.
- ***Technical Details***: 먼저, 모델의 기본 능력 및 확장 응용에 대한 평가를 위한 벤치마크 유형을 검토합니다. 데이터 수집, 주석, 평가 방법으로 구성된 벤치마크 구축 과정과, 기존 모델의 강점과 단점을 분석하는 시스템적 평가 방법을 설명합니다. 또한, 향후 개발될 벤치마크의 방향성을 정의하는 데 있어 잘 정립된 능력 분류와 더 많은 모달리티를 통합하는 방법을 논의합니다.
- ***Performance Highlights***: 논문에서 언급된 여러 벤치마크 결과에 따르면, 최신의 독점 및 오픈 소스 MLLMs가 특정 작업에서 사람 수준의 성과에 상당히 못 미치는 경우가 많습니다. 예를 들어, GPT-4 및 다른 독점 모델은 다중 이미지 이해 및 인터리브된 데이터 평가에서 여전히 실질적인 도전을 받고 있으며, 이는 향후 연구 방향을 제시합니다.

### [DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting](https://arxiv.org/abs/2411.17223)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17223.png)

Vote: 5

Authors: Siyu Du, Lu Zhang, Yunzhi Zhuge, Yicheng Yang, Liqian Ma, Xu Jia, Pengxiang Li, Huchuan Lu, Ping Hu

- ***What's New***: DreamMix는 사용자 정의 이미지 인페인팅(Customized Image Inpainting)에서 객체 속성을 분리하여 더욱 뛰어난 편집 가능성을 제공하는 새로운 접근법을 소개합니다. 이 모델은 텍스트 기반 속성 가이드의 다양성과 차별성을 향상시키고, 사용자가 지정한 위치에 타겟 객체를 삽입하면서 해당 객체의 속성을 문자 지침에 따라 변화할 수 있는 능력을 갖췄습니다.
- ***Technical Details***: DreamMix는 디퓨전 기반의 생성 모델로, 텍스트 주도형 인페인팅에 접근하여 속성 수준의 정확성과 데이터 효율성을 제공합니다. 우리는 대상 객체의 정확한 국부 삽입과 전체적인 시각적 일관성을 동시에 향상시키기 위한 '분리 된 인페인팅 프레임워크(disentangled inpainting framework)'를 도입하였습니다. 또한 '속성 분리 메커니즘(Attribute Decoupling Mechanism; ADM)'과 '텍스트 속성 교체 모듈(Textual Attribute Substitution; TAS)'을 제안하여 텍스트 기반의 속성 편집 능력을 한층 강화했습니다.
- ***Performance Highlights***: DreamMix는 다양한 객체 중심의 인페인팅 응용에서 기존 방법보다 탁월한 성능을 보여주었습니다. 실험 결과, 대부분의 각종 응용 시나리오에서 ID 보존과 속성 편집에 있어서 뛰어난 성능을 입증하였습니다. 특히 CLIP-I, CLIP-T, 그리고 DINO 지표에서 높은 점수를 기록하며, 이는 DreamMix가 원본 속성을 유지하면서도 텍스트 지시사항에 따른 변경 효과를 정확하게 생성할 수 있음을 나타냅니다.

### [FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity](https://arxiv.org/abs/2411.15411)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15411.png)

Vote: 6

Authors: Hang Hua, Jing Shi, Jianming Zhang, Jiebo Luo, Yilin Wang, Qing Liu, Zhifei Zhang, Lingzhi Zhang

- ***What's New***: FINECAPTION은 임의의 마스크를 참조 입력으로 인식하고 고해상도 이미지를 처리하여 다양한 세분화 수준에서 합성 이미지 자막을 생성할 수 있는 혁신적인 비전-언어 모델(Vision-Language Model; VLM)입니다. 이를 지원하기 위해 우리가 제안한 새로운 데이터셋인 COMPOSITIONCAP은 다중-세분화 지역 조성 이미지 자막의 작업을 도입합니다.
- ***Technical Details***: 우리의 제안 모델은 마스크-인식 이미지 인코더와 고해상도 인코더(Such as ConvNeXT and SAM)를 통합하여 이미지의 세부 정보를 인식하고 마스크 참조 지역을 정확히 인식할 수 있습니다. Alpha-CLIP 방법을 따르는 마스크-인식 인코딩 기법을 채택하여 RGB 이미지의 알파 채널로 이진 마스크를 통합합니다. COMPOSITIONCAP 데이터셋은 18개의 다양한 조성 속성을 포함하며, 지역 속성-인식 자막(Attribute-Aware Regional Captioning), 지역 밀집 자막(Regional Dense Captioning), 종합적 글로벌 이미지 자막(Comprehensive Global Image Captioning)이라는 세 가지 수준의 자막 세분화를 제공합니다.
- ***Performance Highlights***: FINECAPTION은 지역 조성 이미지 자막 작업에서 다른 강력한 VLM들, 특히 GPT-4 및 LLaMA-3.2를 능가하는 성능을 보였습니다. 제로샷 학습 환경에서 다른 모델들과 비교할 때, 대부분의 모델이 정확한 지역 참조 및 속성 지시를 인식하는 데 어려움을 겪는 반면, FINECAPTION은 데이터 기반 트레이닝을 통해 의미 있는 성과 향상을 보여주었습니다. 예를 들어, AARC(속성 인식 지역 자막) 작업에서 56.84를, RDC(지역 밀집 자막) 작업에서 83.49를 기록하며 모든 베이스라인 모델을 뛰어넘는 성능을 나타냅니다.

### [Pathways on the Image Manifold: Image Editing via Video Generation](https://arxiv.org/abs/2411.16819)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16819.png)

Vote: 23

Authors: Noam Rotstein, Ron Kimmel, Gal Yona, Daniel Silver, David Bensaïd, Roy Velich

- ***What's New***: 이 논문은 이미지 편집을 이미지-비디오 모델(Image-to-Video Models)을 활용하여 수행하는 새로운 접근 방식을 제안합니다. 기존의 이미지 편집을 비디오 생성 작업으로 재구성하여 원본 이미지의 핵심 요소를 유지하면서 편집의 정확성과 일관성을 향상시키는 방법을 소개합니다.
- ***Technical Details***: Frame2Frame(F2F)이라는 구조화된 파이프라인을 제안합니다. 이 파이프라인은 사전 학습된 비디오 모델을 사용하여 편집 과정을 시간적 진화로 재구성하며, Temporal Editing Captions, 영상 기반의 편집 생성, 자동 프레임 선택과 같은 세 가지 주요 구성 요소로 구성됩니다. 이를 통해 원본 이미지와 목표 편집 사이의 자연스러운 변환을 생성하고, 이는 시각적 및 시간적 일관성을 유지합니다.
- ***Performance Highlights***: 제안된 방법인 Frame2Frame은 TedBench 및 PosEdit 데이터셋에서 현재 최고 수준의 성능을 보이며, 기존 이미지 편집 방식과 비교하여 편집의 정확성 및 원본 이미지 보존 측면에서 높은 평가를 받았습니다. 이 방법은 인체 자세 편집을 포함한 보다 일반적인 컴퓨터 비전 문제에서도 유망한 결과를 보여줍니다.

### [SketchAgent: Language-Driven Sequential Sketch Generation](https://arxiv.org/abs/2411.17673)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17673.png)

Vote: 10

Authors: Alex Zhao, Antonio Torralba, Kristine Zheng, Tamar Rott Shaham, Yael Vinker, Judith E Fan

- ***What's New***: SketchAgent는 인공지능이 사전에 학습된 대형 멀티모달 모델(Multimodal Large Language Models; LLMs)의 풍부한 사전 지식을 활용하여 훈련 없이 텍스트 기반으로 스케치를 순차적으로 생성하는 혁신적인 접근법을 제시합니다. 인간 사용자와의 상호작용을 통해 스케치를 생성 및 편집할 수 있으며, 직관적인 스케칭 언어를 도입하여 언어적 맥락에서 '그릴' 수 있도록 설계되었습니다.
- ***Technical Details***: SketchAgent는 사전 학습된 멀티모달 LLM을 이용하여 스케치의 순차적 생성을 지원합니다. 그리드 캔버스를 도입하여 모델이 특정 좌표를 참조하도록 하고 있으며, 스케치 표현은 의미적으로 각 좌표 시퀀스가 정의된 일련의 스트로크로 구성됩니다. 스트로크는 매끄러운 베지어 곡선(Bezier Curves)으로 처리되어 픽셀 캔버스에 렌더링됩니다. In-Context Learning(ICL)과 Chain-of-Thought(CoT) 기법을 활용하여 모델의 계획 능력을 강화합니다.
- ***Performance Highlights***: SketchAgent는 다양한 개념의 텍스트를 기반으로 한 스케치 생성에 성공적이며, 인간 사용자와의 실시간 협업을 통해 의미 있고 새로운 스케치를 효과적으로 생성할 수 있는 능력을 보여줍니다. SketchAgent는 기존의 스케치 생성 방법과 달리 사전 학습된 LLM을 어떻게 활용할 수 있는지를 보여주는 최초의 사례로, 추가적인 훈련 없이 일반 목적의 인공지능 스케칭 시스템의 가능성을 열어줍니다.

### [Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/abs/2411.17116)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17116.png)

Vote: 25

Authors: Fei Jia, Boris Ginsburg, Shantanu Acharya

- ***What's New***: Star Attention은 긴 시퀀스에 대한 효율적인 추론을 가능하게 하는 새로운 블록-스파스 주의 메커니즘을 도입했습니다. 이 알고리즘은 두 단계로 나뉘며, 첫 번째 단계에서는 컨텍스트 토큰을 블록단위 로컬 주의로 처리하고, 두 번째 단계에서 쿼리와 응답 토큰이 모든 이전에 캐시된 토큰에 시퀀스-글로벌 주의(Sequence-Global Attention)를 적용합니다.
- ***Technical Details***: Star Attention은 두 단계로 작동합니다: (1) '컨텍스트 인코딩'에서는 긴 컨텍스트가 인접한 블록으로 분할되고 '앵커 블록(Anchor Block)'으로 선행되어 처리되며, (2) '쿼리 인코딩 및 토큰 생성'에서는 쿼리가 전체 캐시된 토큰에 글로벌 주의를 통해 처리됩니다. 이 과정은 대부분의 Transformer 기반 LLMs (Large Language Models)에 추가적인 미세 조정 없이 적용 가능하며, 메모리 효율성을 극대화 합니다.
- ***Performance Highlights***: Star Attention은 Llama3.1-8B와 Llama3.1-70B 모델에 대해 최대 11배 빠른 추론 속도를 구현하면서 95-100%의 정확도를 유지하여 Ring Attention에 비해 상당한 성능 향상을 보여주었습니다. 이는 긴 시퀀스에서 LLM의 추론 시간을 크게 단축하고 메모리 요구 사항을 줄입니다.

### [All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages](https://arxiv.org/abs/2411.16508)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16508.png)

Vote: 6

Authors: Ashmal Vayani, Ketan More, Olympiah Otieno, Pramesh Shakya, Hasindri Watawana, Mihail Mihaylov, Branislava Jankovic, Johan Obando-Ceron, Muhammad Saad, Nathan Xavier, Naman Bhatia, Salman Khan, Wafa Al Ghallabi, Mahardika Krisna Ihsani, Muhammad Ridzuan, Noor Ahsan, Kartik Kuckreja, Aman Chadha, Monojit Choudhury, Sebastian Cavada, Maheen Fatima, Amrin Kareem, Thao Nguyen, Ashay Srivastava, Alvaro Cabrera, Abduragim Shtanchaev, Harsh Singh, Sanoojan Baliah, Jenny Chim, Dilshod Azizov, Muztoba Rabbani, Omkar Thawakar, Yahya Hmaiti, Kamila Zhumakhanova, Amirpouya Ghasemaghaei, Dinura Dissanayake, Abdelrahman M Shaker, Hawau Toyin, Fahad Khan, Toluwani Aremu, Thamar Solorio, Amandeep Kumar, Rao Muhammad Anwer, Henok Biadglign Ademtew, Chao Qin, Mykola Maslych, Nevasini Sasikumar, Endre Hamerlik, Sanjay Manjunath, Amiel Esplana, Amirbek Djanibekov, Feno Heriniaina Rabevohitra, Monil Gokani, Mike Zhang, Amit Bhatkal, Daniya Kareem, Michael Felsberg, Jorma Laaksonen, Ivan Laptev, Mubarak Shah, Santosh Sanjeev, Fathinah Asma Izzati, Rohit Gupta, Azril Amirudin, Fabian Farestam, Shachar Mirkin, Hisham Cholakkal, Fadillah Adamsyah Maani, Kunyang Li

- ***What's New***: 이 논문은 기존 대형 멀티모달 모델(Large Multimodal Models; LMMs)이 특정 지역과 언어에 집중되어 문화적 맥락 이해와 저자원 언어 지원이 부족한 문제를 해결하기 위해, 100개 언어의 문화적으로 다양한 평가를 위한 ALM-bench를 제안합니다. 이 벤치마크는 다양한 질문 유형을 포함하여 모델이 문화적으로 다양한 이미지와 텍스트를 이해하고 추론할 수 있는 능력을 평가합니다.
- ***Technical Details***: ALM-bench는 100개의 언어로 구성된 가장 대규모의 멀티모달 벤치마크로, 13개의 문화적 측면(전통, 축제, 음식 등)과 6개의 일반적인 측면을 아우르는 콘텐츠로 구성되었습니다. 이 벤치마크는 73개국에 걸쳐 24개의 서로 다른 스크립트를 포함하며, 다양한 질문 형식(선다형, 참/거짓 등)을 통해 모델의 시각 및 언어적 추론 능력의 다양한 난이도를 평가합니다.
- ***Performance Highlights***: 실험 결과, 대규모 멀티모달 모델(GPT-4o 등)의 경우 고자원 언어에 비해 저자원 언어에서는 성능이 크게 저하되었습니다. 예를 들어, GPT-4o는 영어에서 88.4%의 정확도를 기록했으나, 아마릭어에서는 50.8%로 떨어졌습니다. 이러한 결과는 LMMs가 저자원 언어와 복잡한 문화적 맥락에서의 성능이 아직 미흡함을 보여주며, 이 방향으로의 추가 연구 필요성을 제시합니다.

