## Daily Papers (2024-11-21)

### [Stylecodes: Encoding Stylistic Information For Image Generation](https://arxiv.org/abs/2411.12811)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12811.png)

Vote: 5

Authors: Ciara Rowles

- ***What's New***: StyleCodes는 새로운 오픈소스 스타일 인코더 아키텍처입니다. 스타일코드는 20자리 Base64 코드로 이미지 스타일을 표현하여, 이미지 생성 시 스타일 정보를 보다 쉽게 공유하고 조건화할 수 있습니다. 이는 기존의 스타일 참조 코드 사용의 한계를 극복한 방식입니다.
- ***Technical Details***: StyleCodes의 모델 아키텍처는 기본적인 주의 기반 오토인코더(Attention-based Autoencoder)와 제어 네트워크(ControlNet) 스타일 UNet 디코더의 결합을 특징으로 합니다. 인코딩된 스타일 코드는 임베딩 공간에 위치하며, 스타일코드 조건 모델은 학습된 UNet의 내부 상태에 잔여적으로 영향을 미쳐 스타일 정보를 반영합니다. 또한, 이미지 인코더로는 SigLip을 활용하여 보다 효율적인 이미지 임베딩을 제공합니다.
- ***Performance Highlights***: StyleCode를 사용한 결과, 스타일 보존이 효과적으로 이루어졌으며, 원본 모델을 교체하여도 최소한의 성능 저하만 발생했습니다. 이는 StyleCode가 다양한 스타일 이미지와 함께 호환성이 뛰어남을 보여줍니다. 또한, 고정된 베이스 모델로 인해 다른 모델들과의 호환성도 유지됩니다.

### [SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization](https://arxiv.org/abs/2411.11909)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11909.png)

Vote: 18

Authors: Hongrui Jia, Ji Zhang, Wei Ye, Shikun Zhang, Ming Yan, Chaoya Jiang, Haiyang Xu, Mengfan Dong, Fei Huang

- ***What's New***: SymDPO는 기호 시연 직접 선호 최적화(Symbol Demonstration Direct Preference Optimization)를 통해 대형 멀티모달 모델(Large Multimodal Models; LMMs)의 상황별 학습(In-Context Learning)을 강화하는 혁신적인 방법입니다. 이 방법은 기호를 사용하여 모델이 텍스트 패턴에 의존하지 않고 시각 정보를 정확히 이해하고 활용하도록 설계되었습니다.
- ***Technical Details***: SymDPO는 전통적인 텍스트 기반 응답을 무작위 기호로 대체하여 모델이 시연 이미지와 기호 간의 관계를 구축할 수 있도록 합니다. 이러한 기호 대체를 통해 모델은 시각적 요소와 기호적 표현을 연결하여, 기호적 텍스트만으로는 답할 수 없는 질문에 정확히 답변할 수 있도록 유도합니다. SymDPO는 본질적으로 모델이 시각적 맥락을 무시하지 않고, 시각적 텍스트와 결합된 이해를 통해 정확한 발생을 가능하게 합니다.
- ***Performance Highlights***: 다양한 벤치마크에서 SymDPO는 기존의 기호 없던 최적화 방법에 비해 모든 평가 기준에서 성능을 향상시켰습니다. 예를 들어, COCO Caption과 VQAv2 같은 이미지 캡션 및 질문 응답 벤치마크에서 모델의 성능이 지속적으로 향상됨을 보였습니다. 이는 SymDPO가 보다 통합된 지식 활용을 통해 모델의 멀티모달 이해를 촉진함을 의미합니다.

### [SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory](https://arxiv.org/abs/2411.11922)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11922.png)

Vote: 12

Authors: Wenhao Chai, Cheng-Yen Yang, Zhongyu Jiang, Jenq-Neng Hwang, Hsiang-Wei Huang

- ***What's New***: SAMURAI는 Segment Anything Model(SAM 2)을 기반으로 하여 실제 환경에서도 실시간 고정밀 추적을 가능하게 하는 zero-shot 시각 객체 추적 알고리즘입니다. 주요 개선점은 모션 정보를 활용하여 복잡한 장면에서 객체의 이동을 예측하고 마스크 선택을 정제하는 능력입니다. SAMURAI는 학습이나 미세 조정 없이 다양한 벤치마크에서 뛰어난 성능을 보여줍니다.
- ***Technical Details***: SAMURAI 모델은 Kalman 필터 기반의 모션 모델링을 통한 마스크 선택 개선 및 하이브리드 스코어 시스템을 활용한 메모리 선택 최적화를 도입합니다. 이를 통해 SAM 2의 메모리 관리 문제를 해결하고 추적 정확도를 높였습니다. 각 프레임에 대해 계산된 모션 점수와 마스크 친화도 점수를 결합하여 최적의 마스크를 선택하고, 불필요한 프레임은 메모리에서 제외하여 에러 전파를 최소화합니다.
- ***Performance Highlights***: SAMURAI는 LaSOT, LaSOText, GOT-10k 등의 시각 객체 추적 벤치마크에서 이미 훈련된 최고 성능의 방법들과 견줄만한 성과를 보였습니다. LaSOT에서 74.2% AUC를 기록하며 기존의 방법 대비 성능 향상을 보여주었고, LaSOText에서는 61.0% AUC로 대폭 개선된 결과를 얻었습니다. 이는 기존의 SAM 2 기반 시스템에 비해 5% 이상의 성능 향상입니다.

### [ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models](https://arxiv.org/abs/2411.10867)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10867.png)

Vote: 1

Authors: Samyak Rajesh Jain, Aman Chadha, Amitava Das, Aishwarya Naresh Reganti, Vipula Rawte, Vinija Jain, Sarthak Jain, Garv Kaushik, Prathiksha Rumale Vishwanath, Aarush Sinha, Aman Bansal, Amit P. Sheth

- ***What's New***: ViBe는 대형 멀티모달 모델(LMMs)에서 발생하는 환각 현상을 체계적으로 평가하기 위해 텍스트-비디오(T2V) 모델의 환각 영상을 탐구하고 분류하는 대규모 데이터셋을 도입했습니다. ViBe는 텍스트 입력과 생성된 비디오 간의 일관성 및 정확성을 확인하는 새로운 벤치마크로, T2V 모델의 신뢰성 평가와 환각 감지 개선을 위한 기반을 제공합니다.
- ***Technical Details***: ViBe 데이터셋은 MS COCO 캡션으로부터 임의로 선택된 700개의 캡션을 사용하여, MS1.7B, MagicTime, AnimateDiff-MotionAdapter, Zeroscope V2 XL 등의 10개의 오픈 소스 모델에서 비디오를 생성하여 구축되었습니다. 환각 유형은 인물 소실(Vanishing Subject), 수치 변동(Numeric Variability), 시간 왜곡(Temporal Dysmorphia), 누락 오류(Omission Error), 물리적 부조화(Physical Incongruity)로 분류되며, 3,782개의 비디오가 각 유형으로 주석 처리되었습니다.
- ***Performance Highlights***: 성능 평가에서 TimeSFormer + CNN 조합은 최고 성능을 보여줬으며, 정확도 0.345와 F1 스코어 0.342를 기록했습니다. ViBe 벤치마크는 T2V 모델에서 발견되기 쉬운 환각을 체계적으로 분류하고 감소시키기 위한 연구를 추진할 수 있는 기회를 제공합니다.

### [VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models](https://arxiv.org/abs/2411.13503)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.13503.png)

Vote: 23

Authors: Yu Qiao, Chenyang Si, Qianli Ma, Limin Wang, Nattapol Chanpaisit, Xiaojie Xu, Yuming Jiang, Yaohui Wang, Ziqi Huang, Ziyue Dong, Yinan He, Xinyuan Chen, Jiashuo Yu, Dahua Lin, Ying-Cong Chen, Ziwei Liu, Fan Zhang

- ***What's New***: VBench++는 비디오 생성 모델(Video Generative Models)의 평가를 위한 포괄적이고 다목적 벤치마크 스위트입니다. 각 비디오 생성 품질을 구체적이고 계층적이며 분리된 차원으로 나누어 평가하며, 인간의 인식과 조화를 이루도록 설계되었습니다. 텍스트-비디오(Text-to-Video), 이미지-비디오(Image-to-Video) 생성 작업을 모두 평가할 수 있으며, 생성된 비디오의 신뢰성을 평가하여 모델 성능을 더욱 포괄적으로 이해할 수 있도록 합니다.
- ***Technical Details***: 16가지 비디오 생성 평가 차원을 포함하며, 각 차원은 개별적인 평가 메서드와 프롬프트로 설정됩니다. VBench++는 다양한 콘텐츠 유형에 대한 텍스트-비디오 및 이미지-비디오 생성 모델을 평가하도록 설계되었습니다. 또한 고해상도 이미지 스위트를 포함하여 다양한 이미지-비디오 설정 간의 공정한 평가를 가능하게 합니다. 모든 평가는 인간의 선호도 표본과 일치하도록 설계되었으며 평가 결과는 인간의 인식에 잘 맞도록 검증되었습니다.
- ***Performance Highlights***: 다양한 비디오 생성 모델들이 각기 다른 평가 차원에서 성과를 나타냅니다. 텍스트-비디오 모델들 중에서는 LaVie와 같은 모델이 특정 평가 차원에서 높은 점수를 얻었습니다. 이미지-비디오 모델에서는 DynamiCrafter-1024가 여러 차원에서 우수한 성능을 보였습니다. 또한, 신뢰성 평가에서 산업 기반의 모델들이 학술 기관의 모델들보다 상대적으로 우수한 모습을 보입니다.

### [ORID: Organ-Regional Information Driven Framework for Radiology Report Generation](https://arxiv.org/abs/2411.13025)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.13025.png)

Vote: 2

Authors: Kaicheng Yang, Weidong Cai, Tiancheng Gu, Ziyong Feng, Xiang An, Dongnan Liu

- ***What's New***: 이번 연구는 Organ-Regional Information Driven (ORID) 프레임워크를 도입하여, 방사선 이미지에서 기계학습을 활용한 자동화된 방사선 보고서 생성을 제안합니다. 이는 특히 관련 없는 장기들로부터 발생할 수 있는 노이즈를 줄이고, 다중 형태 정보(Multi-modal Information)를 효과적으로 통합하는 혁신적인 방법론을 제시합니다.
- ***Technical Details***: ORID 프레임워크는 크게 두 가지 모듈로 구성되어 있습니다: 장기 기반 교차 모달 융합 모듈(Organ-based Cross-modal Fusion Module)은 장기-부위 진단 설명과 방사선 이미지를 효율적으로 결합하고, 장기 중요도 계수 분석 모듈(Organ Importance Coefficient Analysis Module)은 그래프 신경망(Graph Neural Network; GNN)을 활용하여 각 장기 영역의 교차 모달 정보의 상호 연결성을 분석합니다. 이를 통해 라벨 방식의 데이터에서 성능을 강화하고자 합니다. 또한, 약 10,000개의 질문-답변 쌍으로 구성된 방사선 이미지 기반의 명령어 데이터셋을 구성하여, LLaVA-Med-RRG 모델을 개발하였습니다.
- ***Performance Highlights***: 제안된 ORID 프레임워크는 IU-Xray와 MIMIC-CXR 두 가지 공공 벤치마크 상에서 새로운 최고 성능을 달성하였습니다. 특히, BLEU, METEOR, ROUGE-L 등 다양한 자연어 생성(Natural Language Generation; NLG) 메트릭에서 기존의 최첨단 모델보다 우수한 성과를 보여주었습니다. 이는 종합적인 장기별 질병 분석 성능이 향상되었음을 시사합니다.

### [When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training](https://arxiv.org/abs/2411.13476)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.13476.png)

Vote: 6

Authors: Chao Du, Qian Liu, Cunxiao Du, Kenji Kawaguchi, Tianyu Pang, Tongyao Zhu, Haonan Wang

- ***What's New***: 이 논문은 BFloat16 부동소수점 정밀도가 Rotary Positional Embedding (RoPE)의 상대 위치 인코딩 속성을 장기 컨텍스트(Long-context) 훈련에서 손상시킨다는 중요한 문제를 식별합니다. 이를 해결하기 위해, AnchorAttention이라는 주목할 만한 새로운 주의 메커니즘을 소개하여 BFloat16이 발생시키는 수치적 문제를 완화하고, 훈련을 가속화하며, 일관된 위치 ID로 첫 번째 토큰을 공유 앵커로 취급하여 모든 문서에 보이도록 합니다.
- ***Technical Details***: AnchorAttention은 RoPE와 BFloat16이 결합될 때 발생하는 수치적 문제를 해결하기 위해 개발되었습니다. 이 방법은 각 문서의 첫 번째 토큰을 수정된 위치 ID로 취급하고 다른 문서의 토큰이 서로 보이지 않도록 보장합니다. 이로 인해 불필요한 주의 계산을 줄이고 일관된 의미적 일관성을 유지하며 계산 효율성을 높입니다.
- ***Performance Highlights***: AnchorAttention은 표준 전체 주의 메커니즘 대비 훈련 시간을 50% 이상 줄이면서 장기 컨텍스트 성능을 상당히 개선했습니다. RULER 벤치마크와 같은 장기 컨텍스트 평가에서, AnchorAttention은 8K부터 128K까지의 길이에서 뛰어난 성능을 보였으며, LongBench와 같은 실제 장기 컨텍스트 벤치마크에서도 일반 작업에서의 모델 성능을 크게 보존했습니다.

### [Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents](https://arxiv.org/abs/2411.06559)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.06559.png)

Vote: 9

Authors: Yu Gu, Kai Zhang, Sanjari Srivastava, Boyu Gou, Huan Sun, Boyuan Zheng, Yanan Xie, Cheng Chang, Yu Su, Peng Qi

- ***What's New***: WEBDREAMER는 복잡한 웹 환경에서 LLM을 세계 모델(World Models)로 활용하여 효율적인 계획을 가능하게 하는 혁신적인 접근 방식을 소개합니다. 이 연구는 웹 인터페이스의 복잡함에도 불구하고, LLM이 웹사이트의 구조와 기능에 관한 포괄적 지식을 암시하고 있다는 통찰에서 출발합니다.
- ***Technical Details***: WEBDREAMER는 LLM을 사용하여 각 후보 행동의 결과를 시뮬레이션(simulate)하고, 자연어 설명을 통해 최상의 행동을 평가합니다. 이를 위해 LLM은 시뮬레이션 기능(sim)과 평가 기능(score)을 담당합니다. 시뮬레이션은 행동이 실행된 후의 상태 변화를 예측하며, 결과를 바탕으로 후속 행동을 생성하여 모든 가능한 경로를 평가합니다. 계획은 Model Predictive Control(MPC)을 기반으로 하며, 각 상태에서 미래 경로를 시뮬레이션하고 평가하여 최적의 행동을 선택합니다.
- ***Performance Highlights***: WEBDREAMER는 VisualWebArena와 Mind2Web-live 벤치마크에서 기존의 반응형 에이전트에 비해 상당한 성능 향상을 보여주었습니다. VisualWebArena에서 반응형 에이전트에 비해 33.3% 상대 성능 향상을 기록하였고, Mind2Web-live에서는 2.9% (13.1% 상대 증가) 성능을 향상시켰습니다. 이는 LLM 기반의 세계 모델을 활용한 모델 기반 계획이 실제 웹 탐색에서의 유연성과 실용성을 제공한다는 것을 강조합니다.

### [SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2411.10958)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10958.png)

Vote: 33

Authors: Pengle Zhang, Jun Zhu, Haofeng Huang, Jianfei Chen, Jintao Zhang, Jia Wei

- ***What's New***: SageAttention2는 주목 프로세스에서 매우 빠른 4비트 행렬 곱셈(Matrix Multiplication; Matmul)을 사용하여 정확성을 유지하면서 속도를 크게 향상시킬 수 있는 새로운 기술을 소개합니다. 이를 통해 플러그 앤 플레이 방식으로 추론 가속화를 가능하게 합니다.
- ***Technical Details***: SageAttention2는 행 단위(warp-level)로 행렬 Q, K를 INT4로 양자화하고 행렬 eP, V를 FP8로 양자화합니다. 이 방법은 Q, K에 대해 주목 정확성을 개선하기 위해 부드럽게(smooth) 하는 방법을 사용하며, 단계와 계층을 넘어 다양한 모델의 종단간 메트릭을 보장하기 위한 적응형 양자화 방법을 제안합니다. RTX4090 GPU 상에서 SageAttention2는 FlashAttention2와 xformers보다 약 3배, 5배 더 높은 OPS를 달성합니다.
- ***Performance Highlights***: SageAttention2는 FLOPS 당 485 TOPS를 기록하며, FlashAttention2와 xformers보다 각각 약 3.1배, 5.4배 빠른 성능을 보여줍니다. 다양한 모델 실험에서 종단간 메트릭 손실이 거의 없음을 확인했으며, 이는 언어, 이미지 및 비디오 생성 모델 전반에 걸쳐 적용 가능합니다.

### [VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation](https://arxiv.org/abs/2411.13281)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.13281.png)

Vote: 14

Authors: Haoning Wu, Mohan Kankanhalli, Junnan Li, Jing Ma, Ziyang Luo, Dongxu Li

- ***What's New***: VideoAutoArena는 비디오 분석을 위한 대형 멀티모달 모델(Large Multimodal Models; LMMs)을 자동으로 평가하기 위한 새로운 아레나 스타일의 벤치마크입니다. 기존의 고비용 및 시간 소모적인 인간 주석 작업과 달리, 사용자 시뮬레이션(User Simulation)을 활용하여 효율적이고 확장 가능한 평가를 진행합니다.
- ***Technical Details***: VideoAutoArena는 동일한 비디오에 대해 두 모델이 질문에 응답하고, 자동화된 심판이 더 나은 응답을 결정하는 피어 배틀(Peer Battles)을 포함합니다. 또한, 모델의 성능에 따라 질문의 난이도를 점차 높이는 오류 기반 진화(Fault-Driven Evolution) 전략을 도입하여 보다 엄격한 테스트 환경을 제공합니다.
- ***Performance Highlights***: VideoAutoArena는 11개의 SOTA LMMs을 평가하여 오픈 소스 모델인 Aria가 폐쇄형 모델인 GPT-4o에 비해 상당한 성능 격차(-385.7)를 보여줍니다. 질문의 난이도가 높아지면 격차가 더욱 두드러지며, 특히 긴 비디오나 사용자 배경 관련성 및 응답의 유용성에서 성능 차이가 더욱 큽니다. 또한 우리의 자동 채점은 인간 선호 선택과 87.29%의 일관성을 보여 효율적이고 신뢰성 있는 평가를 제공합니다.

### [Loss-to-Loss Prediction: Scaling Laws for All Datasets](https://arxiv.org/abs/2411.12925)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12925.png)

Vote: 2

Authors: David Brandfonbrener, Nikhil Vyas, Nikhil Anand, Sham Kakade, Eran Malach

- ***What's New***: 본 논문에서는 다양한 분포로부터의 스케일링 법칙(Scaling Laws)을 예측하는 새로운 방법론인 손실 대 손실 예측(Loss-to-Loss Prediction)을 제안하고, 이를 통해 사전 훈련 데이터셋 간의 손실을 예측하는 전략을 제시합니다. 특히, 기존의 단일 데이터 분포에 의존하지 않고도 다양한 작업 데이터에 대해 손실을 예측할 수 있는 능력을 탐구합니다.
- ***Technical Details***: 이 연구는 손실과 손실 간의 관계를 이용하여 스케일링 법칙을 전환하는 방법론을 제안합니다. 세 가지 주요 관계인 트레인-트레인(Train-to-Train), 트레인-테스트(Train-to-Test), 테스트-테스트(Test-to-Test)의 예측 관계를 정의하고 이를 수학적으로 설명합니다. 특히, 특정 데이터셋에 대한 스케일링 법칙을 다른 데이터셋의 결과로 번역할 수 있는 수학적 프레임워크를 도출합니다.
- ***Performance Highlights***: 6개의 다양한 사전 훈련 데이터셋과 11개의 다운스트림 작업에서의 손실 예측을 통해 제안된 방법론의 효과성을 입증합니다. 실험 결과, 손실 대 손실 예측을 사용한 스케일링 법칙이 독립적인 스케일링 법칙보다 더 정확한 예측을 보여주며, 특히 새로운 데이터셋에 대한 예측에서 탁월한 성과를 보였습니다.

