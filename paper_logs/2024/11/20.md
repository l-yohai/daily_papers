## Daily Papers (2024-11-20)

### [Building Trust: Foundations of Security, Safety and Transparency in AI](https://arxiv.org/abs/2411.12275)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12275.png)

Vote: 6

Authors: Mark Bestavros, Garth Mollett, Huzaifa Sidhpurwala, Huamin Chen, Emily Fox

- ***What's New***: 이 논문은 공개적으로 이용 가능한 AI 모델 생태계의 급속한 진화를 탐구하며, 이에 따른 보안(Security) 및 안전(Safety) 환경에 대한 잠재적 영향을 설명합니다. AI 모델의 보급이 증가함에 따라 모델의 위험성과 취약성을 이해하는 것이 중요합니다. 따라서 모델 개발자와 최종 사용자 모두를 위한 보안 및 안전을 강화하기 위한 포괄적인 전략을 제공합니다.
- ***Technical Details***: AI 보안(Security)과 AI 안전(Safety)은 각각 기술적 위협, 허가되지 않은 접근, 공격으로부터 AI 시스템을 보호하는 것과 시스템의 의도된 운용이 사용자나 사회에 해를 끼치지 않도록 보장하는 것에 중점을 둡니다. AI 모델의 보안 취약점은 주로 기밀성, 무결성, 가용성의 손실과 관련되어 있으며, AI 안전 문제는 불의의 콘텐츠 생성, 의사 결정의 편향 및 사회적 규범 위반 등을 포함합니다.
- ***Performance Highlights***: 본 논문에서는 AI의 보안성과 안전성 관리를 위한 새로운 접근법을 제안하며, 이를 위해 AI 보안 취약점 추적 및 보고 메커니즘을 개선하는 다양한 방법론과 프레임워크를 언급합니다. 현재의 AI 안전 평가의 표준화 부재에 대해서도 논의하며, 안전 평가를 위한 공통 기준을 마련할 것을 제안합니다.

### [SEAGULL: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning](https://arxiv.org/abs/2411.10161)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10161.png)

Vote: 4

Authors: Zewen Chen, Bing Li, Jian Guo, Weiming Hu, Juan Wang, Chunfeng Yuan, Sunhan Xu, Hang Xiong, Shuxun Wang, Wen Wang, Yun Zeng

- ***What's New***: 이 논문에서는 SEAGULL이라는 새로운 네트워크를 제안하여 주목 영역(Region of Interest; ROI)을 비참조 이미지 품질 평가(No-reference Image Quality Assessment; NR-IQA) 가능하게 합니다. 이는 대형 비전-언어 모델(Vision-Language Model; VLM)을 사용하여 고해상도 품질 분석을 수행하며, Segment Anything Model(SAM)이 생성한 마스크를 사용하여 ROI를 명확히 지정합니다.
- ***Technical Details***: SEAGULL은 VLM 및 정교하게 설계된 마스크 기반 특징 추출기(Mask-based Feature Extractor; MFE)로 구성되어, 지정된 ROI의 전역 및 지역 토큰을 추출하여 정확한 세부 품질 평가를 가능하게 합니다. 이를 위해 SEAGULL-100w와 SEAGULL-3k라는 두 개의 ROI 기반 IQA 데이터셋이 구축되었습니다. SEAGULL-100w는 약 100만 개의 합성 왜곡 이미지로 이루어져 있으며, SEAGULL-3k는 3,261개의 실제 왜곡 ROI를 포함합니다.
- ***Performance Highlights***: SEAGULL은 SEAGULL-100w에서 사전 학습하고 SEAGULL-3k에서 미세 조정을 통해 주목 영역의 고해상도 품질 평가에서 탁월한 성능을 보였습니다. 실험 결과 SEAGULL이 기존의 첨단 IQA 모델 및 VLM을 ROI 품질 분석 측면에서 유의미하게 능가함을 입증하며, 코드 및 데이터셋은 공개되어 있습니다.

### [Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages](https://arxiv.org/abs/2411.12240)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12240.png)

Vote: 2

Authors: D. J. Bora, S. Tamang

- ***What's New***: 이 연구는 인도의 22개 공식 언어에 걸친 대형 언어 모델(Large Language Models; LLM)에 사용되는 토크나이저(tokenizer) 평가에 초점을 맞추고 있으며, SUTRA 토크나이저가 12개의 다양한 LLM을 넘어서 14개 언어에서 가장 우수한 성능을 보였음을 밝혔습니다. 이는 인도의 복잡한 언어 구조를 다루는 데 있어 토크나이저 개발의 중요성을 강조합니다.
- ***Technical Details***: 연구에서는 인도의 모든 공식 언어에 대해 LLM의 토크나이저 성능을 평가하는데, Normalized Sequence Length (NSL) 지표를 사용했습니다. 토크나이저는 주어진 예제 문장의 인코딩된 시퀀스 길이를 바탕으로 평가되었으며, 12개의 모델이 시험되었습니다. 본 연구는 특히 GPT-4o의 인도어 처리에서의 진보를 발견하였으며, 프로젝트 '인더스'(Project Indus)의 제한된 성능을 확인하였습니다.
- ***Performance Highlights***: SUTRA 토크나이저가 총 22개 언어 중 14개 언어에서 가장 우수한 성능을 기록했으며, 이는 다른 토크나이저보다 훨씬 낮은 NSL 값을 보였습니다. 다른 모델로는 MBZUAI의 난다(Nanda)가 6개 언어에서 최상의 성능을 보였고, 오픈AI의 GPT-4o는 5개 언어에서 우수한 성능을 나타냈습니다. 이는 토크나이저 설계 개선을 통해 다국어 모델 효율성을 높이는 방향성을 제시합니다.

### [FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations](https://arxiv.org/abs/2411.10818)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10818.png)

Vote: 6

Authors: Hmrishav Bandyopadhyay, Yi-Zhe Song

- ***What's New***: FlipSketch는 정적인 드로잉을 텍스트 지침에 따라 스케치 애니메이션으로 변환하는 시스템으로 소개됩니다. 이는 기존의 벡터 애니메이션 알고리즘이 제공하지 못했던 장면 수준의 역동성을 지원하며, 입력 스케치를 바탕으로 애니메이션을 제어하여 스케치의 정체성을 유지하면서도 동적인 변형을 가능하게 합니다.
- ***Technical Details***: FlipSketch는 큰 첨단 모션 사전(Motion Priors)을 텍스트-비디오 확산 모델(Text-to-Video Diffusion Models)을 통해 활용합니다. 세 가지 혁신을 통해 스케치 애니메이션을 생성하는데: (i) 스케치 스타일 프레임 생성에 대해 파인 튜닝, (ii) 입력 스케치의 시각적 무결성을 유지하는 레퍼런스 프레임(reference frame) 메커니즘, (iii) 시각적 일관성을 잃지 않고 유동적 모션을 가능하게 하는 듀얼 주의 집중(dual attention composition)을 적용합니다.
- ***Performance Highlights***: FlipSketch는 기존의 벡터 기반 기술에 비해 훨씬 더 자유롭고 동적인 애니메이션을 수행할 수 있습니다. 비교 연구 결과, 동적 범위와 애니메이션 품질에서 최첨단 벡터 스케치 애니메이션을 능가하여 사용자 연구에서도 높은 평가를 받았습니다. 또한, 프레임 정렬 및 주의 집중 메커니즘을 통한 향상된 프레임 일관성을 통해 스케치의 정체성을 보존합니다.

### [ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements](https://arxiv.org/abs/2411.12044)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12044.png)

Vote: 10

Authors: Gozde Unal, Yusuf H. Sahin, M. Arda Aydın, Efe Mert Çırpar, Elvin Abdinli

- ***What's New***: ITACLIP은 Vision Language Models (VLMs)인 CLIP의 성능을 강화하여 Zero-Shot Open-Vocabulary Semantic Segmentation (OVSS) 작업에서 최고의 결과를 제공합니다. 이 논문은 CLIP 모델의 이미지 및 텍스트 기능을 증대시켜 학습 필요 없이 개선된 성능을 이끌어 냅니다.
- ***Technical Details***: ITACLIP은 Vision Transformers(ViT)의 마지막 레이어에서 자기 주의 메커니즘을 수정하고 중간 레이어의 주의 맵을 최종 레이어와 결합하여 아키텍처를 개선합니다. 또한, 이미지 엔지니어링(Image Engineering)을 통해 입력 이미지 표현을 강화하고, 대형 언어 모델(Large Language Models; LLMs)을 사용하여 각 클래스 이름에 대한 정의와 동의어를 생성하여 CLIP의 Open-Vocabulary 기능을 활용합니다.
- ***Performance Highlights***: ITACLIP은 COCO-Stuff, COCO-Object, Pascal Context, Pascal VOC 벤치마크에서 현재 최첨단(SOTA) 방법들을 뛰어넘는 성능을 보였습니다. 특히, VOC 데이터셋에서는 67.9%의 mIoU를 달성하며, 다른 모델들에 비해 우수한 성능을 나타냅니다.

### [Soft Robotic Dynamic In-Hand Pen Spinning](https://arxiv.org/abs/2411.12734)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12734.png)

Vote: 8

Authors: Jean Oh, Christopher G. Atkeson, Uksang Yoo, Yunchao Yao, Jeffrey Ichnowski

- ***What's New***: SWIFT는 소프트 로봇 손을 이용하여 실제 데이터에 기반해 다이나믹한 작업을 학습하여 펜 스피닝을 수행하는 시스템입니다. 사전 물리적 지식 없이도 각기 다른 무게와 분포를 가진 펜에 대해 100% 성공률을 달성하며, 다양한 물체의 변화를 견딜 수 있는 일반성과 견고성을 입증합니다.
- ***Technical Details***: SWIFT는 다이나믹 인-핸드 펜 스피닝을 위한 소프트 Multi-finger Omnidirectional End-effector(MOE)를 사용합니다. 펜 스피닝의 작업은 그랩(grasping)과 스피닝, 캐칭(catching)으로 구성되며, RGB-Depth 카메라를 통한 시각적 피드백을 사용하여 최적화 목표를 정의합니다. Covariance Matrix Adaptation Evolution Strategy (CMA-ES)를 사용하여 파라미터 공간을 탐색하고, 각 물체에 최적의 파라미터를 찾아냅니다.
- ***Performance Highlights***: SWIFT는 3개의 무게 및 분포가 다른 펜에 대해 130번의 시도 이후 100% 성공률을 달성했습니다. 또한, 브러쉬와 스크류드라이버와 같은 다른 모양과 무게를 가진 아이템도 각각 10/10, 5/10의 성공률로 스피닝하는 데 성공하여 SWIFT 시스템의 범용성과 적응성을 강조합니다.

### [RedPajama: an Open Dataset for Training Large Language Models](https://arxiv.org/abs/2411.12372)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12372.png)

Vote: 20

Authors: Kezhen Chen, Ben Athiwaratkun, Xiaozhe Yao, Maurice Weber, Virginia Adams, Percy Liang, Quentin Anthony, Shane Adams, Tri Dao, Yonatan Oren, Huu Nguyen, Anton Alexandrov, Irina Rish, Max Ryabinin, Xiaozhong Lyu, Daniel Fu, Christopher Ré, Ce Zhang, Rahul Chalamala

- ***What's New***: RedPajama는 현대 대형 언어 모델(Large Language Models; LLMs)의 훈련을 위한 투명한 오픈 데이터셋을 선보이며, 특히 RedPajama-V1은 LLaMA 모델의 데이터셋을 재구성하였고, RedPajama-V2는 웹 크롤링을 기반으로 한 대규모 데이터셋으로, 총 100조 개 이상의 토큰을 포함하고 있습니다.
- ***Technical Details***: RedPajama-V1은 CommonCrawl, GitHub, Wikipedia 등의 일곱 개 데이터셋에서 추출된 데이터를 통해 LLaMA 훈련 데이터를 재구성하였습니다. RedPajama-INCITE 모델들은 이 데이터셋을 대상으로 3B 및 7B 규모의 모델로 훈련되었습니다. RedPajama-V2는 2014년부터 2023년까지의 웹 데이터를 포함한 84개 월별 스냅샷을 활용했으며, 각 문서에는 46가지의 품질 신호와 메타데이터가 첨부되어 효율적인 데이터 필터링을 지원합니다.
- ***Performance Highlights***: RedPajama-INCITE 3B 모델은 Pythia-2.8B 및 GPT-Neo 모델보다 향상된 zero-shot 및 few-shot 성능을 보였으며, 7B 모델은 Llama-7B에 근접한 성능을 보여주었습니다. RedPajama-V2로부터 생성된 하위 데이터셋들은 다양한 NLP 벤치마크에서 경쟁력 있는 성능을 입증하였습니다.

### [Continuous Speculative Decoding for Autoregressive Image Generation](https://arxiv.org/abs/2411.11925)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11925.png)

Vote: 13

Authors: Shiming Xiang, Zili Wang, Qi Yang, Fei Li, Kun Ding, Robert Zhang

- ***What's New***: 이 연구는 연속 값(Continuous-valued) 시공간 공간에서의 자가 회귀 이미지 생성(Autoregressive Image Generation)을 가속화하기 위한 새로운 방법인 연속 투기적 디코딩(Continuous Speculative Decoding)을 제안합니다. 이는 기존의 LLM(대형 언어 모델)에서 사용되던 투기적 디코딩을 연속 공간으로 확장한 최초의 시도입니다.
- ***Technical Details***: 연구에서는 확산(distribution)의 고유한 특성을 분석하여 수용 기준(Acceptance Criterion)을 설정하고, 투기적 디코딩의 출력 분포에서 발생하는 불일치를 극복하기 위해 잡음 경로 정렬(Denoising Trajectory Alignment) 및 토큰 사전 채우기(Token Pre-filling) 방법을 소개합니다. 또한, 수용-거절 표집법(Acceptance-Rejection Sampling)을 통해 복잡한 적분을 피하고 한계 상한을 설정하여 문제를 해결합니다.
- ***Performance Highlights***: 제안된 연속 투기적 디코딩 방법은 기존 오픈소스 모델에서 2.33배까지의 속도 향상을 실현하면서도 원본 출력 분포와 높은 생성 충실도(Generation Fidelity)를 유지합니다. 실험은 FID (Fréchet Inception Distance)와 IS (Inception Score)를 사용하여 ImageNet 256 × 256 생성에서 성능을 평가하였으며, 결과는 효과적인 이미지 품질을 보여줍니다.

