## Daily Papers (2024-11-11)

### [Balancing Pipeline Parallelism with Vocabulary Parallelism](https://arxiv.org/abs/2411.05288)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.05288.png)

Vote: 10

Authors: Penghui Qi, Min Lin, Xinyi Wan, Man Tsung Yeung

- ***What's New***: 이 연구는 Transformer 기반의 대규모 언어 모델(Large Language Models; LLMs)의 학습 시 Pipeline Parallelism에서 발생하는 계산 및 메모리 불균형 문제, 특히 Vocabulary Layers의 불균형 문제를 해결합니다. 이를 위해 Vocabulary Parallelism을 제안하여 계산 및 메모리의 균형을 맞춥니다.
- ***Technical Details***: Vocabulary Parallelism은 Vocabulary Layers를 파이프라인 내의 모든 장치로 균등하게 분할하여 작업을 그룹화하고 기존 Pipeline Schedule에 통합합니다. 이 방법은 소량의 상수 활성화 메모리만을 초과하여 파라미터 메모리와 계산을 효과적으로 균형 있게 유지합니다. 주목할 만한 점은, V-Half와 같은 Activation Memory 균형 일정을 사용하면 메모리와 계산 모두에서 완벽한 균형을 이룰 수 있다는 것입니다.
- ***Performance Highlights***: 이 연구 결과, Vocabulary Parallelism은 계산을 안정적으로 유지하며, 일정 스케일의 Vocabulary에서 5%에서 51%까지 향상된 처리량을 보였으며, 특히 큰 Vocabulary 크기에서 최고 메모리 사용량을 크게 줄였습니다. 이 방법은 분산 학습 시 기존의 나이브한 방법 대비 실질적인 성능 향상을 보여줍니다.

### [Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding](https://arxiv.org/abs/2411.04282)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.04282.png)

Vote: 4

Authors: Huan Wang, Yihao Feng, Zuxin Liu, Shelby Heinecke, Silvio Savarese, Phil Mui, Akshara Prabhakar, Weiran Yao, Haolin Chen, Caiming Xiong, Ricky Ho

- ***What's New***: 이 논문에서는 LaTent Reasoning Optimization (LaTRO)라는 새로운 프레임워크를 소개하여, 언어 모델(LLM)의 잠재적인 추론 능력을 자가 개선 방식으로 최적화합니다. LaTRO는 외부 피드백이나 보상 모델 없이도 모델이 자체적으로 추론 경로 및 품질 평가 능력을 동시에 향상시킬 수 있게 합니다.
- ***Technical Details***: LaTRO는 추론을 잠재 분포로부터의 샘플링으로 개념화하고 이를 변분적 접근법을 통해 최적화합니다. 이 방법은 LLM이 주어진 질문에 대해 여러 추론 단계를 생성하고, 올바른 답을 생성할 확률을 평가한 후, 고품질의 추론을 선호하도록 매개변수를 업데이트합니다. 구체적인 알고리즘은 LaTRO 알고리즘을 통해 제공됩니다.
- ***Performance Highlights***: GSM8K와 ARC-Challenge 데이터셋에서 실험을 수행한 결과, LaTRO는 제로샷 정확도에서 기본 모델 대비 최대 12.5% 향상된 성능을 보였으며, 감독요약 모델보다 평균 9.6% 더 높은 성능을 보였습니다. 이러한 결과는 LLM이 이미 잠재적으로 강력한 추론 능력을 가지고 있으며, LaTRO를 통해 이를 보다 효과적으로 활용할 수 있음을 시사합니다.

### [DELIFT: Data Efficient Language model Instruction Fine Tuning](https://arxiv.org/abs/2411.04425)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.04425.png)

Vote: 4

Authors: Lucian Popa, Marina Danilevksy, Ishika Agarwal, Krishna Killamsetty

- ***What's New***: DELIFT(DELIFT: Data Efficient Language model Instruction Fine-Tuning)는 대규모 언어 모델(LLMs)의 미세 조정을 위한 데이터 효율적 최적화 알고리즘을 소개합니다. 이 알고리즘은 다양한 미세 조정 단계(Instruction Tuning, Task-Specific Fine-Tuning, Continual Fine-Tuning)에서 데이터 선택을 최적화하여 데이터 크기를 최대 70%까지 줄일 수 있습니다.
- ***Technical Details***: DELIFT는 쌍별 유틸리티 메트릭(Pairwise Utility Metric)을 사용하여 데이터 샘플의 유익성을 평가하고 이를 기반으로 서브모듈러 함수(Submodular Functions)를 활용하여 각 미세 조정 단계에 유용한 최적 데이터 하위 집합을 선택합니다. 이 메트릭은 정보의 상대적 가치를 측정하여 모델의 성능을 효과적으로 개선합니다.
- ***Performance Highlights***: DELIFT는 다양한 작업과 모델 스케일에서 실험을 통해 기존 방법들에 비해 최대 26%까지 성능을 개선하였으며, 데이터 크기를 70% 줄인 상태에서도 성능 저하 없이 효율성을 증명하였습니다. 특히, 일부 작업에서는 전체 데이터를 사용하는 경우보다 더 나은 결과를 달성하기도 했습니다.

### [LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation](https://arxiv.org/abs/2411.04997)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.04997.png)

Vote: 11

Authors: Xufang Luo, Xiyang Dai, Qi Dai, Aoqi Wu, Weiquan Huang, Yifan Yang, Lili Qiu, Dongdong Chen, Yuqing Yang, Chong Luo, Liang Hu

- ***What's New***: LLM2CLIP는 대형 언어 모델(LLM)이 CLIP의 시각 표현 학습을 향상시킬 수 있는 방법을 제안합니다. LLM의 강력한 언어 이해를 통해, CLIP는 더 긴복잡한 텍스트를 더 효과적으로 처리할 수 있으며, 이는 기존 CLIP의 한계를 극복할 수 있게 도와줍니다.
- ***Technical Details***: LLM2CLIP은 LLM의 캡션 공간에서 대조 학습(contrastive learning)을 통하여 텍스트의 변별성을 향상시키고, 이러한 LLM을 CLIP의 시각적 인코더의 강력한 교사 모델로 사용합니다. LLM의 내재된 개방 세계 지식을 활용하여 더 길고 복잡한 캡션을 처리할 수 있습니다.
- ***Performance Highlights***: LLM2CLIP은 기존 SOTA EVA02 모델의 성능을 긴 텍스트와 짧은 텍스트 검색 작업 모두에서 16.5% 향상시켰으며, 단일 언어로 훈련된 CLIP 모델을 최신 크로스-링구얼 모델로 변환했습니다. 멀티모달 훈련에 통합 시, 거의 모든 벤치마크에서 CLIP를 능가하면서 포괄적인 성능 향상을 보여줍니다.

### [Improving the detection of technical debt in Java source code with an enriched dataset](https://arxiv.org/abs/2411.05457)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.05457.png)

Vote: 2

Authors: Davide Di Ruscio, Anh M. T. Bui, Phuong T. Nguyen, Nam Le Hai, Rick Kazman

- ***What's New***: 이 연구는 Java 소스 코드에서 기술 부채(Technical Debt; TD) 탐지를 향상시키기 위한 개선된 데이터셋을 소개합니다. 이 데이터셋은 코드 주석을 분석하여 기술 부채를 식별하며 이는 첫 번째 시도입니다. 기존 텍스트 기반의 탐지 방법 외에도 실제 소스 코드의 정보를 통합하여 기술 부채를 더욱 정확하게 예측하려는 접근 방식을 제안합니다.
- ***Technical Details***: 이 연구에서는 SATD(Self-Admitted Technical Debt) 탐지를 위한 개선된 파이프라인을 제안합니다. 먼저 공개된 Java 프로젝트에서 주석과 대응하는 소스 코드를 분석하여 기술 부채를 식별하고, BERT, RoBERTa와 같은 기계 학습 모델을 통해 성능을 평가했습니다. 주석과 소스 코드 모두를 활용하여 모델의 예측 정확도를 향상시키기 위한 다양한 맥락 길이의 코드 통합 방법(StrConcat, CodeAtt)을 사용했습니다.
- ***Performance Highlights***: Kommentar와 주석만을 이용한 SATD 탐지 모델의 성능을 비교한 결과, 소스 코드 정보가 추가됨으로써 모든 모델에서 성능이 향상됨을 보였습니다. 특히 CodeBERT와 GraphCodeBERT 모델은 각각 4.88%, 4.86%의 성능 향상을 보였습니다. 그러나 소스 코드만으로 기술 부채를 탐지할 경우, 대부분의 모델의 성능이 50%에 미치지 않아, 더욱 발전된 접근 방식이 필요함을 시사합니다.

### [Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study](https://arxiv.org/abs/2411.02462)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.02462.png)

Vote: 4

Authors: André Storhaug, Jingyue Li

- ***What's New***: 이 논문은 대형 언어 모델(Large Language Models; LLMs)을 단위 테스트 생성에 특화하여 파라미터-효율적 미세 조정(Parameter-Efficient Fine-Tuning; PEFT)을 처음으로 광범위하게 평가한 연구입니다. PEFT 방법이 전체 미세 조정과 비교해도 거의 동등한 성능을 제공한다는 사실을 발견했습니다. 특히 프롬프트 튜닝(Prompt Tuning)은 비용과 자원 사용 면에서 가장 효율적이며, LoRA는 여러 경우에 있어 전체 미세 조정에 근접한 효과를 보였습니다.
- ***Technical Details***: PEFT 방법 중 LoRA, (IA)3, 프롬프트 튜닝이 탐구되었습니다. 각기 다른 모델 아키텍처와 크기에서의 성능을 비교하기 위해 잘 정립된 벤치마크 데이터셋을 사용하였습니다. 연구는 3개의 오픈 소스, 디코더 전용 LLM 패밀리에서 다양한 크기의 10개의 LLM을 대상으로 하여 수행되었습니다. 각 PEFT 방법의 성능은 잘 알려진 벤치마크 데이터세트를 통해 평가되었습니다.
- ***Performance Highlights***: 전체 미세 조정이 비효율적이며, 많은 계산 자원이 필요함을 확인했습니다. 반면, PEFT 방법들은 이와 같은 자원 소비를 줄이면서도 전체 미세 조정과 비슷한 성능을 보여주었습니다. 특히, 프롬프트 튜닝은 모델 크기가 클수록 더 효과적인 경향을 보였으며, LoRA는 안정성이 뛰어나 대체할 만한 신뢰할 수 있는 방법으로 평가되었습니다. 

### [RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models](https://arxiv.org/abs/2411.04097)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.04097.png)

Vote: 3

Authors: Jean-Benoit Delbrouck, Curtis Langlotz, Akshay Chaudhari, Maya Varma, Zhihong Chen

- ***What's New***: RAVL은 세분화된 이미지 특성을 사용하여 비정상적 상관관계를 발견하고 완화하는 비전-언어 모델(Vision-Language Models; VLMs)의 강인성을 향상시키기 위한 새로운 연구입니다. 이는 기존의 전역 이미지 수준에서 작동하는 방법과는 달리, 지역적 이미지 특성을 통해 보다 미세한 수준에서 상관관계를 확인하고 완화합니다.
- ***Technical Details***: RAVL은 특성 클러스터링 접근법을 사용하여 이미지의 오차를 유도하는 특정 이미지 특성을 식별하고, 새로운 지역 인식 손실 함수(region-aware loss function)를 도입하여 모델이 관련 있는 지역에 집중하도록 유도합니다. 실험은 654개의 VLMs에서 다양한 모델 아키텍처와 데이터 도메인을 기반으로 수행되었으며, RAVL은 클러스터의 성능 간극 및 인플루언스 점수를 계산하여 정확한 비정상적 상관관계를 발견합니다.
- ***Performance Highlights***: RAVL은 비정상적 상관관계를 기존 방법보다 191% 더 정확하게 발견하며, 이미지 최악 그룹 분류 정확도를 8.2% 향상시켰습니다. 이는 VLMs의 실제 및 합성 데이터에서의 정성적 평가로 확인되었습니다.

### [StdGEN: Semantic-Decomposed 3D Character Generation from Single Images](https://arxiv.org/abs/2411.05738)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.05738.png)

Vote: 9

Authors: Yanning Zhou, Kaiwen Xiao, Yong-Jin Liu, Wang Zhao, Wei Yang, Xiao Han, Yuze He, Zhongkai Wu

- ***What's New***: StdGEN은 단일 이미지에서 고품질의 의미론적인 분해가 가능한 3D 캐릭터를 생성할 수 있는 혁신적인 파이프라인입니다. 이 파이프라인은 가상 현실, 게임, 영화 제작 등의 광범위한 응용 분야에 적용될 수 있습니다. StdGEN은 첫 번째로 의미론적 대형 복원 모델(S-LRM; Semantic-aware Large Reconstruction Model)을 소개합니다. 이 모델은 다중 뷰 이미지에서 기하학, 색상 및 의미를 동시에 복원할 수 있습니다.
- ***Technical Details***: StdGEN의 핵심은 S-LRM으로, 다중 뷰에서의 입력 이미지로부터 변환 기반의 트라이플레인(Triplane) 표현을 생성합니다. 이러한 트라이플레인 특징은 다시 해석되어 색상 및 의미론적 필드를 산출합니다. 또한, 차별 가능한 다중 레이어 의미론적 서페이스 추출 기법과 효율적인 다중 뷰 확산 모델이 통합되어 고품질 의미분해 3D 캐릭터 생성을 지원합니다.
- ***Performance Highlights***: Anime3D++ 데이터셋에서 StdGEN은 기존의 모든 기준을 능가하는 성능을 발휘했습니다. 특히 임의의 포즈 및 A-포즈 3D 캐릭터 생성에서 최첨단 성능을 달성했으며, 특히 지오메트리와 텍스처의 세밀한 묘사에서 뛰어난 결과를 보여줍니다.

### [The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities](https://arxiv.org/abs/2411.04986)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.04986.png)

Vote: 3

Authors: Jiasen Lu, Yoon Kim, Xinyan Velocity Yu, Dani Yogatama, Zhaofeng Wu

- ***What's New***: 이 논문에서는 현대 AI 언어 모델(Language Models; LMs)이 이질적인 데이터 유형, 즉 여러 언어와 모달리티에서 입력을 처리할 수 있는 능력을 가지며, 이는 다양한 데이터 유형을 통한 공유 표현 공간을 학습하는 것임을 제안합니다. 이 가설을 'Semantic Hub Hypothesis'라고 명명하며, 이는 언어와 모달리티 간에 모델이 의미론적 유사성을 가진 입력 데이터를 가까운 표현 공간으로 배치할 수 있음을 설명합니다.
- ***Technical Details***: Semantic Hub Hypothesis는 중간 레이어에서 서로 다른 언어로 된 의미론적으로 동등한 입력이 유사한 표현을 가지며, 이는 모델의 지배적인 사전 학습 언어, 일반적으로 영어를 통해 해석될 수 있음을 보여줍니다. 추가로 수학적 표현, 코드, 시각적/오디오 입력과 같은 다른 데이터 유형에도 확장됩니다. 또한, 공유 표현 공간의 개입이 한 데이터 유형의 모델 출력에 예측 가능한 영향을 미친다는 점을 강조합니다.
- ***Performance Highlights***: 실험들은 다양한 데이터 유형, 예를 들어 여러 언어, 산술 표현, 코드, 포멀 시맨틱 구조, 다중 모달 입력 간에 공유 표현 공간이 존재함을 일관되게 보여줍니다. 이는 LMs가 의미론적으로 유사한 데이터를 중간 레이어에서 가까이 배치함으로써 표현한다는 것을 나타냅니다. 이러한 공유 표현 공간은 데이터 유형 처리 시에 모델의 실행 메커니즘을 쉽게 해석하고 제어할 수 있는 방법을 제공합니다.

