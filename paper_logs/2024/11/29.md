## Daily Papers (2024-11-29)

### [LongKey: Keyphrase Extraction for Long Documents](https://arxiv.org/abs/2411.17863)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17863.png)

Vote: 2

Authors: Cinthia Obladen de Almendra Freitas, Radu State, Jeovane Honorio Alves, Jean Paul Barddal

- ***What's New***: LongKey는 긴 문서에 대한 핵심 구문 추출(Keyphrase Extraction)을 수행하는 새로운 시스템으로, 인코더 기반 언어 모델을 사용하여 긴 텍스트의 복잡성을 포착합니다. LongKey는 LDKP 데이터셋과 여섯 개의 미시행된 데이터셋을 통해 기존의 비지도 학습 및 언어 모델 기반 핵심 구문 추출 방법을 능가하는 성능을 입증했습니다.
- ***Technical Details***: LongKey는 초기 단어 임베딩(Word Embedding), 핵심 구문 후보 임베딩(Keyphrase Candidate Embedding), 후보 점수 측정의 세 가지 단계로 작동합니다. Longformer 모델을 사용하여 긴 문서의 임베딩을 생성하며, 각 핵심 구문 후보의 발생을 최적화하기 위해 컨볼루션 네트워크를 활용합니다. 최대 풀링(max pooling) 연산을 통해 후보자의 다양한 나타남을 단일 종합 표현으로 집계합니다.
- ***Performance Highlights***: LongKey는 LDKP3K 테스트 셋에서 가장 높은 F1@5 점수인 39.55%와 F1@O 점수 41.84%를 기록하며 최고 성능을 보였습니다. LDKP10K 데이터셋에서도 F1@5가 41.81%로 우수한 성능을 검증하였습니다. 미시행된 데이터셋 평가에서도 대부분의 데이터셋에서 최상의 성능을 기록했습니다.

### [ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting](https://arxiv.org/abs/2411.17176)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17176.png)

Vote: 11

Authors: Zhuohang Dang, Chengyou Jia, Minnan Luo, Hangwei Qian, Weijia Wu, Changliang Xia

- ***What's New***: ChatGen은 사용자가 자유로운 채팅 스타일로 요구 사항을 설명할 수 있도록 하는 자동 텍스트-이미지(T2I; Text-to-Image) 생성 방식을 제안합니다. 이를 위해 새로운 벤치마크인 ChatGenBench를 도입하여 다양한 스타일과 모달리티를 가진 입력을 평가할 수 있습니다.
- ***Technical Details***: ChatGenBench는 6,807개의 맞춤화된 모델로부터 얻은 고품질의 페어링 데이터를 제공합니다. ChatGen-Evo라는 멀티스테이지 진화 전략이 사용되어 모델이 점진적으로 필수 자동화 기술을 습득할 수 있게 합니다. 이 전략은 모델을 3단계로 훈련시켜 프롬프트 생성, 모델 선택, 인수 설정을 수행합니다.
- ***Performance Highlights***: ChatGen-Evo는 다양한 단계의 정확성과 이미지 품질 평과 기준에서 기존 방법들에 비해 뛰어난 성능을 발휘했습니다. 특히 ChatGen-Evo는 8B 파라미터 모델과 유사한 성능을 2B 파라미터에서 달성하면서 스케일링 법칙을 탐구할 수 있는 가능성을 보여줍니다.

### [Morph: A Motion-free Physics Optimization Framework for Human Motion Generation](https://arxiv.org/abs/2411.14951)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14951.png)

Vote: 2

Authors: Ruibing Hou, Hong Chang, Zimo Liu, Hao Liu, Chen Li, Xin Zhao, Zhuo Li, Mingshuang Luo

- ***What's New***: 본 논문에서는 인간 모션 생성에 물리적 제약을 강화할 목적으로 모션 프리 피직스 최적화 프레임워크(Morph)를 제안합니다. Morph는 실제 모션 데이터 없이도 물리적으로 타당한 모션을 생성할 수 있는 효율적이고 모델-독립적인 솔루션을 제공합니다.
- ***Technical Details***: Morph는 두 개의 주요 모듈로 구성되어 있습니다: 모션 생성기(Motion Generator)와 모션 물리 정제 모듈(Motion Physics Refinement module; MPR). MPR 모듈은 물리 시뮬레이터 내에서 캐릭터 에이전트를 제어하여 물리적 제약을 강화하고 물리적으로 타당한 모션으로 변환합니다. 이러한 물리적 모션은 모션 생성기를 미세 조정하는 데 사용됩니다. 또한, 강화 학습을 사용하여 MPR 모듈의 모션 모방기를 최적화하며 모션 판별기를 통해 모션 품질을 자연스럽게 유지합니다.
- ***Performance Highlights***: Morph는 두 가지 작업, 텍스트-모션 및 음악-댄스 생성에서 가장 최신의 모션 생성 품질을 달성하며 물리적 타당성을 크게 향상시켰습니다. 다양한 생성 모델과 결합하여 물리적 오류 메트릭에서 상당한 개선을 보이며, 실제 모션 데이터 없이도 경쟁력 있는 생성 메트릭을 유지하였습니다.

### [AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering Benchmark Dataset](https://arxiv.org/abs/2411.15640)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15640.png)

Vote: 0

Authors: Naome A. Etori, Tassallah Abdullahi, Jonas Kemp, Irfan Essa, Aimérou Ndiaye, Bonaventure F. P. Dossou, Folafunmi Omofoye, Stephen Edward Moore, Foutse Yuehgoh, Evans Doe Ocansey, Charles Nimo, Timothy Faniran, Emmanuel Ayodele, Chinemelu Aka, Chris Fourie, Wendy Kinara, Katherine Heller, Mardhiyah Sanni, Chidi Asuzu MD, Ifeoma Okoh, Mercy Nyamewaa Asiedu, Abraham Owodunni, Jude Chidubem Omeke, Tobi Olatunji, Michael Best, Moshood Yekini

- ***What's New***: AfriMed-QA는 첫 번째 대규모 범아프리카 다전문 의료 질의응답 데이터세트로, 아프리카 대륙에서 16개국의 60개 이상의 의과대학에서 출처된 총 15,000개의 질문을 포함합니다. 이 데이터세트는 32개의 의료 전공을 아우르며, LLM(대형 언어 모델)의 성능을 정확성과 인구통계학적 편향성 측면에서 평가합니다.
- ***Technical Details***: AfriMed-QA 데이터세트는 MCQ(다항 선택 질문) 4,000개와 SAQ(단답형 질문) 1,200개, CQ(소비자 질의) 10,000개로 구성되며, 아프리카 지역의 의학적 다양성을 엄격하게 평가하기 위해 설계되었습니다. 30개의 대형 언어 모델(LLMs)을 평가하여 각 모델의 정확도와 지역별 편향성을 분석했습니다.
- ***Performance Highlights***: AfriMed-QA를 통해 LLM들의 전공별 및 지리적 성능 차이가 확인되었습니다. 특히, 대형 모델들은 약 75% 이상의 성과를 보였으나, 소형 모델들은 40~60%의 범위에 머물렀습니다. 또한, 일반 모델이 생의학 모델보다 더 나은 성과를 보이는 역설적인 결과가 나타났습니다.

### [TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models](https://arxiv.org/abs/2411.18350)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18350.png)

Vote: 4

Authors: Barbara Hammer, Riza Velioglu, Petra Bevandic, Robin Chan

- ***What's New***: 이 논문은 Virtual Try-Off (VTOFF)라는 새로운 작업을 소개합니다. 이는 옷을 입고 있는 한 명의 사진에서 표준화된 의류 이미지를 생성하는 것을 목표로 합니다. 기존의 Virtual Try-On (VTON)과 달리 VTOFF는 의류의 형태, 질감, 복잡한 무늬를 포착하는 데 중점을 둡니다. TryOffDiff라는 모델을 제시하며, Stable Diffusion과 SigLIP 기반의 비주얼 컨디셔닝을 활용하여 높은 충실도와 세부 사항을 유지합니다.
- ***Technical Details***: TryOffDiff는 Stable Diffusion을 활용하여 VTOFF에 맞도록 사전 학습된 확산 모델을 적응시켰습니다. 이미지 특성을 텍스트 기반 확산 사전 조건과 정렬하여 높은 시각적 충실도와 일관된 제품 세부 사항을 보장합니다. VITON-HD 데이터셋을 수정하여 실험을 수행했으며, 자세 전환 기반과 가상 착용의 기존 방법보다 적은 전처리 및 후처리 단계를 통해 뛰어난 성능을 보였습니다.
- ***Performance Highlights***: TryOffDiff는 VITON-HD 데이터셋에서 다양한 벤치마크 지표에 대해 다른 방법보다 뛰어난 성능을 보였습니다. DISTS를 포함한 주요 성능 지표에서 기준 방법들을 능가하며, 특히 패턴과 로고와 같은 세부 사항을 보존하는 데 있어 우수한 결과를 보여줬습니다. 이는 VTOFF가 전자 상거래 응용 프로그램의 제품 이미지 향상 및 생성 모델 평가 개선에 미칠 잠재력을 강조합니다.

### [Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning](https://arxiv.org/abs/2411.18203)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18203.png)

Vote: 17

Authors: Di Zhang, Junxian Li, Zonglin Yang, Wanli Ouyang, Jianbo Wu, Jingdi Lei, Yujie Liu, Weida Wang, Dongzhan Zhou, Xunzhi Wang, Peng Ye, Suorong Yang, Jiatong Li

- ***What's New***: Critic-V는 멀티모달 추론에서의 VLM 오류를 검출하는 새로운 프레임워크로, Actor-Critic 패러다임에서 영감을 받아 VLM(Vision-Language Models)의 추론 능력을 향상시키는 것을 목표로 합니다. 이 프레임워크는 'Reasoner'와 'Critic'이라는 두 가지 독립적인 구성 요소를 통합하여, 시각 및 텍스트 입력에 기반한 추론 경로를 생성하고 이를 정교하게 비판하여 경로를 개선합니다.
- ***Technical Details***: Critic-V는 In-Context Reinforcement Learning(ICRL)을 활용하여 텍스트와 시각적 설명을 연계하며, 발생된 추론 경로가 Critic의 피드백을 바탕으로 지속적으로 평가 및 개선됩니다. Critic 모델은 Direct Preference Optimization(DPO)으로 훈련되어, Rule-based Reward(RBR)를 통해 비판의 질을 평가합니다. Critic 모델은 피드백을 통해 Reasoner의 추론 전략을 세밀하게 조정하여 보다 정확하고 신뢰성 있는 결과를 도출할 수 있게 합니다.
- ***Performance Highlights***: Critic-V 프레임워크는 8개의 벤치마크 중 5개에서 기존 방법들을 능가하며, 특히 추론의 정확성과 효율성 면에서 두각을 드러냈습니다. 특히, 수학적 추론 작업에서 눈에 띄는 향상을 보여주었으며, Qwen2-VL-7B는 MathVista 데이터셋에서 11.8%의 성능 향상을, MathVerse 데이터셋에서 7.1%의 향상을 기록했습니다. 또한, 일반적인 추론 작업 지원에 대한 가능성을 시사하며, 다양한 복잡한 작업에 대한 VLM의 견고성을 크게 향상시킬 수 있습니다.

### [Free^2Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models](https://arxiv.org/abs/2411.17041)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17041.png)

Vote: 3

Authors: Bryan S Kim, Jaemin Kim, Jong Chul Ye

- ***What's New***: Free2Guide는 대형 비전-언어 모델(Large Vision-Language Models; LVLMs)과 경로 적분 제어(Path Integral Control)를 활용하여 추가적인 모델 학습 없이 텍스트-비디오 생성 시 가장 적절한 텍스트 정렬을 달성하는 혁신적인 경사도 없는 프레임워크입니다. 이 방법은 비차별화 보상 함수(non-differentiable reward functions)를 사용하여 강력한 블랙박스 비전-언어 모델을 보상 모델로 통합할 수 있게 합니다.
- ***Technical Details***: Free2Guide는 비디오와 텍스트 프롬프트 간의 정렬을 평가하기 위해 LVLM API를 활용하여 비차별화된 보상 모델을 사용합니다. 이 프레임워크는 여러 대규모 이미지 기반 모델을 조합하여 비디오 생성 시 텍스트-비디오 정렬을 개선하며, 경사도 없는 제어를 위한 경로 적분 제어(principles from path integral control)를 적용하여 비디오 생성 과정을 안내합니다. 다양한 실험을 통해 제안된 방법이 텍스트 정렬 및 생성 비디오의 질을 개선함을 보여줍니다.
- ***Performance Highlights***: 텍스트-비디오 정렬 측면에서 CLIP[29] 또는 ImageReward[44]와 같은 대규모 이미지 모델과 GPT-4o(비전-언어 모델)을 조합한 Free2Guide는 모든 기준에서 기본 모델보다 향상된 성능을 발휘하였습니다. 특히, 공간적 관계(Spatial Relationships) 및 일관성 보정에 있어 상당한 개선을 보였습니다. 이는 LVLM이 시간적 동작을 이해하면서 비디오 가이던스를 강화할 수 있음을 나타냅니다.

