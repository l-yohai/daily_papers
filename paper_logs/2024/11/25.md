## Daily Papers (2024-11-25)

### [One to rule them all: natural language to bind communication, perception and action](https://arxiv.org/abs/2411.15033)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15033.png)

Vote: 2

Authors: Giuseppe Boccignone, Simone Colombani, Dimitri Ognibene

- ***What's New***: 이 논문에서는 자연어로 전달되는 인간 명령을 이해하고 로봇 행동으로 변환하는 새로운 로봇 계획 아키텍처를 제시합니다. 이 시스템은 대형 언어 모델(LLMs; Large Language Models)을 활용하여, 소통, 인식 및 계획을 통합하여 다채로운 환경에서의 사용자 요청을 실시간으로 처리할 수 있습니다.
- ***Technical Details***: 수정된 ReAct 프레임워크 내에 LLMs를 내장하여 로봇이 환경 정보와 실시간 피드백을 기반으로 명령을 해석하고 실행할 수 있도록 했습니다. 장면 그래프(scene graphs)를 이용한 강력하고 역동적인 의미론적 지도 표현을 결합하였으며, 다양한 오류 처리 및 적응 계획 전략을 통해 로봇의 적응성과 작업 실행 효율성을 높였습니다.
- ***Performance Highlights***: RoBee라는 인지형 휴머노이드 로봇 위에서 구현된 본 시스템은 다양한 환경에서의 실시간 변화에 성공적으로 반응하며, 사용자 요청을 고급 행동으로 변환하여 실행할 수 있는 가능성을 보여주었습니다. 로봇 작업 실행과 자연어 처리에서의 기존 기술과의 비교 평가가 아직 진행 중이나, 복잡한 요청에 대해서는 성공률이 25%로 낮은 기초 결과가 보고되었음.

### [WildLMa: Long Horizon Loco-Manipulation in the Wild](https://arxiv.org/abs/2411.15131)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15131.png)

Vote: 5

Authors: Ge Yang, Yuchen Song, Xuanbin Peng, Xueyan Zou, Xiaolong Wang, Chengzhe Jia, Minghuan Liu, Sai Aneesh Suryadevara, Ri-Zhao Qiu, Mazeyu Ji, Ruihan Yang

- ***What's New***: WildLMa는 다시 구성 가능한 기술 라이브러리(WildLMa-Skill)와 LLM 플래너(WildLMa-Planner)를 사용하여 장기 과제를 수행할 수 있는 기능을 갖춘 로봇의 현실 세계 환경 적용을 목표로 합니다. 이렇게 함으로써 다양한 환경에서 일반화된 이동 조작을 수행할 수 있는 능력을 연구합니다.
- ***Technical Details***: WildLMa는 세 가지 주요 구성 요소로 나뉘어집니다. 첫째, VR 기반 전신 원격 조작을 지원하는 학습된 저수준 컨트롤러를 적응시킵니다. 둘째, CLIP를 활용하여 언어 조건화 모방 학습을 통해 종합 가능한 기술을 제공하는 WildLMa-Skill을 제안합니다. 마지막으로 WildLMa-Planner를 통해 LLM 플래너와의 인터페이스를 통해 장기 계획 실행을 지원합니다. 이러한 모든 요소는 Unitree B1 사족 로봇과 Unitree Z1 팔을 통해 구현되었습니다.
- ***Performance Highlights***: WildLMa는 ACT 및 RL 모델보다 우수한 성능을 보여줍니다. I.D. 설정에서 94.4%의 탁월한 성공률을 달성했으며 특히 O.O.D. 설정에서도 75%의 성공률을 나타냈습니다. 또한, 전신 컨트롤러를 사용한 원격 조작의 효율성을 개선하여 훈련 비용을 26.9% 절감시키며 다양한 작업에서 더 높은 성공률을 보였습니다.

### [Novel View Extrapolation with Video Diffusion Priors](https://arxiv.org/abs/2411.14208)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14208.png)

Vote: 6

Authors: Kunhao Liu, Ling Shao, Shijian Lu

- ***What's New***: ViewExtrapolator는 새로운 시점 외삽(novel view extrapolation)을 위해 안정적인 비디오 확산(Stability Video Diffusion; SVD)의 생성 사전(generative priors)을 활용한 획기적인 접근법입니다. 이는 훈련된 보기의 범위를 훨씬 넘어서는 새로운 시점을 실제에 가깝게 합성할 수 있는 기술입니다. 이러한 접근은 방사 필드나 포인트 클라우드가 생성한 인공 조각을 제거하고 보다 현실적인 렌더링을 제공합니다.
- ***Technical Details***: ViewExtrapolator는 다양한 3D 렌더링 접근법에 적용할 수 있는 범용적인 시점 외삽기입니다. SVD의 미세 조정 없이도 작동하여 데이터와 계산이 효율적입니다. SVD의 네트워크를 조정하여 인공 조각을 줄이고, 두 가지 핵심 기법인 지도 어닐링(guidance annealing)과 재샘플링 어닐링(resampling annealing)을 설계하여 영상 내 보이지 않는 영역을 고품질로 보강합니다.
- ***Performance Highlights***: ViewExtrapolator는 실험에서 다른 최신 기법보다 뛰어난 성능을 보여 줍니다. SSIM, PSNR, LPIPS와 같은 척도로 평가했을 때, 더 향상된 시각적 충실도를 입증하였으며, 낮은 아티팩트를 보였고, 특히 개념을 넘어선 뷰의 생성에서 뛰어난 성능을 기록했습니다.

### [OminiControl: Minimal and Universal Control for Diffusion Transformer](https://arxiv.org/abs/2411.15098)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15098.png)

Vote: 19

Authors: Xinchao Wang, Songhua Liu, Qiaochu Xue, Zhenxiong Tan, Xingyi Yang

- ***What's New***: OminiControl은 이미지를 조건으로 사용하여 사전 학습된 확산 변환기(Diffusion Transformer; DiT) 모델에 최소한의 매개변수만을 추가하여 통합할 수 있는 범용적이고 효율적인 프레임워크를 소개합니다. 기존의 복잡한 인코더 모듈을 사용하는 방법과는 달리, OminiControl은 단 0.1%의 추가 파라미터만으로 피사체 기반 생성과 공간 정렬 조건을 포함한 다양한 이미지 조건 부여 작업을 효과적이고 효율적으로 수행할 수 있습니다.
- ***Technical Details***: OminiControl은 사전 학습된 확산 변환기에 이미지 조건을 통합하기 위해 멀티 모달 주의(Multi-modal Attention) 프로세서를 활용하는 메커니즘을 사용합니다. 이로써 DiT의 이미 존재하는 VAE 인코더를 재사용하여 조건 이미지를 처리하고, 학습 가능한 위치 임베딩을 통해 코드 처리 파이프라인을 확장합니다. 이 방법은 조건 및 생성 토큰 간의 상호작용을 도모하며 효율적인 정보 교환을 가능하게 합니다.
- ***Performance Highlights***: OminiControl은 피사체 유지 기반 이미지 생성을 위한 ID-Adapter와 같은 기존 모델들을 뛰어넘는 성능을 보이며, Canny에서 이미지로의 전환, 딥쓰나 이미지 생성, 지역 특정 편집, 아이덴티티 유지 생성 등의 다양한 제어 작업에서 기존 방법들을 지속적으로 능가합니다. 새로운 Subjects200K 데이터셋은 20만 장 이상의 고품질 이미지로 구성되어 연구 공동체에 유용한 자원을 제공합니다.

### [A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection](https://arxiv.org/abs/2411.12946)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12946.png)

Vote: 15

Authors: Shing Yee Chan, Gabriel Chua, Shaun Khoo

- ***What's New***: 이 논문은 대형 언어 모델들(Large Language Models; LLMs)의 비정상적인 프롬프트 사용(Off-Topic Prompt)을 감지하기 위한 새로운 유연한 가드레일 개발 방법론을 소개합니다. 이 방법론은 비정상적 사용을 방지하고, 모델 배포 전 단계에서 학습 가능한 합성 데이터셋을 생성함으로써 기존 휴리스틱 접근 방식을 능가하는 가드레일을 구축합니다.
- ***Technical Details***: 이 연구에서는 문제 공간을 정성적으로 정의하고, LLM을 활용하여 다양한 프롬프트를 생성함으로써 합성 데이터셋을 구성했습니다. 이 데이터셋은 비정상적인 프롬프트를 감지할 수 있는 분류기를 훈련하는 데 사용됩니다. 이 과정에서 임베딩 모델과 크로스 인코더 모델을 미세조정하여 높은 정확도의 가드레일을 개발했고, 결과적으로 잘못된 양성률을 낮추었습니다.
- ***Performance Highlights***: 실험 결과, 미세조정된 크로스 인코더 분류기는 기존 방법보다 높은 정밀도와 재현율을 보였으며, 특히 옵션 분류(Precision)에서 더 적은 오탐률을 기록했습니다. 더불어, 향후 연구와 개발을 지원하기 위해 합성 데이터셋과 비정상 프롬프트 가드레일 모델을 오픈소스로 제공하고 있습니다.

### [MyTimeMachine: Personalized Facial Age Transformation](https://arxiv.org/abs/2411.14521)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14521.png)

Vote: 7

Authors: Luchao Qi, Jiaye Wu, Annie N. Wang, David W. Jacobs, Bang Gong, Roni Sengupta

- ***What's New***: MyTimeMachine(MyTM)은 개인화된 사진 컬렉션을 사용하여 개인적인 연령 변화를 학습하는 새로운 방법론을 제안합니다. 이 모델은 StyleGAN2를 기반으로 하며, 개인화된 사진이 적게는 50장 정도만 있어도 고품질의 아이덴티티 유지 연령 변환을 달성할 수 있습니다. MyTM은 이미지를 넘어 동영상에도 적용 가능하며, 기존의 기술을 뛰어넘는 성능을 보입니다.
- ***Technical Details***: MyTM은 StyleGAN2의 잠재 공간을 활용하여 전역적 얼굴 노화 특징과 개인화된 노화 특징을 결합한 어댑터 네트워크(Adapter Network)를 도입합니다. 세 가지 손실 함수(personalized aging loss, extrapolation regularization, adaptive w-norm regularization)를 사용하여 개인화된 변환을 학습합니다. 전역적 노화 프라이어와 개인적 사진컬렉션을 결합하여 타겟 노화를 학습합니다.
- ***Performance Highlights***: 계량 및 질적 평가에서 MyTM은 기존의 전역 및 개인화 기술을 능가합니다. 연령 회귀(regression) 실험에서 ID 보존(IDsim)에서 평균 0.67의 점수를 기록하였으며, 이는 이론도 IDsim 점수에 있어 타의 추종을 불허하는 성과입니다. 또한 본 방법은 동영상 노화에 있어 정치성을 유지하면서 일관성 있는 결과를 생성할 수 있음을 보여줍니다.

### [VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection](https://arxiv.org/abs/2411.14794)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14794.png)

Vote: 8

Authors: Shifeng Zhang, Xiaojuan Qi, Xu Zhou, Hairong Shi, Le Zhuo, Yue Liao, Songhao Han, Si Liu, Wei Huang, Xiu Su

- ***What's New***: VideoEspresso는 대규모 문제 해결이 가능한 세밀한 비디오 추론을 위한 체인-오브-생각(Chain-of-Thought; CoT) 데이터셋을 특징으로 하는 새로운 비디오 질문 응답(VideoQA) 데이터셋입니다. 주요 프레임 선택을 통해 시공간적 맥락을 유지하며, GPT-4o를 활용해 세밀한 VideoQA 쌍을 자동으로 생성합니다.
- ***Technical Details***: VideoEspresso는 의미 기반의 핵심 정보 추출 방법을 사용해 데이터 중복성을 줄이고, 주요 프레임을 선택하여 정확한 비디오 이해를 돕습니다. LVLMs의 협력을 통해 저비용의 고정밀 비디오 추론을 가능하게 하는 구조로, 작은 모델로 핵심 프레임을 선택하고 추론 LVLM이 체인-오브-생각(CoT) 추론을 수행하는 하이브리드 프레임워크를 제안합니다.
- ***Performance Highlights***: VideoEspresso를 기반으로 구축된 벤치마크에서 14개의 과제를 기준으로 9개의 인기 있는 LVLMs와 성능을 비교하였으며, 대부분의 과제에서 기존의 기준 모델보다 우수한 성능을 나타냈습니다. 이는 새로운 데이터셋과 하이브리드 프레임워크의 합작 효과로 인한 결과를 보여주며, 추가 실험에서는 비디오 추론 작업에서 CoT의 중요성을 강조했습니다.

### [VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement](https://arxiv.org/abs/2411.15115)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15115.png)

Vote: 4

Authors: Jaemin Cho, Mohit Bansal, Daeun Lee, Jaehong Yoon

- ***What's New***: VIDEOREPAIR는 기존의 텍스트-비디오(T2V) 변환 모델들이 겪는 텍스트와 비디오 간의 불일치 문제를 해결하기 위한 혁신적이고 훈련이 필요 없는 모델 불가지론적인 비디오 개선 프레임워크입니다. 이 프레임워크는 자동으로 세밀한 텍스트-비디오 비일치 현상을 식별하고 구체적인 공간적 및 텍스트 피드백을 생성하여, T2V 확산 모델이 목표로 하는 위치에 맞게 로컬 정련을 수행할 수 있도록 지원합니다.
- ***Technical Details***: VIDEOREPAIR는 (1) 비디오 평가, (2) 정련 계획, (3) 지역 분해, (4) 로컬 정련의 네 단계로 구성됩니다. 이 과정에서 인간 언어 모델(MLLM)을 통해 미세한 평가 질문을 생성하고 객체 지향적 질문-응답 쌍을 통해 생성된 비디오의 오류를 식별합니다. 이후 정의된 키 개념을 유지하고 운동 잡음을 재정의하여 특정 지역의 재정련을 진행합니다.
- ***Performance Highlights***: VIDEOREPAIR는 EvalCrafter와 T2V-CompBench와 같은 두 가지 인기 있는 비디오 생성 벤치마크에서 시험을 통해 다양한 텍스트-비디오 정렬 메트릭에서 최신 정련 방법을 크게 능가했습니다. 이 모델은 비디오 생성의 조성적 프롬프트에서의 다양한 객체 수, 속성 및 위치에 따라 최근의 정련 방법들보다 뛰어난 성과를 보여, 향후 비주얼 생성 작업의 자동 정련 프레임워크 발전을 촉진하기를 기대합니다.

### [Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction](https://arxiv.org/abs/2411.14762)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14762.png)

Vote: 8

Authors: Sihyun Yu, Pieter Abbeel, Huiwon Jang, Younggyo Seo, Jinwoo Shin

- **What's New**: CoordTok는 좌표 기반 패치 복원을 통해 긴 비디오를 효율적으로 토큰화할 수 있는 스케일러블 토크나이저를 소개합니다. 이 접근 방식은 3D 생성 모델의 최근 발전에서 영감을 받아 비디오를 요인화된 트라이플레인 표현으로 인코딩하고 랜덤으로 샘플링된 (x, y, t) 좌표에 해당하는 패치를 복원함으로써 긴 비디오의 시간적 일관성을 활용하여 더 적은 토큰 수로 인코딩을 가능하게 합니다.
- **Technical Details**: CoordTok는 비디오를 공간-시간 패치로 분할하고 트라이플레인(triplane) 표현으로 인코딩합니다. 인코더는 비디오 특징을 추출하고, 크로스-셀프(cross-self) 레이어는 이 특징을 요인화된 표현으로 변환합니다. 디코더는 비디오 패치의 중심 좌표를 샘플링하고, 트라이플레인으로부터 좌표 기반 표현을 쿼리하여 각 좌표로부터 픽셀을 복원합니다. 이 접근은 전체 프레임을 복원할 필요가 없어 메모리 및 계산 요구를 줄입니다.
- **Performance Highlights**: CoordTok는 UCF-101 데이터셋에서 기존 토크나이저보다 적은 토큰(1280개)으로 128프레임 비디오를 인코딩할 수 있으며, rFVD 점수에서 탁월한 성능을 보여 프레임 재구성 품질의 다양한 평가 메트릭에서 뛰어납니다. 또한 디퓨전 트랜스포머를 메모리 효율적으로 훈련하여 한 번에 긴 비디오를 생성할 수 있습니다.

### [Adapting Vision Foundation Models for Robust Cloud Segmentation in Remote Sensing Images](https://arxiv.org/abs/2411.13127)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.13127.png)

Vote: 3

Authors: Shiying Wang, Congyan Lang, Xuechao Zou, Kai Li, Lei Jin, Pin Tao, Junliang Xing, Shun Zhang

- ***What's New***: 이 논문에서는 원격 감지 이미지에서의 강력한 구름 분할을 위해 비전 파운데이션 모델(Vision Foundation Models; VFM)을 적응시키는 기술을 제안합니다. 이 방법은 Pre-trained VFM을 동결한 채로 경량의 Cloud-Adapter를 도입하여 구름 분할의 정확성과 강건성을 향상시킵니다. Cloud-Adapter는 적은 양의 추가 파라미터만을 필요로 하며, 다양한 위성 데이터 셋에서 일관되게 최첨단(State-of-the-Art) 성능을 달성합니다.
- ***Technical Details***: Cloud-Adapter는 Pre-trained VFM을 기반으로 하여, 이를 동결시킨 채 적응 모듈을 추가하여 구름 분할 성능을 향상시킵니다. 이는 CNN(Convolutional Neural Network)을 사용하여 밀집 공간 표현을 추출하고, 이를 다중 스케일 컨텍스트 정보로 집계하여 VFM의 Transformer Layer를 효율적으로 조절합니다. 실험 결과, Backbone의 0.6% Trainable Parameters만을 사용하여 다양한 인기 데이터셋에서 성능 향상을 달성하였습니다.
- ***Performance Highlights***: Cloud-Adapter는 여러 구름 분할 데이터셋에서 최첨단 성능을 달성하였으며, 특히 HRC WHU 데이터셋에서 mIoU 89.05%, GF12MS WHU GF1 데이터셋에서 mIoU 92.55% 등 매우 높은 정확도를 달성하였습니다. 또한, CloudSEN12 High L1C 데이터셋에서도 74.18% mIoU를 기록하여 경쟁 모델들을 상회하는 뛰어난 성능을 보여줍니다.

### [TÜLU 3: Pushing Frontiers in Open Language Model Post-Training](https://arxiv.org/abs/2411.15124)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15124.png)

Vote: 36

Authors: Jena D. Hwang, Jacob Morrison, Hamish Ivison, Oyvind Tafjord, Yizhong Wang, Noah A. Smith, Lester James V. Miranda, Faeze Brahman, Pradeep Dasigi, Victoria Graf, Hannaneh Hajishirzi, Shane Lyu, Valentina Pyatkin, Saumya Malik, Yuling Gu, Luca Soldaini, Chris Wilhelm, Ronan Le Bras, Shengyi Huang, Alisa Liu, Nathan Lambert, Nouha Dziri, Jiangjiang Yang

- ***What's New***: TÜLU 3는 Llama 3.1 기반의 최신 오픈 오픈 소스 언어 모델로, 데이터, 코드, 훈련 레시피 등을 전면 공개하며 학술 연구와 기존 비공개 방법을 결합하여 최첨단 모델 성능을 제공합니다. TÜLU 3는 새로운 Reinforcement Learning with Verifiable Rewards (RLVR) 방법을 도입하여 특히 수학 및 명령어 준수와 같은 검증 가능한 답변을 요구하는 분야에서 현저한 성능 향상을 보여줍니다.
- ***Technical Details***: TÜLU 3의 훈련은 Supervised Finetuning (SFT), Direct Preference Optimization (DPO), Reinforcement Learning with Verifiable Rewards (RLVR) 등의 단계로 나누어집니다. 각 단계에서 다양한 데이터셋과 알고리즘적 혁신을 활용하여 모델을 최적화하였으며, 전체적인 목표를 설정하고 훈련을 통해 개선을 이끌어내기 위한 평가 도구 또한 함께 제공됩니다.
- ***Performance Highlights***: TÜLU 3의 70B 모델은 MMLU, PopQA 등 다양한 평가에서 기존의 강력한 비공개 모델들을 능가하는 성능을 보여주며, 특히 수학적 문제 해결에서 높은 성과를 기록했습니다. RLVR은 GSM8K와 MATH에서 유의미한 개선을 보였으며, 이는 정책 모델이 올바른 답변을 생성할 경우에만 보상을 제공하는 간단한 기법으로 인해 가능합니다.

### [BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games](https://arxiv.org/abs/2411.13543)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.13543.png)

Vote: 11

Authors: Rob Fergus, Tim Rocktäschel, Samuel Coward, Jack Parker-Holder, Davide Paglieri, Akbir Khan, Jakob Nicolaus Foerster, Maciej Wolczyk, Ulyana Piterbarg, Lerrel Pinto, Łukasz Kuciński, Bartłomiej Cupiał, Eduardo Pignatelli

- ***What's New***: BALROG는 대형 언어 모델(LLMs)과 비전 언어 모델(VLMs)의 에이전트 능력을 다양한 난이도의 복잡한 게임 세트를 통해 평가하기 위한 새로운 벤치마크입니다. BALROG는 기존의 강화 학습 환경을 통합하여, 인간 비전문가가 몇 초 안에 풀 수 있는 것부터 수년이 걸릴 수 있는 매우 어려운 작업까지 포함되어 있습니다.
- ***Technical Details***: BALROG 벤치마크는 BabyAI, Crafter, TextWorld, Baba Is AI, MiniHack, 및 NetHack과 같은 다양한 강화 학습 게임 환경을 통합하여 실험을 수행했습니다. 각 환경은 규칙에 따라 절차적으로 생성되며, 같은 인스턴스를 두 번 만나는 것은 드뭅니다. 이 벤치마크는 모형이 임베디드된 실제 세계 작업을 해결할 수 있는 범용 에이전트를 개발할 수 있도록 설계되었습니다. 평가 지표는 작업 완료와 환경 다이나믹스 발견, 장기 계획과 같은 에이전트의 능력을 정밀하게 평가하도록 설정되어 있습니다.
- ***Performance Highlights***: 최상의 성능을 보이는 모델인 GPT-4o는 BabyAI와 Crafter에서 단순한 작업에는 어느 정도 성공했지만, 더욱 복잡한 작업에서는 완전히 실패했습니다. 특히 NetHack과 같은 어려운 경우에는 o1-preview 모델이 평균 게임 진행 1.5%로 최고 성능을 보였지만 매우 낮은 진전을 보였습니다. LLMs는 게임 메커니즘에 대한 지식을 갖고 있음에도 불구하고 실습에서는 이를 활용하지 못하며, 시각적 입력이 제공될 때 성능이 크게 저하되는 것으로 나타났습니다.

### [Style-Friendly SNR Sampler for Style-Driven Generation](https://arxiv.org/abs/2411.14793)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14793.png)

Vote: 27

Authors: Sungroh Yoon, Heeseung Kim, Jooyoung Choi, Chaehun Shin, Yeongtak Oh

- ***What's New***: Style-Driven Generation을 위한 Style-Friendly SNR 샘플러는 스타일 템플릿을 참조 이미지로부터 학습하여 개인화된 콘텐츠 생성능력을 향상시킵니다. 이는 Fine-tuning 과정에서 신호 대 잡음 비율(Signal-to-Noise Ratio; SNR) 분포를 높은 소음 수준으로 집중시켜 스타일 특징을 보다 효과적으로 학습할 수 있도록 합니다.
- ***Technical Details***: Style-Friendly SNR 샘플러는 Fine-tuning 시 SNR 분포를 조정하여 스타일 특징이 나타나는 범위로 집중합니다. 이 방식은 기존의 model training에 사용되었던 목적 함수와 소음 수준 분포를 뛰어넘어 스타일 표현의 Key 요소인 색상 체계, 레이아웃, 조명 등을 더욱 강화하여 학습합니다.
- ***Performance Highlights***: 우리의 접근법은 FLUX-dev와 Stable Diffusion 3.5와 같은 최신 모델들에서 스타일 템플릿을 높은 정확도로 학습하게 하여 스타일-구동 생성에서 전례 없는 성능을 제공합니다. 우리는 스타일-구동 생성이 참조 스타일 이미지의 독특성을 충실히 반영할 수 있음을 시연하며, 스타일과 텍스트 정렬 모두에서 상대적 우위를 보여주었습니다.

### [Large Multi-modal Models Can Interpret Features in Large Multi-modal Models](https://arxiv.org/abs/2411.14982)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14982.png)

Vote: 11

Authors: Ziwei Liu, Yifei Shen, Kaichen Zhang, Bo Li

- ***What's New***: 이 논문은 대형 멀티모달 모델(Large Multimodal Models; LMMs)의 내부 표현을 이해하고 해석하기 위한 범용 프레임워크를 제시합니다. 특히 희소 자동인코더(Sparse Autoencoder; SAE)를 사용하여 사람 이해가 가능한 특징으로 표현을 분리하여 대형 멀티모달 모델 내의 의미를 명확히 해석하려 합니다.
- ***Technical Details***: 희소 자동인코더(SAE)를 LMM의 특정 계층에 통합하여 비활성화된 특징을 예방하는 보조 손실을 결합한 손실 함수와 함께 원활한 해석을 제공합니다. 그리고 자동 해석 파이프라인을 통해 LMM이 학습한 개방형 의미 특징을 분석합니다. SAE에서 학습된 특징들은 LLaVA-OV-72B 모델을 통해 해석되어 이들 특징들이 모델의 행동을 유도하는 데 효과적일 수 있음이 입증되었습니다.
- ***Performance Highlights***: 실험을 통해 LMM이 특정 작업에서 탁월한 성능을 보이는 이유와 실수의 본질을 깊이 이해할 수 있게 되었습니다. 또한 인간 두뇌의 인지 과정과의 유사성을 제안하며 희소 자동인코더가 모델의 행동을 유도하는 데 효과적이라는 증거를 제공합니다. 이러한 결과는 LMM의 해석 가능성과 신뢰성을 개선할 수 있는 잠재 전략을 제안합니다.

