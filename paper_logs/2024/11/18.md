## Daily Papers (2024-11-18)

### [Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement](https://arxiv.org/abs/2411.06558)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.06558.png)

Vote: 24

Authors: Zhibo Chen, Jian Yang, Yajie Li, Zhennan Chen, Qian Wang, Ying Tai, Haofan Wang, Zhengkai Jiang, Jun Li

- **What's New**: RAG는 조화롭고 일관된 생성 결과를 달성하기 위한 새로운 다중 지역 제어 기술을 소개합니다. 이는 튜닝 없는(tuning-free) 방식으로, 사용자가 필요에 따라 생성된 이미지의 특정 부분을 자유롭게 수정할 수 있도록 합니다.
- **Technical Details**: RAG는 비교적 간단한 두 하위 작업, 즉 'Regional Hard Binding'과 'Regional Soft Refinement'로 구성됩니다. 'Region-Aware Hard Binding'은 디노이징 과정 초기에 각 지역 프롬프트를 정확하게 실행하고, 'Soft Refinement'는 크로스 어텐션(cross-attention) 레이어 내에서 시각적 경계를 제거하고 인접 지역 간의 상호작용을 강화합니다.
- **Performance Highlights**: RAG는 T2I-CompBench 벤치마크에서 RPG를 비롯한 기존 최첨단 메소드들보다 속성 바인딩(attribute binding)과 객체 관계에서 우수한 성능을 보여주었습니다. 특히 공간적 관계를 포함하는 프롬프트의 경우 29% 이상의 향상을 기록하며 복잡한 다중 지역 프롬프트를 처리할 수 있는 능력을 입증했습니다.

### [LLaVA-o1: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10440.png)

Vote: 60

Authors: Li Yuan, Peng Jin, Lichao Sun, Guowei Xu, Li Hao, Yibing Song

- ***What's New***: LLaVA-o1 모델은 비전-언어 모델(Vision-Language Models; VLMs)에서 체계적인 추론을 단계별로 수행하는 새로운 접근법을 소개합니다. 이 모델은 기존의 Chain-of-Thought(COT) 프롬프트와 달리, 자동으로 요약(Summary), 캡션(Caption), 추론(Reasoning), 결론(Conclusion)을 생성하여 논리적 추론을 수행합니다.
- ***Technical Details***: LLaVA-o1은 100k 샘플로 구성된 LLaVA-o1-100k 데이터셋을 사용하여 다양한 시각적 질문 응답(VQA) 소스의 샘플을 통합하고 구조화된 추론 주석을 제공합니다. 또한, 추론 시간(stage-level) 빔 검색(stage-level beam search) 방법을 제안하여 효율적인 추론 시간 확장을 지원합니다. 이 방법은 각 단계에서 여러 후보 결과를 생성하고 최적의 것을 선택하여 추론 과정을 계속합니다.
- ***Performance Highlights***: LLaVA-o1은 MMStar, MMBench, MathVista 등의 다양한 멀티모달 추론 벤치마크에서 기존 모델보다 평균 8.9% 높은 성능을 보여주었으며, Gemini-1.5-pro, GPT-4o-mini 같은 더 큰 크기의 모델도 능가하였습니다.

### [Xmodel-1.5: An 1B-scale Multilingual LLM](https://arxiv.org/abs/2411.10083)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10083.png)

Vote: 7

Authors: Wang Qun, Liu Yang, Jiang Ling, Lin Qingquan

- ***What's New***: Xmodel-1.5는 1억 매개변수를 가진 새로운 다국어 대형 모델로, 약 2조 개의 토큰으로 사전 학습되었습니다. 이 모델은 여러 언어에서 강력한 성능을 보이며, 특히 태국어, 아랍어, 불어에서 주목할 만한 결과를 나타냅니다. 또한 우리는 연구 커뮤니티에 태국어 평가 데이터셋을 공개하여 다국어 AI 연구의 발전에 기여하고자 합니다.
- ***Technical Details***: Xmodel-1.5는 약 30개의 언어로 구성된 Wiki 데이터와 27개의 언어로 구성된 CulturaX 데이터 등 다양한 다국어 데이터를 통합하여 사전 학습되었습니다. 모델은 RoPE(Rotary Positional Embedding)를 통해 장문 이해 능력을 개선하였으며, SwiGLU 활성화 함수와 RMSNorm을 적용하여 성능을 최적화하였습니다. 학습 과정에서는 Distributed Data Parallel(DDP)과 FlashAttention-V2를 사용하여 효율성을 최대화하였습니다.
- ***Performance Highlights***: Xmodel-1.5는 대화형 AI 평가에서 다수의 경쟁 모델보다 뛰어난 성능을 보이며, 특히 TinyLLaMA보다 높은 평균 점수를 기록했습니다. 태국어, 아랍어, 불어 등의 다국어 평가에서도 뛰어난 능력을 입증하였으며, 여러 벤치마크에서 PolyLM 1.7B 모델을 초과하는 성능을 나타냈습니다. 이 모델은 다국어 처리와 관련된 여러 공통 과제를 해결하며 차별화된 장점을 보였습니다.

### [Number it: Temporal Grounding Videos like Flipping Manga](https://arxiv.org/abs/2411.10332)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10332.png)

Vote: 9

Authors: Yuyang Sun, Yongliang Wu, Xu Yang, Yizhou Zhou, Xinting Hu, Wenbo Zhu, Fengyun Rao, Bernt Schiele

- ***What's New***: NumPro (Number-Prompt)은 비디오 프레임에 숫자 아이덴티티를 추가하여 비디오 대형 언어 모델(Vid-LLMs)의 비디오 시간적 그라운딩(VTG) 능력을 강화하는 혁신적인 방법입니다. 이는 비디오를 만화처럼 순차적으로 넘기며 사건 타임라인을 시각적으로 읽고 정확한 시간 정보를 제공하도록 지원합니다.
- ***Technical Details***: NumPro는 각 비디오 프레임에 고유한 숫자 식별자를 부여하여, Vid-LLMs가 시간적 순서에 따라 시각 콘텐츠를 '읽을' 수 있도록 합니다. OCR 기능을 활용하여 이 숫자를 인식하고, 프레임 번호와 언어적 질문을 연결합니다. 이로 인해 추가 학습이나 모델 구조 변경 없이 VTG 성능을 개선할 수 있습니다. 또한, NumPro-FT라는 데이터세트에서 미세 조정을 통해 더 나은 성능을 달성합니다.
- ***Performance Highlights***: NumPro는 ActivityNet에서 이전 최고 성능을 9.9% 늘려, mIoU에서 새 최고 성과를 기록했습니다. Qwen2-VL-7B 모델의 경우 mIoU가 평균 24.7% 증가했으며, LongVA-7B-DPO는 미세 조정 없이도 mIoU에서 41.4%를 기록하여 모든 메트릭에서 새로운 SOTA를 설정했습니다. Highlight Detection에서도, VID-LLMs의 mAP를 평균 1.55% 개선하여 성능을 더욱 강화했습니다.

### [The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use](https://arxiv.org/abs/2411.10323)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10323.png)

Vote: 14

Authors: Mike Zheng Shou, Siyuan Hu, Mingyu Ouyang, Difei Gao

- ***What's New***: Claude 3.5 Computer Use는 최초로 그래픽 사용자 인터페이스(GUI) 에이전트로 공개 베타 버전의 컴퓨터 사용 기능을 제공하는 프론티어 AI 모델로 주목받고 있습니다. 이 사례 연구에서는 Claude 3.5의 데스크탑 작업 자동화 능력을 다양한 도메인과 소프트웨어를 통해 탐구하고 그 용량과 한계를 분석합니다.
- ***Technical Details***: Claude 3.5 Computer Use는 API 기반 GUI 자동화 모델의 성능을 평가하기 위해 웹 탐색, 전문 도구, 게임 등 다양한 도메인에서 실세계 데스크탑 환경을 바탕으로 포괄적인 사례 연구를 제안합니다. 평가의 주요 차원으로는 사용자의 쿼리로부터 실행 가능한 계획을 생성하는 능력인 Planning, GUI 요소와의 상호작용을 Grounding하고 실행하는 능력인 Action, 그리고 변화하는 환경에 대한 적응력과 작업 완료 시 수행을 멈추는 Critic 기능이 포함됩니다.
- ***Performance Highlights***: Claude 3.5는 다양한 도메인에서 뛰어난 End-to-End 언어와 데스크탑 행동 능력을 보여주었습니다. 예를 들어, Amazon에서 ANC 헤드폰을 검색하고 장바구니에 추가하는 작업을 성공적으로 수행했으며, Excel에서 제품을 기록하는 멀티 앱 연동 작업도 정확하게 수행했습니다. 반면, Fox Sports 구독과 같은 몇몇 작업에서 오류가 발생하며 Task Planning 오류를 드러냈습니다. 이러한 결과는 GUI 자동화 모델이 실환경의 복잡성을 더 효과적으로 처리하기 위한 추가 개선이 필요함을 보여줍니다.

### [MARS: Unleashing the Power of Variance Reduction for Training Large Models](https://arxiv.org/abs/2411.10438)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10438.png)

Vote: 2

Authors: Quanquan Gu, Yifeng Liu, Shuang Wu, Xun Zhou, Huizhuo Yuan

- ***What's New***: 이번 연구에서는 큰 모델(large models)을 효율적으로 학습하기 위한 새로운 최적화 프레임워크인 MARS(Make vAriance Reduction Shine)가 제안되었습니다. MARS는 적응적 경사 방식(adaptive gradient methods)과 분산 감소(variance reduction)를 통합하여 훈련 효율성을 높이는 혁신적인 접근법을 제공합니다.
- ***Technical Details***: MARS는 확장된 확률적 재귀적 모멘텀(scaled stochastic recursive momentum)을 통해 분산 감소와 전처리된(preconditioned) 경사 업데이트를 조화롭게 결합한 최적화 프레임워크입니다. 세 가지 변형인 MARS-AdamW, MARS-Lion, MARS-Shampoo가 있으며 각각의 방법은 기존 최적화 알고리즘인 AdamW, Lion, Shampoo와 높은 호환성을 가지고 있습니다.
- ***Performance Highlights***: MARS는 GPT-2 모델의 학습에 있어서 AdamW를 큰 폭으로 능가하는 성능을 보여줍니다. 구체적으로, OpenWebText 데이터를 사용한 실험에서 MARS는 50 억 개의 토큰으로 학습한 AdamW보다 더 낮은 검사 손실(validation loss)을 기록했습니다. 또한, Hellaswag 다운스트림 작업에서 44.20%의 정확도를 달성하여 AdamW의 42.31% 보다 높은 성능을 보였습니다.

### [GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation](https://arxiv.org/abs/2411.08033)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08033.png)

Vote: 19

Authors: Bo Dai, Fangzhou Hong, Chen Change Loy, Shangchen Zhou, Yushi Lan, Xingang Pan, Shuai Yang, Zhaoyang Lyu

- ***What's New***: GAUSSIANANYTHING는 3D 생성을 위한 새로운 프레임워크로, 포인트 클라우드(Point Cloud) 기반의 상호작용 가능한 잠재 공간(latent space)을 통해 고품질의 3D 콘텐츠 생성 및 편집을 지원합니다. 이 프레임워크는 단일 또는 다중 뷰 이미지, 캡션을 조건으로 하여 3D 콘텐츠를 생성할 수 있으며, 지오메트리와 텍스처를 자연스럽게 분리하여 3D 인식 편집을 가능하게 합니다.
- ***Technical Details***: 제안된 3D VAE는 다중 뷰로 렌더링된 RGB-D-N(법선) 이미지를 입력으로 채택하고, 3D 모양 정보를 보존하는 독특한 잠재 공간 설계를 사용하여 지오메트리-텍스처 분리에 효과적인 잠재 디퓨전 모델(cascaded latent diffusion model)을 통합합니다. 이는 포인트 클라우드 구조의 잠재 공간으로 변환되어, 3D 인식 편집이 가능하게 합니다. 최종적으로 생성된 잠재 포인트 클라우드는 고밀도 서펠 가우시언(surfel Gaussian)으로 디코딩되어 고해상도의 렌더링을 지원합니다.
- ***Performance Highlights***: GAUSSIANANYTHING는 텍스트 및 이미지 조건의 3D 생성에서 기존 방법들을 능가하는 성능을 보여주며, G-Objaverse, 3DTopia 등의 여러 데이터셋에서 뛰어난 효과를 보였습니다. 특히 텍스트와 이미지에 조건된 3D 생성에서 높은 정밀도와 효율성을 입증하였습니다. 실험 결과, 기존의 많은 모델들에 비해 더 나은 텍스트와 3D 간의 일치도를 보여 주었습니다. 

