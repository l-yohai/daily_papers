## Daily Papers (2024-11-19)

### [Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering](https://arxiv.org/abs/2411.09213)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.09213.png)

Vote: 6

Authors: Chien Van Nguyen, Franck Dernoncourt, Nghia Trung Ngo, Thien Huu Nguyen

- ***What's New***: 이 연구는 의학적 질문답변 시스템에서 검색 강화 생성 시스템(Retrieval-Augmented Generation; RAG) 평가를 위한 포괄적 프레임워크를 제시합니다. MedRGB(Medical Retrieval-Augmented Generation Benchmark)는 4개의 의료 QA 데이터셋에 다양한 테스트 요소를 추가하여 대형 언어 모델(LLMs)의 특정 상황 처리 능력을 평가합니다.
- ***Technical Details***: MedRGB는 4개의 의료 QA 데이터셋(MMLU, MedQA, PubMedQA, BioASQ)을 기반으로 하며, 주요 테스트 시나리오는 Standard-RAG, sufficiency, integration, robustness입니다. 각 시나리오는 여러 문서 검색 조건에서 모델의 능력을 포괄적으로 평가하기 위해 고안되었습니다.
- ***Performance Highlights***: 현재의 RAG 시스템은 검색된 문서 내 잡음과 잘못된 정보 처리에 제한적인 능력을 보였습니다. MedRGB를 통해 평가된 7개의 LLMs는 정확성을 요구하는 복잡한 시나리오에서 전반적으로 한계를 보여주었으며, 이는 신뢰할 수 있는 의료 AI 시스템 개발을 위한 향후 연구 방향을 제시합니다.

### [Evaluating the role of `Constitutions' for learning from AI feedback](https://arxiv.org/abs/2411.10168)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10168.png)

Vote: 3

Authors: Adam Mahdi, Saskia Redgate, Andrew M. Bean

- ***What's New***: 이 논문은 AI 피드백 학습에서 'constitutions'의 역할을 평가하는 연구로, 특히 환자 중심의 의사소통(communication)을 개선하기 위한 다양한 constitution을 사용하여 이들의 효과를 비교합니다. 인간 피드백 대신 LLMs의 피드백을 사용하는 방법을 제안하며, 'constitutions'의 선택이 방법의 결과에 어떻게 영향을 미치는지를 조사합니다.
- ***Technical Details***: 'Constitutions'에 기반한 피드백 학습은 In-context learning을 통해 AI의 대화 생성 품질을 개선하는 과정을 포함합니다. 환자와 의사 간의 대화를 시뮬레이션하여 다양한 constitution을 적용하였으며, 'Fostering Relationship', 'Decision Making', 'Responding to Emotions'과 같은 감정적 차원에서 가장 구체적인 constitution이 더욱 효과적임을 발견했습니다. 각 구성 요소는 Claude 3.5 Sonnet 모델 인스턴스로 이루어져 있으며, 환자, 의사, 중재자, 비평가의 역할을 맡습니다.
- ***Performance Highlights***: 215명의 인간 평가자가 참여한 실험 결과, 가장 구체적인 'Best Practices' constitution이 감정적 차원에서 더 선호되는 대화를 생성하는 데 효과적이었습니다. 그러나 정보 수집과 제공과 같은 실용적 차원에서는 해당 constitution이 뚜렷한 차별성을 보이지 못했습니다. 이는 AI 피드백이 특정 유형의 행동 개선에는 적합할 수 있음을 시사합니다.

### [SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers](https://arxiv.org/abs/2411.10510)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10510.png)

Vote: 8

Authors: Joseph Liu, Mahesh Kumar Nandwana, Ziyu Guo, Haomiao Jiang, Joshua Geddes

- ***What's New***: SmoothCache는 Diffusion Transformer(DiT) 모델의 추론을 효과적으로 가속화하는 범용적인 기술로 소개됩니다. 이 기술은 인접한 층 출력 간의 높은 유사성을 활용하여 다양한 모달리티에서 최적의 캐싱 강도를 적응적으로 결정하며, 이미지, 비디오, 오디오 생성 모델에서의 성능을 크게 개선했습니다.
- ***Technical Details***: SmoothCache는 어떠한 DiT 아키텍처에서도 적용 가능하며, 모델 또는 데이터셋에 특정되지 않은 보편적인 캐싱 방법을 제공합니다. 인접한 시간 단계에서의 층별 표현 오류를 분석하여, 최소 오류로 기능을 캐싱하고 재사용할 수 있습니다. 이를 통해 대부분의 계산 집약적인 층의 작업 수를 줄일 수 있으며, 실시간 응용 프로그램을 가능하게 합니다.
- ***Performance Highlights***: SmoothCache는 다양한 모달리티에 대해 8%에서 71%까지의 속도 향상을 이루어 내었으며, 성능 저하 없이 품질을 유지하거나 개선했습니다. 특히, 이미지, 비디오, 오디오 분야에서 최첨단(SOTA) 캐싱 방식에 필적하거나 이를 초과하는 성능을 입증했습니다.

### [VeGaS: Video Gaussian Splatting](https://arxiv.org/abs/2411.11024)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11024.png)

Vote: 5

Authors: Przemysław Spurek, Marcin Mazur, Dawid Malarz, Jan Kaczmarczyk, Weronika Smolak-Dyżewska, Kornel Howil

- ***What's New***: VeGaS는 비디오 데이터의 비선형 구조를 포착하기 위해 새로운 폴드 가우시안(Folded-Gaussian) 분포를 도입하여 비디오 데이터를 보다 현실적으로 수정할 수 있는 비디오 가우시안 스플래팅(Video Gaussian Splatting, 3DGS) 모델을 제안합니다.
- ***Technical Details***: VeGaS 모델은 3D 가우시안(3D Gaussian)을 2D 비디오 데이터에 적용하여 각 프레임을 3D 스페이스 내의 평행 평면으로 취급하고 비디오 스트림의 비선형 구조를 효과적으로 모델링하는 Folded-Gaussian 분포를 사용하여 만든 모델입니다. 각 프레임은 시간 조건에 따른 2D 가우시안으로 모델링되며, 이는 고품질의 비디오 데이터 렌더링과 다양한 수정 작업을 가능하게 합니다.
- ***Performance Highlights***: VeGaS 모델은 비디오 프레임 재구성 작업에서는 기존 최첨단 모델을 능가하며, PSNR(33.31) 및 SSIM(0.902) 등의 평가 척도에서 높은 성능을 기록했습니다. 또한, 연속적으로 인코딩된 비디오 데이터의 프레임 간 보간 작업에서도 우수한 품질의 결과를 보여줍니다.

### [Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts](https://arxiv.org/abs/2411.10669)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10669.png)

Vote: 7

Authors: Yizhao Gao, Hongpeng Lin, Jinqiang Long, Nanyi Fei, Zhiwu Lu, Yanqi Dai, Guoxing Yang

- ***What's New***: Awaker2.5-VL은 대형 멀티모달 언어 모델(Multimodal Large Language Models; MLLMs)의 '다중 과제 충돌(multi-task conflict)' 문제를 해결하기 위해 Mixture of Experts(MoE) 아키텍처를 도입한 새로운 모델입니다. 이 모델은 여러 전문가 모델들을 통해 과제별 능력을 획득하며, MoE/Lora 모듈만 훈련하여 비용 효율성을 높였습니다.
- ***Technical Details***: Awaker2.5-VL는 LoRA 구조로 각 전문가(Experts)를 구성하며, MoE 라우팅 전략을 새롭게 설계하여 MLLMs의 안정성을 높였습니다. 기본 모델(Qwen2-VL-7B-Instruct)은 동결 상태로 유지되고, 훈련은 MoE/Lora 모듈만을 대상으로 하여 비용을 절감합니다. Gate 네트워크는 간단한 선형 레이어로 되어 있으며, 데이터가 처리되면서 어떤 전문가가 활성화될지 결정합니다.
- ***Performance Highlights***: Awaker2.5-VL는 여러 최신 벤치마크(MME-RealWorld, MMBench)에서 높은 성능을 나타내며, 특히 MME-RealWorld-CN 벤치마크에서는 모든 모델 중 가장 높은 점수를 기록했습니다. 중국어 시나리오에서 Perception과 Reasoning 점수가 각각 6점과 3점 향상되어 주목할 만한 성과를 보였습니다.

### [Generative World Explorer](https://arxiv.org/abs/2411.11844)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11844.png)

Vote: 34

Authors: Jieneng Chen, Daniel Khashabi, Tianmin Shu, Taiming Lu, Alan Yuille

- ***What's New***: Generative World Explorer (Genex)는 인간이 실제 물리적 탐색을 하지 않고도 3D 세상을 심적 상상으로 탐험하고 관찰하며 신념을 업데이트할 수 있는 프레임워크를 제안합니다. Genex는 대규모 3D 도시 장면을 상상 관찰하며 기존 의사 결정 모델을 개선합니다.
- ***Technical Details***: Genex는 에고센트릭(first-person) 파노라마 뷰를 이용한 비디오 생성 모델입니다. 학습을 위해 Genex-DB라는 합성 도시 장면 데이터셋을 구축했습니다. 모델은 파노라마 이미지를 사용해 세계 탐험의 일관성을 유지하며, 동영상 확산 모델(video diffusion model)을 활용하여 성능을 극대화합니다. 상상 탐험으로 신념을 업데이트하여 Partially Observable Markov Decision Processes (POMDP)에 기반해 행동하는 에이전트를 지원합니다.
- ***Performance Highlights***: 실험 결과, Genex는 고품질 동영상을 생성하며, 상상 관찰을 통해 에이전트가 더 나은 계획을 세울 수 있게 해 줍니다. 체계적인 상상 탐험을 통해 물리적 탐험과 유사한 신념 업데이트를 구현하며, 다양한 신 시나리오와 복잡한 상황에서도 잘 일반화됩니다.

### [FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on](https://arxiv.org/abs/2411.10499)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10499.png)

Vote: 4

Authors: Donghao Luo, Jiangning Zhang, Chengjie Wang, Xiaobin Hu, Yunsheng Wu, Chengming Xu, Boyuan Jiang, Yanwei Fu, Qingdong He, Jinlong Peng

- ***What's New***: FitDiT는 가상 시착(Virtual Try-on; VTON)을 향상시키기 위한 고해상도 텍스처 및 사이즈 피팅을 강화한 새로운 접근법입니다. Diffusion Transformers(DiT)를 사용하여 복잡한 패턴 및 텍스트와 같은 고주파 의류 세부사항을 더 잘 인식하며 고해상도 특징에 더 많은 주의를 기울입니다.
- ***Technical Details***: FitDiT는 두 가지 주요 문제를 해결합니다: (1) 텍스처 인지를 위해 의류 패턴 인식에 강점을 두는 DiT 기반 LDM을 도입하고, (2) 사이즈 피팅 문제를 해결하기 위해 희소하며 유연한 마스크 전략을 사용합니다. 또한, 기존 모델에서 나타나는 구조적 중복성을 해결하기 위해 DiT 구조를 슬림화하고 의류 조건 조절 및 특징 주입을 통해 성능을 최적화했습니다. 주목할 만한 개념은 의류 선행 진화 전략으로, 의류 데이터에 기초하여 피처 추출기를 미세 조정하고, 주파수 공간에서의 복잡한 텍스처를 유지하기 위해 주파수 거리 손실을 도입한 것입니다.
- ***Performance Highlights***: FitDiT는 실제 패턴 및 텍스처 보존에서 탁월한 성능을 발휘하며, 대칭적인 시나리오를 다루는데 있어 다른 모델을 능가합니다. 실제 테스트에서는 1024 × 768 이미지당 4.57초의 추론 시간을 기록하여 빠른 처리 속도를 보여줍니다. VITON-HD와 DressCode 데이터셋에서의 정량적 평가에서도 FID와 KID 점수에서 다른 첨단 모델을 능가하며, CVDD와 같은 복잡한 데이터셋에서도 특히 강력한 성능을 보입니다.

### [StableV2V: Stablizing Shape Consistency in Video-to-Video Editing](https://arxiv.org/abs/2411.11045)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11045.png)

Vote: 5

Authors: Kaidong Zhang, Chang Liu, Dong Liu, Rui Li, Yunwei Lan

- ***What's New***: StableV2V는 비디오 편집에서 형태 일관성을 유지하는 혁신적인 방법을 제안합니다. 이 방법은 매끄러운 모션과 편집된 콘텐츠 간의 불일치 문제를 해결하는데 초점을 맞추었으며, 새롭게 구축된 DAVIS-Edit 벤치마크를 통해 다양한 프롬프트와 난이도에 대한 종합적인 평가가 가능합니다.
- ***Technical Details***: StableV2V 방법은 'Prompted First-frame Editor (PFE)', 'Iterative Shape Aligner (ISA)', 'Conditional Image-to-video Generator (CIG)'의 세 가지 주요 구성 요소로 구성됩니다. PFE는 외부 프롬프트를 받아 첫 번째 프레임을 편집하며, ISA는 원본 비디오의 깊이 맵과 광학 흐름을 활용해 편집된 비디오의 모든 프레임에 콘텐츠를 전파합니다. 마지막으로, CIG는 형태 안내 깊이 맵을 사용하여 최종 편집된 비디오를 생성합니다.
- ***Performance Highlights***: StableV2V는 DAVIS-Edit 벤치마크에서 다른 최신 연구들과 비교하여 비주얼 품질, 일관성, 추론 효율성 등 여러 측면에서 우수한 성능을 보여줍니다. 특히 상당한 형태 변화를 포함하는 더 복잡한 시나리오에서도 동작과 사용자 프롬프트 사이의 일관성을 보장하여 더 우수한 결과물을 생성합니다.

### [AnimateAnything: Consistent and Controllable Animation for Video Generation](https://arxiv.org/abs/2411.10836)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10836.png)

Vote: 10

Authors: Hong Li, Rong Zhang, Weiwei Xu, Guojun Lei, Chi Wang, Yikai Wang

- ***What's New***: AnimateAnything은 다양한 제어 신호를 통합하여 일관되고 제어 가능한 비디오 생성을 가능하게 하는 새로운 접근법을 제시합니다. 이는 참조 이미지를 기반으로 다양한 캐릭터를 애니메이션화하고 카메라 경로, 텍스트 프롬프트, 사용자 모션 주석과 같은 조건 하에서 일관된 비디오를 생성할 수 있습니다.
- ***Technical Details***: AnimateAnything은 다단계 제어 기능 융합 네트워크를 사용하여 공통 모션 표현을 구축하며, 모든 제어 정보를 프레임별 광학 흐름(Optical Flow)으로 변환합니다. 여기서 중요한 구성 요소로는 주파수 기반 안정화 모듈이 있으며, 이는 대규모 모션에서 발생할 수 있는 깜빡임 현상을 최소화합니다. 비디오 생성은 두 단계로 진행되며, 첫 번째 단계에서는 다양한 모션 제어 신호를 하나의 통합된 광학 흐름으로 변환하고, 두 번째 단계에서 이를 최종 비디오 생성에 사용합니다.
- ***Performance Highlights***: AnimateAnything은 여러 테스트에서 최신 기법보다 우수한 결과를 보였습니다. 특히 카메라 궤도 및 사용자 주석 기반 영상 생성에서 뛰어난 성능을 나타냈습니다. 비디오 품질 평가 지표인 Fréchet Inception Distance(FID), SSIM 등 다양한 평가에서 다른 방법들에 비해 높은 점수를 기록하였습니다.

### [BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices](https://arxiv.org/abs/2411.10640)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10640.png)

Vote: 29

Authors: Yan Hu, Yanzhou Yang, Aojun Zhou, Hongsheng Li, Yinghao Chen, Zhaoxiong Wang, Boheng Chen, Cheng Chen, Guanxin Tan, Hui Tan, Long Liu, Yi Zeng, Shuai Ren, Xudong Lu, Han Xiao, Yafei Wen, Xiaoxin Chen, Lei Wu, Renshou Wu, Liuyang Bian, Yina Xie, Rui Hu

- ***What's New***: BlueLM-V-3B는 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)을 모바일 플랫폼에 효율적으로 배포하기 위해 알고리즘과 시스템을 공동 설계한 새로운 접근 방식입니다. 특히 메인스트림 MLLMs에서 채택한 동적 해상도(Dynamic Resolution) 방식을 다시 설계하고, 하드웨어 인식 배포를 위한 시스템 최적화를 구현하여 모바일 기기에서 원활한 모델 추론을 가능하게 합니다.
- ***Technical Details***: BlueLM-V-3B는 NLP 모델로 2.7B 파라미터, 그리고 비전 인코더로 400M 파라미터를 탑재하고 있습니다. MediaTek Dimensity 9300 프로세서에서 4-bit LLM 가중치 양자화를 통해 24.4토큰/초의 생성 속도를 달성합니다. 알고리즘 측면에서는 이미지 확장을 감소시키기 위해 완화된 가로세로비 일치 방식(Relaxed Aspect Ratio Matching)을 제안하고, 시스템 측면에서는 이미지 패치 병렬 처리 등 하드웨어 최적화를 도입합니다.
- ***Performance Highlights***: BlueLM-V-3B는 OpenCompass 벤치마크에서 4B 파라미터 이하의 모델 중 가장 높은 평균 점수(66.1)를 기록하며, 훨씬 큰 파라미터 수를 가진 일련의 모델(예: MiniCPM-V-2.6, InternVL2-8B)을 능가합니다. 또한, 모바일 장치로의 배포 시 높은 처리 효율성을 보장하며, 768×1536 해상도의 이미지를 약 2.1초 만에 인코딩하고 2.2GB의 메모리만을 요구합니다.

### [Top-nσ: Not All Logits Are You Need](https://arxiv.org/abs/2411.07641)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.07641.png)

Vote: 10

Authors: Hongli Xu, Jianchun Liu, Chenxia Tang, Liusheng Huang

- ***What's New***: 이 연구에서는 기존의 논리를 바탕으로 한 토큰 샘플링 방법에 도전하여 pre-softmax 로짓을 직접 이용하는 새로운 샘플링 방법인 top-nσ을 소개합니다. 이 방법은 확률 기반의 복잡한 조작 없이 간단한 통계적 임계값을 사용하여 노이즈 토큰이 아닌 정보성 토큰을 효율적으로 필터링합니다. top-nσ은 현재의 샘플링 방법들이 다양한 온도에서 노이즈 토큰을 포함하게 되는 문제점을 해결하여, 온도 스케일링과 관계없이 안정적인 샘플링 공간을 유지합니다.
- ***Technical Details***: top-nσ 샘플링은 pre-softmax 로짓 분포를 분석하여 크게 노이즈 영역과 정보성(Informative) 영역으로 나뉜다는 것을 발견한 것에 기반합니다. Gaussian 분포를 따르는 노이즈 토큰과 주목할 만한 이탈자인 정보성 토큰들을 통계적 속성을 통해 구분하며, sorting나 softmax 변환이 불필요하여 계산 효율성이 높습니다. 이 방법은 정렬 없는(Streamlined) 예방 민감 객체 탐지 기반으로 동작하며, 기본적으로 고온 샘플링에서도 성능을 보장합니다.
- ***Performance Highlights***: top-nσ 기법은 네 가지 데이터셋에 대한 실험에서 기존의 샘플링 방식뿐만 아니라 그리디 디코딩을 능가하는 성능을 보였습니다. 특히, 고온(T=3.0)에서도 안정적인 성능을 유지하며, GPQA 데이터셋에서 기존 샘플링 기법들이 완전히 실패하는 온도에서 25.00%와 GSM8K에서 74.61%의 정확도를 보였습니다. 전통적인 그리디 디코딩을 넘어서는 성과를 보여주는 이 결과는 높은 온도에서도 탐색과 활용의 균형을 유지하는 데 탁월함을 보여줍니다.

### [Drowning in Documents: Consequences of Scaling Reranker Inference](https://arxiv.org/abs/2411.11767)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11767.png)

Vote: 8

Authors: Michael Carbin, Andrew Drozdov, Mathew Jacob, Omar Khattab, Matei Zaharia, Erik Lindgren

- ***What's New***: 이 논문은 침수된 문서 환경에서 대규모 리랭커 추론(Reranker Inference)의 영향에 대해 탐구하고 있습니다. 기존 리랭커가 대량의 문서를 평가할 때 점차적으로 감소하는 성능을 보이며, 일정한 한도를 넘어서는 경우 품질이 저하될 수 있음을 밝혀냈습니다.
- ***Technical Details***: 이 연구는 여러 개의 공개 및 비공개 리랭커와 리트리버를 사용하여 다양한 IR 벤치마크를 분석하였습니다. 리랭커의 효율성을 이해하기 위해 다양한 문서 수(k)로 리랭킹을 수행하고, 각 리랭커의 리콜(Recall)을 측정하였습니다. 실험 결과는 대규모 언어 모델을 사용한 리스트형 리랭킹(listwise reranking)이 리랭커의 품질을 향상시킬 수 있음을 보여줍니다.
- ***Performance Highlights***: 리랭커는 소량의 문서(K)가 주어졌을 때 리콜을 개선하는 경향을 보였으나, 너무 많은 문서를 리랭킹할 경우 오히려 리트리버보다 성능이 낮아졌습니다. 이로 인해, 대규모 리랭커가 대량의 문서를 처리할 경우 품질이 저하될 수 있으며, 이는 리랭커의 개선 필요성을 시사합니다.

### [Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering](https://arxiv.org/abs/2411.11504)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11504.png)

Vote: 12

Authors: Xinyu Lu, Xianpei Han, Jie Lou, Yanjiang Liu, Hongyu Lin, Le Sun, Bowen Yu, Xinyan Guan, Ben He, Yaojie Lu, Boxi Cao

- ***What's New***: 이 논문은 파운데이션 모델(Foundation Models)의 사후 훈련을 위한 새로운 패러다임인 'Verifier Engineering'을 제안합니다. 이 접근 방식은 '검색(Search)', '검증(Verify)', '피드백(Feedback)'의 세 가지 주요 단계로 구성되며, 자동화된 검증기를 활용하여 파운데이션 모델에 의미 있는 피드백을 제공합니다.
- ***Technical Details***: Verifier Engineering은 파운데이션 모델의 훈련 이후 활용되는 자동화된 검증 작업을 통해 다양한 검증기를 통합합니다. 이 과정은 목표에 맞는 샘플링(Search), 후보 응답에 대한 적절한 검증 수행(Verify), 그리고 검증 결과를 기반으로 모델의 출력 분포를 최적화(Feedback)하는 일련의 절차를 통해 수행됩니다.
- ***Performance Highlights***: Verifier Engineering은 기존의 RLHF(Reinforcement Learning from Human Feedback) 및 기타 기존 방법들과 비교하여 다중 검증원을 통합하여 보다 정확하고 일반화된 피드백 신호를 제공합니다. 이 접근법을 통해 파운데이션 모델의 범용 인공지능 능력 향상을 위한 시스템 구축을 제안합니다.

### [Adaptive Decoding via Latent Preference Optimization](https://arxiv.org/abs/2411.09661)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.09661.png)

Vote: 5

Authors: Jack Lanchantin, Jason Weston, Ilia Kulikov, Asli Celikyilmaz, Shehzaad Dhuliawala, Ping Yu, Sainbayar Sukhbaatar

- **What's New**: 본 논문은 Latent Preference Optimization(LPO)을 활용한 적응적 디코딩(Adaptive Decoding) 기법을 도입하여, 토큰 또는 예제 단위에서 디코딩 온도를 동적으로 선택함으로써 언어 모델의 성능을 최적화하고자 합니다. 이는 고정된 디코딩 온도 대신, 문맥에 맞는 이상적인 온도를 학습하여 다양한 종류의 과제에서 향상된 성능을 보여줍니다.
- **Technical Details**: ADAPTIVEDECODER라는 새로운 학습 가능한 모듈을 Transformer 아키텍처의 마지막 숨은 상태에 추가하여, 주어진 컨텍스트에 따라 다음 토큰의 디코딩 온도를 이상적으로 선택할 수 있게 합니다. 온도 선택을 위해 LPO 방법론을 통한 학습을 수행하며, 이 모듈은 시퀀스 수준에서 단일 온도를 예측하거나 토큰 수준에서 토큰별로 새로운 온도를 예측할 수 있습니다. LPO는 보상 모델을 활용하여 생성된 응답에 대한 선호 쌍(Preference Pairs)을 구축, 학습합니다.
- **Performance Highlights**: ADAPTIVEDECODER를 통해 고정된 디코딩 온도를 사용하는 기존 방법을 넘어서는 성능 향상을 달성했습니다. UltraMathStories라는 데이터셋에서 테스트한 결과, 수학 문제와 같이 정확한 답변이 필요한 경우 낮은 온도를, 창의적인 글쓰기가 필요한 경우 높은 온도를 선택하는 등 이상적인 온도를 학습하는데 성공했습니다. 이는 특히 제한된 창의적 글쓰기와 같은 복잡한 과제에서도 효과적으로 적응함을 보여주며, 단일 온도 전략에 비해 더 유리한 결과를 보여줍니다.

### [LLäMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.11171.png)

Vote: 7

Authors: Andreas Hotho, Julia Wunderle, Jan Pfister

- ***What's New***: LLäMmlein은 독일어 전용 디코더 모델인 LLäMmlein 120M과 1B를 처음부터 투명하게 제작하였으며, 독일 NLP 연구 커뮤니티를 위해 공개합니다. RedPajama V2 데이터셋을 필터링 및 전처리하여 고품질 독일어 데이터를 만들고, 맞춤형 독일어 토크나이저를 구축하여 학습했습니다.
- ***Technical Details***: 모델 개발에는 데이터셋 전처리, 토크나이저 학습, 오토리그레시브 언어 모델(pretrain) 학습 및 SuperGLEBer 벤치마크에서의 성능 평가 등을 포함합니다. 독일어 전용 데이터셋에서 토큰화 및 학습을 진행하며, 다양한 중간 체크포인트를 저장하여 학습 동역학 분석을 실시합니다.
- ***Performance Highlights***: SuperGLEBer 벤치마크에서 LLäMmlein 모델은 동급 크기의 다른 모델들을 능가하거나 일관되게 상회하는 성능을 보여주었습니다. 특히, 독일어 전용 모델로서 다국어 지원 모델보다 우수한 성과를 보였으며, 토크나이저 성능도 기존 독일어 토크나이저와 비교하여 경쟁력 있음을 입증했습니다. LLäMmlein 1B 모델은 크기 대비 비교적 우수한 성능을 발휘했으며, 작은 모델에 비해 더 높은 정확도를 제공했습니다.

### [SlimLM: An Efficient Small Language Model for On-Device Document Assistance](https://arxiv.org/abs/2411.09944)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.09944.png)

Vote: 10

Authors: Thang M. Pham, Phat T. Nguyen, Franck Dernoncourt, Seunghyun Yoon, Trung Bui, Viet Dac Lai

- ***What's New***: SlimLM은 모바일 기기에서 문서 보조(Document Assistance) 작업을 위한 소형 언어 모델(Small Language Models; SLMs)의 성능과 효율성을 극대화한 새로운 모델 시리즈로, 특히 고급 스마트폰에서의 실행 가능성을 제공합니다. 삼성 갤럭시 S24에서 문서 요약(Summarization), 질문 답변(Question Answering), 질문 제안(Question Suggestion) 작업을 수행하기 위해 최적화된 훈련과정을 통해 서버 비용 절감과 개인정보 보호를 강화할 수 있습니다.
- ***Technical Details***: SlimLM은 SlimPajama-627B 데이터셋에 사전 학습(Pre-training)되었으며, 자체 구축한 DocAssist 데이터셋을 활용해 3가지 문서 보조 작업에 맞춰 미세 조정(Fine-tuning)되었습니다. 이 모델은 125M에서 1B 파라미터까지 다양한 크기로 제공되어, 모바일 디바이스에서 효율적 작동이 가능하게끔 설계되었습니다. 최근 고성능 스마트폰에서도 최대 800개의 문맥 토큰을 효과적으로 처리할 수 있습니다.
- ***Performance Highlights***: SlimLM 모델은 BLEU, ROUGE, STS 및 GEval 같은 표준 메트릭에서 기존의 소형 언어 모델과 비교해 동등하거나 더 높은 성능을 보여주었습니다. SlimLM-125M은 SmolLM-135M-Instruct를 능가하고, SlimLM-1B는 더 큰 모델인 Qwen2-1.5B-Instruct와 거의 비견될 만큼 뛰어난 성능을 자랑합니다. 실질적인 적용 가능성을 보이기 위해 Android 애플리케이션이 개발되었습니다.

