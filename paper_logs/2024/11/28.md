## Daily Papers (2024-11-28)

### [Training and Evaluating Language Models with Template-based Data Generation](https://arxiv.org/abs/2411.18104)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18104.png)

Vote: 1

Authors: Yifan Zhang

- ***What's New***: 이 연구에서는 템플릿 기반 데이터 생성(Template-based Data Generation; TDG)이란 새로운 접근 방식을 통해 대규모, 고품질의 수학 문제를 생성하는 방법을 소개합니다. 특히 GPT-4를 활용하여 메타-템플릿을 자동으로 생성하고, 이를 기반으로 다양한 수학 문제와 솔루션을 무한히 생성할 수 있는 시스템을 구축하였습니다.
- ***Technical Details***: TDG 방법론은 GPT-4를 활용하여 다양한 수학적 문제 구조를 포괄하는 메타-템플릿을 생성하고, 이를 사용해 문제와 솔루션을 동시에 생성합니다. 생성된 데이터는 코드 실행과 LLM기반 검증 과정을 거쳐 정확성을 보장합니다. 이러한 통합 생성 및 검증 프로세스를 통해 데이터 품질을 높이고, 풍부한 문제 구조를 유지합니다.
- ***Performance Highlights***: 이 연구에서 생성된 TemplateGSM 데이터셋은 7백만 개 이상의 초등학교 수학 문제로 구성되어 있으며, 정확하고 신뢰할 수 있는 솔루션을 포함하고 있습니다. 이 데이터셋은 대형 언어 모델을 수학적 추론 과제에서 사전 훈련 및 세부 조정에 사용할 수 있도록 설계되었습니다. 이를 통해 모델의 이해력과 문제 해결 능력을 크게 향상시킬 수 있습니다.

### [Identity-Preserving Text-to-Video Generation by Frequency Decomposition](https://arxiv.org/abs/2411.17440)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17440.png)

Vote: 9

Authors: Liuhan Chen, Yunyuan Ge, Jiebo Luo, Xianyi He, Yujun Shi, Jinfa Huang, Li Yuan, Shenghai Yuan

- ***What's New***: ConsisID는 주파수 분해를 통해 사람의 정체성을 유지하면서 텍스트로부터 비디오를 생성하는 새로운 모델입니다. 이 모델은 주파수 인지적 기술(frequency-aware heuristic)을 사용하여 얼굴 특징을 높은 주파수와 낮은 주파수로 분리하여 보다 효과적인 ID-보존 텍스트-비디오(ID-preserving text-to-video) 생성을 가능하게 합니다.
- ***Technical Details***: ConsisID는 DiT(Diffusion Transformer) 기반의 제어 가능한 IDT2V(Identity-preserving text-to-video) 모델로, 주파수 도메인에서 얼굴 특징을 분리하여 DiT의 특정 위치에 삽입함으로써 ID 기능을 고효율적으로 유지합니다. 저주파 관점에서는 참조 이미지와 얼굴 키포인트를 잠재 공간에 인코딩하여 네트워크의 얕은 계층에 통합하고, 고주파 관점에서는 로컬 얼굴 추출기를 설계하여 고주파 세부 정보를 가져오고 트랜스포머 블록에 주입합니다. 최종적으로, 사전 훈련된 모델을 IPT2V 모델로 변환하고 일반화를 향상시키기 위한 계층적 학습 전략이 제안됩니다.
- ***Performance Highlights***: ConsisID는 높은 품질과 일관된 정체성 보존 비디오를 생성하는 데 있어서 뛰어난 성능을 보여주며, FaceSim-Arc에서 0.58, CLIPScore에서 27.93을 기록했습니다. 이는 최신 모델들에 비해 정체성 보존과 텍스트 일치 측면에서 향상된 성능을 입증합니다.

### [EfficientViM: Efficient Vision Mamba with Hidden State Mixer based State Space Duality](https://arxiv.org/abs/2411.15241)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15241.png)

Vote: 3

Authors: Sanghyeok Lee, Joonmyung Choi, Hyunwoo J. Kim

- ***What's New***: EfficientViM은 효율적인 비전 마음바(Efficient Vision Mamba; EfficientViM)라는 새로운 경량 비전 백본을 소개합니다. 이는 Hidden State Mixer 기반의 State Space Duality(HSM-SSD)를 활용하여, 전역 의존성을 효율적으로 캡처하고 계산 비용을 더욱 줄인 아키텍처입니다.
- ***Technical Details***: EfficientViM은 Hidden State Mixer 기반 SSD(HSM-SSD) 레이어를 도입하여, 숨겨진 상태 내 채널 믹싱 작업을 가능하게 하며, 다단계 숨겨진 상태 결합 방식을 제안하여 모델의 표현력을 강화합니다. SSD를 수정하여 메모리 제한 작업으로 인한 병목 현상을 제거할 수 있도록 설계되었습니다. 이를 통해 실용적인 성능을 실현하였습니다.
- ***Performance Highlights***: ImageNet-1K에서 EfficientViM은 최상의 속도-정확도 절충점을 달성하며, 이전 SOTA 모델인 SHViT을 능가하였고, 최대 0.7%의 성능 향상을 제공합니다. 또한, 이미지 크기 조정 및 증류 훈련을 사용할 때 이전 연구에 비해 처리량과 정확도에서 상당한 개선을 보였습니다.

### [Adaptive Blind All-in-One Image Restoration](https://arxiv.org/abs/2411.18412)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18412.png)

Vote: 3

Authors: David Serrano-Lozano, Shaolin Su, Javier Vazquez-Corral, Luis Herranz

- ***What's New***: Adaptive Blind All-in-One Image Restoration(ABAIR) 모델은 비시각적 조건 하의 이미지 복원 작업을 개선합니다. 이 모델은 다양한 손상(degradations)에 잘 대응하며, 새로운 손상을 추가할 때 일부 파라미터만 학습함으로써 유연하게 적용할 수 있습니다.
- ***Technical Details***: ABAIR 모델은 세 가지 주요 단계로 구성됩니다. 첫째, 다량의 합성 손상과 함께 자연 이미지를 대량 학습하여 기본 모델의 강력한 백본을 만듭니다. 둘째, 독립적인 낮은 계며 어댑터(low-rank adapters)를 사용하여 기본 모델을 다양한 이미지 복원 작업에 적용합니다. 셋째, 적응형 분산 추정기(degradation estimator)를 통해 다양한 어댑터를 조합하여 복잡한 이미지에 효율적으로 대응합니다.
- ***Performance Highlights***: ABAIR 모델은 다섯 가지와 세 가지 이미지 복원 작업에서 최신 방법보다 큰 차이로 성능을 능가하며, 보이지 않는 손상과 혼합 손상에 대한 향상된 일반화 능력을 보여주었습니다. 이 모델은 PASS@1에서 2.91 dB PSNR 향상을 기록하며, 성능이 입증된 최신의 편리성과 유연성을 제공합니다.

### [Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding](https://arxiv.org/abs/2411.18462)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18462.png)

Vote: 4

Authors: Xingyu Chen, Zhaopeng Tu, Rui Wang, Zhiwei He, Jiahao Xu, Ziyin Zhang, Tian Liang

- ***What's New***: 이 논문에서는 새로운 추론 가속 기법인 Speculative Decoding(SD)을 위한 적응형 드래프트 길이 정책을 제안했습니다. 기존 SD 방식은 고정된 드래프트 길이를 사용하지만, 제안된 SVIP(Speculative decoding의 Self-Verification length Policy)는 각 토큰의 엔트로피를 기반으로 길이를 동적으로 조정하여 효율성을 향상시킵니다.
- ***Technical Details***: SVIP는 초임계 토큰의 엔트로피를 이용해 드래프트 시퀀스의 길이를 동적으로 결정합니다. 예를 들어, 간단한 토큰은 길게 드래프트하고 어려운 토큰이 마주할 경우 드래프트를 조기에 종료합니다. 이 기법은 Kullback-Leibler divergence와 핀스커의 불평등을 기반으로 한 이론적 하한을 사용하여 드래프트 토큰 수용률을 추정합니다.
- ***Performance Highlights***: SVIP는 주류 Speculative Decoding 벤치마크와 프레임워크에서 기존 방법들에 비해 최대 20%의 벽시간 속도를 개선하였으며, 8K 토큰 길이의 장문 생성에서는 방해 요소 없이 60%의 속도 개선을 달성했습니다. 또한, SVIP는 추가 훈련이 필요 없고 모든 기존 SD 메서드와 호환이 가능합니다.

### [Video-Guided Foley Sound Generation with Multimodal Controls](https://arxiv.org/abs/2411.17698)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17698.png)

Vote: 2

Authors: Justin Salamon, Bryan Russell, David Bourgin, Prem Seetharaman, Ziyang Chen, Oriol Nieto, Andrew Owens

- **What's New**: 이 연구는 비디오 인식 기반의 Foley 음향 생성 시스템인 MultiFoley를 소개합니다. MultiFoley는 텍스트, 오디오, 비디오 등 여러 모달 입력을 통해 사용자에게 맞춤형으로 고품질 동기화된 오디오를 생성하는 유연한 제어 기능을 제공합니다. 이는 Foley 음향 제작의 새로운 가능성을 열어줍니다.
- **Technical Details**: MultiFoley는 여러 모달리티에서 합동 훈련을 통해 48kHz의 고품질 전체 대역폭 오디오를 생성할 수 있습니다. 이 모델은 영상, 오디오, 텍스트 인코더와 Diffusion Transformer를 사용하며, VGGSound와 Sound Ideas와 같은 데이터세트에서 고음질과 저음질 태그를 이용해 학습됩니다. 특히, VGGSound의 하위 집합에서 성능을 더욱 향상시키기 위한 미세조정 과정을 거쳤습니다.
- **Performance Highlights**: MultiFoley는 두 가지 CLAP 기반 메트릭과 AV-Sync 스코어에서 기존 방법보다 뛰어난 성과를 보였습니다. 특히, 사전 훈련된 모델을 미세 조정하여 다양한 크로스 모달 정렬 메트릭에서 눈에 띄는 성능 향상을 달성하였습니다. 이는 높은 오디오 품질과 크로스 모달 정렬 능력을 입증합니다.

### [UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing](https://arxiv.org/abs/2411.16781)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16781.png)

Vote: 4

Authors: Yiheng Li, Ruibing Hou, Hong Chang, Shiguang Shan, Xilin Chen

- ***What's New***: UniPose는 다양한 형태의 인간 포즈를 이해하고 생성하며 수정할 수 있는 통합 멀티모달 프레임워크입니다. 이 연구는 이미지를 포함한 여러 모달리티에서 인간 포즈를 다룰 수 있는 첫 번째 시도로서 주목받고 있습니다.
- ***Technical Details***: UniPose는 영문자 보정을 지원하는 대규모 언어 모델(LLM; Large Language Models)을 활용하여 포즈적인 정보를 분석합니다. 특히, 3D 포즈를 이산 포즈 토큰으로 변환하는 포즈 토크나이저를 적용하여 LLM 내에서 통합된 어휘 공간을 제공합니다. 더욱 세밀한 포즈 인식을 위해 혼합 시각 인코더 방법을 사용했으며, 이 중 포즈 특정 시각 인코더는 포즈 추정 작업에 사전 학습됩니다. 이러한 접근은 다양한 포즈 관련 작업에 대해 지식 전이를 가능하게 하며, 모델의 적응성을 향상시킵니다.
- ***Performance Highlights***: UniPose는 다양한 포즈 관련 작업에서 경쟁력 있는 성능을 보이며, 특히 포즈 이해, 생성, 편집에서 우수한 결과를 나타냅니다. 이는 포즈 세부사항을 보다 정밀하게 인식할 수 있도록 하는 혼합 시각 인코더와 혼합 주의 메커니즘의 효과에 기인합니다.

### [DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2411.15139)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15139.png)

Vote: 8

Authors: Bo Jiang, Xinggang Wang, Ying Zhang, Haoran Yin, Shaoyu Chen, Xinbang Zhang, Qian Zhang, Bencheng Liao, Xiangyu Li, Cheng Wang, Sixu Yan

- ***What's New***: DiffusionDrive는 새로운 End-to-End 자율주행 모델로, 다중 모드 앵커 기반의 Truncated Diffusion Policy를 도입하여 기존 모델의 모드 붕괴 문제와 높은 계산 비용을 해결했습니다. 또한 효율적인 Transformer 기반의 Diffusion Decoder를 설계하여 조건부 장면 맥락과 상호작용을 향상시켰습니다.
- ***Technical Details***: DiffusionDrive는 Anchored Gaussian Distribution에서 시작하는 Truncated Diffusion Schedule을 통해 denoising 단계 수를 대폭 줄였습니다. 이는 기존 20단계에서 단 2단계로 감소하여 실시간(Real-time) 자율주행에 적합하도록 하였습니다. 또한 ResNet-34 백본과 결합하여 설계된 Cascade Diffusion Decoder는 BEV 및 PV 피처와 Sparse Deformable Attention 기법을 사용하여 향상된 상호작용을 가집니다.
- ***Performance Highlights***: DiffusionDrive는 NAVSIM 데이터셋에서 88.1 PDMS를 달성하여 기존 최고 성능을 갱신하였으며, NVIDIA 4090 GPU에서 45 FPS의 실시간 속도로 실행됩니다. 또한 nuScenes 데이터셋에서는 VAD보다 20.8% 낮은 L2 에러와 63.6% 낮은 충돌률을 기록하며 1.8배 더 빠르게 동작합니다.

### [Optimizing Brain Tumor Segmentation with MedNeXt: BraTS 2024 SSA and Pediatrics](https://arxiv.org/abs/2411.15872)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15872.png)

Vote: 5

Authors: Jaskaran Walia, Mohammad Yaqub, Sarim Hashmi, Fadillah Adamsyah Maani, Alikhan Nurkamal, Juan Lugo, Abdelrahman Elsayed, Mohammed Elseiagy, Dinesh Saggurthi

- ***What's New***: 이 논문은 MedNeXt를 이용하여 BraTS 2024 SSA와 Pediatrics 과제에서 뇌종양 세분화를 최적화하는 방법론을 소개합니다. 이 모델은 다양한 분포 변화에 민감한 기존 모델의 제한을 극복하고, 특히 저소득국가 및 소아 환자들의 MRI 데이터를 효과적으로 세분화할 수 있도록 설계되었습니다.
- ***Technical Details***: MedNeXt는 ConvNeXt 블록을 사용하는 U-Net의 확장형으로 설계되었으며, 네 가지 MRI 모달리티 입력을 처리하여 TC, WT, ET의 출력을 생성합니다. 이 연구에서는 새로운 Schedule-free AdamW Optimizer를 활용하여 MedNeXt-B와 MedNeXt-M 모델을 훈련했으며, fine-tuning과 후처리 기술을 통해 성능을 최적화했습니다. 모델 훈련은 NVIDIA 24GB GPU 환경에서 수행되었습니다.
- ***Performance Highlights***: 제안된 방법은 BraTS-2024 SSA 데이터셋에서 평균 Dice Similarity Coefficient(DSC) 0.896을, BraTS Pediatric Tumor 데이터셋에서 평균 DSC 0.830을 달성했습니다. 또한, 평균 Hausdorff Distance (HD95)는 SSA 데이터셋에서 14.682, Pediatric 데이터셋에서 37.508을 기록하며 뛰어난 세분화 능력을 입증했습니다.

### [Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing](https://arxiv.org/abs/2411.16832)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.16832.png)

Vote: 2

Authors: Zhengzhong Tu, Yue Zhao, Ruizheng Bai, Yihua Zhang, Hanhui Wang, Sijia Liu

- ***What's New***: FACELOCK은 악의적인 이미지 편집으로부터 개인 초상화를 보호하기 위한 혁신적인 방법을 제안합니다. 이 연구는 편집 후 생체 정보를 인식할 수 없게 만들기 위한 적대적 섭동(adversarial perturbations)을 최적화하여, 편집된 이미지와 원본 이미지의 생체적 연결을 차단하는 것을 목표로 합니다.
- ***Technical Details***: FACELOCK은 최첨단 안면 인식 모델을 활용하여 새로운 섭동 방식을 설계하였으며, 디퓨전 프로세스에 포함시켜 편집 중 얼굴 인식을 방해하도록 최적화했습니다. CVLFACE 모델과의 상호작용을 통해 얼굴 특징의 시각적 불일치를 극대화하는 방식을 채택하였습니다.
- ***Performance Highlights***: 다양한 악의적 편집에 대해 FACELOCK은 기존의 모든 기준선 방법을 능가하는 방어 성능을 보여줍니다. 특히, 인식 불가능한 얼굴 특징으로 편집을 변형하여 생체적 인정보호에 탁월한 효과를 입증했습니다. 실험 결과, FACELOCK은 얼굴 인식 유사도 점수(FR) 측면에서도 탁월한 성과를 기록하였습니다.

### [ROICtrl: Boosting Instance Control for Visual Generation](https://arxiv.org/abs/2411.17949)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17949.png)

Vote: 65

Authors: Pingchuan Ma, Yixin Nie, Yunfan Ye, Yuchao Gu, Mike Zheng Shou, Licheng Yu, Kevin Qinghong Lin, Yipin Zhou

- ***What's New***: ROICtrl은 시각적 생성을 위한 인스턴스 제어를 향상시키기 위해 새로운 ROI-Unpool 연산을 도입한 어댑터입니다. 이는 고해상도 특징 맵에서 효율적이고 정확한 ROI(Region of Interest) 조작을 가능하게 하며, 여러 인스턴스 생성에 대한 제어를 대폭 향상시킵니다.
- ***Technical Details***: ROICtrl은 사전 훈련된 확산 모델(Diffusion Models)에 인스턴스 제어 기능을 통합하는 어댑터로, 특히 ROI-Align과 신규 도입된 ROI-Unpool 연산을 결합하여 고해상도 특징 맵에서 효율적이고 정확한 ROI 처리를 가능하게 합니다. 또한, ROICtrl은 공간 기반 추가 기능(예: ControlNet, T2I-Adapter) 및 임베딩 기반 추가 기능(IP-Adapter, ED-LoRA)과의 호환성을 높여 인스턴스 생성에 대한 응용 프로그램을 확장합니다.
- ***Performance Highlights***: 실험 결과, ROICtrl은 인스턴스 제어 능력 평가에서 최고 수준의 성능을 보여주었으며, 계산 비용을 크게 줄였습니다. ROICtrl은 기존의 벤치마크인 InstDiff-Bench와 MIG-Bench에서도 뛰어난 성능을 기록했으며, 새로운 ROICtrl-Bench에서도 강력한 성능을 발휘했습니다.

### [Diffusion Self-Distillation for Zero-Shot Customized Image Generation](https://arxiv.org/abs/2411.18616)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18616.png)

Vote: 3

Authors: Shengqu Cai, Leonidas Guibas, Gordon Wetzstein, Yunzhi Zhang, Eric Chan, Jiajun Wu

- ***What's New***: 이 논문에서는 Diffusion Self-Distillation이라는 새로운 방법을 제안하여 제로샷 맞춤형 이미지 생성(Zero-Shot Customized Image Generation)을 구현합니다. 이 방법은 사전 훈련된 텍스트-이미지 확산 모델을 활용하여 자신의 데이터셋을 생성하며, 추가적인 추론 단계 학습 없이 즉각적인 맞춤화를 제공합니다.
- ***Technical Details***: Diffusion Self-Distillation은 텍스트-이미지 확산 모델의 맥락 내 생성(In-Context Generation) 능력을 활용하여 이미지 그리드를 생성하고, 비전-언어 모델(Vision-Language Models; VLMs)의 도움을 받아 관련 이미지를 정제하여 대규모 쌍 데이터셋을 만듭니다. 이렇게 만들어진 데이터셋을 사용해 모델을 특정 정체성이나 구조를 보전하는 이미지-이미지 전환 모델로 미세 조정합니다.
- ***Performance Highlights***: Diffusion Self-Distillation은 기존의 제로샷 방법보다 더 나은 성능을 보여주며, 추론 단계 튜닝 기법에 비해 경쟁력 있는 성과를 냅니다. 인간적으로 평향되지 않은 평가 메트릭에서도 좋은 결과를 나타내, 참신성과 창의성을 충실히 반영합니다. 특히 IP-Adapter와 같은 기존 방법들이 갖는 '복붙' 현상에서 자유로워 높은 평가를 받았습니다.

### [Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis](https://arxiv.org/abs/2411.17769)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17769.png)

Vote: 5

Authors: Xiaoming Li, Xinyu Hou, Chen Change Loy, Zongsheng Yue

- ***What's New***: Omegance는 단일 파라미터 ω를 활용하여 디퓨전 기반 합성에서 세부 조정이 가능하게 하는 새로운 방법입니다. 모델 재훈련이나 구조 수정 없이 ω 파라미터를 조정하여 생성된 출력의 세부 정도를 조절할 수 있습니다. 또한, 공간 마스크나 시간별 조정을 통해 특정 지역이나 시간 단계에 맞춘 세부 제어가 가능합니다.
- ***Technical Details***: 이 연구에서는 디퓨전 모델의 역 프로세스 중 디노이징 단계에서 소음을 조정함으로써 세부 정도를 조절합니다. ω 값을 사용하여 소음 예측을 스케일링하며, 이는 분야에 종속되지 않고 다양한 네트워크 구조와 스케줄러에 적용될 수 있습니다. 공간적으로는 사용자 지정 마스크를 통해 영역별 디테일 수준을 조정하며, 시간적으로는 단계별 ω 스케줄을 활용하여 전체 레이아웃과 세부사항의 시공간적 변화에 영향을 미칩니다.
- ***Performance Highlights***: Omegance는 Stable Diffusion, FLUX 등의 다양한 디퓨전 기반 생성 작업에 대해 뛰어난 성능을 보여줍니다. 이 방법은 텍스트-이미지, 이미지-이미지, 텍스트-비디오 생성 작업에서 생성된 결과물의 디테일 제어 효과를 입증했으며, 사용자 주도형 제어와 실용적인 응용 가능성을 확장시키는 데 기여합니다.

### [ChatRex: Taming Multimodal LLM for Joint Perception and Understanding](https://arxiv.org/abs/2411.18363)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18363.png)

Vote: 5

Authors: Yuda Xiong, Yihao Chen, Lei Zhang, Qing Jiang, Gen luo, Yuqin Yang, Tianhe Ren, Zhaoyang Zeng

- ***What's New***: ChatRex는 모델 설계와 데이터 개발 측면에서 인식을 강화하여 다중 모드 대형 언어 모델(multimodal large language models; MLLMs)의 시각적 인식 성능 격차를 줄이고자 합니다. ChatRex는 인식과 이해를 분리한 설계를 도입하여, 제안 네트워크에서 출력된 상자(box)를 LLM에 입력으로 주어 해당 상자의 색인(index)을 바탕으로 탐지 결과를 산출합니다.
- ***Technical Details***: ChatRex는 이미지 인식에서 제안 네트워크(Universal Proposal Network; UPN)를 사용하여 결정적 박스 입력을 LLM에 제공하며, 다중 모드 이해에 관해서는 자동 회귀 예측 체계를 유지합니다. 또한, 다단계의 상세화된 데이터셋(Rexverse-2M)을 구축하여 인식과 이해를 동시에 훈련할 수 있도록 했습니다. 이 데이터 엔진은 이미지-영역-텍스트 주석 세 쌍을 다양한 수준의 세부사항으로 생성합니다.
- ***Performance Highlights***: ChatRex는 COCO와 LVIS 데이터셋에서 각각 48.5와 43.1의 mAP를 기록하며, 다른 MLLM들과 비교했을 때 강력한 인식 능력을 보였습니다. 또, RefCOCO 등의 참조 탐지 벤치마크에서도 경쟁력 있는 성과를 보이며 인식과 이해의 융합이 모델의 성능 향상에 긍정적인 영향을 미침을 입증했습니다.

### [DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching](https://arxiv.org/abs/2411.17786)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17786.png)

Vote: 9

Authors: Umberto Michieli, Mete Ozay, Emanuele Aiello, Diego Valsesia, Enrico Magli

- ***What's New***: DreamCache는 기존의 개인화 이미지 생성 방법들의 한계를 극복한 혁신적인 방법입니다. 이 접근법은 U-Net의 일부 레이어에서 참조 이미지를 한 번만 캐싱함으로써 컴퓨팅 및 메모리 요구량을 크게 줄입니다. DreamCache는 추가적인 파인튜닝이나 외부 이미지 인코더, 또는 병렬 참조 처리 없이 실시간 개인화 이미지 생성을 가능하게 합니다.
- ***Technical Details***: DreamCache는 캡션 없는 다중 해상도 참조 이미지 표현을 생성하는 기능 캐싱 접근법을 제안합니다. 사전 학습된주의 기반 조건부 메커니즘을 사용하여 개인화된 이미지 생성을 위한 효율적인 샘플링을 구현하며, 캐시된 특징을 생성 중인 이미지에 주입하여 텍스트 제어 없이 개인화를 수행합니다.
- ***Performance Highlights***: DreamCache는 25M의 추가 파라미터로만 극소수 데이터 비용 및 컴퓨팅 비용으로 개인화 이미지 생성에서 최고의 품질을 달성합니다. 특히, 고해상도 이미지 생성에서 효율성이 뛰어나며 모바일 플랫폼에 쉽게 배포할 수 있습니다.

### [CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models](https://arxiv.org/abs/2411.18613)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18613.png)

Vote: 24

Authors: Aleksander Holynski, Changxi Zheng, Jonathan T. Barron, Rundi Wu, Alex Trevithick, Ruiqi Gao, Ben Poole

- ***What's New***: CAT4D는 단일 카메라 비디오에서 동적 3D(4D) 장면을 생성하는 방법을 제시합니다. 이 방법은 다중 시점 비디오 확산 모델(Multi-View Video Diffusion Model)을 활용하여, 새로운 카메라 위치와 시간에서 새로운 장면을 합성할 수 있게 합니다. 이는 주어진 단안 비디오를 다중 시점 비디오로 변환함으로써, 변형 가능한 3D 가우시안 표현(Deformable 3D Gaussian Representation)의 최적화를 통해 견고한 4D 재구성을 가능하게 합니다.
- ***Technical Details***: CAT4D는 다양한 데이터셋의 조합으로 훈련된 다중 시점 비디오 확산 모델을 사용하여 새로운 시각에서의 장면 합성을 가능하게 합니다. 이 모델은 미리 훈련된 비디오 모델과 다중 시점 이미지 합성 모델을 활용합니다. 모델은 지정된 시각과 시간 위치에서 일관성 있는 프레임 컬렉션을 생성하도록 훈련되었고, 변형 가능한 3D 가우시안 표현을 최적화하여 동적 3D 모델을 재구성합니다.
- ***Performance Highlights***: CAT4D는 새로운 시점 합성과 동적 장면 재구성 벤치마크에서 경쟁력을 보이며, 기존의 상태-of-the-art 모델들과 견줄 수 있는 결과를 보여줍니다. 이는 다양한 감독 신호나 외부 정보를 필요로 하지 않고도 이루어지며, 실제 비디오 및 생성된 비디오로부터 4D 장면 생성을 가능하게 합니다.

### [Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment](https://arxiv.org/abs/2411.17188)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17188.png)

Vote: 18

Authors: Shu Pu, Pan Zhou, Caixi Chen, Ruoxi Chen, Benlin Liu, Yue Huang, Yao Wan, Dongping Chen, Ranjay Krishna, Zhaoyi Liu, Yanru Wu

- ***What's New***: 이 연구는 ISG(Interleaved Scene Graph)라는 새로운 평가 프레임워크를 소개하여 텍스트와 이미지가 교차로 생성되는 컨텐츠의 일관성과 유창성을 평가합니다. ISG는 시각적 요소와 텍스트 요소 간의 연결을 평가하는 씬 그래프(structure graph) 구조를 활용합니다.
- ***Technical Details***: ISG는 전체적, 구조적, 블록 단위, 이미지 단위의 네 가지 세분화된 수준으로 멀티모달 생성 모델을 평가합니다. 이를 위해 ISG-BENCH라는 독창적인 벤치마크가 구축되었으며, 1,150개의 샘플로 구성되어 있으며 8개의 범주와 21개의 하위 범주를 포함하고 있습니다. 이 벤치마크는 시각 중심(vision-centric) 작업을 더 잘 평가할 수 있도록 설계되었습니다.
- ***Performance Highlights***: 현재의 통합(unified) 비전-언어 모델은 일관된 교차 콘텐츠 생성에 있어 여전히 개선의 여지가 많습니다. 통합 모델 대비 병합(compositional) 접근 방식은 더 높은 평균 점수를 기록했으나, 여전히 블록 및 이미지 수준에서의 정확한 생성에서는 성능이 저조했습니다. ISG-AGENT라는 기준 에이전트는 '계획-실행-개선' 파이프라인을 통해 122%의 성능 향상을 달성하여 우수한 평가를 받았습니다.

### [3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes](https://arxiv.org/abs/2411.14974)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14974.png)

Vote: 6

Authors: Silvio Giancola, Anthony Cioppa, Abdullah Hamdi, Marc Van Droogenbroeck, Bernard Ghanem, Adrien Deliege, Jan Held, Andrea Vedaldi, Renaud Vandeghen

- ***What's New***: 3D Convex Splatting (3DCS)는 다중 보기 이미지에서 3D 스무스 컨벡스 매개체를 사용하여 기하학적으로 의미 있는 Radiance Field를 모델링하는 새로운 방법론을 소개합니다. 이 방법은 기존 3D Gaussian Splatting (3DGS)의 한계를 극복하고 하드 엣지를 더욱 정확하게 표현하며, 적은 기본 요소로 고품질 장면을 재구성합니다.
- ***Technical Details***: 3DCS는 스무스 컨벡스(3D Smooth Convexes)를 이용하여 3D 장면의 Radiance Field를 표현합니다. CUDA 기반의 고성능 래스터라이저를 사용하여 실시간 렌더링을 지원하며 최적화를 가속화합니다. 3DGS와 비교하여 Superior한 성능을 보이는 MipNeRF360, Tanks and Temples 및 Deep Blending 벤치마크에서 0.81 PSNR 및 0.026 LPIPS 개선을 달성하였습니다.
- ***Performance Highlights***: 3DCS는 3DGS보다 적은 수의 기본 요소를 사용하여 더 높은 렌더링 품질과 메모리 효율성을 제공합니다. 특히 Tanks&Temples 데이터셋에서 3DGS보다 1.73 PSNR 이상 뛰어난 성능을 보여주며, Mip-NeRF360 대비 훈련 시간을 48시간에서 63분으로 단축시켰습니다. 3DCS는 복잡한 장면에서도 시각적 충실도를 잃지 않고 높은 적응력을 발휘하여 메모리 사용을 3DGS의 70% 수준으로 감소시켰습니다.

### [Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient](https://arxiv.org/abs/2411.17787)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17787.png)

Vote: 9

Authors: Xinyin Ma, Zigeng Chen, Gongfan Fang, Xinchao Wang

- ***What's New***: Collaborative Decoding(CoDe)는 Visual Auto-Regressive(VAR) 모델의 효율성을 높이기 위해 새로운 디코딩 전략을 제공합니다. CoDe는 큰 모델과 작은 모델 사이의 협업을 통해 메모리 사용량을 대폭 줄이고 속도를 크게 향상시킵니다.
- ***Technical Details***: CoDe는 VAR 모델의 다단계 추론 프로세스를 큰 모델이 저주파수 내용을 생성하고 작은 모델이 고주파수 세부사항을 예측하는 협력 과정으로 분할합니다. 이러한 구조를 통해 연산의 효율성을 극대화하며, 두 모델을 해당 예측 스케일에 맞게 세밀하게 조정해 성능을 향상시킵니다.
- ***Performance Highlights***: CoDe는 초기 VAR-d30 모델 대비 1.7배 더 빠른 속도와 절반에 가까운 메모리 사용량을 보이며 이미지 품질의 손실은 거의 없습니다(FID가 1.95에서 1.98로 증가). 또한, NVIDIA 4090 GPU에서 256x256 해상도를 가진 이미지를 초당 41장 생성할 수 있으며, FID 2.27을 유지합니다.

### [MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation](https://arxiv.org/abs/2411.17945)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17945.png)

Vote: 19

Authors: Muhammad Usama, Didier Stricker, Shino Sam, Mohammad Sadil Khan, Sk Aziz Ali, Sankalp Sinha, Muhammad Zeshan Afzal

- ***What's New***: MARVEL-40M+는 7개의 주요 3D 데이터셋에서 수집된 890만 개 이상의 3D 자산에 대해 총 4천만 건의 텍스트 주석을 제공하는 새로운 대규모 데이터셋입니다. 이 데이터셋은 새로운 멀티레벨 주석 생성 파이프라인을 도입하여, 다양한 세부 묘사부터 간략한 의미 태그에 이르는 다단계 설명을 자동 생성합니다. 이를 통해 정교한 3D 재구성과 급속한 프로토타이핑을 지원합니다.
- ***Technical Details***: MARVEL-40M+ 주석 파이프라인은 오픈소스 멀티뷰 VLMs와 대형 언어 모델(LLMs)을 통합하여 고품질의 텍스트 설명을 생성합니다. 이 과정에서는 원본 데이터셋의 사람 메타데이터를 통합하여 도메인별 정보를 추가하고 VLM의 환각을 줄입니다. 또한, 두 단계로 구성된 텍스트-3D 변환 파이프라인인 MARVEL-FX3D를 개발하여, 이 데이터셋을 통해 세밀한 3D 재구성 이미지를 개선하고 사전 훈련된 이미지-3D 변환 네트워크를 사용해 15초 이내에 3D 메시를 생성합니다.
- ***Performance Highlights***: MARVEL-40M+는 기존 데이터셋보다 주석의 품질과 언어적 다양성에서 월등한 성능을 보이며, GPT-4와 인간 평가자에게서 각각 72.41%와 73.40%의 우수한 승률을 기록했습니다. 또한, MARVEL-FX3D는 고품질의 텍스처드 3D 메시를 빠르게 생성하며, 다른 최신 텍스트-3D 생성 방법보다 프롬프트 충실도와 전체 선호도에서 높은 점수를 받았습니다.

### [Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters](https://arxiv.org/abs/2411.18197)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18197.png)

Vote: 6

Authors: Jinxu Xiang, Ran Zhang, Houqiang Li, Kai Ma, Zhiyang Guo, Wengang Zhou

- ***What's New***: Make-It-Animatable는 다양한 포즈와 형태의 3D 휴머노이드 모델을 1초 내에 애니메이션 가능한 상태로 만드는 데이터 기반의 효율적인 새로운 방법입니다. 이 프레임워크는 고품질의 blend weights, bones, pose transformations을 생성하며, 입자 기반의 shape autoencoder를 포함하여 포즈와 상관없이 다양한 3D 표현을 지원합니다.
- ***Technical Details***: Make-It-Animatable 프레임워크는 많은 입자(3D Gaussian Splats)를 통해 모델링된 3D 캐릭터에 대해 고급 코스-투-파인(coarse-to-fine) 표현과 구조 인식 모델링을 채택합니다. 이 방법은 체골 구조와 포즈 변환을 정확하게 관리하면서 다양한 형태와 비표준 해골 구조의 캐릭터도 다룰 수 있습니다. 주요 요소로는 입자 기반의 shape autoencoder와 구조 인식 transformer가 있어 정밀한 예측이 가능합니다.
- ***Performance Highlights***: 기존 방법들과 비교하여, Make-It-Animatable은 품질과 속도에서 상당한 향상을 보여주며, 모든 3D 캐릭터의 릭(rigging) 및 스키닝(skinning)을 매우 빠르게 처리합니다. 평균 1초 이하의 처리 시간으로 다양한 캐릭터 애니메이션을 가능하게 하여 가상 현실, 게임, 실시간 시뮬레이션 같은 응용 분야에서 높은 커스터마이즈와 빠른 반응을 제공합니다.

### [Large Language Model-Brained GUI Agents: A Survey](https://arxiv.org/abs/2411.18279)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18279.png)

Vote: 13

Authors: Yu Kang, Qingwei Lin, Qi Zhang, Chaoyun Zhang, Shilin He, Si Qin, Minghua Ma, Jiaxu Qian, Dongmei Zhang, Bowen Li, Liqun Li, Saravan Rajmohan

- ***What's New***: 이 논문은 대형 언어 모델(LLM)이 GUI(GUI Agents) 환경에서 웹, 모바일, 데스크톱 응용 프로그램과 상호작용할 수 있도록 설계된 프레임워크 및 기술을 체계적으로 조사하고 있습니다. GUI 에이전트를 통해 사용자는 자연어 명령을 통해 복잡한 작업을 수행할 수 있습니다.
- ***Technical Details***: 이 논문에서는 GUI 에이전트의 발전을 위해 여러 핵심 구성 요소와 첨단 기술을 소개합니다. 여기에는 환경 인식 기술, 프롬프트 엔지니어링, 모델 추론, 행동 실행 및 메모리 사용이 포함됩니다. 또한, 다중 에이전트 프레임워크(multi-agent framework)의 이점을 강조하며, GUI 요소 해석 및 상호작용을 위한 컴퓨터 비전 기반의 GUI 파싱(computer vision-based GUI parsing)을 제안합니다.
- ***Performance Highlights***: 대형 멀티모달 모델(Large Multimodal Models; LMMs) 및 LAMs(Large Action Models)의 발전을 통해, GUI 에이전트는 웹, 모바일 및 데스크톱 환경에서 더 나은 적응성과 수행력을 보입니다. 특히, GPT-4V와 같은 멀티모달 입력을 활용하는 에이전트는 복잡한 작업에서 높은 정확성을 보여주며, 웹 및 모바일 플랫폼에서 상호작용의 성공률을 높입니다.

### [VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format](https://arxiv.org/abs/2411.17991)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.17991.png)

Vote: 3

Authors: Dongyan Zhao, Jiansheng Wei, Jianxin Liang, Huishuai Zhang, Yuxuan Wang, Yueqian Wang, Xiaojun Meng

- ***What's New***: 이 논문에서는 비디오 LLM (Large Language Models)의 사용자-모델 상호작용 포맷의 한계를 극복하기 위해 비디오-텍스트 듀엣 상호작용 포맷을 소개합니다. 이 포맷은 비디오가 지속적으로 재생되는 동안, 사용자와 모델이 텍스트 메시지를 임의의 지점에 삽입할 수 있습니다. 새로운 MAGQA (Multi-Answer Grounded Video Question Answering) 태스크도 제안되었습니다.
- ***Technical Details***: 비디오-텍스트 듀엣 상호작용 포맷은 사용자가 취할 수 있는 전통적인 '저전달' 방식 대신, 비디오가 프레임 단위로 입력되며, 각 프레임 뒤에 사용자 및 모델의 텍스트 메시지를 삽입할 수 있는 형태로 구성됩니다. 이를 위해 MMDuetIT라는 데이터셋이 구성되었으며, 이는 밀집 비디오 캡셔닝과 임시 비디오 그라운딩 데이터를 포함합니다. MMDuet은 이 포맷을 구현한 비디오 LLM으로, LLaVA-OneVision을 초기화하고 MMDuetIT로 훈련되었습니다.
- ***Performance Highlights***: MMDuet은 다양한 시간 민감 태스크에서 상당한 성능 향상을 보여주었습니다. 특히 YouCook2 밀집 비디오 캡셔닝에서 76%의 CIDEr 점수를, QVHighlights 하이라이트 검출에서 90%의 mAP를, Charades-STA 임시 비디오 그라운딩에서 25%의 R@0.5를 기록했습니다. 기존의 모델들과 비교했을 때, 실시간으로 영상이 플레이 되면서 응답을 생성할 수 있는 능력이 MMDuet의 큰 장점입니다.

