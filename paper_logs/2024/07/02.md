## Daily Papers (2024-07-02)

### [We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?](https://arxiv.org/abs/2407.01284)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.01284.png)

Vote: 56

Authors: Yifan Zhang, Muxi Diao, Runfeng Qiao, Xiao Zong, Yida Xu, Zhimin Bao, Chen Li, Shanglin Lei, Chong Sun, Runqi Qiao, Honggang Zhang, Minhui Wu, Qiuna Tan, Xiaoshuai Song, Guanting Dong, Miaoxuan Zhang, Zhuoma GongQue, Zhe Wei

- **What's New**: 최근의 연구는 인간 인지 및 추론 패턴이 심층 학습의 진보에 큰 영향을 미쳤음을 강조합니다. 트랜스포머(Transformers) 및 대형 언어 모델(LLMs)과 대형 다중 모달 모델(LMMs)이 사람 수준의 추론 능력을 보여주며, 알고리즘이 수학추론 과제에 있어 인간과 유사한 성과를 보이고 있습니다. 이를 체계적으로 평가하기 위해 We-Math라는 새로운 벤치마크가 도입되었습니다.
- **Technical Details**: We-Math는 6.5K 이상의 시각적 수학 문제로 구성되어 있으며, 67개의 지식 개념으로 세분화된 5단계의 지식 세밀도를 포함합니다. 이에 따라 복합 문제를 개별 지식 개념에 따라 두 단계로 분리하여 모델의 추론 능력을 평가합니다. We-Math는 지식 개념을 기반으로 한 네 차원 평가를 도입하여 LMMs의 문제 해결 능력의 내재적 격차를 정확히 평가합니다. 또한, 67개 지식 개념에 대한 Wikipedia와 교과서를 이용한 지식 개념 증강(KCA) 전략을 제안합니다.
- **Performance Highlights**: {'GPT-4o': 'GPT-4o는 여러 시각적 수학 카테고리에서 가장 높은 성능을 보였습니다.', 'LMMs with Larger Parameters': 'LLaVA-NeXT-110B 등이 포함된 큰 파라미터 규모의 LMMs는 일반적으로 뛰어난 시각적 수학 추론 능력을 보여줍니다.', 'Multi-step Problems': '대부분의 LMMs가 다단계 문제에서 성능이 저하되며 이는 지식 개념의 수가 문제의 난이도와 정비례하고 성능과 반비례함을 나타냅니다.', 'Specialized Disciplines': "계산 분야에서는 뛰어난 성능을 보이나 '각도와 길이'와 같은 세부 시각 측정에서는 일관되게 어려움을 겪습니다.", 'Knowledge Gaps': '기본적인 지식 부족 문제(IK)는 대부분의 LMMs에서 특히 더 작은 규모의 모델(예: LLaVA-1.6-7B, DeepSeek-VL-1.3B)에서 두드러지게 나타났습니다. GPT-4o는 이러한 지식 격차를 크게 해소하며 지식 일반화 단계로 나아가고 있습니다.'}

### [ROS-LLM: A ROS framework for embodied AI with task feedback and structured reasoning](https://arxiv.org/abs/2406.19741)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19741.png)

Vote: 43

Authors: Xinyu Zhang, Jianye Hao, Jinlong Wang, Davide Tateo, Guangjian Tian, Jun Wang, Haitham Bou-Ammar, Anbang Zhai, Hongzhan Yu, Christopher E. Mower, Marco Hutter, Yao Zhao, Yuzheng Zhuang, Antoine Grosnit, Jan Peters, Matthieu Zimmer, Puze Liu, Cesar Cadena, Jonas Gonzalez-Billandon, Xingyue Quan, Kun Shao, Yuhui Wan

- **What's New**: 이번 연구는 비전문가도 자연어 프롬프트와 로봇 환경에서의 문맥 정보를 사용해 로봇을 프로그래밍할 수 있는 프레임워크를 소개합니다. 이러한 접근 방식은 팀 전문가들의 의존성을 줄이고 빠른 프로토타이핑과 로봇 시스템 배치를 가능하게 합니다.
- **Technical Details**: 이 프레임워크는 종속성의 주요 병목 현상인 전문가 팀 의존성을 해결하고자 합니다. 로봇 사용자를 위해 경로 및 행동 시퀀스를 정의하는 데 있어 자연어 처리의 최신 발전을 활용하여, 사용자가 자연어로 쉽게 로봇의 동작을 정의할 수 있도록 지원합니다. 기술적 접근은 ROS (Robot Operating System)과 통합되며, 강화 학습 (RL), 모방 학습, 최적 제어를 사용한 낮은 수준의 정책 표현을 포함합니다. 자연어 모델은 행동 시퀀스를 유도하고 이를 통해 동적인 환경과 인간 피드백에 적응할 수 있게 설계되었습니다.
- **Performance Highlights**: 프레임워크는 키친 환경에서의 로봇 설정과 다양한 실험을 통해 테스트되었습니다. 실험에서는 자연어 모델의 성능을 향상시키기 위한 프롬프트 전략에서 중요한 교훈이 도출되었습니다. 또한, 비전문가 사용자가 텔레오퍼레이션이나 운동 학습을 통해 여러 데모를 제공함으로써 시스템의 원자 행동 라이브러리를 업데이트 할 수 있는 인터페이스를 개발했습니다. 장거리 감독 통제 사용자 연구에서도 유럽의 운영자가 아시아의 로봇을 테이블 정렬 작업을 수행하도록 조작하는 데 성공하였습니다.

### [MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation](https://arxiv.org/abs/2407.00468)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.00468.png)

Vote: 30

Authors: Yichi Zhang, Haozhe Zhao, Bohan Wu, Wei Ju, Fu Zeng, Tianyu Liu, Jinsheng Huang, Baobao Chang, Ming Zhang, Zhihui Guo, Luchen Liu, Jingyang Yuan, Liang Chen, Yusheng Zhao, Taian Guo, Ye Yuan

- **What's New**: 최근의 연구에서 GPT4-o, Gemini-1.5, Qwen-VL 및 LLaVA와 같은 대규모 멀티모달 모델(LMM)의 평가 신뢰성에 대한 문제를 다뤘습니다. 다중 선택 질문(MCQ) 포맷은 여전히 널리 사용되지만, 이 방식의 평가가 모델의 실제 능력을 정확히 반영하지 않을 수 있음을 여러 연구들이 지적하고 있습니다. 특히, 이 논문에서는 모델이 시각적 데이터를 처리하지 않고도 높은 점수를 얻을 수 있다는 점을 발견했습니다.
- **Technical Details**: 논문에서는 주로 MMMU, ScienceQA 및 MathVista라는 세 가지 유명한 멀티모달 벤치마크를 분석했습니다. Seeing-or-Not Comparison 실험을 통해 시각적 데이터를 보거나 보지 않고도 LLM과 LMM의 점수 차이가 크지 않음을 확인했습니다. 또, Answer Consistency Test를 통해 모델이 실제 이해 없이도 올바른 답변을 도출할 수 있다는 Type-I Error를 발견했습니다. 이를 개선하기 위해 MMEvalPro를 제안했으며, 이는 원래의 MCQ에 전제 인지와 지식 질문을 추가하여 모델의 진정한 멀티모달 능력을 평가합니다.
- **Performance Highlights**: MMEvalPro는 기존 벤치마크보다 더 신뢰성 있는 평가를 제공합니다. 예를 들어, MMEvalPro에서는 최고의 LLM이 최고의 LMM보다 23.09% 정도 뒤처지지만, 기존 벤치마크에서는 그 차이가 14.64%에 불과했습니다. 또한, 가장 발전된 모델조차도 인간과 비교할 때 30% 이상의 성능 차이를 보였습니다. 이는 모델이 진정한 멀티모달 이해 능력을 갖추기 위해서는 아직 갈 길이 멀다는 것을 시사합니다.

### [LiteSearch: Efficacious Tree Search for LLM](https://arxiv.org/abs/2407.00320)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.00320.png)

Vote: 29

Authors: Dong Yu, Ye Tian, Haitao Mi, Ante Wang, Linfeng Song, Dian Yu, Jinsong Su, Baolin Peng

- **What's New**: 이 연구는 복잡한 수학적 추론 작업을 처리할 수 있는 효율적이고 적응 가능한 방법을 개발하는 데 집중하고 있습니다. 특히, 우리는 노드 선택 및 노드 수준 탐색 예산 계산을 통한 Guided Tree Search 알고리즘을 도입했습니다. 목표는 중간 수준의 비용으로 성능을 유지하는 것입니다.
- **Technical Details**: 구체적으로, 우리는 가치 점수를 가이드로 사용하여 다음 작업을 위한 가장 유망한 노드를 선택하고, 동적으로 계산된 예산 크기 내에서 이를 확장합니다. 탐색-활용(exploration-exploitation) 균형을 유지하기 위해 선택 및 확장 작업을 계속 반복합니다. 노드의 컴퓨팅 예산은 가치 점수와 반비례합니다. 가치 점수가 높은 노드는 확장 시 정답을 산출할 가능성이 더 높기 때문에, 불필요한 계산을 피하기 위해 적은 자원을 할당합니다. 이는 빠른 수렴을 촉진하고 충분한 상태 공간 탐색을 통해 성능을 유지합니다.
- **Performance Highlights**: 우리는 GSM8K 및 TabMWP에서 실험을 수행한 결과, 다른 베이스라인과 비교하여 약 5배 적은 계산비용으로 경쟁력 있는 성능을 제공하는 것을 확인했습니다. 각 구성요소의 유용성을 확인하고 다양한 설정에 맞는 더 실질적인 옵션을 제공했습니다. 이 연구 경로의 한계도 식별하고 이를 해결할 수 있는 방법을 제안했습니다.

### [RegMix: Data Mixture as Regression for Language Model Pre-training](https://arxiv.org/abs/2407.01492)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.01492.png)

Vote: 21

Authors: Min Lin, Tianyu Pang, Niklas Muennighoff, Longxu Dou, Xiaosen Zheng, Qian Liu, Jing Jiang, Guangtao Zeng

- **What's New**: 본 연구에서는 효율적이고 확장 가능한 데이터 혼합 최적화 방법인 RegMix를 소개합니다. 이는 소규모 모델을 통해 데이터 혼합을 예측하고 이를 대규모 모델의 훈련에 적용하는 방식을 제안합니다.
- **Technical Details**: RegMix는 네 단계로 구성되어 있습니다: 1) 무작위 데이터 혼합을 생성하고 소규모 프록시 모델(proxy models)을 훈련, 2) 혼합 데이터를 특징으로, 타겟 값을 라벨로 사용하는 회귀 모델(linear regression model)을 적합, 3) 데이터 혼합 공간을 시뮬레이션하고 회귀 모델을 이용해 최적의 데이터를 식별, 4) 식별된 데이터 혼합을 이용해 대규모 모델 훈련. 이를 통해 소규모 모델을 병렬로 훈련시켜 효율성을 크게 개선했습니다.
- **Performance Highlights**: RegMix를 통해 얻은 최적 데이터 혼합은 인간이 선택한 혼합보다 우수한 성능을 보이며, 더 적은 계산량으로 기존의 DoReMi 방법과 동등한 성능을 달성합니다. 특히, 데이터 혼합이 다운스트림 성능에 큰 영향을 미치는 것을 발견했고, 일반 웹 코퍼스(CommonCrawl)가 위키피디아(Wikipedia)보다 더 큰 성능 향상을 가져오는 것을 확인했습니다.

### [Step-Controlled DPO: Leveraging Stepwise Error for Enhanced Mathematical Reasoning](https://arxiv.org/abs/2407.00782)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.00782.png)

Vote: 18

Authors: Weikang Shi, Mingjie Zhan, Zimu Lu, Junting Pan, Houxing Ren, Aojun Zhou, Ke Wang

- **What's New**: 이 논문은 대형 언어 모델(LLMs)에 대한 수학적 문제 해결 능력을 향상시키기 위해 Step-Controlled Direct Preference Optimization (SCDPO) 알고리즘을 제안합니다. SCDPO는 인간의 추가적인 주석 없이도 단계별로 우선응답 및 비우선응답을 생성할 수 있도록 설계되었습니다. 이를 통해 여러 수학 문제에 대해 정확한 해결책을 제공하고 모델의 논리적 추론 능력을 강화할 수 있습니다.
- **Technical Details**: 논문에서 제안한 SCDPO 알고리즘은 두 가지 주요 단계로 구성됩니다: 단계별 데이터 생성과 단계 인식 DPO 훈련. 첫 번째 단계에서는 초기 모델을 사용하여 수학 문제에 대한 올바른 해결책을 생성하고, 이후 특정 단계에서 오류를 포함하는 비우선응답을 자동으로 생성합니다. 두 번째 단계에서는 이러한 데이터를 사용하여 DPO 훈련을 통해 모델의 수학적 추론 능력을 최적화합니다. 또한, 이 알고리즘은 과거의 RL 기법과 달리, 인간의 정교한 주석을 필요로 하지 않습니다.
- **Performance Highlights**: 연구 결과에 따르면, SCDPO를 사용한 InternLM2-20B 모델은 GSM8K 벤치마크에서 88.5%, MATH 벤치마크에서 58.1%의 정확도를 기록하여, 기존의 다른 오픈 소스 모델들과 비슷한 성능을 보였습니다. 이는 SCDPO가 대형 언어 모델의 수학적 문제 해결 능력을 크게 향상시킬 수 있음을 시사합니다.

### [ColPali: Efficient Document Retrieval with Vision Language Models](https://arxiv.org/abs/2407.01449)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.01449.png)

Vote: 16

Authors: Tony Wu, Céline Hudelot, Hugues Sibille, Gautier Viaud, Pierre Colombo, Manuel Faysse

- **What's New**: Document Retrieval 시스템에서 비주얼 요소를 포함한 평가의 중요성을 강조하면서, ViDoRe라는 새로운 벤치마크를 공개했습니다. ViDoRe는 페이지 단위의 문서 검색을 평가하며 다양한 도메인, 비주얼 요소, 언어를 포함합니다.
- **Technical Details**: 문서 검색은 주로 PDF 파싱, OCR, 문서 레이아웃 감지 및 청킹(chuncking) 전략 등을 포함한 데이터 인제션 파이프라인에서 성능 병목이 발생합니다. 제안된 시스템인 ColPali는 Vision Language Models (VLMs)을 기반으로 문서의 비주얼 피처를 사용해 빠른 질의 매칭을 수행합니다.
- **Performance Highlights**: ViDoRe 벤치마크 상에서 ColPali는 모든 기존 검색 시스템을 능가하며, 빠르고 엔드 투 엔드(end-to-end) 훈련이 가능합니다.

### [DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image Restoration Models](https://arxiv.org/abs/2407.01519)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.01519.png)

Vote: 16

Authors: Zhixiang Wang, Yu-Lun Liu, Chi-Wei Hsiao, Chin-Yang Lin, Ting-Hsuan Chen, Chang-Han Yeh

- **What's New**: 이 논문에서는 영상 복원을 위한 최첨단 기법을 소개합니다. 특히, 대규모 데이터를 학습한 Convolutional Neural Networks(CNNs)나 Transformers를 사용한 기존 방법들이 매우 효과적이지만, 이러한 기법들은 종종 현실적인 디테일이 부족한 흐릿한 출력물을 만들어내는 한계가 있습니다. 이에 반해, 본 연구에서는 학습이 필요 없는 두 가지 모듈— 계층적 잠재 워핑(hierarchical latent warping)과 하이브리드 흐름 유도 공간 인식 토큰 병합(hybrid flow-guided spatial-aware token merging) —을 설계하여 시간적 일관성을 유지하는 새로운 제로샷(video 접근법)을 제안합니다.
- **Technical Details**: 기존의 영상 복원 기법들은 주로 선형 회귀에 기반한 방법이나, 특별히 정의된 저하(degradations)를 처리하기 위해 특정 모델을 학습시키는 방식입니다. 반면, 본 연구에서는 미리 학습된 이미지 확산 모델(pre-trained image diffusion model)을 이용하며 추가적인 학습이 필요 없습니다. 우리가 제안한 계층적 잠재 워핑과 하이브리드 흐름 유도 공간 인식 토큰 병합 모듈은 각각 잠재 및 토큰 공간에서 시간적 일관성을 강화합니다. 이는 잠재 공간에서 일관성을 유지하기 위해 계층적인 접근 방식을 취하고, 흐름 대응과 공간 정보를 이용해 토큰 병합을 개선합니다.
- **Performance Highlights**: 제안된 방법은 극단적인 저하 조건에서도 영상 품질과 시간적 일관성 면에서 state-of-the-art 기법들을 능가하는 복원 성능을 보여주었습니다. 특히, 전통적인 회귀 기법에 비해 다양한 저하 수준을 단일 모델로 다룰 수 있어 일반화 및 견고성에서 더 뛰어난 성능을 발휘합니다.

### [Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP](https://arxiv.org/abs/2407.00402)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.00402.png)

Vote: 13

Authors: Reut Tsarfaty, Alon Jacovi, Aviv Slobodkin, Ido Dagan, Omer Goldman, Aviya Maimon

- **What's New**: 대규모 언어 모델(LLMs)의 긴 문맥 처리 능력을 평가하는 새로운 연구 패러다임을 제안합니다. 현재의 연구는 주로 문맥의 길이만을 중심으로 LLMs의 성능을 평가하지만, 본 논문은 문맥 길이 외에도 다양한 요소들을 고려한 세부적인 분류 체계가 필요하다고 주장합니다.
- **Technical Details**: 문맥 길이를 기반으로 한 기존의 평가 방식에서 벗어나, 더 복잡하고 세분화된 분류 체계를 제안합니다. 본 논문은 두 가지 주요 축을 중심으로 긴 문맥 작업을 분류합니다: (I) 필요한 정보를 찾고 추출하는 난이도(정보 확산 정도), 그리고 (II) 과제를 해결하기 위해 필요한 정보의 절대량(정보 범위). 이러한 분류 체계를 통해, 서로 다른 작업이 어떻게 다르게 긴 문맥을 필요로 하고, 이에 따라 LLMs의 성능을 어떻게 평가할 수 있는지에 대해 조명합니다.
- **Performance Highlights**: 긴 문맥 작업을 평가하는 새로운 분류 체계를 통해, 정보가 널리 퍼져 있고 추출하기 어려운 상황에서 LLMs의 성능을 비교적 낮게 평가할 수 있다는 점을 발견했습니다. 이는 문맥 길이만을 기준으로 모델을 평가하는 기존 방식의 한계를 극복하는 데 중요한 통찰을 제공합니다.

### [MIRAI: Evaluating LLM Agents for Event Forecasting](https://arxiv.org/abs/2407.01231)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.01231.png)

Vote: 13

Authors: Wei Wang, Yanqiao Zhu, Chenchen Ye, Zijie Huang, Mingyu Derek Ma, Ziniu Hu, Yihe Deng

- **What's New**: 국제 사건 예측을 위해 설계된 새로운 벤치마크 환경인 Mirai를 소개합니다. 이 벤치마크는 GDELT 데이터베이스에서 파생된 실제 국제 사건 데이터를 사용하여 다양한 시점에서 LLM 에이전트의 성능을 평가합니다.
- **Technical Details**: Mirai는 이벤트를 타임스탬프(t), 주체/객체 국가(s, o), 그리고 관계 유형(r)의 형태로 표현합니다. 관계 유형은 CAMEO 온톨로지에 따라 단계별로 분류되며, 국가 간의 관계를 예측합니다. LLM 에이전트는 API를 통해 관계형 및 텍스트 데이터베이스와 상호 작용하여 정보를 자율적으로 수집하고 처리합니다.
- **Performance Highlights**: ['시간 예측 작업은 LLM 에이전트에게 도전적이며, GPT-4o 에이전트가 두 번째 수준의 관계 예측 작업에서 29.6의 F1 점수를 기록했습니다.', '장기 및 세밀한 사건 예측 작업은 더욱 어려운 과제로 나타났습니다.', "GPT-4o만이 'Code Block' 도구 사용 전략을 효과적으로 활용하고 이점이 있음을 보였습니다."]

### [E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS](https://arxiv.org/abs/2406.18009)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.18009.png)

Vote: 12

Authors: Zhen Xiao, Chung-Hsien Tsai, Sheng Zhao, Manthan Thakker, Min Tang, Zirun Zhu, Yanqing Liu, Naoyuki Kanda, Canrun Li, Hemin Yang, Xiaofei Wang, Sefik Emre Eskimez, Xu Tan

- **What's New**: 최근 몇 년간 텍스트-음성 변환(TTS) 시스템은 인간의 음성과 구별할 수 없을 정도의 자연스러움을 달성하며 크게 발전했습니다. 이에 따라 짧은 음성 샘플, 즉 오디오 프롬프트로부터 자연스러운 음성을 생성하려는 연구가 이루어졌습니다. 이번 연구에서는 'Embarrassingly Easy TTS (E2 TTS)'라는 매우 단순한 아키텍처를 갖춘 완전 비자기회귀(non-autoregressive, NAR) 제로샷 TTS 시스템을 제안합니다.
- **Technical Details**: {'Architecture': 'E2 TTS는 두 개의 모듈로 이루어져 있습니다: 바로 멜 스펙트로그램 생성기와 보코더(vocoder)입니다. 텍스트 입력을 문자 시퀀스로 변환하고 멜 필터뱅크 시퀀스의 길이에 맞춰 필러 토큰을 추가합니다. 이후 멜 스펙트로그램 생성기는 U-Net 스타일의 스킵 연결을 사용한 기본 Transformer로 구성되며, 이는 말 채우기(speech infilling) 작업을 통해 훈련됩니다.', 'Training Process': '모델은 조건부 흐름 매칭(conditional flow-matching)을 사용하여 텍스트 입력과 음성 출력 사이의 분포를 학습합니다. 훈련 과정에서 이진 시간 마스크를 사용하여 제로샷 TTS를 위한 음성 생성기를 학습시킵니다.'}
- **Performance Highlights**: E2 TTS는 Voicebox와 NaturalSpeech 3을 포함한 기존의 최신 연구들과 동등하거나 이를 능가하는 성능을 가지고 있습니다. 또한, 매우 단순한 아키텍처 덕분에 유연한 입력 표현이 가능합니다.

### [InstantStyle-Plus: Style Transfer with Content-Preserving in Text-to-Image Generation](https://arxiv.org/abs/2407.00788)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.00788.png)

Vote: 11

Authors: Hao Ai, Qixun Wang, Haofan Wang, Renyuan Huang, Peng Xing, Xu Bai

- **What's New**: 최근 미세 조정이 필요 없는 방법들(tuning-free methods)이 이미지 생성에서 큰 주목을 받고 있으며, 새로운 스타일에 빠르게 적응할 수 있는 능력을 보여주고 있습니다. InstantStyle-Plus는, 기존의 InstantStyle[20]을 개선한 방식으로, 스타일 전이(stylization) 과정에서 컨텐츠를 유지하는 능력을 대폭 향상시켰습니다.
- **Technical Details**: InstantStyle-Plus는 스타일 주입(style injection), 공간 구조 보존(spatial structure preservation), 의미론적 컨텐츠 보존(semantic content preservation)이라는 세 가지 세부 과제로 작업을 분해하여 스타일과 컨텐츠 간의 균형을 맞추려 노력합니다. 이를 위해 컨텐츠의 초벌 잠재(latent)와 Tile ControlNet[25]을 활용하여 공간 구성을 유지하고, 전역 이미지 어댑터(global image adapter)[24]를 통해 의미론적 보존을 강화합니다. 추가적으로, 스타일 디스크리미네이터(style discriminator)인 CSD[18] 모델을 사용하여 스타일 가이던스(style guidance)를 제공합니다.
- **Performance Highlights**: 기존 스타일 전이 모델들은 스타일 일관성에 중점을 두었지만, InstantStyle-Plus는 컨텐츠 보존 측면에서 혁신적인 접근 방식을 제시합니다. 특히, Tile ControlNet[25]이 공간 구성 유지에 매우 효과적이며, 내용 노이즈 역전(inverting content noise)이 섬세한 내용의 세부 요소들을 보존하는 데 도움이 된다는 점을 발견했습니다. 또한, 추가적인 스타일 가이던스를 통한 균형 유지가 강조되었습니다.

### [Wavelets Are All You Need for Autoregressive Image Generation](https://arxiv.org/abs/2406.19997)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19997.png)

Vote: 10

Authors: Shai Dekel, Nir Sharon, Idan Levy, Wael Mattar

- **What's New**: 현대 인공지능의 가장 주목할만한 업적 중 하나는 고해상도 시각 정보 생성입니다. 본 논문에서는 기존의 자기회귀 모델(autoregressive model) 기반 이미지 생성 연구를 수학적으로 견고하게 개선한 방법을 제안합니다. 이 방법은 전통적인 이미지 처리 기술인 웨이블릿(wavelet) 이미지 코딩을 활용합니다.
- **Technical Details**: 웨이블릿은 비선형, 적응형 근사(appoximation)를 위한 도구로서, 이미지를 희소한 방식으로 나타냅니다. 논문에서는 7개의 토큰만으로 이미지의 시각적 정보를 조밀한 정보부터 세부 사항까지 순차적으로 나타내는 점진적 웨이블릿 이미지 코딩(progressive wavelet image coding)을 사용합니다. 이를 위해, NLP의 디코더 전용 트랜스포머(transformer) 구조를 웨이블릿 언어에 맞게 재설계하였습니다.
- **Performance Highlights**: 이 새로운 접근법은 유동적인 토큰 시퀀스 길이를 제공하며, 더 긴 시퀀스는 더욱 상세하거나 고해상도의 이미지를 의미합니다. 텍스트 프롬프트나 클래스 레이블을 추가하여 다양하고 의미 있는 이미지 생성을 유도할 수 있습니다. 단일 텍스트 프롬프트에서 서로 다른 이미지들을 생성할 수 있으며, 생성 과정에서 지역적 프롬프트를 전환하는 것이 가능합니다.

### [RealTalk: Real-time and Realistic Audio-driven Face Generation with 3D Facial Prior-guided Identity Alignment Network](https://arxiv.org/abs/2406.18284)

![](/avatars/d6310ed861972fd691687d8f47413f33.svg)

Vote: 7

Authors: Chengjie Wang, Jiangning Zhang, Jian Yang, Ying Tai, Junwei Zhu, Xiaozhong Ji, Xiaobin Hu, Zhonggan Ding, Chuming Lin, Donghao Luo

- **What's New**: 본 논문에서는 고화질 고속 음성 기반 얼굴 생성(Audio-driven face generation)을 위한 새로운 프레임워크인 RealTalk를 소개합니다. 이 프레임워크는 오디오를 3D 표현 계수로 변환하는 오디오-표현 변환기와, 이 3D 표현 계수를 이용해 정밀하게 얼굴을 렌더링하는 표현-얼굴 렌더러(expressions-to-face renderer)로 구성되어 있습니다.
- **Technical Details**: RealTalk는 세 가지 주요 설계를 통해 성능 및 효율성을 높입니다. 첫째, 크로스 모달 어텐션(cross-modal attention)을 통해 3D 안면 사전(facial prior)을 개선했습니다. 이는 오디오 입력 외에도 3D 형태와 과거 표현 계수를 활용하여 얼굴 표현을 예측하는 방법입니다. 둘째, 학습 가능한 얼굴 마스크(learnable facial mask)를 도입하여 오디오-표현 변환기와 표현-얼굴 렌더러 사이의 연결 고리로 활용합니다. 셋째, 고속 추론을 위한 효율적인 네트워크 설계를 적용한 FIA 모듈(Facial Identity Alignment)을 도입하여 높은 품질의 얼굴 이미지를 실시간으로 생성합니다.
- **Performance Highlights**: RealTalk는 기존 방법들보다 뛰어난 입술 움직임 제어력과 텍스처 참조 기능을 자랑합니다. 또한, 초당 30프레임(30 FPS)으로 실시간에 가까운 성능을 달성하면서도 고화질 얼굴 영상을 생성할 수 있습니다.

### [OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents](https://arxiv.org/abs/2407.00114)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.00114.png)

Vote: 6

Authors: Zhancun Mu, Shaofei Cai, Haowei Lin, Yitao Liang, Qing Li, Xiaojian Ma, Ceyao Zhang, Zihao Wang, Anji Liu, Xuejie Liu

- **What's New**: 최근 프리트레인된 대형 언어 모델(LLMs)의 성공과 멀티모달 언어 모델(MLMs)의 발전에 이어 Vision-Language-Action(VLA) 모델 개발이 활발히 진행되고 있습니다. VLA 모델은 자율 에이전트를 구축하기 위한 유망한 경로로, 특히 오픈 월드 환경에서 다양한 추론 및 행동 과제를 수행할 수 있는 능력을 가지고 있습니다. OmniJARVIS는 이러한 VLA 모델의 한계점을 극복하고자 제안된 새로운 모델입니다.
- **Technical Details**: OmniJARVIS는 비전, 언어, 행동 데이터를 통합하여 모델링하는 VLA 모델로, 자체 지도 학습 방법을 통해 행동 인코더를 학습하고, 이를 통해 행동 궤적을 토큰화합니다. 이후, MLM에 행동 토큰을 추가하여 멀티모달 인터랙션 데이터를 하나의 통합된 토큰 시퀀스로 만들고, 오토레그레시브 모델링 목표로 학습합니다. 이 모델은 특히 Minecraft Universe 환경에서 종합적인 평가를 실시하여 우수한 성능을 보였습니다.
- **Performance Highlights**: OmniJARVIS는 다양한 원자적, 프로그램적, 그리고 열린 결말의 Minecraft 과제들에서 인상적인 성과를 보였습니다. 데이터 형성, 토큰화, 그리고 모델의 확장 가능성에 있어서도 중요한 설계 선택들이 확인되었습니다.

### [Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language Models by Learning from Knowledge Graphs](https://arxiv.org/abs/2407.00653)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.00653.png)

Vote: 6

Authors: Sirui Xia, Yifei Zhang, Jiaqing Liang, Xintao Wang, Yanghua Xiao, Lida Chen

- **What's New**: 최신 연구에서는 Chain-of-Knowledge(CoK)라는 새로운 학습 프레임워크를 소개하여 대규모 언어 모델(LLM)의 지식 추론 능력을 강화하려고 합니다. 이 프레임워크는 주로 지식 그래프(KGs)를 활용하여 데이터를 구성하고 이를 통해 모델을 학습시킵니다. CoK는 데이터셋 구성 및 모델 학습의 종합적인 방법론을 제공하며, 지식 추론을 위한 새로운 과제를 제시합니다.
- **Technical Details**: CoK 프레임워크는 지식 그래프로부터 규칙을 채굴하고 관련 삼중항을 추출한 후 이를 자연어 샘플로 변환하는 세 단계로 데이터셋을 구성합니다. 규칙 채굴(rule mining), 지식 선택(knowledge selection), 샘플 생성(sample generation)을 포함합니다. 모델 학습에는 행동 복제(behavior cloning) 방식과 시도-오류(trial-and-error) 메커니즘을 사용하여 LLMs의 일반화 성능을 향상시킵니다. CoK는 익명화된 실험 설정과 일반적인 실험 설정 모두에서 평가되었습니다.
- **Performance Highlights**: 광범위한 실험을 통해 CoK의 효과를 검증하였으며, 데이터 누수를 방지하기 위해 익명화된 실험 설정을 사용하였습니다. CoK는 실제 지식 추론뿐만 아니라 다른 추론 벤치마크에서도 유용성을 입증하였습니다. 특히 CoK는 새로운 규칙과 도전과제에 대한 높은 일반화 성능을 보여주었습니다.

### [UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI](https://arxiv.org/abs/2407.00106)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.00106.png)

Vote: 4

Authors: Eleni Triantafillou, Ilia Shumailov, Itay Yona, Jamie Hayes, Eugene Bagdasaryan, Matthew Jagielski, Nicolas Papernot, Heidi Howard, Guillermo Ortiz-Jimenez

- **What's New**: 최근 대형 언어 모델(Large Language Models, LLMs)에서 바람직하지 않은 목적으로의 사용에 대한 우려가 커지고 있다. 이러한 문제 해결을 위해 'Unlearning'이라는 개념이 등장했고, 이는 원래 프라이버시 민감 정보를 제거하기 위해 개발된 기술이다(Bourtoule et al., 2021). 최근에는 이를 사용해 유해 기능(Lynch et al., 2024)이나 응답(Yao et al., 2023; Liu et al., 2024) 제거, 백도어 지우기(Liu et al., 2022) 등 다양한 응용 분야에 적용하려는 시도가 있어왔다. 이 논문은 LLMs에서 광범위하게 허용되지 않는 지식을 제거하기 위한 Unlearning의 적용 가능성을 논의하며, 이를 통해 'UnUnlearning'이라는 개념을 소개한다. 이는 모델이 특정 지식을 성공적으로 제거하더라도 맥락 상호작용을 통해 다시 등장할 수 있다는 뜻이다.
- **Technical Details**: 논문에서는 몇 가지 주요 용어를 사용하여 내용을 전개한다. 지식(Knowledge)은 모델에 이용 가능한 정보이며, 이는 맥락 내 제공된 입력, 모델 파라미터에 저장된 정보, 또는 검색 가능한 증거 등을 포함한다. 콘텐츠 필터링(Content Filtering)은 특정 모형/응답을 걸러내는 과정이다. Unlearning은 모델에서 특정 지식을 제거하는 과정으로, Unlearning for privacy와 Unlearning for content regulation으로 구분된다. 후자는 불법적이거나 허용되지 않는 콘텐츠를 생성하는 것을 방지하기 위해 지식을 제거하는 것이다. 또한 In-Context Learning(맥락 학습)은 언어 모델이 학습 데이터에 없는 작업 설명으로부터 일반화하는 능력을 의미한다.
- **Performance Highlights**: Unlearning이 이론적으로 완벽하게 수행되었을 때조차도, 모델이 여전히 존재하는 지식을 종합하여 허용되지 않는 작업을 수행할 가능성을 시사한다. 예를 들어, 폭탄의 성질에 대한 기본 화학 지식을 갖고 있는 모델은 폭탄 제조 방법을 도출할 수 있다. 따라서 UnUnlearning이 문제가 되는 경우, 이는 불완전한 Unlearning 설정에서는 더욱 악화될 수밖에 없다. 논문은 이러한 '지식 복원' 문제를 해결하기 위한 효과적인 콘텐츠 규제 메커니즘의 필요성을 강조한다.

### [T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge](https://arxiv.org/abs/2407.00088)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.00088.png)

Vote: 3

Authors: Lingxiao Ma, Jianyu Wei, Yanyong Zhang, Ting Cao, Mao Yang, Shijie Cao, Lei Wang

- **What's New**: 대규모 언어 모델(LLMs)의 클라이언트 기기 배포가 더욱 활발해지고 있습니다. 본 논문은 다양한 비트 폭(quantization)에서 혼합 정밀도 행렬 곱셈(mpGEMM)을 직접적이고 효율적으로 지원하는 mpGEMM 커널 설계를 목표로 하고 있습니다. 이 방식을 통해 비트 감소에 따른 성능 향상을 달성하고자 합니다.
- **Technical Details**: 기존 하드웨어에서는 고정된 비트 폭과 대칭 연산을 지원하지만, 비트 단위 계산을 활용하여 표준 곱셈 알고리즘 대신 테이블 조회 방법을 제안합니다. 특히, 비트 패턴에 따라 예측된 결과를 테이블에 저장하고 조회하는 방식으로 곱셈을 줄이는 방법을 도입합니다. 이를 통해 LUT(Look-Up Table) 기반 mpGEMM 커널을 제안하며, 테이블의 랜덤 접근 비용을 줄이기 위해 다양한 기술을 제안합니다. 시스템 측면에서는 빠른 온칩 메모리에서 테이블을 유지하고 병렬 조회를 가능하게 하며, 알고리즘 측면에서는 테이블 양자화 및 미러 통합을 통해 테이블 크기를 줄입니다.
- **Performance Highlights**: 도입된 T-MAC mpGEMM 커널 라이브러리는 Apple M2 Ultra, Jetson AGX Orin, Surface Book 3 및 Raspberry Pi 5와 같은 일반 엣지 장치에서 구현되었습니다. 전반적인 LLM 추론 속도가 llama.cpp와 비교하여 평균 3.6배, 최대 6.6배의 속도 향상을 보였습니다. 특히 Raspberry Pi에서도 높은 성능을 보여 BitNet 모델에서 11.1 토큰/초의 속도를 기록했습니다. 에너지 소비는 llama.cpp 대비 60-70% 절감되었습니다.

### [Towards Robust Speech Representation Learning for Thousands of Languages](https://arxiv.org/abs/2407.00837)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.00837.png)

Vote: 3

Authors: Xinjian Li, Jinchuan Tian, Karen Livescu, Shinji Watanabe, Soumi Maiti, Wangyou Zhang, William Chen, Xuankai Chang, Yifan Peng, Jiatong Shi

- **What's New**: 이 논문에서는 XEUS (Cross-lingual Encoder for Universal Speech)라는 새로운 멀티링구얼 자기지도학습(Self-Supervised Learning, SSL) 모델을 소개합니다. XEUS는 4,057개의 언어를 포함하는 1백만 시간 이상의 공개 데이터를 활용해 사전 학습 되었으며, 다양한 녹음 조건을 처리할 수 있는 다목적 음성 인코더입니다. 저자들은 또한 7,413시간의 라벨 없는 음성을 포함하는 새로운 SSL 코퍼스를 공개합니다.
- **Technical Details**: XEUS는 E-Branchformer라는 구조를 사용하며, 37개의 기존 코퍼스로부터 데이터를 큐레이션하여 다양한 연설 및 녹음 조건을 반영합니다. 새로운 SSL 목적은 모델이 실내에서 녹음된 소리를 예측할 수 있도록 하는 '음향 디리버베이션(acoustic dereverberation)'을 포함합니다. 이 모델은 HuBERT 스타일의 마스킹 예측과 WavLM 스타일의 잡음 제거를 결합하여 더욱 강력한 성능을 발휘합니다.
- **Performance Highlights**: XEUS는 다양한 다운스트림 작업에서 SOTA(Self-Supervised Learning) 모델보다 우수한 성능을 보였습니다. ML-SUPERB 멀티링구얼 ASR 기준에서 MMS, w2v-BERT 2.0 등 기존 모델을 능가했으며, ST, 스피치 재합성 등 다양한 작업에서도 뛰어난 성능을 확인했습니다. 영어 전용 SUPERB 벤치마크에서도 4개의 작업에서 새로운 SOTA를 달성했습니다.

### [SVG: 3D Stereoscopic Video Generation via Denoising Frame Matrix](https://arxiv.org/abs/2407.00367)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.00367.png)

Vote: 3

Authors: Yinda Zhang, Qiangeng Xu, Sean Fanello, Peng Dai, David Futschik, Ruofei Du, Xiaojuan Qi, Feitong Tan

- **What's New**: 이 논문은 VR/AR 기술의 발전에 따라 사용자에게 몰입형 3D 경험을 제공하는 필요성이 증가함에 따라, 단일 카메라 비디오(monocular video)를 3D 스테레오스코픽(stereoscopic) 비디오로 변환하는 새로운 프레임워크를 제안합니다. 이 방법은 기존의 카메라 포즈 추정(camera pose estimation)을 사용하지 않으며, 사전 학습된 비디오 생성 모델(video generation model)의 추론만을 이용하여 높은 품질의 3D 스테레오스코픽 비디오를 생성합니다.
- **Technical Details**: 이 프레임워크는 2D에서 3D로 이미지 변환하는 기존 방법을 비디오 도메인으로 확장하여, 먼저 단일 카메라 비디오를 생성한 후 각 프레임의 단일 카메라 깊이를 추정하여 좌안과 우안을 각각 재투영합니다. 이어서 비디오 생성 모델의 덴워프(denoised warped) 프레임을 이용하여 비노출된 영역을 인페인트(inpaint)합니다. 이를 위해 제안된 프레임 매트릭스(frame matrix) 표현은 여러 시점에서 본 프레임 시퀀스를 포함하며, 시간적-공간적 일관성을 유지합니다. 마지막으로, 디노이징 단계에서 생성된 콘텐츠를 디졸루션 영역에 다시 주입(re-injection scheme)함으로써, 비노출된 영역에서 발생하는 아티팩트를 효과적으로 줄입니다.
- **Performance Highlights**: 제안된 방법은 Sora, Lumiere, WALT, Zeroscope 등 다양한 단일 카메라 비디오 생성 모델에서 생성된 비디오를 3D 스테레오스코픽 비디오로 변환하는 데 있어 우수한 성능을 보였습니다. 정성적 및 정량적 평가에서 제안된 프레임워크는 기존의 3D 스테레오스코픽 비디오 생성 방법보다 일관성과 품질 면에서 뛰어난 결과를 나타냈습니다.

### [Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs](https://arxiv.org/abs/2406.20086)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.20086.png)

Vote: 2

Authors: Sheridan Feucht, David Bau, Byron Wallace, David Atkinson

- **What's New**: 이 논문에서는 대형 언어 모델(LLMs)이 다중 토큰 단어 및 명명을 어떻게 '이해'하고 처리하는지에 대한 새로운 통찰을 제공하며, 특히 초기 레이어에서 토큰 수준 정보를 '지우는' 메커니즘을 조사합니다. 연구진은 LLM이 훈련 중에 의미 있는 어휘 항목을 암묵적으로 매핑하는 어휘를 개발한다는 가설을 세웠습니다.
- **Technical Details**: 이 연구는 Llama-2-7b와 Llama-3-8b 모델을 대상으로, 각 층과 토큰 위치에서 숨겨진 상태 덩어리를 조사하기 위해 선형 프로브(linear probe)를 훈련시켰습니다. 훈련 데이터로는 Pile 데이터셋을 사용했으며, 16 에포크 동안 AdamW 옵티마이저로 학습을 진행했습니다. 또한, 학습된 프로브를 CounterFact 데이터셋을 사용해 검증했습니다.
- **Performance Highlights**: 테스트 결과, CounterFact 데이터셋의 마지막 토큰에서는 토큰 정확도가 크게 떨어지는 '지우기 효과(erase effect)'가 관찰되었습니다. 이는 선형 프로브가 특정 토큰 위치에서 특정 주제나 개념을 나타낼 때, 해당 토큰과 관련된 최초의 토큰 정보를 잃는 과정을 의미합니다.

### [Accurate Prediction of Ligand-Protein Interaction Affinities with Fine-Tuned Small Language Models](https://arxiv.org/abs/2407.00111)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.00111.png)

Vote: 2

Authors: Ben Fauber

- **What's New**: 최근 연구에서 매우 큰 매개변수를 가진 프리트레인된 소형 언어 모델(SLM)을 사용하여 리간드-단백질 상호작용(LPI)의 친화도를 예측하는 새로운 방법을 소개했습니다. 이 연구는 드럭-타겟 상호작용(DTI) 예측의 정확성을 향상시키기 위한 시도 중 하나입니다.
- **Technical Details**: 기존의 기계학습 및 자유 에너지 방해 방법에 비해 모델의 정확도를 크게 개선하기 위해, 도메인 특화 데이터를 몇 에포크 동안 학습시킨 소형 언어 모델을 사용했습니다. 모델 입력으로는 리간드의 SMILES 문자열과 타겟 단백질의 아미노산 서열만을 사용했습니다. 이는 복잡한 다변수 최적화 문제를 효과적으로 해결하기 위해 거대한 데이터세트를 활용한 접근 방식입니다.
- **Performance Highlights**: 연구 결과, 새로운 모델은 기존의 ML 및 FEP 방법을 능가하는 성능을 보여줍니다. 다양한 리간드-단백질 상호작용의 친화도를 정확하게 예측할 수 있으며, 이는 어려운 치료용 표적에 대한 약물 발견 캠페인을 가속화할 수 있는 잠재력을 보여줍니다.

### [Auto Cherry-Picker: Learning from High-quality Generative Data Driven by Language](https://arxiv.org/abs/2406.20085)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.20085.png)

Vote: 2

Authors: Xiangyu Zhao, Xiangtai Li, Yanhong Zeng, Kai Chen, Yicheng Chen, Jianzong Wu, Yining Li

- **What's New**: 본 논문에서는 새로운 트레이닝 데이터 생성 파이프라인인 'Auto Cherry-Picker(ACP)'를 제안합니다. 이 시스템은 객체 리스트에 기반하여 이미지와 상세한 설명 및 레이아웃 주석을 동시에 생성할 수 있습니다. 이를 통해 기존의 텍스트나 시각적 주석 없이 데이터 생성이 가능합니다.
- **Technical Details**: ACP는 사전학습된 생성 모델을 기반으로 하며, 두 가지 주요 컴포넌트로 구성됩니다: raw data generator와 comprehensive data filter입니다. raw data generator는 LLM을 사용하여 세부적인 장면 설명을 샘플링하고, T2I 모델을 통해 이미지를 생성합니다. 이를 통해 다양한 객체 리스트에서 손쉽게 합성 예제를 확대할 수 있습니다. 또한, 데이터 필터는 Composite Layout and Image Score (CLIS)라는 포괄적인 메트릭을 사용하여 품질을 평가합니다. CLIS는 레이아웃 타당성을 평가하는 CLIS-L과 이미지 품질을 평가하는 CLIS-I로 구성됩니다.
- **Performance Highlights**: ACP 파이프라인을 통해 생성된 데이터는 다양한 원거리 및 오픈-어휘 시나리오에서 성능 향상을 보여줍니다. LVIS 데이터셋 실험에서 Mask R-CNN을 사용하여 long-tail setting에서 +5.2%의 성능 향상을, Grounding DINO를 사용하여 open-vocabulary setting에서 +1.3%의 성능 향상을 관찰했습니다. 또한, MME benchmark에서 +80.1점의 스코어를, GQA benchmark에서 +0.4점의 향상을 기록했습니다.

### [Show Less, Instruct More: Enriching Prompts with Definitions and Guidelines for Zero-Shot NER](https://arxiv.org/abs/2407.01272)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.01272.png)

Vote: 2

Authors: Marco Ernandes, Andrea Zugarini, Leonardo Rigutini, Marco Maggini, Andrew Zamai

- **What's New**: 이번 연구에서는 SLIMER (Show Less, Instruct More - Entity Recognition)이라는 새로운 접근 방식을 소개합니다. 이 접근 방식은 제한된 훈련 데이터 샘플을 사용하면서도 각 엔티티에 대한 정의와 지침을 포함한 프롬프트를 활용하여 성능을 극대화하는 것을 목표로 합니다. 이는 특히 처음 보는 엔티티 태그에 대한 제로샷(Zero-shot) NER 성능을 향상시키기 위한 것입니다.
- **Technical Details**: 기존의 제로샷 NER 모델은 대규모 엔티티 태그와 예시를 기반으로 훈련되었습니다. SLIMER은 이러한 데이터의 일부만을 사용하여 모델을 훈련하지만, 정의와 지침을 포함하는 고유한 프롬프트로 모델을 안내합니다. 여기서 정의는 해당 엔티티 태그를 설명하는 짧은 문장으로 구성되며, 지침은 모델의 레이블링을 원하는 주석 체계와 맞추기 위한 주석 지침을 제공합니다. 이러한 지침들은 모델이 특정 경계 사례를 레이블링하지 않도록 유도하거나 엔티티의 예시를 제공하는 역할을 합니다.
- **Performance Highlights**: SLIMER는 MIT와 CrossNER 같은 OOD(Named Entity Recognition) 벤치마크에서 테스트되었습니다. 또한, BUSTER라는 문서 단위 NER 데이터셋에서도 처음 보는 NE 태그에 대한 성능을 평가하였습니다. 실험 결과, 정의와 지침이 없는 기본 모델과 비교했을 때 SLIMER는 더 깊은 이해를 보였고, 더 빠르고 안정적인 학습을 하였으며, 제로샷 성능도 높았습니다. 이는 제한된 데이터와 적은 엔티티 태그로 훈련되었음에도 불구하고, 최첨단 방법들과 비교 가능한 성과를 보였습니다.

### [DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging](https://arxiv.org/abs/2407.01470)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.01470.png)

Vote: 2

Authors: Yun-Nung Chen, Chen-An Li, Tzu-Han Lin, Hung-yi Lee

- **What's New**: 최신 대형 언어 모델인 GPT-4와 Gemini는 탁월한 성능을 보이고 있지만, 이들의 행동을 인간 선호도에 맞추는 데는 여전히 어려움이 있습니다. 본 논문에서는 이러한 문제를 해결하기 위해 Domain knowledge merged Reward Model (DogeRM)이라는 새로운 접근 방식을 제안합니다. DogeRM은 일반 공개된 선호 데이터 세트로 학습된 보상 모델(reward model, RM)과 수학, 코딩과 같은 도메인 특화 데이터 세트로 미세 조정된 언어 모델을 병합합니다.
- **Technical Details**: DogeRM의 개발 방법론은 사전 학습된 트랜스포머 기반 언어 모델의 디코딩 레이어를 선형 회귀 레이어로 교체하는 것으로 시작합니다. 이 레이어는 로짓을 스칼라 값으로 변환하여 준 최적 응답(chosen response)와 거부된 응답(rejected response)의 보상을 평가합니다. 구체적으로, 새로운 보상 모델은 입력 프롬프트, 준 최적 응답 및 거부된 응답에 대해 로지스틱 함수를 이용한 손실 함수로 최적화됩니다. 또한 DogeRM은 도메인 특화 언어 모델의 파라미터와 보상 모델의 파라미터를 병합하여 도메인 지식을 통합합니다. 모델 병합 과정에는 단순한 평균화 방법에서 가중치를 적용한 평균화 방법, 태스크 벡터 생성 방법 등이 사용됩니다.
- **Performance Highlights**: DogeRM의 성능은 RewardBench, Auto-J Eval, GSM8K 및 MBPP 테스트에서 평가되었으며, 전반적으로 성능이 향상되었음을 확인했습니다. 다양한 모델 아키텍처에 대해 일반화 가능하다는 점도 입증되었습니다. 본 연구는 모델 병합이 효과적인 방법임을 보여주며, 도메인 지식을 성공적으로 통합하여 보상 모델의 성능을 향상시키는 데 중요한 기여를 합니다.

### [ProgressGym: Alignment with a Millennium of Moral Progress](https://arxiv.org/abs/2406.20087)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.20087.png)

Vote: 1

Authors: Xuchuan Huang, Jiaming Ji, Yang Zhang, Yaodong Yang, Tianyi Qiu, Jasmine Xinze Li

- **What's New**: AI 시스템이 인간의 신념과 가치에 미치는 영향이 증가함에 따라 이러한 시스템이 현대 사회에 바이어스와 잘못된 개념을 심화시킬 위험이 있다. 새로운 연구는 AI의 가치 고착 현상을 해결하기 위해 '진보 정렬(progress alignment)' 개념을 도입했다. 이는 디지털 시대의 도덕적 진보를 모방하는 AI 정렬 방법을 말한다.
- **Technical Details**: 이 연구는 진보 정렬 문제를 시간 인지 불확실성 마르코프 결정 과정(POMDP)으로 개념화했다. AI 에이전트가 변화하는 인간 가치를 학습하고 상호작용하는 시스템을 통해 구체화한다. ProgressGym 실험 프레임워크를 통해 다양한 문제와 도전 과제를 벤치마킹하고 평가할 수 있도록 한다.
- **Performance Highlights**: ProgressGym은 9세기에 걸친 방대한 역사적 텍스트 데이터(38GB)와 LLMs(최대 70B 파라미터)를 제공하며, 진보 정렬을 위한 실험 프레임워크를 오픈소스로 제공한다. 이 프레임워크는 시간적 차원을 포함한 첫 번째 정렬 실험 환경이며, 다양한 알고리즘 평가와 데이터 세트를 지원한다.

