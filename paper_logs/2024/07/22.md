## Daily Papers (2024-07-22)

### [Internal Consistency and Self-Feedback in Large Language Models: A Survey](https://arxiv.org/abs/2407.14507)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14507.png)

Vote: 31

Authors: Zifan Zheng, Xunkai Li, Hanyu Wang, Zhiyu Li, Xun Liang, Shichao Song, Rong-Hua Li, Qingchen Yu, Feiyu Xiong

- **What's New**: 이번 연구는 대형 언어 모델(LLMs)의 내부 일관성(Internal Consistency)에 대한 체계적인 분석을 제공하며, 이 현상이 모델의 추론 능력과 환각(hallucinations) 문제의 근본 원인임을 강조합니다.
- **Technical Details**: 내부 일관성을 개선하기 위해 'Internal Consistency Mining'이라는 전략을 사용하며, 여기에는 자기 평가(Self-Evaluation), 일관성 신호 획득(Consistency Signal Acquisition), 자기 업데이트(Self-Update)와 같은 방법들이 포함됩니다. 이러한 방법들은 체계적인 학습 과정을 통해 모델이 자신의 출력을 평가하고 개선할 수 있도록 돕습니다.
- **Performance Highlights**: 모델의 내부 일관성을 향상시키기 위해 스케일링 외에도 소형 언어 모델의 장점을 극대화하는 전략과 인간의 사고 과정을 모방하도록 설계된 방법들이 제안되었습니다. 이러한 접근 방법은 다양한 피드백 신호를 포착하고 모델의 결과물의 신뢰성을 높이는 데 기여합니다.

### [EVLM: An Efficient Vision-Language Model for Visual Understanding](https://arxiv.org/abs/2407.14177)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14177.png)

Vote: 25

Authors: Changyi Liu, Kaibing Chen, Dewen Fan, Bin Wen, Huihui Xiao, Huasong Zhong, Di Xu, Yifei Hu, Hanwen Zhong, Tianke Zhang, Fan Yang, Di Zhang, Kui Xia, Jiahong Wu, Dong Shen, Wei Yuan, Size Li

- **What's New**: 최근 연구에서는 시각-언어 모델의 성능을 향상시키기 위해 cross-attention 메커니즘을 도입한 새로운 비주얼-언어 모델(EVLM)을 제안하였습니다. 이 모델은 긴 시각 토큰을 처리할 때 계산 오버헤드를 줄이면서도 충분한 시각 정보를 언어 모델에 전달할 수 있도록 설계되었습니다.
- **Technical Details**: 모델 아키텍처는 Flamingo 모델을 기반으로 하며, 시각 인코더와 대형 언어 모델, Gated Cross Attention Layer로 구성됩니다. 시각 인코더로는 4.4B EVA2-CLIP-E-Plus를 사용하여 계층적 시각 특징을 추출하고, 학습 가능한 시각 특징을 포함하는 learnable tokens를 도입하였습니다. Cross Attention을 통해 비주얼 및 텍스트 간의 상호작용이 이루어집니다.
- **Performance Highlights**: EVLM은 대규모 이중 언어 이미지-텍스트 쌍 데이터셋을 통해 사전 훈련 되어 경쟁력 있는 scores를 달성하였으며, 이미지 및 비디오 캡셔닝과 같은 작업에서 뛰어난 성능을 보였습니다. 이러한 접근 방식은 계산 효율성을 유지하면서도 정보의 풍부함을 고려한 디자인을 통해 FLOPs를 현저히 감소시켜 효율적인 훈련이 가능하게 합니다.

### [LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference](https://arxiv.org/abs/2407.14057)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14057.png)

Vote: 18

Authors: Minsik Cho, Sachin Mehta, Mohammad Rastegari, Qichen Fu, Mahyar Najibi, Thomas Merth

- **What's New**: LazyLLM은 LLM의 prefilling 속도를 향상시키기 위해 설계된 새로운 기법으로, 중요도가 낮은 토큰에 대한 KV(cache)를 지연 계산하는 방식으로 TTFT(time-to-first-token)를 최적화합니다.
- **Technical Details**: LazyLLM은 각 생성 단계에서 다음 토큰 예측에 중요한 토큰에 대해서만 KV를 선택적으로 계산하고, 나머지 토큰의 계산은 필요할 때로 미룹니다. 그리고 Aux Cache라는 추가 캐싱 메커니즘을 사용하여 제거된 토큰의 hidden states를 캐시하여, 더 효율적으로 재사용할 수 있도록 합니다.
- **Performance Highlights**: LazyLLM은 기존의 transformer 기반 LLM에 통합이 용이하며, 추가적인 파라미터 조정 없이 바로 적용 가능하고, 실험 결과에 따르면 6개 언어 작업의 16개의 표준 데이터셋에서 LLM의 prefilling 및 decoding 단계에서 추론 속도를 현저히 개선하는 것으로 나타났습니다.

### [ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities](https://arxiv.org/abs/2407.14482)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14482.png)

Vote: 14

Authors: Mohammad Shoeybi, Peng Xu, Wei Ping, Xianchao Wu, Zihan Liu, Bryan Catanzaro

- **What's New**: 오픈 LLM 커뮤니티는 Llama-3-70B-Instruct (Meta-AI, 2024) 등과 같은 새로운 오픈 액세스 대형 언어 모델의 성능을 크게 향상시켰습니다. 그러나 여전히 GPT-4-Turbo (OpenAI, 2023)와 같은 상업 모델과 성능 차이가 있습니다. 기존의 RAG 성능을 결합하여 GPT-4-Turbo 수준의 긴 컨텍스트 이해 능력을 갖춘 ChatQA 2를 소개합니다.
- **Technical Details**: 이 연구에서는 Llama3-70B의 컨텍스트 창을 8K에서 128K로 확장하고, 세 가지 단계로 구성된 지시 튜닝을 수행하여 모델 성능을 향상시켰습니다. 특히, SlimPajama (Soboleva et al., 2023)와 함께 고급 시퀀스를 사용하여 지속적으로 재훈련하는 방법을 사용했습니다.
- **Performance Highlights**: 결과적으로, Llama3-ChatQA-2-70B-128K 모델은 많은 실제 긴 컨텍스트 이해 작업에서 GPT-4-Turbo-2024-04-09와 유사하거나 약간 못 미치는 성능을 보였으며, RAG 및 대화형 QA 작업에서는 GPT-4-Turbo보다 우수한 성과를 보였습니다.

### [Stable Audio Open](https://arxiv.org/abs/2407.14358)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14358.png)

Vote: 10

Authors: Zack Zukowski, Josiah Taylor, Zach Evans, Jordi Pons, Julian D. Parker, CJ Carr

- **What's New**: 이 연구는 Creative Commons (CC) 라이센스 오디오를 기반으로 한 텍스트 조건 생성 모델을 공개하며, 공개 모델 가중치와 코드, 사용된 데이터에 대한 저작권 정보를 제공합니다.
- **Technical Details**: 모델 아키텍처는 latent diffusion model로, 텍스트 프롬프트에서 44.1kHz 스테레오 오디오를 생성합니다. 이 모델은 오토인코더(156M 파라미터), T5 기반 텍스트 임베딩(109M 파라미터), 변이형 오토인코더 내에서 작동하는 transformer 기반 확산 모델(DiT, 1057M 파라미터)으로 구성되어 있습니다.
- **Performance Highlights**: 본 모델은 소비자 등급 GPU에서 실행 가능하며, 가변 길이 오디오 생성과 고품질 오디오 생성을 지원합니다. 평가 과정에서 모델은 훈련 데이터의 암기(Memorization) 특성을 보이지 않았습니다.

### [The Vision of Autonomic Computing: Can LLMs Make It a Reality?](https://arxiv.org/abs/2407.14402)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14402.png)

Vote: 9

Authors: Gong Cheng, Dongmei Zhang, Zhiyang Zhang, Fangkai Yang, Qi Zhang, Xiaoting Qin, Qingwei Lin, Saravan Rajmohan, Jue Zhang

- **What's New**: 이 논문은 LLM(대형 언어 모델)을 활용하여 마이크로서비스의 자가 관리(self-management) 시스템을 구축하는 새로운 프레임워크를 제안합니다. 특히, 이 연구는 LLM 기반의 다중 에이전트 구조를 이용하여 클라우드 서비스의 자가 관리를 실현하고자 하며, 이를 통해 자율 시스템의 비전을 실현할 수 있을지 탐구합니다.
- **Technical Details**: 제안된 시스템은 계층적 다중 에이전트 아키텍처를 채택합니다. 상위 그룹 관리자는 여러 서비스 구성 요소를 아우르는 선언적 작업을 수행하며, 최적의 지연 시간(200 ms 이하)을 목표로 합니다. 하위 자율 에이전트는 각 서비스 구성 요소 내의 특정 작업에 집중합니다. 이 시스템에 대한 평가는 마이크로서비스 데모 프로젝트인 Sock Shop을 기반으로 한 온라인 평가 벤치마크를 통해 수행됩니다.
- **Performance Highlights**: 연구 결과는 LLM 기반 다중 에이전트 프레임워크가 자율 서비스 유지 관리에서 5단계 분류법의 3단계 자율성을 달성했음을 보여 줍니다. 이 시스템은 문제를 효과적으로 감지하고 특정 필수 작업을 수행하지만, 근본 원인 분석(root cause analysis)과 이슈 완화(issue mitigation) 능력에서의 개선 기회가 있습니다.

### [VisFocus: Prompt-Guided Vision Encoders for OCR-Free Dense Document Understanding](https://arxiv.org/abs/2407.12594)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12594.png)

Vote: 7

Authors: R. Manmatha, Srikar Appalaraju, Ofir Abramovich, Sharon Fogel, Ron Litman, Royee Tichauer, Niv Nayman, Shai Mazor, Inbal Lavi, Shahar Tsiper

- **What's New**: 이 논문에서는 OCR(Optical Character Recognition) 없이 문서 이해를 개선하기 위한 새로운 접근 방식인 VisFocus를 제안합니다. VisFocus는 사용자 프롬프트에 정보 집합을 기반으로 한 시각적 피쳐 생성 방식을 도입하여 문서에서 텍스트와 비주얼 정보를 더 잘 정렬시키고, 중요 정보를 더 잘 식별할 수 있도록 합니다.
- **Technical Details**: VisFocus는 두 가지 주요 구성 요소를 포함합니다: (1) Vision-Language Merging Attention (ViLMA) 계층은 시각적 인코더 아키텍처에 통합되어 사용자 프롬프트와의 상호작용을 가능하게 합니다. (2) Localized Masked Prompt Modeling (LMPM)이라는 새로운 프리트레이닝 태스크는 모델이 프롬프트와 관련된 텍스트 패치를 집중적으로 탐색하도록 유도합니다. 이러한 구성 요소들은 함께 시각적 피쳐와 언어 정보를 정렬시키기 위한 교차 주의 메커니즘을 기반으로 동작합니다.
- **Performance Highlights**: 제안된 방법은 여러 벤치마크에서 기존의 OCR-Free 방법과 비교하여 최첨단 성능을 달성하며, ViLMA 계층과 LMPM 태스크 간의 상호작용으로 인해 성능 개선이 이루어졌음을 실증적으로 보여줍니다.

### [SciCode: A Research Coding Benchmark Curated by Scientists](https://arxiv.org/abs/2407.13168)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.13168.png)

Vote: 6

Authors: Cunwei Fan, Shengyan Liu, Chenyu Tian, Di Luo, Pan Ji, Yanyu Xiong, Kilian Lieret, Kha Trinh, Zihan Wang, Yao Li, Shengzhu Yin, Shizhuo Dylan Zhang, Roland Haas, Minhui Zhu, Bohao Wu, +, Minyang Tian, Xinan Chen, Xuefei Guo, Yutao Ma, Kittithat Krongchon, Hao Tong, Luyu Gao

- **What's New**: 이 논문에서는 SciCode라는 새로운 벤치마크를 제안합니다. SciCode는 자연 과학 분야에서의 코드 생성 문제를 포함하여, 수학, 물리학, 화학, 생물학 및 재료 과학 등에서 총 80개의 주요 문제와 338개의 하위 문제를 가지고 있습니다. 이 벤치마크는 현실적이고 까다로운 평가를 통해 현재 LM(언어 모델)의 성능을 평가하는 데 중점을 두고 있습니다.
- **Technical Details**: SciCode는 고품질 데이터를 기반으로 하고 있으며, 모든 문제는 대표적인 과학 분야에 있는 최소 두 명의 박사 과정 학생 이상이 주석을 달고 교정한 후 검증을 받은 문제들로 구성되어 있습니다. 각 주요 문제는 여러 하위 문제로 세분화되어 있으며, 모델은 각 하위 문제에 대해 파이썬 함수를 구현한 후 이를 통합하여 전체 문제의 해결책을 제출해야 합니다.
- **Performance Highlights**: 현재 테스트된 모델들 가운데 대다수는 SciCode 벤치마크의 문제들을 성공적으로 해결하지 못했습니다. Claude3.5-Sonnet 모델은 4.6%의 주요 문제를 해결하고, GPT-4o와 같은 다른 강력한 모델은 1.5%에 불과하며, 오픈 소스 모델인 Deepseek-Coder-v2는 3.1%의 문제를 해결했습니다. 문제 해결 시 배경 지식을 제공할 경우 모델의 성능이 크게 향상되는 것으로 나타났지만, 최선의 모델조차도 12.3%의 문제만 해결할 수 있었습니다.

### [Fast Matrix Multiplications for Lookup Table-Quantized LLMs](https://arxiv.org/abs/2407.10960)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10960.png)

Vote: 6

Authors: Han Guo, Jonathan Ragan-Kelley, Yoon Kim, Eric P. Xing, William Brandon, Radostin Cholakov

- **What's New**: 이 논문은 FLUTE라는 유연한 lookup-table 엔진을 소개하며, 이는 Low-bit 및 비균일(non-uniform) 양자화 설정을 지원하여 LLM의 weight quantization을 효율적으로 처리하기 위한 솔루션입니다.
- **Technical Details**: FLUTE는 (1) 오프라인 weight 재구성, (2) 효과적인 비균일 양자화를 위한 공유 메모리 lookup 테이블, 및 (3) 최적화된 작업 분배를 위한 Stream-K 파티셔닝의 조합을 사용하여 도전 과제를 해결합니다. 이 방법을 통해 양자화된 weight를 GPU의 로컬 SRAM으로 이동하고, 비트 폭이 낮은 구조에서도 효율적으로 매트릭스 곱(matmul)을 수행할 수 있습니다.
- **Performance Highlights**: FLUTE는 128개의 그룹에서 4비트 양자화된 weights에 대한 혼합 정밀도(matmul) 설정에서 기존의 비균일 양자화 커널보다 뛰어난 성능을 보여주며, 일부 경우에는 단순한 균일 양자화 커널과 유사한 성능을 기록합니다. FLUTE를 LLaMA3에 통합하여 1.5배에서 2배의 throughput 증가를 달성했습니다.

### [Visual Text Generation in the Wild](https://arxiv.org/abs/2407.14138)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14138.png)

Vote: 6

Authors: Jiawei Liu, Zhibo Yang, Peng Wang, Cong Yao, Yuanzhi Zhu, Xinggang Wang, Fei Huang, Wenyu Liu, Feiyu Gao

- **What's New**: 본 논문에서는 SceneVTG라는 새로운 방법을 제안하여 실제 환경에서 시각적 텍스트 생성을 해결합니다. 이 방법은 두 단계의 패러다임을 따르며, 텍스트 영역 및 콘텐츠 생성기(TRCG)와 지역 시각적 텍스트 렌더러(LVTR)로 구성됩니다.
- **Technical Details**: TRCG는 다중 모달 대형 언어 모델(MLLMs)의 시각적 추론 기능을 활용하여 배경 이미지에서 텍스트 영역을 식별하고 맥락적으로 일치하며 시각적으로 적합한 콘텐츠를 추천합니다. LVTR는 지역 조건부 확산 모델을 사용하여 임의의 스케일에서 텍스트 생성을 활성화합니다.
- **Performance Highlights**: SceneVTG는 기존의 렌더링 기반 및 확산 기반 방법에 비해 충실도(fidelity)와 합리성(reasonability) 면에서 유의미한 성과를 보이며, 생성된 이미지는 텍스트 탐지 및 인식 작업에서 우수한 유용성을 제공합니다.

### [Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders](https://arxiv.org/abs/2407.14435)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14435.png)

Vote: 5

Authors: Arthur Conmy, Nicolas Sonnerat, János Kramár, Tom Lieberum, Neel Nanda, Senthooran Rajamanoharan, Vikrant Varma

- **What's New**: 이 논문에서는 JumpReLU sparse autoencoders (SAEs)를 소개합니다. JumpReLU는 원래의 ReLU 기반 SAE 아키텍처에서의 작은 수정으로, SAE 인코더의 ReLU 활성화 함수가 JumpReLU 활성화 함수로 교체되어 양수 임계값 이하의 전활성화를 제로로 설정합니다. 또한, L2 재구성 오류 항과 L0 희소성 패널티의 가중 합으로 구성된 손실 함수를 사용하여 JumpReLU SAEs를 훈련합니다.
- **Technical Details**: JumpReLU SAEs는 사전 훈련된 언어 모델의 활성화를 근거로 하여 희소 선형 분해를 수행합니다. 이 아키텍처는 여러 레이어의 Gemma 2 9B 잔여 스트림에서 평가되었으며, JumpReLU 활성화 기능을 통해 뚜렷한 성능 개선이 있음을 확인했습니다. 손실 함수는 생태적으로 상수이며, JumpReLU의 임계값에 대해 이상의 기울기를 제공하여 효율적인 훈련이 가능합니다.
- **Performance Highlights**: JumpReLU SAEs는 주어진 희소성과 상관없이 Gated SAEs보다 더 신뢰성 있는 재구성을 제공하는 동시에 TopK SAEs와 유사하거나 때로는 더 뛰어난 성능을 보였습니다. JumpReLU는 단일 순방향 및 역방향 통과로 훈련 가능하여, Gated 또는 TopK SAEs보다 더 효율적으로 훈련할 수 있습니다.

### [Phi-3 Safety Post-Training: Aligning Language Models with a "Break-Fix" Cycle](https://arxiv.org/abs/2407.13833)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.13833.png)

Vote: 4

Authors: Gary Lopez, Solianna Herrera, Thomas Portet, Bolor-Erdene Jagdagdorj, Shiven Chawla, Daniel Perez-Becker, David Majercak, Wen Wen, Blake Bullwinkel, Amit Garg, Emman Haider, Jianwen Zhang, Piyush Madan, Nina Chikanov, Shahed Warreth, Martin Pouliot, +, Dongwoo Kim, Amanda Minnich, Raja Sekhar Rao Dheekonda, Maggie Engler, Ziyi Yang, Hiteshi Sharma

- **What's New**: Microsoft가 최근에 공개한 Phi-3 시리즈의 작은 언어모델(SLM)들은 성능을 유지하면서도 메모리와 연산 비용이 낮은 방식으로 설계되었습니다. 특히 Phi-3-mini 모델은 MMLU에서 69%를 달성하며 Mixtral 8x7B 및 GPT-3.5와 경쟁력을 갖추고 있습니다.
- **Technical Details**: Phi-3 모델들의 안전성을 높이기 위해 Microsoft는 iterative한 접근 방식을 사용하여 다섯 가지 주요 단계인 Safety Dataset Curation, Safety Post-Training, Quantitative and Qualitative RAI Evaluations, AI Red Teaming, Vulnerability Identification을 통해 모델의 취약점을 지속적으로 점검하였습니다. 이를 통해 모델을 다양한 맥락에서 안전한 응답을 생성하도록 조정했습니다.
- **Performance Highlights**: 모델의 안전성을 평가하기 위해 다수의 public datasets와 Microsoft 내부 데이터를 활용하여 Phi-3 모델의 성능을 비교했습니다. 특히, Phi-3는 다양한 유해 콘텐츠를 다루는 실험에서 유의미한 결과를 보여주었으며, Mistral-7B, Gemma-7B 및 Llama-3-In 모델과 비교되었습니다. 평가 결과는 모델이 제시한 응답의 적합성 및 잠재적인 해를 기준으로 분석되었습니다.

### [SparseCraft: Few-Shot Neural Reconstruction through Stereopsis Guided Geometric Linearization](https://arxiv.org/abs/2407.14257)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14257.png)

Vote: 3

Authors: Amine Ouasfi, Adnane Boukhayma, Mae Younes

- **What's New**: SparseCraft라는 새로운 프레임워크를 소개하며, 이는 최소한의 입력으로 정확한 3D 재구성과 새로운 시점 합성을 가능하게 한다.
- **Technical Details**: 이 연구는 SDF(Signed Distance Function)와 방사선 함수를 자가 감독 방식으로 학습하고, MVS(Multi-View Stereo) 기하학과 색상 단서를 통해 학습을 정규화한다. 특히, 새로운 손실 함수인 Taylor 확장을 활용하여 비선형성을 줄임으로써 SDF 학습을 개선한다.
- **Performance Highlights**: 본 연구는 데이터 선행 학습 없이도 최첨단 성능을 달성하며, 신속한 훈련 시간으로 이전 방법들보다 우수한 시각적 결과를 보여준다.

### [PlacidDreamer: Advancing Harmony in Text-to-3D Generation](https://arxiv.org/abs/2407.13976)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.13976.png)

Vote: 3

Authors: Pengfei Wan, Yanmin Xiong, Zixuan Wang, Shikun Sun, Yuan Zhang, Xiaoyu Qin, Shuo Huang, Jia Jia, Di Zhang

- **What's New**: PlacidDreamer라는 새로운 프레임워크를 도입하여 고화질의 text-to-3D 생성을 지원합니다. 이 프레임워크는 Latent-Plane 모듈과 Balanced Score Distillation 알고리즘을 활용하여 일관된 생성 프로세스를 개선합니다.
- **Technical Details**: SlacitDreamer는 다중 뷰의 생성과 텍스트 조건의 생성을 단일 다중 뷰 확산 모델과 조화롭게 결합하며, 새로운 score distillation 알고리즘을 사용하여 균형 잡힌 포화(Over-Saturation)를 달성합니다. Latent-Plane 모듈은 빠른 기하학 재구성과 향상된 다중 뷰 이미지 생성을 가능하게 하는 기능을 포함하고 있으며, Balanced Score Distillation(BSD) 알고리즘은 최적화 방향을 동적으로 조정해서 파레토 최적 점에 수렴합니다.
- **Performance Highlights**: PlacidDreamer는 기존의 최첨단 방법들보다 최소 5포인트 이상의 성능을 발휘하여, T3Bench 벤치마크에서 생성 품질 및 정렬 지표에서 일관되게 우수한 결과를 나타냈습니다. 실험 결과, 다양한 개방형 소스 text-to-3D 프레임워크에서 BSD 알고리즘이 성능 향상에 기여했음을 입증했습니다.

### [Qalam : A Multimodal LLM for Arabic Optical Character and Handwriting Recognition](https://arxiv.org/abs/2407.13559)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.13559.png)

Vote: 2

Authors: El Moatez Billah Nagoudi, Fakhraddin Alwajih, Muhammad Abdul-Mageed, Gagan Bhatia

- **What's New**: 본 연구에서는 아랍어 필기 인식(HWR)과 광학 문자 인식(OCR) 작업을 위한 새로운 모델 아키텍처를 제안합니다. 특히 아랍어의 복잡한 diacritics를 효과적으로 처리할 수 있는 능력이 강조됩니다.
- **Technical Details**: 모델 아키텍처는 Hidden Markov Models (HMMs), CTC (Connectionist Temporal Classification) 기반, RNN (Recurrent Neural Networks), CNN-RNN, Encoder-Decoder 구조, Transformer 등을 포함합니다. 모델의 하이퍼파라미터로는 Learning Rate, Batch Size, Optimizer(Adam) 등이 있습니다. 특히, 학습률(Learning Rate)은 5×10^{-5}로 설정되었으며, 총 학습 배치 크기는 64입니다.
- **Performance Highlights**: 실험 결과, 새로운 모델은 다양한 아랍어 텍스트 및 글꼴에서 뛰어난 인식률을 보여줍니다. HWR 및 OCR 작업 모두에서 일정하게 높은 성능을 유지하며, 특히 다양한 diacritics 처리 능력이 뛰어난 것으로 나타났습니다. 예를 들어, OnlineKHATT 데이터셋에서 HWR의 인식률은 74.76%에 도달했습니다.

### [Efficient Audio Captioning with Encoder-Level Knowledge Distillation](https://arxiv.org/abs/2407.14329)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14329.png)

Vote: 2

Authors: Mark D. Plumbley, Xuenan Xu, Mengyue Wu, Haohe Liu, Wenwu Wang

- **What's New**: 이번 연구에서는 자동 오디오 캡셔닝(AAC)에서의 모델 압축 기술에 대해 다룹니다. 기존의 대형 모델들이 많은 계산 자원과 메모리를 요구함에도 불구하고, 이에 대한 연구는 거의 없었습니다. 저자들은 지식 증류(knowledge distillation, KD) 방법을 통해, AAC에서의 효율적인 압축을 위한 새로운 방법론을 제안합니다.
- **Technical Details**: 제안된 KD 프레임워크는 오디오 입력을 다루기 위해 compact encoder와 decoder를 사용하며, 두 가지 손실 함수를 통해 모델을 최적화합니다. 첫번째는 평균 제곱 오차(mean squared error, MSE) 손실로, 두번째는 대조 손실(contrastive loss)입니다. 이 방법들은 학생(학생 모델) 인코더가 교사(교사 모델)와 유사한 오디오 구분 능력을 학습하게 하는데 중점을 두고 있습니다.
- **Performance Highlights**: 제안된 모델은 기존 교사 모델의 약 6.5%의 매개변수만으로도 거의 SOTA 수준의 성능을 달성하였습니다. 또한, 데이터 부족 상황에서도 우수한 성능을 보여주며, 비주얼 데이터를 활용한 학습 또한 성능 개선에 기여했습니다.

