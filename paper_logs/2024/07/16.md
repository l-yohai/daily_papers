## Daily Papers (2024-07-16)

### [Qwen2 Technical Report](https://arxiv.org/abs/2407.10671)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10671.png)

Vote: 91

Authors: Chang Zhou, Jin Xu, Haoran Wei, Fei Huang, Baosong Yang, Guanting Dong, Jingren Zhou, Jianwei Zhang, Jianxin Ma, Chengpeng Li, Bowen Yu, Huan Lin, Jialin Wang, Jianhong Tu, Jialong Tang, Jinze Bai, Chengyuan Li, Binyuan Hui, +, Dayiheng Liu, Jian Yang, Bo Zheng, An Yang

- **What's New**: 최근 ChatGPT(2022, OpenAI)의 등장 이후 대형 언어 모델(LLM)에 대한 열기가 전 세계적으로 상승하고 있습니다. 특히 오픈소스 커뮤니티에서는 GPT 수준의 로컬 LLM에 큰 관심을 가져왔습니다. Llama 시리즈(Touvron et al., 2023)의 출시가 이러한 관심을 더욱 고조시켰습니다. 이번에는 Qwen2라는 새로운 LLM과 멀티모달 모델을 소개합니다. Qwen2는 Transformer 아키텍처(Vaswani et al., 2017)를 기반으로 하며, 다음 토큰 예측을 사용해 훈련되었습니다.
- **Technical Details**: Qwen2 모델 시리즈는 기본 언어 모델과 인스트럭션 튜닝된(instruction-tuned) 모델로 구성됩니다. 이 모델들은 0.5억, 1.5억, 7억, 72억 개의 파라미터, 그리고 57억 개의 파라미터를 가진 혼합 전문가 모델(MoE)을 포함합니다. 이를 처리하기 위해 대규모 고품질 데이터셋에서 7조 개 이상의 토큰을 사용해 사전 훈련을 진행했으며, 그 후 인간의 피드백을 학습하여 인간의 선호에 맞게 맞춤 튜닝(Supervised Fine-Tuning and Direct Preference Optimization)을 거쳤습니다.
- **Performance Highlights**: Qwen2 모델은 다양한 기준에서 경쟁 모델들을 능가했습니다. 예를 들어 Qwen2-72B-Instruct 모델은 MT-Bench(Zheng et al., 2023)에서 9.1점, Arena-Hard(Chiang et al., 2024)에서 48.1점, LiveCodeBench(Jain et al., 2024)에서 35.7점을 기록했습니다. Qwen2-72B 기본 언어 모델은 MMLU(Hendrycks et al., 2021a)에서 84.2점, GPQA(Rein et al., 2023)에서 37.9점, HumanEval(Chen et al., 2021)에서 64.6점, GSM8K(Cobbe et al., 2021)에서 89.5점, BBH(Suzgun et al., 2023)에서 82.4점을 달성했습니다.

### [Learning to Refuse: Towards Mitigating Privacy Risks in LLMs](https://arxiv.org/abs/2407.10058)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10058.png)

Vote: 19

Authors: Tong Zhu, Wenliang Chen, Chuanyuan Tan, Zhenhua Liu

- **What's New**: 새로운 연구에서는 대규모 언어 모델(LLMs)의 개인 정보 보호 이슈를 해결하기 위한 기계적인 '잊혀지기' (Machine Unlearning, MU) 접근법을 제안하고 있습니다. 본 연구는 개인의 민감 정보를 보호하는 새로운 데이터셋인 RETURN(Real-world pErsonal daTa UnleaRNing)을 개발하였습니다.
- **Technical Details**: RETURN은 2,492명의 실존 인물의 이름과 각 개인에 대한 20개의 질문-응답(QA) 쌍으로 구성되어 있습니다. 연구팀은 '이름 인식 비학습 프레임워크'(Name-Aware Unlearning Framework, NAUF)를 통해 특정 인물의 정보를 보호하려는 시도를 했습니다. NAUF는 이름 인식 거부 응답(Name-Aware Refusal Answer)과 대조적 데이터 증강(Contrastive Data Augmentation) 두 가지 핵심 요소로 구성되어 있습니다.
- **Performance Highlights**: RETURN 데이터셋을 사용한 평가 결과, 제안된 NAUF 방법이 평균 비학습 점수에서 최상의 기준 방법보다 5.65점 더 높은 성과를 기록했습니다. 이는 '잊혀야 할' 세트의 개인 정보를 보호하면서도, LLM의 다른 기능의 성능은 유지하는 데 크게 기여합니다.

### [Q-Sparse: All Large Language Models can be Fully Sparsely-Activated](https://arxiv.org/abs/2407.10969)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10969.png)

Vote: 13

Authors: Hongyu Wang, Furu Wei, Shuming Ma, Ruiping Wang

- **What's New**: Q-Sparse라는 새로운 기법을 소개합니다. Q-Sparse는 대형 언어 모델(LLMs)의 활성화(activations)에 전면적인 희소성(sparsity)을 부여하는 간단하지만 효과적인 접근법입니다. 이 접근법은 LLM의 효율성을 크게 향상시켜, 실제 응용 프로그램에서의 배포 가능성을 높여줍니다.
- **Technical Details**: Q-Sparse 아키텍처는 Transformer 구조를 기반으로 하며, 활성화 희소성을 달성하기 위한 여러 수정 사항을 포함합니다. 주요한 변경 사항은 linear projection(행렬 곱셈) 단계로, Top-K sparsification 함수를 도입하여 활성화 텐서에서 상위 K개의 활성화를 선택합니다. 역전파(backpropagation) 단계에서는 Straight Through Estimator를 사용하여 활성화의 경사도를 계산하며, 피드포워드(feed-forward) 층에서는 Squared ReLU 함수를 도입하여 활성화 희소성을 더욱 개선합니다. Q-Sparse는 정밀한(full-precision) 모델과 양자화(quantized)된 모델 모두에서 사용할 수 있습니다.
- **Performance Highlights**: Q-Sparse는 여러 설정에서 테스트되었으며, 전훈련(training-from-scratch), 기존 LLMs의 지속 훈련(continue-training), 그리고 미세 조정(finetuning)에서도 탁월한 성능을 보여줍니다. 활성화된 파라미터 수(activated parameters)나 FLOPs와 같은 같은 추론 계산 예산 하에서 희소 모델은 조밀한(dense) 모델보다 더 나은 성능을 보여주었으며, 약 40%의 희소성 비율에서 동일한 모델 크기와 훈련 토큰을 가진 조밀 모델과 비슷한 성능을 발휘했습니다. 최적의 희소성 비율은 45.58%이고, 이는 1.84 * Na의 파라미터를 가진 모델이 최고 성능을 발휘할 수 있음을 확인했습니다. 1.58-bit 모델에서는 최적의 희소성 비율이 61.25%입니다.

### [GRUtopia: Dream General Robots in a City at Scale](https://arxiv.org/abs/2407.10943)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg)

Vote: 13

Authors: Sizhe Yang, Peizhou Cao, Tai Wang, Jiangmiao Pang, Tao Huang, Jiahe Chen, Boyu Mi, Huiling Wang, Dahua Lin, Zhongying Tu, Junfeng Long, Siheng Zhao, Ying Zhao, Qingwei Ben, Jialun Li, Yilun Chen, Zirui Wang, Wenye Yu, Zichao Ye, Wensi Huang, Yu Qiao, Hanqing Wang

- **What's New**: 이 논문은 새로운 머신 러닝 모델(Machine Learning Model)을 제안합니다. 이 모델은 기존의 딥러닝(Deep Learning) 구조를 개선하여 보다 효율적인 학습을 가능하게 합니다.
- **Technical Details**: 이 모델은 트랜스포머(Transformer) 아키텍처와 결합된 컨볼루셔널 뉴럴 네트워크(Convolutional Neural Network, CNN)를 사용합니다. 또한, 새로운 손실 함수(Loss Function)와 학습률 조정(Learning Rate Adjustment) 기법을 도입하여 모델의 성능을 최적화하였습니다.
- **Performance Highlights**: 이 모델은 여러 벤치마크 데이터셋(Benchmark Datasets)에서 기존 최고 성능(SOTA, State-of-the-Art)을 능가하는 결과를 보였습니다. 특히, 이미지 분류(Image Classification)와 자연어 처리(Natural Language Processing, NLP) 분야에서 탁월한 성능을 발휘했습니다.

### [The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism](https://arxiv.org/abs/2407.10457)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10457.png)

Vote: 13

Authors: Sujian Li, Yifan Song, Bill Yuchen Lin, Guoyin Wang

- **What's New**: 이번 연구는 대형 언어 모델(LLM)의 비결정성(non-determinism) 문제를 깊이 탐구합니다. 특히, 기존에는 LLM의 평가에서 단일 출력을 기준으로 하는 경향이 있었으나, 이번 연구는 서로 다른 디코딩 설정(greedy decoding, nucleus sampling)을 비교하여 LLM의 성능과 그 변동성을 분석합니다. 이는 LLM 평가에서 새로운 패러다임을 제공할 수 있습니다.
- **Technical Details**: ['비결정성(non-determinism)은 동일한 입력에 대해 LLM이 다양한 출력물을 생성하는 현상으로, greedy decoding과 nucleus sampling 등 여러 디코딩 설정에 따라 출력물이 달라질 수 있습니다.', '연구팀은 AlpacaEval 2, Arena-Hard, WildBench v2, MixEval, MMLU-Redux, GSM8K, HumanEval 등 여러 벤치마크를 사용해 LLM의 성능을 평가했습니다.', '각 벤치마크에서 greedy decoding과 sampling 방법의 성능 차이를 비교하고, 다양한 디코딩 설정의 변동성을 분석했습니다.', '연구는 GPT-4-Turbo와 같은 상용 LLM 외에도 Llama-3-Instruct, Yi-1.5-Chat, Qwen-2-Instruct, Mistral 등 오픈소스 LLM을 포함하여 다양한 크기와 패밀리의 모델을 평가 대상으로 삼았습니다.', 'Alignement 기법(DPO, KTO, IPO, ORPO, RDPO, SimPO)을 적용한 모델들의 성능 차이와 샘플링 변동성도 분석했습니다.']
- **Performance Highlights**: ['대부분의 벤치마크에서 greedy decoding이 sampling보다 우수한 성능을 보였습니다. 그러나 AlpacaEval에서는 sampling이 높은 승률을 기록했습니다.', 'LLM의 세부적인 작업 카테고리에서 역시 greedy decoding과 sampling 간 성능 차이가 나타났습니다. 예를 들어, Arena-Hard 벤치마크에서는 Qwen2-7B가 greedy decoding에서 Llama-3-8B보다 약간 우수했으나, sampling 설정에서는 그 반대 결과가 나왔습니다.', '수학추론(math reasoning)과 코드 생성(code generation) 작업에서 sampling 변동성의 영향이 가장 컸습니다.', '특정 alignement 방법(DPO 등)이 샘플링 변동성을 크게 줄일 수 있으며, 높은 temperature 설정이 reasoning과 code generation 능력에 악영향을 미치는 반면, 높은 repetition penalty는 AlpacaEval에서 성능 향상을 가져왔습니다.', 'Best-of-N 샘플링 설정에서는 7B 수준의 LLM이 GPT-4-Turbo를 능가할 가능성이 있으며, 최신 보상 모델(reward models)을 사용하면 다중 샘플에서 우수한 응답을 선택할 수 있습니다.']

### [Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion](https://arxiv.org/abs/2407.10973)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10973.png)

Vote: 7

Authors: Tingqiang Xu, Guangqi Jiang, Huazhe Xu, Furong Huang, Kaizhe Hu, Yongyuan Liang

- **What's New**: 이번 논문에서는 전통적인 행동(replay buffer)이나 행동 시범을 사용하여 정책(policy)나 궤적 모델을 학습하는 대신, 오프라인 데이터에서 서브옵티멀(suboptimal) 궤적을 이용해 최적의 정책 네트워크 파라미터를 역으로 예측하는 새로운 접근 방식을 제안합니다.
- **Technical Details**: 이 논문의 주요 기술적 기여는 다음과 같습니다:
1) 정책 네트워크를 컴팩트한 잠재 표현 (latent representations)으로 인코딩하는 오토인코더(autoencoder)를 제안했습니다.
2) 장기 궤적과 그 성공 또는 미래 상태 간의 상호 정보를 포착하는 대조 학습(contrastive learning)을 사용하여 새로운 효율적인 행동 임베딩(behavior embedding)을 제공합니다.
3) 학습된 행동 임베딩을 조건으로 하는 확산 모델(diffusion model)을 이용해 정책 파라미터 표현을 생성하고, 이를 사전 학습된 디코더를 통해 배포 가능한 정책으로 디코딩 합니다.
4) 정책 네트워크 파라미터와 대응하는 수행된 궤적의 사전 학습 데이터셋을 구축했습니다.
- **Performance Highlights**: Make-An-Agent 시스템은 다양한 테이블탑 조작 및 실세계 이동 작업과 같은 세 가지 연속 제어 영역에서 테스트되었습니다. 결과는 기존의 멀티태스크(multi-task) 및 메타러닝(meta learning), 그리고 하이퍼네트워크(hypernetwork) 기반의 생성 방법들보다 높은 성능을 보였습니다. 특히, Make-An-Agent는 에이전트 행동 임베딩을 조건으로 다양한 크기의 정책 네트워크를 생성할 수 있는 확장성을 보였습니다. 또한, 미지의 행동이나 작업에서 고성능 정책을 생성해내는 일반화 능력과 환경의 무작위성 속에서도 강력한 성능을 유지하는 로버스트함을 보여 주목받고 있습니다.

### [Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity](https://arxiv.org/abs/2407.10387)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10387.png)

Vote: 5

Authors: Chunghsin Yeh, Ioannis Tsiamas, Santiago Pascual, Joan Serrà

- **What's New**: 최근 몇 년 동안 오디오-비주얼 크로스 모달 생성(Audio-visual cross-modal generation)이 큰 주목을 받고 있습니다. 이번 연구에서는 Masked Generative Video-to-Audio Transformer (MaskVAT)라는 새로운 V2A(video-to-audio) 시스템을 제안합니다. 이 시스템은 최신 오디오 코덱(Descript audio codec)과 마스크 기반 생성 모델링을 결합하여 다양한 멀티모달 오디오-비주얼 특성을 바탕으로 V2A 생성을 이끌어냅니다.
- **Technical Details**: MaskVAT는 전체 대역폭 오디오 코덱을 활용하여 오토리그레시브(AR) 및 마스크 기반 생성 모델이 작동하는 이산(latent) 공간에서 작업을 수행합니다. 이 모델은 시퀀스-투-시퀀스(sequence-to-sequence) 아키텍처를 사용하여 생성된 오디오의 입력 비디오와의 시간적 정렬을 보장하며, 정규화 손실(regularization loss) 및 동기화(pre-trained synchronicity features) 특성을 통합합니다. 또한, CLIP 및 CLAP 임베딩을 사용하여 멀티모달 특성을 효율적으로 매핑합니다.
- **Performance Highlights**: 이 연구에서는 세 가지 주요 성과 측면에서 MaskVAT의 성능을 조사합니다. 첫째, 전체 대역 오디오 코덱을 활용하여 생성된 오디오 품질을 극대화합니다. 둘째, 사전 학습된 모델을 연결하여 의미적 매칭을 유사 방식으로 처리합니다. 셋째, 시퀀스-투-시퀀스 모델 아키텍처를 통해 입력 비디오와 생성된 오디오의 시간적 정렬 문제를 중점적으로 다룹니다.

### [SHERL: Synthesizing High Accuracy and Efficient Memory for Resource-Limited Transfer Learning](https://arxiv.org/abs/2407.07523)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07523.png)

Vote: 4

Authors: Long Chen, Huchuan Lu, Xu Jia, Yunzhi Zhuge, Bo Wan, Ying Zhang, Haiwen Diao

- **What's New**: 최근 비전, 언어, 멀티모달 도메인에서 대규모 사전 훈련된 모델들이 뛰어난 일반화와 표현 능력을 선보였습니다. 그러나 이러한 모델의 전면 미세 조정(fine-tuning)은 비용이 너무 많이 들기 때문에 파라미터 효율적 전이 학습(parameter-efficient transfer learning, PETL) 전략이 증가하고 있습니다. 이 논문에서는 새로운 튜닝 패러다임인 SHERL을 제안합니다.
- **Technical Details**: SHERL은 초기 집계를 통해 교차 계층 피처 간의 유사성 비율을 정량화한 다음, 교차 계층 중복의 부정적 간섭을 최소화하기 위해 집계 가중치를 정규화하는 메커니즘을 적용합니다. 이후 결과 표현은 사후 bare 사전 훈련된 레이어와 호환성을 극대화하여, 'maximalist' 사전 훈련된 지식을 'minimalist' 훈련 메모리 오버헤드 아래 유지합니다.
- **Performance Highlights**: SHERL은 이미지-텍스트 검색, 비디오-텍스트 검색, 시각적 질문 응답, 조합적 질문 응답, 시각적 그라운딩 등의 다섯 가지 도전적인 비전-언어 과제와 GLUE 벤치마크를 포함한 여러 순수 언어 작업에서 평가되었습니다. 기존 PETL 변종들과 비교하여 훈련 비용이 최소화되면서도 최적의 성능과 효율성을 보였습니다. 또한, SHERL은 다른 PETL 방법들과 원활하게 협력할 수 있으며 다양한 네트워크 백본(CNN, 단일 또는 크로스 모달 Transformer, T5 또는 MDETR 같은 Encoder-Decoder 아키텍처)에서 좋은 적용성을 보입니다.

### [Video Occupancy Models](https://arxiv.org/abs/2407.09533)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09533.png)

Vote: 4

Authors: Philippe Hansen-Estruch, Alex Lamb, Matthew E. Taylor, Philip Bachman, Sergey Levine, Manan Tomar, John Langford

- **What's New**: 본 논문은 Video Occupancy Models (VOCs)라는 새로운 생성 모델을 제안합니다. 이 모델은 관찰의 표현을 기반으로 할인된 미래 분포를 예측합니다. 이를 통해 픽셀 단위 예측을 피하고, 특정 시간 단계에서 예측하지 않도록 설계되었습니다. 이 모델은 높은 차원의 영상 데이터에서도 효율적으로 작동합니다.
- **Technical Details**: VOCs는 자가 회귀 변환기(autoregressive transformer) 아키텍처를 사용하여 잠재 표현 공간에서 작동합니다. 이는 양자화 인코더(VQ-VAEs), 역동학 모델링(inverse dynamics modeling), 그리고 자가 지도 학습(self-supervised distillation) 기법을 사용해 설계됩니다. 특히, GPT-2 모델을 사용하여 시퀀스 토큰을 자가 회귀적으로 예측합니다.
- **Performance Highlights**: VOCs는 낮은 수준의 픽셀 예측 없이 시간 단계마다 예측하지 않는 방식으로 환경의 동적 정보를 효율적으로 캡처합니다. 이는 후속의 제어 작업에서 우수한 성능과 빠른 예측 역량을 보여줍니다.

### [DataDream: Few-shot Guided Dataset Generation](https://arxiv.org/abs/2407.10910)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10910.png)

Vote: 3

Authors: Stephan Alaniz, Jae Myung Kim, Zeynep Akata, Cordelia Schmid, Jessica Bader

- **What's New**: 이번 연구에서는 Stable Diffusion을 활용한 text-to-image 생성 모델이 이미지 분류 작업에 미치는 영향을 분석하고, 이를 개선하기 위한 새로운 접근법인 DataDream을 제안합니다. DataDream은 소수의 실제 데이터를 사용하여 생성모델을 조정함으로써, 하위 작업 학습에 더욱 유용한 실내(인-디스트리뷰션) 이미지를 생성할 수 있도록 합니다.
- **Technical Details**: DataDream은 Stable Diffusion 모델을 LoRA (Low-Rank Adaptation) 기법으로 두 가지 방식으로 조정합니다: DataDreamcls와 DataDreamdset. DataDreamcls는 클래스별로 LoRA를 학습하고, DataDreamdset는 모든 클래스를 위해 하나의 LoRA를 학습합니다. 이 방식은 기존의 방법들이 동결된 사전학습 생성 모델을 활용하는 것과 달리, 생성모델 자체를 조정하는 첫 번째 시도입니다. 본 연구에서는 몇 샷 샘플을 생성 프롬프트에 포함하여 실제 데이터와 합성 데이터의 분포 정렬을 향상합니다.
- **Performance Highlights**: DataDream은 다양한 데이터셋에서 실험을 통해 우수한 성능을 입증하였으며, 전적으로 합성 데이터를 사용하여 몇 샷 분류에서 최첨단 성능을 달성했습니다. 특히, 10개의 데이터셋 중 7개에서 최고 성능을 기록했습니다. 또한, 실제 몇 샷 데이터와 합성 데이터를 함께 사용했을 때, 데이터 분포 정렬 측면에서 기존 방법보다 높은 일치도를 나타냈습니다. 이를 통해 더 많은 합성 데이터 포인트와 실제 샘플을 추가함으로써 데이터셋의 확장 가능성도 탐구하였습니다.

### [LAB-Bench: Measuring Capabilities of Language Models for Biology Research](https://arxiv.org/abs/2407.10362)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10362.png)

Vote: 3

Authors: Siddharth Narayanan, Samuel G. Rodriques, Joseph D. Janizek, Andrew D. White, Michael Ruzo, Jon M. Laurent, Michael J. Hammerling, Manvitha Ponnapati, Michaela M. Hinks

- **What's New**: 최근 몇 년간 대형 언어 모델(LLM)과 이와 관련된 AI 보조 시스템은 모델 크기와 애플리케이션 확장에서 엄청난 성장을 보였습니다. 이와 같은 발전을 통해 일반적인 모델도 명시적으로 학습되지 않은 복잡한 문제를 해결하는 능력을 나타냈습니다. 이러한 LLM을 생물학 연구 분야에서 실질적인 작업을 수행하도록 평가하는 것을 목표로 한 새로운 벤치마크, 'Language Agent Biology Benchmark(LAB-Bench)'를 소개합니다.
- **Technical Details**: LAB-Bench는 2,400개 이상의 객관식 질문을 포함하고 있으며, 생물학 연구 전반에 걸쳐 보편적인 중요한 실질적인 연구 과제를 다루고 있습니다. LAB-Bench는 다양한 작업 범주로 나뉩니다. 예를 들어, 문헌 회상 및 추론(LitQA2, SuppQA), 그림 해석(FigQA), 표 분석(TableQA), 데이터베이스 접근(DbQA), 프로토콜 작성(ProtocolQA), DNA 및 단백질 서열 이해 및 조작(SeqQA, CloningScenarios) 등이 있습니다.
- **Performance Highlights**: LAB-Bench의 성능 평가 결과, 최첨단 상용 및 오픈 소스 모델의 성능을 생물학 박사급 연구원의 성능과 비교한 결과들이 포함되어 있습니다. 주요한 결과로는 특정 하위 작업에 대한 후보 질문을 생성하는 효과적인 전략을 식별했고, 모델이 고품질 질문을 쉽게 대답하지 못하도록 하는 접근 방식을 개발했습니다.

### [Noise Calibration: Plug-and-play Content-Preserving Video Enhancement using Pre-trained Video Diffusion Models](https://arxiv.org/abs/2407.10285)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10285.png)

Vote: 3

Authors: Menghan Xia, Xiaodong Cun, Haoxin Chen, Qinyu Yang, Yong Zhang, Zhixun Su, Ying Shan

- **What's New**: 최근 확산 모델(diffusion models)이 기존의 생성적 적대 신경망(GANs) 및 변분 오토인코더(VAEs)와는 다른 유형의 생성 모델로 급부상하였습니다. 특히, 텍스트-비디오(T2V) 확산 모델을 활용한 비디오 품질 향상에 대한 연구가 진전을 이루고 있습니다. 이 연구는 새로운 비디오 품질 향상 방법론을 제시하여 원본 비디오의 내용과 일관성을 유지하면서 품질을 향상시킬 수 있는 새로운 해결책을 소개하고 있습니다.
- **Technical Details**: 연구에서는 노이즈 도입 및 제거를 통해 예측된 고품질 비디오를 만들고, 이를 통해 원본 비디오의 품질을 향상시킵니다. 내용 일관성을 보장하기 위해 추가적인 손실 함수(content loss)를 제안하며, 이 함수를 통해 개선된 비디오와 원본 비디오 간의 내용 간극을 최소화합니다. 도입된 기술은 '노이즈 캘리브레이션(Noise Calibration)'이라 불리며, 초기 랜덤 노이즈를 1-3회만 정제하여 원본 비디오의 내용을 보존하면서도 품질을 크게 향상시킬 수 있습니다.
- **Performance Highlights**: 실험 결과, 제안된 방법론인 Noise Calibration이 기존의 SDEdit 기반 비디오 향상 작업에 효과적으로 적용되어 더 제어 가능한 이미지/비디오 생성이 가능함을 입증했습니다. 또한 내용 보존 성능과 비디오 품질 향상 사이의 균형을 잘 유지하여, SOTA(state-of-the-art) 시각적 정제 모델의 성능을 효과적으로 강화할 수 있는 플러그인으로 활용될 수 있습니다.

### [LLM Circuit Analyses Are Consistent Across Training and Scale](https://arxiv.org/abs/2407.10827)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10827.png)

Vote: 2

Authors: Michael Hanna, Curt Tigges, Stella Biderman, Qinan Yu

- **What's New**: 이 논문에서는 LLMs(대규모 언어 모델)의 메커니즘을 설명하는 서킷(circuits)을 탐구합니다. 특히, 모델의 훈련 중에 서킷과 그 구성 요소들이 언제 어떻게 발생하는지, 그리고 훈련 및 다양한 모델 스케일에서의 일관성을 연구했습니다. Pythia suite 모델을 사용하여 7000만에서 28억 파라미터에 걸친 모델들을 연구했으며, 결과적으로 서킷과 그 속성이 스케일과 훈련 전반에 걸쳐 일관되게 나타난다는 것을 발견했습니다.
- **Technical Details**: 이 논문에서는 서킷(circuits)을 모델이 주어진 작업을 수행하는 데 사용되는 최소한의 컴퓨팅 서브 그래프로 정의합니다. 대명사 식별 작업(IOI; Wang et al., 2023)에서 모델이 'John이 Mary에게 드링크를 주었다'라는 입력에 대해 올바른 출력을 하는지를 평가합니다. 서킷의 충실도를 검증하기 위해, 서킷 외부 간선을 파괴하여 모델의 행동을 관찰합니다. 중요한 방법론으로는 edge attribution patching with integrated gradients (EAP-IG; Hanna et al., 2024)을 사용하여 서킷을 식별했습니다. 이 방법은 모델의 모든 간선 중요도를 평가하는 데 몇번의 forward와 backward pass만 필요합니다.
- **Performance Highlights**: 연구 결과, 다양한 모델 스케일에서도 서킷의 특성이 일관되게 발생하며, 이전 연구에서 발견된 바와 같이 유도 헤드는 모든 스케일의 모델에서 20억에서 50억 토큰 사이에서 발생합니다. 또한, 서킷 알고리즘은 구성 요소가 변하더라도 안정적으로 유지되며, 이는 다양한 모델 스케일에서도 해당 서킷이 일반화될 수 있다는 것을 시사합니다. 이 결과는 작은 모델에서 훈련 완료 후 연구된 서킷이 더 큰 모델 또는 더 긴 훈련 기간을 가진 모델에서도 유용할 수 있음을 나타냅니다.

### [MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with Open-domain Information Extraction Large Language Models](https://arxiv.org/abs/2407.10953)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10953.png)

Vote: 2

Authors: Hexiang Huang, Shijian Wang, Xinyang He, Younghun Lim, Hanjun Wei, Chengguang Gan, Tatsunori Mori, Yunhao Liang, Qinghao Zhang, Shiwen Ni, Qingyu Yin

- **What's New**: 논문에서는 정보 추출(IE) 작업에서 각기 다른 세부 과제들 간의 상호 연결성을 탐구하는 새로운 연구 방향인 상호 강화 효과(Mutual Reinforcement Effect, MRE)를 소개합니다. MRE는 텍스트 수준의 과제와 단어 수준의 과제로 IE 세부 과제를 분류하고, 이를 통해 각각의 과제 성능을 향상시키는 방법론을 제시합니다. 또한, 다양한 언어로 번역된 다국어 MRE 믹스 데이터셋(MMM)도 새롭게 개발되었습니다.
- **Technical Details**: 이 논문에서는 기존 대형 언어 모델(LLM)보다 성능이 떨어지는 문제를 해결하고자 새로운 입력-출력 체계를 개발했습니다. 이를 통해 정보 추출 과제의 성능을 최적화했습니다. 데이터를 번역하는 새로운 프레임워크를 제안하여 부족한 언어 데이터를 수집하고, 다국어 MRE 믹스 데이터셋을 구성합니다. 또한, 새롭게 개방된 도메인 NER 데이터셋을 포함하여 MRE 믹스 데이터셋을 확장했습니다. 이는 모델의 훈련 성능을 높이는 데 중요한 역할을 합니다.
- **Performance Highlights**: 최적화된 LLM(OIELLM)은 다양한 데이터셋에서 기존 훈련 방법보다 우수한 성능을 보여주었습니다. MRE 믹스 데이터셋을 확장하여 더 많은 데이터를 활용함으로써 모델의 지식 활용을 극대화하고, 전체 업무 성능을 향상시켰습니다.

### [Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation](https://arxiv.org/abs/2407.10817)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10817.png)

Vote: 1

Authors: Chris Tar, Tu Vu, Yun-Hsuan Sung, Salaheddin Alzubi, Kalpesh Krishna, Manaal Faruqui

- **What's New**: 대규모 언어 모델(LLM)의 발전에 따라 이들의 장문 출력(long-form output)을 정확히 평가하는 방법이 큰 도전 과제로 떠오르고 있습니다. 최근 연구는 LLM 자체를 자동 평가자(autorater)로 사용하는 가능성을 제시합니다. 이를 위해, 이전 연구에서 수집한 인간 평가 데이터를 이용하여 다양한 평가 작업을 수행할 수 있는 FLAMe라는 데이터 컬렉션을 구축했습니다. 이 데이터는 100개 이상의 평가 작업과 530만 개 이상의 인간 판단을 포함합니다.
- **Technical Details**: FLAMe 데이터 컬렉션은 공개적으로 이용 가능한 인간 평가 데이터를 표준화하고 통합한 것으로, 기계 번역 품질 평가부터 AI 어시스턴트의 사용자 지시 준수도 평가까지 다양한 작업을 포함합니다. 데이터 수집 과정에서는 각 데이터셋의 모호성을 해결하기 위해 원 저자와의 상담도 했습니다. 이러한 데이터에 기반하여 LLM 자동 평가자는 여러 작업에서 원활한 전이 학습(transfer learning)을 수행할 수 있도록 텍스트-텍스트 형식으로 변환된 평가 작업 예제를 활용하여 지도 멀티태스크(fine-tuning) 방식으로 훈련되었습니다. 이를 통해, PaLM-2-24B와 같은 LLM을 훈련시켜 다양한 측정 작업에서 우수한 성능을 보였습니다.
- **Performance Highlights**: FLAMe 데이터 컬렉션으로 훈련된 PaLM-2-24B 모델은 기존의 여러 유명한 자동 평가자 모델들(GPT-4, Claude-3, Llama-3)을 능가하는 성과를 보여줬습니다. 특히, 보상 평가(Reward Modeling) 작업에서는 FLAMe-RM 모델이 RewardBench에서 정확도 87.8%를 기록하며 GPT-4 모델을 앞질렀습니다. 또한, FLAMe-Opt-RM 모델의 경우 최적화된 멀티태스크 혼합 데이터를 사용해 5000걸음만에 경쟁력 있는 성과를 거두었으며, 이는 기존보다 약 25배 적은 훈련 데이터로 이루어진 결과입니다.

### [Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?](https://arxiv.org/abs/2407.10956)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10956.png)

Vote: -

Authors: Hongcheng Gao, Haoyuan Wu, Ruoxi Sun, Kai Yu, Jixuan Chen, Ansong Ni, Qian Liu, Xinzhuang Xiong, Lu Chen, Caiming Xiong, Wenjing Hu, Victor Zhong, Danyang Zhang, Sida Wang, Ruisheng Cao, Yuchen Mao, Tianbao Xie, Hongshen Xu, +, Pengcheng Yin, Hanchong Zhang, Fangyu Lei, Yeqiao Fu

- **What's New**: 본 논문에서는 새로운 딥러닝 기반 모델을 제안하여 자연어 처리(Natural Language Processing, NLP) 분야에서의 문제를 해결하고자 합니다. 특히, 기존 모델들 대비 성능 개선을 목표로 하는 혁신적인 알고리즘을 소개합니다.
- **Technical Details**: 제안된 모델은 Transformer 아키텍처의 변형을 기반으로 하며, self-attention 메커니즘을 개선하여 더 효율적인 텍스트 처리와 문맥 이해를 가능하게 합니다. 또한, 모델의 학습 속도를 높이기 위해 고급 최적화 기법을 도입하였습니다. 이 모델은 BERT와 GPT-3와 같은 기존 모델들과의 비교 실험을 통해 우수한 성능을 보였습니다.
- **Performance Highlights**: 실험 결과, 제안된 모델은 다양한 벤치마크 데이터셋에서 최고 성능을 기록하였습니다. 특히, 텍스트 분류와 자연어 생성(Natural Language Generation) 태스크(task)에서 기존 선도 모델들보다 높은 정확도와 효율성을 보여주었습니다. 예를 들어, GLUE 벤치마크에서는 기존 최고 성능을 2% 이상 상회하는 결과를 달성하였습니다.

