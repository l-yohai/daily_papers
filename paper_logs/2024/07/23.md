## Daily Papers (2024-07-23)

### [NNsight and NDIF: Democratizing Access to Foundation Model Internals](https://arxiv.org/abs/2407.14561)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14561.png)

Vote: 26

Authors: Francesca Lucchetti, Can Rager, Alexander R Loftus, Aaron Mueller, Jonathan Bell, Jaden Fiotto-Kaufman, Samuel Marks, Eric Todd, Byron Wallace, Arjun Guha, Adam Belfki, Caden Juang, David Bau, Michael Ripa, Carla Brodley, Jannik Brinkmann, Sumeet Multani, Nikhil Prakash, Arnab Sen Sharma, Koyena Pal

- **What's New**: 본 연구에서는 AI 연구의 두 가지 주요 과제를 해결하기 위해 NNsight 라이브러리와 National Deep Inference Fabric (NDIF)를 소개합니다. 이들은 투명한 모델 접근 및 적절한 계산 자원의 부족 문제를 해결하려는 목표를 가지고 있습니다.
- **Technical Details**: NNsight는 PyTorch 기반 모델에 대한 투명한 접근을 제공하며, 사용자 코드로 모델 내부를 수정하고, 활성화 및 경량 알고리즘을 검사할 수 있는 체계적인 API를 제공합니다. NDIF는 원격 NNsight 요청을 지원하는 서비스로, 사용자가 대규모 모델 인스턴스를 공유하고 활용할 수 있게 합니다.
- **Performance Highlights**: NNsight와 NDIF는 오픈 소스 프로젝트로서, 대규모 모델을 연구하기 위한 서버리스 인프라를 제공하며, 사용자에게는 대형 모델을 테스트하고 디버깅할 수 있는 동일한 경험을 제공합니다. NDIF는 여러 사용자가 동시에 대규모 모델을 공유하여 고비용의 자원 할당 없이 효율적인 연구를 가능하게 합니다.

### [Knowledge Mechanisms in Large Language Models: A Survey and Perspective](https://arxiv.org/abs/2407.15017)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15017.png)

Vote: 22

Authors: Yong Jiang, Mengru Wang, Yunzhi Yao, Ziwen Xu, Fei Huang, Ningyu Zhang, Pengjun Xie, Xiang Chen, Shumin Deng, Huajun Chen, Jia-Chen Gu, Shuofei Qiao, Peng Wang

- **What's New**: 이 논문은 LLMs(대형 언어 모델)에서의 지식 메커니즘을 전체 지식 생애 주기 전체에 걸쳐 혁신적으로 검토하였습니다. 또한 LLM에서의 지식 활용과 진화 관련하여 새로운 분류 체계를 제안합니다.
- **Technical Details**: 이 논문에서는 섹션을 통해 지식 활용 메커니즘에 대한 새로운 관점을 제공하고, 지식 진화를 위한 기본 원리를 탐구합니다. LLM의 지식 활용은 특정 시점의 정적 지식에 초점을 맞추고, 지식 진화는 개별 및 집단 LLM의 장기 동적 발전을 조사합니다.
- **Performance Highlights**: 지식의 활용에서는 기억화(Memorization), 이해 및 적용(Comprehension & Application), 그리고 창출(Creation) 과정을 다루며, 각 과정에서 사용되는 다양한 지식 유형을 분류하고 있습니다. 지식 진화 섹션에서는 개별 LLM의 진화 경로와 이를 위한 사전 훈련 및 후 훈련의 중요성을 강조하고 있습니다.

### [SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models](https://arxiv.org/abs/2407.15841)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15841.png)

Vote: 18

Authors: Haiming Gang, Kai Kang, Zhe Gan, Afshin Dehghan, Mingze Xu, Mingfei Gao, Hong-You Chen, Zhengfeng Lai

- **What's New**: 본 연구에서는 훈련이 필요 없는 비디오 대형 언어 모델 SlowFast-LLaVA(SF-LLaVA)를 제안합니다. 이는 LLaVA-NeXT를 기반으로 하며, 기존 SFT 비디오 모델의 단점인 프레임 수의 제한과 적절한 시간 모델링 설계의 부족을 해결합니다.
- **Technical Details**: SlowFast-LLaVA는 두 개의 경로(Slow and Fast)를 사용하여 세밀한 공간 의미와 장기적인 시간 맥락을 캡처합니다. Slow 경로는 낮은 프레임 속도에서 고해상도 공간 세부 사항을 유지하며, Fast 경로는 높은 프레임 속도에서 빠른 공간 풀링을 수행하여 움직임 단서를 집약합니다. 프레임 특징은 시각 인코더를 통해 독립적으로 추출되고, 두 경로를 통해 LLM으로 입력됩니다.
- **Performance Highlights**: SF-LLaVA는 오픈 엔디드 비디오 QA, 다중 선택 비디오 QA, 텍스트 생성과 같은 3가지 비디오 작업에서 8가지 벤치마크를 사용하여 광범위한 평가를 수행하였으며, 기존의 훈련 없는 방법들과 비교해 뚜렷한 우위를 보였고, 전문적으로 조정된 SFT 모델의 성능과 동등하거나 더 나은 결과를 기록하였습니다.

### [Compact Language Models via Pruning and Knowledge Distillation](https://arxiv.org/abs/2407.14679)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14679.png)

Vote: 15

Authors: Sharath Turuvekere Sreenivas, Bryan Catanzaro, Raviraj Joshi, Mostofa Patwary, Saurav Muralidharan, Marcin Chochowski, Pavlo Molchanov, Mohammad Shoeybi, Jan Kautz

- **What's New**: 이 논문은 LLM(대형 언어 모델)의 구조적 프루닝(structured pruning)과 재교육(retraining)에 대한 철저한 경험적 탐색을 제공합니다. 특히, 원래의 훈련 데이터의 소량만을 사용하여 대형 모델에서 작은 모델을 훈련하고 더 높은 정확성을 이끌어낼 수 있는 방법을 제안합니다.
- **Technical Details**: 모델 프루닝은 뉴런, 다중 헤드 주의(multi-head attention) 레이어, 임베딩 채널 및 모델 깊이를 포함하는 여러 축(axis)에서 이루어집니다. 프루닝 기법으로는 뉴런 프루닝, 주의 헤드 프루닝, 합성곱 필터 및 깊이 프루닝 등이 있습니다. 이 과정은 각 축에 대한 중요도를 계산한 후, 이를 바탕으로 프루닝된 모델을 생성하는 방식으로 진행됩니다.
- **Performance Highlights**: Minitron 모델은 Nemotron-4 15B 모델에서 직접 프루닝하여 생성된 모델입니다. Minitron 8B는 40배 적은 훈련 토큰을 사용하여 Nemotron-3 8B보다 더 나은 정확성을 달성했으며, LLaMa-2 7B 및 Mistral-7B와 유사한 성능을 또 다른 다양한 크기의 모델들과 비교하여 훌륭한 성능을 입증했습니다.

### [POGEMA: A Benchmark Platform for Cooperative Multi-Agent Navigation](https://arxiv.org/abs/2407.14931)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14931.png)

Vote: 12

Authors: Aleksandr Panov, Konstantin Yakovlev, Anatolii Borzilov, Anton Andreychuk, Alexander Chernyavskiy, Alexey Skrynnik

- **What's New**: 이번 논문에서는 멀티 에이전트 강화 학습(MARL)에 대한 새로운 환경 및 평가 도구인 POGEMA를 소개합니다. POGEMA는 여러 변종의 멀티 로봇 내비게이션 문제를 지원하는 빠르고 유연한 환경을 제공하며, 문제 인스턴스 생성을 위한 생성기와 성능 정보를 시각화할 수 있는 도구를 포함하고 있습니다.
- **Technical Details**: 멀티 에이전트 경로 찾기 문제(Multi-agent Pathfinding problem)와 같은 고차원 문제를 다루는 POGEMA는 주로 강화 학습(Reinforcement Learning)과 모방 학습(Imitation Learning) 같은 최신 기계 학습 기법을 활용하여 전통적인 해결책의 효율성을 증가시키는 데 초점을 맞추고 있습니다. 이 환경은 Python으로 구현되어 있으며, 하드웨어에 독립적이고 수천 개의 에이전트가 넘는 확장성을 제공합니다.
- **Performance Highlights**: POGEMA를 통해 얻어진 성능 평가 기준은 성공률(success rate)과 경로 길이(path length) 같은 지표를 포함하여 공정하게 비교할 수 있는 평가 프로토콜을 정의합니다. 이를 통해 최신 MARL 기법과 전통적인 탐색 기반 방법들 간의 비교 결과가 제시되어, 다양한 기술 간의 성능 차이를 명확히 분석할 수 있습니다.

### [LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding](https://arxiv.org/abs/2407.15754)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15754.png)

Vote: 12

Authors: Bei Chen, Junnan Li, Dongxu Li, Haoning Wu

- **What's New**: 본 연구에서는 LongVideoBench라는 새로운 비디오 이해 벤치를 도입하였습니다. 이는 1시간 길이의 자막이 포함된 비디오에서 LMMs(Long-context Multimodal Models)의 진행 상황을 측정하는 benchmark입니다.
- **Technical Details**: LongVideoBench는 3,763개의 비디오에 대해 6,678개의 다중 선택 질문을 포함하고 있으며, 각 질문은 특정 비디오 맥락을 참조하는 'referring reasoning' 작업에 기반합니다. 이 작업은 LMM이 특정 멀티모달 세부 정보를 인식하고 다루는 능력을 평가합니다. 질문은 두 가지 수준으로 구분되며, (L1) 지각(perception)과 (L2) 관계(relation)로 나뉘어져 있습니다.
- **Performance Highlights**: LongVideoBench는 단일 프레임 편향 문제를 해소하기 위해 설계되었으며, 모델이 더 많은 프레임을 효과적으로 처리할 수 있을 때 성능이 일관되게 향상됨을 발견하였습니다. 기존 모델들에서 나타나는 결함에 대한 깊이 있는 통찰을 제공하며, 멀티모달 긴 맥락 이해에 대한 향후 연구 방향을 제시합니다.

### [VideoGameBunny: Towards vision assistants for video games](https://arxiv.org/abs/2407.15295)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15295.png)

Vote: 12

Authors: Cor-Paul Bezemer, Mohammad Reza Taesiri

- **What's New**: 이번 연구에서는 reinforcement learning (강화 학습) 알고리즘을 사용하여 여러 고전적인 게임에서 AI의 성능을 향상시키는 방법에 대해 다룹니다. 특히, 새로운 training (훈련) 방법론과 생성된 data (데이터)의 중요성을 강조합니다.
- **Technical Details**: 저자들은 다양한 neural network architectures (신경망 구조)를 실험하여 agent (에이전트)가 복잡한 환경에서 효과적으로 학습하도록 합니다. 강화 학습의 기존 접근 방식과 비교하여 sample efficiency (샘플 효율성)와 convergence speed (수렴 속도)의 개선을 보여줍니다.
- **Performance Highlights**: 이 연구는 여러 benchmark games (벤치마크 게임)에서 state-of-the-art (최신 기술) 성능을 달성하였으며, 특히 multi-agent environments (다중 에이전트 환경)에서의 협력적인 학습 과정이 두드러진 성과를 나타냈습니다.

### [BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis in Large-scale Scenes](https://arxiv.org/abs/2407.15848)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15848.png)

Vote: 11

Authors: Yu-Lun Liu, Chih-Yao Hu, Jie-Ying Lee, Shr-Ruei Tsai, Chih-Hai Su, Chin-Yang Lin

- **What's New**: 본 연구에서는 BoostMVSNeRFs라는 파이프라인을 제안하여 대규모 및 무한한 장면에서의 렌더링 품질을 개선합니다. 이 방법은 모든 MVS 기반 NeRF와 호환 가능하며, 새로운 시각을 합성하기 위해 3D 가시성 점수를 사용하여 각 입력 뷰의 기여도를 나타냅니다.
- **Technical Details**: BoostMVSNeRF에서는 3D 가시성 점수를 샘플링하여 2D 가시성 마스크로 렌더링하여 각 비용 볼륨의 기여도를 결정합니다. 또한, 여러 비용 볼륨을 조합하여 새로운 뷰 포트의 커버리지를 확장하고 아티팩트를 줄이도록 합니다. 이를 통해 우리가 제안하는 알고리즘은 최적의 지원 비용 볼륨 집합 선택을 근사합니다.
- **Performance Highlights**: Free 및 ScanNet 데이터셋을 사용한 실험 결과, 제안한 방법이 다른 개별 장면 훈련 접근 방식이나 일반화된 NeRF에 비해 우수한 성능을 보였습니다. 특히, 자유 카메라 경로와 무제한 외부 장면에서 렌더링 품질을 개선하였고, 이는 실제 응용 사례에서 중요한 장점으로 작용합니다.

### [BOND: Aligning LLMs with Best-of-N Distillation](https://arxiv.org/abs/2407.14622)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14622.png)

Vote: 6

Authors: Geoffrey Cideron, Léonard Hussenot, Johan Ferret, Sarah Perrin, Aliaksei Severyn, Andrea Michi, Pier Giuseppe Sessa, Olivier Bachem, Nikola Momchev, Amélie Héliou, Nino Vieillard, Abe Friesen, Robert Dadashi, Sabela Ramos, Alexandre Ramé, Piotr Stanczyk, Matt Hoffman, Bobak Shariari, Sertan Girgin, Danila Sinopalnikov

- **What's New**: 이 논문에서는 BOND(Best-of-N Distillation)이라는 새로운 RLHF 알고리즘을 제안하여 Best-of-N 샘플링의 강력한 성능을 단일 샘플로 추구할 수 있는 방법을 제공합니다.
- **Technical Details**: BOND 접근법은 두 단계로 구성됩니다. 첫째, Best-of-N 분포의 해석적 표현을 도출하고, 둘째로 이 문제를 분포 일치 문제로 표현하여 정책을 Best-of-N 분포에 더 가깝게 조정합니다. 이를 통해 KL divergence를 최소화하는 여러 방법론을 제시하였습니다.
- **Performance Highlights**: BOND와 J-BOND 알고리즘은 abstractive summarization XSum 작업에서 긍정적인 결과를 보여 주었으며, Gemma 정책에 적용하여 전통적인 RL 알고리즘과 비교했을 때 더 높은 KL-reward Pareto 프론트를 달성하는 성과를 보였습니다.

### [HoloDreamer: Holistic 3D Panoramic World Generation from Text Descriptions](https://arxiv.org/abs/2407.15187)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15187.png)

Vote: 6

Authors: Li Yuan, Wangbo Yu, Xinhua Cheng, Yonghong Tian, Haiyang Zhou

- **What's New**: HoloDreamer라는 새로운 프레임워크를 소개합니다. 이 방법은 텍스트 프롬프트로부터 강력한 렌더링 견고성을 갖춘, 완전하게 폐쇄적이고 뷰 일관성 있는 3D 장면을 생성합니다.
- **Technical Details**: HoloDreamer는 스타일화된 구형 판노라마 생성(Stylized Equirectangular Panorama Generation) 방법을 활용하여 고해상도 360도 구형 판노라마를 직접 생성하고, 3D Gaussian Splatting(3D-GS)을 사용하여 장면의 세밀한 디테일을 표현합니다. 또한, 다각적 제약을 통한 3D 장면 복원 및 결함 보완을 위해 두 단계의 최적화 과정을 도입합니다.
- **Performance Highlights**: HoloDreamer는 텍스트 기반 입력에 대해 높은 뷰 일관성과 몰입감을 갖춘 3D 장면을 생성하며, 실험 결과 다른 텍스트 기반 3D 장면 생성 방법보다 시각적 일관성과 조화, 복원 품질, 렌더링 견고성에서 우수한 성과를 보였습니다.

### [AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?](https://arxiv.org/abs/2407.15711)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15711.png)

Vote: 5

Authors: Chaitanya Malaviya, Ori Yoran, Ben Bogin, Samuel Joseph Amouyal, Ofir Press, Jonathan Berant

- **What's New**: 이 논문에서는 AssistantBench라는 새로운 벤치마크를 소개합니다. 이 벤치마크는 214,214,214개의 다양하고 현실적인 작업으로 구성되어 있어 웹 에이전트가 전체 웹을 탐색하고 실제 소요 시간이 긴 작업을 해결할 수 있는 능력을 평가할 수 있습니다.
- **Technical Details**: AssistantBench의 작업은 사용자가 웹에서 검색할 수 있는 정보 요구 사항에 기반하고 있으며, 에이전트는 자율적으로 웹을 탐색하고 관련 웹 페이지와 동적으로 상호작용하여 결과를 생성해야 합니다. 연구진은 전문가와 일반 사용자가 제공한 실시간 정보 탐색 작업으로부터 이 작업을 수집했습니다.
- **Performance Highlights**: SPA(SeePlanAct)라는 새로운 에이전트 변형을 도입하여 기존의 SeeAct 모델보다 약 7,777점 더 높은 정확도로 질문에 답변할 수 있음을 보여주었습니다. 전체 모델의 성능이 낮고, 최고 정확도는 25명 점이었습니다. 연구 결과, 전문가가 제공한 작업이 가장 도전적이며, 웹 탐색 중 오류가 빈번히 발생하는 것으로 나타났습니다.

### [Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models](https://arxiv.org/abs/2407.15642)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15642.png)

Vote: 5

Authors: Yu Qiao, Cunjian Chen, Yaohui Wang, Gengyu Jia, Xinyuan Chen, Yuan-Fang Li, Xin Ma

- **What's New**: Cinemo는 정적인 이미지를 애니메이션으로 변환하는 모델로서 이미지 일관성과 모션 제어 능력을 동시에 향상시킬 수 있도록 설계되었습니다. 이 모델은 기존의 T2V(Text-to-Video) 모델을 기반으로 하며, 모션 잔차의 분포를 학습하는 새로운 전략을 도입하여 이미지의 세부 정보를 보존할 수 있도록 합니다.
- **Technical Details**: Cinemo는 기존 T2V 모델에서 노이즈된 모션 잔차에 입력 정적 이미지의 외관 정보를 추가하고 이를 입력으로 사용하여 모션 잔차 예측을 효과적으로 유도합니다. 또한, SSIM(Structural Similarity Index)을 활용하여 모션 강도를 세밀하게 조절할 수 있는 효과적인 전략을 도입합니다. DCTInit라는 전략은 입력 정적 이미지의 저주파수 대칭 코사인 변환 계수를 사용하여 초기 추론 노이즈를 조정합니다.
- **Performance Highlights**: Cinemo 모델은 이미지 일관성과 모션 제어 능력에서 타 모델 대비 최신 성능을 입증합니다. 이 모델은 애니메이션 비디오를 생성할 때 입력 이미지의 세부 정보를 잘 유지하며, 다양한 응용 프로그램(예: 비디오 편집, 모션 전이)으로 쉽게 확장될 수 있습니다.

### [Conditioned Language Policy: A General Framework for Steerable Multi-Objective Finetuning](https://arxiv.org/abs/2407.15762)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15762.png)

Vote: 5

Authors: Geoffrey Cideron, Christoph Dann, Léonard Hussenot, Johan Ferret, Yunxuan Li, Andrea Michi, Kaiwen Wang, Olivier Bachem, Aranyak Mehta, Raghav Gupta, Amr Ahmed, Edouard Leurent, Marco Gelmi, Ryan Sullivan, Avinava Dubey, Hongkun Yu, Rahul Kidambi, Alekh Agarwal, Le Hou, Alexandre Ramé

- **What's New**: 이 논문에서는 Conditional Language Policy (CLP)를 제안하여 다목적 강화 학습(MOFT) 프레임워크를 통해 다목적 언어 모델(LM)을 학습할 수 있는 새로운 방법론을 소개합니다. 이는 다중 목표 보상을 효과적으로 다룰 수 있도록 설계되었습니다.
- **Technical Details**: CLP는 매개변수 공간 조건화(parameter-space conditioning) 및 다중 작업 학습(multi-task training)을 활용하여 각기 다른 보상 가중치에 맞춰 LM의 출력을 최적화합니다. 이는 기존의 단일 목적 미세 조정(single-objective finetuning, SOFT) 방법의 한계를 극복하고 다양한 사용자 요구에 맞는 출력을 생성할 수 있게 합니다.
- **Performance Highlights**: CLP는 다양한 실험 조건에서 기존 방법론보다 출력 품질과 조작 가능성(steerability)이 향상되었음을 입증했습니다. Gemini 1.0 Ultra를 활용한 자동화 평가에서도 CLP가 다른 기준선보다 더 조작 가능하고 높은 품질의 응답을 생성함을 보여줍니다. 이 연구는 다목적 RL 기술의 발전을 위한 중요한 기여를 하고 있습니다.

### [MIBench: Evaluating Multimodal Large Language Models over Multiple Images](https://arxiv.org/abs/2407.15272)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15272.png)

Vote: 4

Authors: Ming Yan, Fei Huang, Yaya Shi, Haiyang Xu, Chaoya Jiang, Ji Zhang, Chunfeng Yuan, Xi Zhang, Haowei Liu, Bing Li, Weiming Hu

- **What's New**: 이 논문에서는 MLLMs(Multimodal Language Models)의 다중 이미지(multi-image) 기능을 평가하기 위해 새로운 대규모 벤치마크인 MIBench를 제안합니다. 이 벤치마크는 13,000개의 고품질 샘플을 포함하는 3가지 시나리오와 13가지 작업을 아우릅니다.
- **Technical Details**: MIBench는 Multi-Image Instruction (MII), Multimodal Knowledge-Seeking (MKS), Multimodal In-Context Learning (MIC) 등 세 가지 주제를 중심으로 구성됩니다. MII는 복수의 이미지 간 인식, 비교 및 추론을 포함하고, MKS는 이미지-텍스트 데이터가 제공되며 질문은 단일 이미지에 대해 하는 방식입니다. MIC는 여러 멀티모달 데모를 바탕으로 모델이 질문에 응답하는 시나리오입니다. 각 시나리오는 다양한 작업으로 세분화되어, 정확한 성능 평가를 위해 정확도와 정확한 일치를 메트릭으로 사용합니다.
- **Performance Highlights**: 다양한 기존 MLLMs를 MIBench에서 평가한 결과, 특히 오픈 소스 모델이 다중 이미지 시나리오에서 주요한 결함을 보였으며, 멀티모달 지식 탐색 시나리오에서 성능이 낮은 것으로 나타났습니다. 연구자들은 MIBench를 통해 MLLMs의 다중 이미지 능력 개선을 촉진할 것으로 기대하고 있습니다.

### [Local All-Pair Correspondence for Point Tracking](https://arxiv.org/abs/2407.15420)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15420.png)

Vote: 4

Authors: Honggyu An, Seokju Cho, Jisu Nam, Joon-Young Lee, Jiahui Huang, Seungryong Kim

- **What's New**: LocoTrack는 고급 4D 상관관계를 활용하여 포인트 추적 문제를 해결하는 새로운 접근 방식을 제안한다. 기존의 지역 기반 포인트-지역 대응 방법론 대신, 모든 쌍의 대응을 찾는 지역 4D 상관관계 문제로 포인트 추적을 재구성하였다.
- **Technical Details**: LocoTrack는 지역 4D 상관관계를 구축하여 원하는 쿼리 포인트 주변의 지역과 대상 프레임의 해당 지역 간 모든 쌍을 매칭하고, 경량 상관 관계 인코더를 통해 고차원 상관관계를 효율적으로 처리한다. Transformer 아키텍처를 사용하여 시간을 넘어서는 정보를 통합하고, 상대 위치 편향을 통해 가변 길이의 시퀀스 처리를 가능하게 한다.
- **Performance Highlights**: LocoTrack은 TAP-Vid-DAVIS 데이터셋에서 Cotracker보다 +2.5 AJ 보다 나은 성능을 보였고, 3.5배 더 빠른 추론 속도를 제공한다. 크기가 작은 버전은 최신 모델 대비 빠르면서도 높은 정확도를 유지하며, 이는 3개의 Transformer 층으로 구성되어 있다.

### [MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation](https://arxiv.org/abs/2407.15060)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15060.png)

Vote: 4

Authors: Hao-Chung Cheng, Yi-Hsuan Yang, Wen-Yi Hsiao, Yun-Han Lan

- **What's New**: MusiConGen은 텍스트에서 음악으로의 생성 모델로, 리듬과 코드에 대한 향상된 제어 기능을 제공하는 첫 번째 Transformer 기반 모델이다. 이 모델은 사용자 제공 텍스트 입력을 지원하며, 레퍼런스 오디오 없이도 작동할 수 있다.
- **Technical Details**: MusiConGen은 기존의 MusicGen 모델을 기반으로 하여 악기 음악에서 백킹 트랙 음악으로의 도메인 전환을 위해 직접 파인튜닝(direct finetuning) 방법을 채택하였다. 또한, 'adaptive in-attention' 조건부 방식을 도입하여 리듬과 코드 제어를 시간적으로 향상시킬 수 있도록 설계하였다. 학습 데이터는 YouTube에서 수집한 5,000개의 텍스트-오디오 쌍을 사용하여 구축되었다.
- **Performance Highlights**: MusiConGen은 MUSDB18과 RWC-pop-100의 공개 데이터셋을 사용해 평가된 결과, 기존 MusicGen 모델보다 주어진 조건에 더 정밀하게 음악을 생성하는 능력을 보여주었다. 또한, 4개의 RTX-3090 그래픽카드를 사용하여 소비자 수준의 GPU에서 효율적인 훈련 구성으로 구축될 수 있다.

### [Temporal Residual Jacobians For Rig-free Motion Transfer](https://arxiv.org/abs/2407.14958)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14958.png)

Vote: 4

Authors: Niladri Shekhar Dutt, Noam Aigerman, Vladimir Kim, Siddhartha Chaudhuri, Sanjeev Muralikrishnan, Matthew Fisher, Niloy J. Mitra

- **What's New**: 이 논문에서는 전통적인 리깅(rigging) 방법 없이도 3D 캐릭터 애니메이션에서 모션을 더 자연스럽고 부드럽게 전이하는 새로운 접근법을 제안합니다. Temporal Residual Jacobians라는 새로운 표현 방식을 통해, 지역적인 시공간 변화를 학습하여 다양한 캐릭터에 대한 현실적인 모션 전이를 가능하게 합니다.
- **Technical Details**: 이 방법은 스틱 피규어의 관절 각도를 기반으로 한 모션을 리깅 없이 지정된 메쉬(target shape)에 전이하는 데 초점을 맞춥니다. 우리는 시간에 따른 공간 예측을 연결하기 위해 Temporal Residual Jacobians를 사용하며, 두 개의 신경망을 훈련하여 각각 지역적인 공간 변화와 시간적 변화를 예측합니다. 이는 공간적 통합과 시간적 통합을 통해 부드러운 애니메이션을 생성합니다.
- **Performance Highlights**: 제안된 방법은 AMASS, COP3D, 4DComplete 데이터셋을 사용하여 다양한 캐릭터(인물, 동물 등)에 대해 다양한 동작(걷기, 뛰기, 점프하기 등)에 대한 질적 및 양적 결과를 보였습니다. 리깅이 필요하지 않으며, 고정된 템플릿이나 형상으로부터 파라미터화된 학습이 필요 없으므로, 활용 가능성이 매우 높습니다.

### [Consent in Crisis: The Rapid Decline of the AI Data Commons](https://arxiv.org/abs/2407.14933)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14933.png)

Vote: 3

Authors: Ariel Lee, Caroline Chitongo, Da Yin, William Brannon, Damien Sileo, Nayan Saxena, Ahmad Anis, +, Kevin Klyman, Cole Hunter, Hamidah Oderinwale, Manuel Cherep, Tobin South, Robert Mahari, Hailey Schoelkopf, Christopher Klamm, Campbell Lund, Nikhil Singh, Deividas Mataciunas, Shayne Longpre, Naana Obeng-Marnu, An Dinh, Diganta Misra

- **What's New**: 이 논문에서는 새로운 접근법으로서 Neural Light Field (NLF) 기술을 소개하고 있습니다. 이는 3D (3차원) 환경에서의 렌더링을 혁신적으로 개선할 것으로 기대되고 있습니다.
- **Technical Details**: NLF는 multi-view (다중 시점) 이미지를 위해 neural networks (신경망) 구조를 활용하며, 다차원 공간에서의 데이터 샘플링을 통해 고해상도 이미지를 생성합니다. 이 방법은 기존의 voxel (복셀) 기반 기술보다 훨씬 더 효율적이고 빠르게 작업을 수행할 수 있습니다.
- **Performance Highlights**: 실험 결과, NLF는 이전의 방식보다 rendering speed (렌더링 속도)가 크게 향상되었으며, 이미지 품질 또한 더 높은 해상도를 유지함을 보여줍니다. 따라서 NLF는 게임 및 영화 산업에서의 실시간 Graphics (그래픽스) 처리에 있어 큰 잠재력을 지닌 기술로 평가되고 있습니다.

### [Artist: Aesthetically Controllable Text-Driven Stylization without Training](https://arxiv.org/abs/2407.15842)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15842.png)

Vote: 3

Authors: Ruixiang Jiang, Changwen Chen

- **What's New**: 이 논문은 텍스트 기반 이미지 스타일화에서 내용(content)과 스타일(style) 생성을 분리하는 새로운 접근법을 제안합니다. 기존의 diffusion 모델의 한계를 극복하기 위해 보조 diffusion 가지(auxiliary diffusion branches)를 도입하여 내용 보존과 스타일 생성을 분리하는 것입니다.
- **Technical Details**: 이 논문은 diffusion 과정에서 내용과 스타일의 생성을 정교하게 조절하기 위해 feature-level soft constraints를 적용하며, 이를 통해 미적 수준(aesthetic-level)의 제어를 가능하게 합니다. 제안된 방법은 VLMs(Visual-Language Models)를 사용하여 스타일화 결과를 미적 수준에서 평가합니다.
- **Performance Highlights**: 다양한 실험에서 제안된 방법이 질적 및 양적으로 기존 방법들을 능가하며, 제공된 프롬프트(prompts)에 잘 맞는 고품질 스타일화 이미지를 생성함을 보여줍니다.

### [ThermalNeRF: Thermal Radiance Fields](https://arxiv.org/abs/2407.15337)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15337.png)

Vote: 3

Authors: Yvette Y. Lin, Gordon Wetzstein, Xin-Yi Pan, Sara Fridovich-Keil

- **What's New**: 이 연구는 장파 적외선 (LWIR) 이미지를 기반으로 한 3D 열 장면 복원 방법을 최초로 제안합니다. 이는 핸드헬드 RGB 및 열 카메라의 상대 포즈를 교차 보정하여 열 카메라의 포즈를 추정하고, 다중 뷰 열 및 RGB 측정의 융합을 통해 열 초해상도를 지원합니다.
- **Technical Details**: 연구에서는 열 카메라의 저해상도 문제를 해결하기 위해 RGB 카메라의 정보를 활용하며, 열 및 가시광선의 개별 흡수 특성을 나타내도록 방사선 필드 모델을 확장합니다. 이를 통해 물질 속성을 복원하고 복원 품질을 개선합니다.
- **Performance Highlights**: 연구에서 제시된 방법은 다양한 재료로 촬영된 다중 뷰 열 및 RGB 카메라 데이터셋에 적용되어 높은 충실도의 3D 열 복원을 보여줍니다. 그 데이터셋에는 현실 세계의 아홉 개 장면과 하나의 합성 장면이 포함되어 있으며, 스카이디오 드론 데이터셋에 대한 결과도 포함됩니다.

### [GET-Zero: Graph Embodiment Transformer for Zero-shot Embodiment Generalization](https://arxiv.org/abs/2407.15002)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15002.png)

Vote: 3

Authors: Shuran Song, Austin Patel

- **What's New**: GET-Zero는 로봇의 그래프 연결성을 활용하여 다양한 그래프 구조로 제어할 수 있는 모델을 도입합니다. 이는 기존의 정책을 재훈련하지 않고도 하드웨어 구성의 변화에 효과적으로 적응할 수 있는 가능성을 제공합니다.
- **Technical Details**: GET-Zero는 Graph Embodiment Transformer (GET) 모델 아키텍처를 기반으로 하며, 조인트를 별도의 토큰으로 인코딩하고 변형자의 주의 메커니즘에서 학습된 그래프 바이어스를 활용합니다. 기존의 전이 학습 방법과 달리, GET-Zero는 행동 클로닝을 통해 형태 특정 전문가의 지식을 증류하여 불특정 형태의 로봇을 제어하는 능력을 향상시킵니다.
- **Performance Highlights**: LEAP Hand를 기반으로 한 실험에서 GET-Zero는 다양한 하드웨어 구성의 다관절 로봇 손 제어에서 우수한 성능을 보여주었습니다. 실험 결과는 GET-Zero가 링크 기하학 및 그래프 연결성의 변화에 대한 단일 네트워크 가중치 세트로 전이하는 데 성공적임을 나타냅니다.

### [Visual Haystacks: Answering Harder Questions About Sets of Images](https://arxiv.org/abs/2407.13766)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.13766.png)

Vote: 2

Authors: Joseph E. Gonzalez, Trevor Darrell, Ritwik Gupta, Jerome Quenum, David M. Chan, Giscard Biamby, Tsung-Han Wu

- **What's New**: 이 논문에서는 다중 이미지 시각 질문 응답(MIQA) 시스템의 필요성을 강조하며, 새로운 벤치마크인 'Visual Haystacks (VHs)'를 소개하여 기존 모델의 성능을 평가합니다.
- **Technical Details**: VQs 시스템의 한계를 극복하기 위해, MIRAGE (Multi-Image Retrieval Augmented Generation)라는 새로운 훈련 패러다임을 도입하였습니다. 이 방식은 LLaVA 모델을 확장하여 MIQA 작업을 다루며, 이미지 인코더를 보강하고 검색 기반 쿼리 인식을 활용한 관련성 필터를 포함합니다.
- **Performance Highlights**: VHs 벤치마크 및 기존 MIQA 벤치마크에서의 실험 결과, MIRAGE는 기존 검색 증강 방법 대비 MIQA 성능에서 최대 11% 향상되었으며, 기존 모델보다 3.4배 더 효율적이라는 결과를 보였습니다.

### [Discrete Flow Matching](https://arxiv.org/abs/2407.15595)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15595.png)

Vote: 2

Authors: Felix Kreuk, Yossi Adi, Yaron Lipman, Itai Gat, Gabriel Synnaeve, Neta Shaul, Ricky T. Q. Chen, Tal Remez

- **What's New**: 최근에 발표된 논문은 새로운 딥러닝(deep learning) 구조를 제안하며, 특정 분야에서의 성능 향상에 중점을 두고 있습니다. 이 구조는 기존 모델보다 더 빠르고 효율적인 학습을 가능하게 합니다.
- **Technical Details**: 제안된 모델은 convolutional neural networks (CNNs)와 recurrent neural networks (RNNs)의 하이브리드(hybrid) 형태로, 복잡한 시간적 패턴을 인식하는 데 강점을 보입니다. 이 모델은 attention mechanism을 활용하여 보다 정확한 예측을 수행합니다.
- **Performance Highlights**: 실험 결과, 새로운 모델은 여러 벤치마크 데이터셋에서 기존의 최첨단 모델들보다 최소 10% 이상의 성능 향상을 보였으며, 특히 이미지와 텍스트에 대한 이해도에서 큰 차이를 보였습니다.

### [CGB-DM: Content and Graphic Balance Layout Generation with Transformer-based Diffusion Model](https://arxiv.org/abs/2407.15233)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15233.png)

Vote: 1

Authors: Jie Wu, Yu Li, Yifan Chen, Yujiu Yang, Gongye Liu

- **What's New**: 이 논문에서는 Content and Graphic Balance Layout Generation with Transformer-based Diffusion Model (CGB-DM)이라는 새로운 방법을 제안합니다. 이는 content-aware layout generation의 과정에서 발생하는 여러 문제점을 해결하기 위한 혁신적인 접근입니다.
- **Technical Details**: CGB-DM은 layout의 공간 구조에 대한 학습과 이미지의 content-aware feature 간의 균형을 조정하기 위해, content와 graphic 균형 가중치(content and graphic balance weight)를 도입합니다. 또한, saliency bounding box를 사용하여 이미지에서 중요한 영역의 공간 정보를 명확하게 추출합니다.
- **Performance Highlights**: 실험 결과, CGB-DM은 PKU와 CGL 데이터셋에서 기존 최첨단 모델들보다 우수한 성능을 보이며, 특히 graphic 성능에서 눈에 띄는 개선을 보여주었습니다. 이는 콘텐츠의 품질과 그래픽 성능을 모두 고려한 혁신적인 방법입니다.

