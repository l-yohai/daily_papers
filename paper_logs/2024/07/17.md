## Daily Papers (2024-07-17)

### [NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?](https://arxiv.org/abs/2407.11963)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11963.png)

Vote: 25

Authors: Mo Li, Songyang Zhang, Kai Chen, Yunxin Liu

- **What's New**: 최신 대형 언어 모델(LLMs)이 긴 문맥을 처리하는 능력이 특히 중요해지고 있습니다. 예를 들어, GPT-4 Turbo(OpenAI, 2023)은 128K 토큰까지, Claude 2.1(Anthropic, 2024a)과 Claude 3 시리즈(2024b)는 200K 토큰과 100만 토큰까지 지원합니다. 새로운 NeedleBench 데이터셋을 소개하며, LLM이 긴 문맥에서 정보를 추출하고 분석하는 능력을 평가합니다.
- **Technical Details**: NeedleBench 데이터셋은 LLM의 긴 문맥 정보 추출 및 분석 능력을 평가하기 위해 설계된 커스터마이징 가능한 프레임워크입니다. 이 데이터셋에는 다양한 길이(4k, 8k, 32k, 128k, 200k, 1000k 이상의 길이)와 텍스트 깊이 범위가 포함됩니다. 주요 데이터 포인트를 다양한 텍스트 깊이 영역에 전략적으로 삽입하여 모델의 정보 회수 및 추론 능력을 엄격하게 테스트합니다. 또한, Ancestral Trace Challenge (ATC) 테스트를 통해 다단계 논리 추론을 측정합니다.
- **Performance Highlights**: ATC 테스트의 실험 결과, 현재의 LLM은 복잡한 논리 관계가 포함된 추론 작업을 처리하는 데 어려움을 겪고 있으며, 이는 2K 토큰보다 짧은 텍스트에서도 마찬가지입니다. NeedleBench와 ATC 테스트는 LLM이 실제 긴 문맥 시나리오에서 얼마나 잘 수행하는지에 대한 더욱 현실적인 평가를 제공합니다.

### [Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes](https://arxiv.org/abs/2407.10957)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10957.png)

Vote: 22

Authors: Peiwen Sun, Dongzhan Zhou, Di Hu, Yaoting Wang, Guangyao Li, Honggang Zhang

- **What's New**: 최근 논문에서는 Referring Audio-Visual Segmentation (Ref-AVS)라는 새로운 과제를 제안하였습니다. 이 과제는 복잡한 오디오-비주얼(오디오비디오) 장면에서 관심 객체를 정확히 찾고 세분화하는 것을 목표로 합니다. 이에 따라 Ref-AVS Bench라는 첫 번째 벤치마크도 도입되었습니다.
- **Technical Details**: Ref-AVS는 동적 오디오비주얼 정보를 포함하는 다중 모달 참조 표현을 사용하여 각 픽셀이 해당 객체인지 예측하는 픽셀 단위 세분화 작업입니다. 이를 위해 Crossmodal Transformer를 사용하여 다중 모달 단서를 효율적으로 처리하는 엔드 투 엔드(end-to-end) 프레임워크가 설계되었습니다. 현재 연구들은 주로 비디오 객체 세분화 (Video Object Segmentation), 참조 비디오 객체 세분화 (Referring Video Object Segmentation), 오디오비주얼 세분화 (Audio-Visual Segmentation) 등의 분리된 접근 방식으로 이어져 왔으나, 이 작업은 이들을 통합한 접근 방식을 제공합니다.
- **Performance Highlights**: 새로운 Ref-AVS 벤치마크는 약 4,000개의 유튜브 audible 비디오 클립과 20,000개 이상의 참조 표현을 포함하며, 60% 이상이 다중 소스 사운드 시나리오입니다. 전문가들에 의해 정확성과 신뢰성이 검증된 이 데이터셋은 모델의 제로샷 시나리오에서의 일반화 능력을 평가하기 위해 특별히 설계된 검증 세트를 포함하고 있습니다. 다양한 실험을 통해 다중 모달 단서를 고려한 비주얼 세분화의 장점을 입증하였으며, 제시된 방법이 모든 하위 집합에서 우수한 성능을 보임을 확인했습니다.

### [Qwen2-Audio Technical Report](https://arxiv.org/abs/2407.10759)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10759.png)

Vote: 16

Authors: Yuanjun Lv, Haojie Wei, Jin Xu, Yichong Leng, Jingren Zhou, Qian Yang, Junyang Lin, Yunfei Chu, Chang Zhou, Jinzheng He, Zhifang Guo, Xipin Wei

- **What's New**: Qwen2-Audio 모델은 최근의 대형 오디오-언어 모델(LALMs) 개발 동향을 기반으로, 다양한 음성 신호 이해, 음성 신호 분석, 그리고 복잡한 추론 능력을 크게 향상시키고 있습니다. 본 보고서에서는 Qwen2-Audio를 개발하여 주로 지시에 따른 명령 이행 능력을 강화하는데 중점을 두었습니다.
- **Technical Details**: Qwen2-Audio는 오디오와 텍스트 입력을 처리하여 텍스트 출력을 생성하는 대형 오디오-언어 모델(LALM)입니다. 훈련 과정에서 더 큰 데이터를 사용하는 한편, 자연어 프롬프트를 사용하여 사전 훈련 단계를 단순화하였습니다. 모델의 오디오 인코더는 Whisper-large-v3 모델을 기반으로 초기화되었고, 최종 모델은 8.2B의 파라미터를 갖습니다.
- **Performance Highlights**: Qwen2-Audio는 Aishell2, FLUERS-zh, VocalSound, AIR-Bench chat benchmark에서 탁월한 성능을 보여주며, 이전 LALMs를 능가했습니다. 또한, 영어 자동 음성 인식(ASR) 성능 측면에서도 모범적 성과를 달성하였습니다. 특히 librispeech test-clean 및 test-other 데이터셋에서 각각 1.6% 및 3.6% WER을 기록하였으며, Fleurs zh 서브셋에서도 Whisper-large-v3보다 더 나은 결과를 보여주었습니다.

### [Scaling Diffusion Transformers to 16 Billion Parameters](https://arxiv.org/abs/2407.11633)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11633.png)

Vote: 10

Authors: Junshi Huang, Changqian Yu, Mingyuan Fan, Debang Li, Zhengcong Fei

- **What's New**: 최근 확산 모델(Diffusion Models)이 이미지, 비디오, 3D 객체 등 다양한 도메인에서 강력한 생성 모델로 주목받고 있습니다. 특히, Transformer 기반 구조가 네트워크 용량을 늘리는 데 매우 효과적이라는 연구 결과가 나오면서 Stable Diffusion 3 같은 대규모 모델(파라미터가 8억 개 이상)들이 등장하고 있습니다. 하지만 이러한 모델을 훈련하고 서비스를 제공하는 데는 높은 비용이 수반됩니다. 이를 해결하기 위한 기법으로 조건부 연산(Conditional Computation)이 주목받고 있으며, 이는 모델 용량을 유지하면서도 훈련과 추론 비용을 절감할 수 있습니다. 본 연구에서는 Diffusion Transformers(DiT)에 조건부 연산을 적용한 DiT-MoE 모델을 제안합니다.
- **Technical Details**: 본 논문에서는 확산 모델과 조건부 연산을 활용한 MoE(Mixture of Experts)에 대해 설명하고, 이를 확산 Transformer에 적용하는 방법을 제시합니다. DiT-MoE는 DiT의 일부 Dense Feed-Forward Layer를 Sparse MoE 레이어로 대체하고, 각 이미지 패치의 토큰을 일부 전문가(MLP 레이어)에 라우팅합니다. 또한, 공통 지식을 포착하는 전문가의 공유 부분과 라우팅된 전문가 간의 중복성을 줄이는 전문가 레벨 균형 손실(balance loss)을 포함하는 두 가지 주요 설계를 도입합니다.
- **Performance Highlights**: DiT-MoE 모델은 ImageNet 벤치마크에서 상태-of-the-art 급 Dense 모델의 성능과 맞먹는 결과를 보여주었으며, 추론 시간은 절반밖에 걸리지 않았습니다. 또 다른 형태인 DiT-MoE-S는 비용 측면에서 DiT-B와 동일한 수준을 유지하면서 더 나은 성능을 보여주었습니다. 추가적인 합성 데이터를 활용하여 모델 파라미터를 16.5B로 확장하면서 3.1B 파라미터만 활성화하여 512x512 해상도에서 새로운 최첨단 FID-50K 점수(1.80)를 달성했습니다. 우리의 DiT-MoE 모델은 안정적으로 훈련되고 효율적인 추론이 가능하며, 코드와 훈련된 모델 체크포인트도 공개되었습니다.

### [DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation](https://arxiv.org/abs/2407.11394)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11394.png)

Vote: 8

Authors: Hyunjung Shim, Jiho Choi, Jaeyo Shin, Jiwook Kim, Seonho Lee

- **What's New**: 이번 연구에서는 새로운 강화 학습 알고리즘(Reinforcement Learning Algorithm)을 소개합니다. 이 알고리즘은 환경의 변화에 빠르게 적응하는 고도화된 정책(Policy)를 개발하는 것을 목표로 하고 있습니다.
- **Technical Details**: 제안된 알고리즘은 정책 네트워크(Policy Network)와 가치 네트워크(Value Network)를 활용하며, 두 네트워크는 공동으로 학습 및 업데이트됩니다. 이 과정에서 Proximal Policy Optimization (PPO)과 같은 최신 기법이 적용되었습니다. 중요한 기술적 기여 중 하나는 보상 함수(Reward Function)의 동적 조정 메커니즘입니다.
- **Performance Highlights**: 제안된 알고리즘은 다양한 환경에서 실험되었으며, 기존 방법론 대비 빠른 수렴 속도와 높은 성능을 보였습니다. 특히, 복잡한 환경에서도 안정적인 정책 학습이 가능함을 입증했습니다. 결과적으로, 이 알고리즘은 실제 응용에서도 높은 활용 가능성을 나타냈습니다.

### [Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning](https://arxiv.org/abs/2407.10718)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10718.png)

Vote: 7

Authors: Lifeng Liu, Tianhao Shen, Yulong Wang, Jian Xie

- **What's New**: 이 논문은 컴퓨터 비전(Computer Vision)과 자연어 처리(NLP)를 위한 새로운 멀티 모달(multimodal) 학습 모델을 소개합니다. 이 모델은 이미지와 텍스트 정보의 상호작용을 좀 더 효율적으로 학습할 수 있도록 설계되었습니다.
- **Technical Details**: 제안된 모델은 transformer 구조를 기반으로 하며, 이미지 인코더(image encoder)와 텍스트 인코더(text encoder)를 포함하고 있습니다. 두 인코더는 잠재 공간(latent space)에서 데이터를 결합하여 학습합니다. 이 과정에서 cross-attention 메커니즘이 사용되어 정보의 상호작용이 강화됩니다. 또한, 여러 데이터셋에서 사전학습(pretraining)을 실시하여 성능을 높였습니다.
- **Performance Highlights**: 이 모델은 다양한 벤치마크(benchmarks)에서 기존의 최첨단 모델들보다 높은 성능을 보였습니다. 특히, 이미지-텍스트 매칭(image-text matching)과 이미지 캡셔닝(image captioning) 과제에서 두드러진 성과를 거두었습니다. 정량적인 평가 결과, 모델은 BLEU, METEOR 등 주요 지표에서 최고점을 기록했습니다.

### [FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models](https://arxiv.org/abs/2407.11522)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11522.png)

Vote: 6

Authors: Qing Li, Song-Chun Zhu, Zhi Gao, Mehrtash Harandi, Tao Yuan, Yuwei Wu, Pengxiang Li, Yunde Jia, Bofei Zhang

- **What's New**: 언어 비전 모델(Vision Language Models, VLMs)을 위한 새로운 데이터셋 FIRE(FIRE 데이터셋 및 평가 벤치마크 시스템)이 출시되었습니다. 이 데이터셋 및 벤치마크는 다양한 작업에서 VLMs의 피드백 통합과 개선 능력을 평가하기 위해 설계되었습니다.
- **Technical Details**: FIRE는 1.1백만의 피드백-개선 대화 기록을 포함하며, 이 중 100K는 GPT-4V를 사용해 생성되고 나머지 1M은 학생과 교사 모델이 자유롭게 대화를 시뮬레이션하여 생성됩니다. 이러한 데이터는 시각적 질문 응답(Visual Question Answering), 이미지 캡셔닝(Image Captioning), OCR(OCR Reasoning), 문서 이해(Document Understanding) 등 다양한 작업을 포함하고 있습니다. FIRE-Bench는 두 가지 평가 설정(고정된 대화 및 자유 대화)을 제공하며, 11K 피드백-개선 대화를 통해 모델의 개선 능력을 평가합니다.
- **Performance Highlights**: FIRE-LLaVA 모델은 FIRE 벤치마크에서 50% 이상의 성능 향상을 보였습니다. 이는 피드백을 통합하여 응답을 개선하는 데 있어서 LLaVA-NeXT 모델보다 뛰어난 결과를 보여줍니다.

### [VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models](https://arxiv.org/abs/2407.11691)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11691.png)

Vote: 6

Authors: Haodong Duan, Yuxuan Qiao, Xiaoyi Dong, Kai Chen, Yuan Liu, Pan Zhang, Yuhang Zang, Dahua Lin, Xinyu Fang, Jiaqi Wang, Junming Yang, Lin Chen

- **What's New**: 아카이브 논문 'VLMEvalKit'에서 소개된 주요사항을 요약합니다. Large Language Models(LLMs)의 급속한 발전과 함께 Large Multi-Modality Models(LMMs)도 큰 발전을 이루었습니다. LMMs는 일반적으로 두 개 이상의 모달리티를 입력으로 받아들이며, 주로 이미지와 텍스트, 오디오텍스트, 비디오, 포인트 클라우드 등을 다룹니다. 이러한 LMMs는 인간과의 다양한 대화 스타일을 통해 뛰어난 일반화 능력을 보여주며, 다양한 혁신적인 응용 프로그램들을 촉진하고 있습니다. 논문에서는 이러한 LMMs의 종합적이고 사용자 친화적인 평가를 돕기 위해 'VLMEvalKit'이라는 오픈 소스 툴킷을 개발했습니다. 이 툴킷은 70개 이상의 LMM과 20개 이상의 다중 모달리티 벤치마크를 지원하며, 간단한 명령어를 통해 평가를 자동화하여 잘 구조화된 평가 결과를 생성합니다.
- **Technical Details**: VLMEvalKit은 여러 모듈로 구성되어 있습니다. '벤치마크(Benchmarks)' 모듈은 다양한 포맷으로 준비된 다중 모달리티 벤치마크들을 처리하며, 'LMMs' 모듈은 통일된 .generate() 인터페이스를 통해 70개 이상의 LMM을 지원합니다. 또한 여러 모달리티 메시지를 처리할 수 있는 기능도 포함되어 있습니다. '다중 모달리티 추론(Multi-modal Inference)' 모듈은 병렬 처리를 통해 상용 API와 오픈 소스 모델 모두를 지원하며, 인터럽트된 추론 과정을 최소 비용으로 재개할 수 있습니다. '다중 모달리티 평가(Multi-modal Evaluation)'는 모델이 제공한 예측을 기반으로 최종 메트릭스를 산출합니다.
- **Performance Highlights**: VLMEvalKit은 기존 벤치마크에서 높은 평가 정확도를 유지하며, 특히 다중 선택 질문(MCQ)과 예 또는 아니오 질문(Y/N)에 대한 LLM-보강 응답 추출을 통합하여 평가의 정확성을 높입니다. CircularEval 전략을 통해 MCQ 벤치마크에서의 모델 이해도를 효과적으로 평가할 수 있습니다. OpenVLM 리더보드에 모든 평가 결과가 공개되어 있으며, 이를 통해 LMM 개발의 진전을 모니터링할 수 있습니다.

### [YouTube-SL-25: A Large-Scale, Open-Domain Multilingual Sign Language Parallel Corpus](https://arxiv.org/abs/2407.11144)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11144.png)

Vote: 6

Authors: Biao Zhang, Garrett Tanzer

- **What's New**: YouTube-SL-25라는 새로운 대규모 다국어 수화 비디오 코퍼스를 소개합니다. 이는 전 세계에서 사용되는 수화의 번역을 위해 설계되었으며, YouTube-ASL 데이터셋의 상위 집합입니다.
- **Technical Details**: YouTube-SL-25 비디오 코퍼스는 YouTube에서 자동 분류기를 이용해 텍스트 메타데이터를 기반으로 관련 비디오를 식별하고, 단일 비원어 사용자가 채널별로 검토해 비디오를 선택하는 과정을 통해 구축되었습니다. 최종 데이터셋은 3,207시간의 비디오와 25개 이상의 수화에서 3,000명 이상의 고유 서명자를 포함합니다. 비디오들은 최소 15시간 이상의 데이터를 가진 25개의 주요 수화로 구성되며, 총 55개의 수화 데이터를 포함합니다.
- **Performance Highlights**: YouTube-SL-25는 수화 이해 작업의 기준치를 설정하며, 다양한 수화 번역 및 식별 작업에서 멀티 태스크 전이가 가능함을 보여줍니다. 특히 비디오 캡션 정렬, 번역, 수화 식별 등의 하위 작업에서 유용하게 사용할 수 있도록 설계되었습니다. 상위 1,000개 채널을 선별하고, 3,000-6,000번 채널 범위의 비디오를 우선 처리하며, 채널 내 비디오를 길이순으로 정렬하여 검토하였습니다.

### [OmniBind: Large-scale Omni Multimodal Representation via Binding Spaces](https://arxiv.org/abs/2407.11895)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11895.png)

Vote: 5

Authors: Ziang Zhang, Luping Liu, Zehan Wang, Hang Zhang, Zhou Zhao, Hengshuang Zhao, Xize Cheng, Rongjie Huang

- **What's New**: 최근 다양한 모달리티를 하나의 모델로 이해하고 생성하는 omni model에 대한 관심이 증가하고 있습니다. 기존 오픈 소스 멀티모달 표현 모델(Multimodal Representation Model)은 주로 소규모로 이미지-텍스트, 오디오-텍스트, 오디오-이미지, 3D-이미지-텍스트 조합 등을 탐구했습니다. 이번 논문에서는 3D, 오디오, 이미지, 언어 모두를 통합한 대규모 모델인 OmniBind를 소개합니다. OmniBind는 여러 미리 학습된 공간을 결합하여, 다양한 대규모 데이터와 방대한 파라미터 수를 가진 앙상블 모델(Ensemble Model)을 개발했습니다.
- **Technical Details**: OmniBind는 3D 포인트, 오디오, 이미지, 언어 등 네 가지 모달리티를 포함하며, 기존의 미리 학습된 모델 공간을 결합하는 방식을 사용합니다. 이를 위해, 가변적인 조합 가중치를 예측하기 위해 모덜리티별로 학습 가능한 라우터(Router)를 사용합니다. 두 가지 주요 학습 목표는 '크로스모달 전체 정렬(Cross-modal overall alignment)'과 '언어 표현 분리(Language representation decoupling)'입니다. 이 두 목표는 라우터가 최적의 조합 가중치를 동적으로 예측하도록 동기 부여하며, 다양한 표현의 식별성을 유지합니다.
- **Performance Highlights**: OmniBind는 7억에서 300억 파라미터에 이르는 세 가지 대규모 모델로 구성되며, 13개의 벤치마크에서 최첨단 성능을 보여줍니다. 3D, 오디오, 이미지 분류 작업에서 뛰어난 제로샷(Zero-shot) 일반화 능력을 입증했으며, 모든 가능한 모달리티 쌍에서 강력한 크로스모달 정렬을 보여줍니다. 또한, 고품질의 옴니(Omni) 의미 정렬을 통해, 정확한 3D-오디오 검색, 임의 쿼리 객체 로컬라이제이션 및 오디오 분리, 복잡한 합성 이해 등의 인상적인 응용 프로그램을 실행할 수 있습니다.

### [Efficient Training with Denoised Neural Weights](https://arxiv.org/abs/2407.11966)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11966.png)

Vote: 5

Authors: Yanyu Li, Andrey Zharkov, Jian Ren, Sergey Tulyakov, Yerlan Idelbayev, Zheng Zhan, Yifan Gong, Yanzhi Wang, Kfir Aberman

- **What's New**: 딥 뉴럴 네트워크(DNN)의 효율적인 학습을 위해, 잘 훈련된 모델 가중치(weight)를 생성해주는 새로운 프레임워크를 제안했습니다. 이를 통해 모델 초기화를 개선하여 효율적인 학습이 가능해집니다. 특히 GAN 모델을 사용한 이미지 변환 작업에 이 프레임워크를 적용했습니다.
- **Technical Details**: 이 연구는 HyperNetwork의 최신 기술에서 영감을 받아 가중치 생성기(weight generator)를 개발하는 것입니다. 가중치 생성기는 처음 본 개념과 스타일에 대한 초기 가중치를 예측해주는 모델입니다. 이를 위해 우리는 Low-Rank Adaptation (LoRA) 기법을 사용하여 모델 파라미터 수를 줄였습니다. 또한, 잘 훈련된 GAN 모델의 가중치 공간을 모델링하기 위해 diffusion process를 사용했습니다. 이 과정에서 블록 인덱스 정보를 활용하여 가중치 생성기의 성능을 높였습니다.
- **Performance Highlights**: 프레임워크는 새로운 개념에 대해 단일 디노이징 과정으로 가중치를 예측하고, 그 예측된 가중치를 사용하여 GAN 모델을 초기화하는 방식으로 작동합니다. 이 초기화된 모델은 빠른 파인튜닝 과정을 통해 고품질 이미지 생성 결과를 얻을 수 있습니다. 전체 훈련 시간을 15배 줄이면서도 이미지 생성 품질을 유지하거나 향상시켰으며, 기존의 효율적인 학습 방법과 비교해도 4.6배의 시간을 절약했습니다.

### [Animate3D: Animating Any 3D Model with Multi-view Video Diffusion](https://arxiv.org/abs/2407.11398)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11398.png)

Vote: 5

Authors: Jin Gao, Chenjie Cao, Chaohui Yu, Fan Wang, Yanqin Jiang, Weiming Hu

- **What's New**: 이번 논문에서는 4D(동적 3D) 생성에 대한 획기적인 접근 방법을 제안합니다. 이는 기존의 3D 구조 모델을 통합하여 스페이셜(spatial) 및 템포럴(temporal) 일관성을 동시에 유지하면서 3D 객체를 애니메이션화하는 방법입니다.
- **Technical Details**: 저자들은 Animate3D라는 새로운 4D 생성 프레임워크를 제안했습니다. 이 프레임워크는 기본적인 4D 생성 모델과 함께 4D Gaussian Splatting(4DGS) 최적화를 포함하며, 이는 MVDream 기반의 Multi-View Video Diffusion Model(MV-VDM)로 이루어졌습니다. spatio-temporal attention 모듈을 포함해 기존의 3D 및 비디오 확산 모델을 기반으로 공간 및 시간적 주의를 학습합니다. MV-VDM은 또한 Multi-view image를 참조함으로써 기존의 3D 객체의 아이덴티티와 세부사항을 충분히 보존합니다.
- **Performance Highlights**: MV-VDM를 활용하여 멀티뷰 동영상 데이터셋(MV-Video)을 구성, 115K 애니메이션과 1.8M 멀티뷰 동영상을 포함. 실험 결과, 이 프레임워크는 이전의 방식들보다 훨씬 나은 공간 및 시간적 일관성을 가진 4D 객체 생성이 가능합니다.

### [From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients](https://arxiv.org/abs/2407.11239)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11239.png)

Vote: 5

Authors: Zhangyang Wang, Lu Yin, Jiawei Zhao, Yuandong Tian, Shiwei Liu, Zhenyu Zhang, Ajay Jaiswal

- **What's New**: 현대의 딥러닝 환경에서 거대한 행렬들이 낮은 랭크(low-rank)의 구조를 보이는 것은 매우 흔한 일입니다. 이 연구는 대규모 언어 모델(LLMs) 내 여러 레이어에서 낮은 랭크 구조가 어떻게 나타나는지 탐구하고, 이러한 구조를 이용해 모델 압축과 메모리 효율적인 파인튜닝(memory-efficient fine-tuning, MEFT)을 가능하게 하는 새로운 기법을 제안합니다.
- **Technical Details**: 이 연구는 Attention 및 MLP 레이어마다 확연히 다른 낮은 랭크 구조의 출현을 살펴봅니다. 초기의 전역 점진적인 백프로파게이션 중 각 레이어의 가중치 매트릭스와 그라디언트를 분석하여, 일부 레이어에서는 그라디언트 매트릭스가 짧은 기간에 걸쳐 포화되지만, 다른 레이어에서는 계속해서 의미 있는 에러 신호를 전달하는 것을 발견했습니다. 이러한 패턴을 통해 '높은 품질의 안정적인 낮은 랭크 서브스페이스'가 가중치 매트릭스에 나타난다는 가설을 수립했습니다.
- **Performance Highlights**: 프로포즈된 Weight Low-Rank Projection (WeLore) 기술은 하나의 데이터 비의존적 레이어 단위 비균일 랭크 감소 기법으로, 높은 압축 비율을 유지하면서도 높은 컴퓨팅 효율성을 제공합니다. 특히, WeLore는 전체 파인튜닝과 유사한 성능을 제공하면서도 훈련 가능한 파라미터 수를 크게 줄이고, GPU 메모리 요구 사항을 현저히 감소시킵니다. 예를 들어, 50% 압축된 LLaMa-2 7B 모델의 경우 WeLore는 훈련 가능한 파라미터 수를 약 65% 줄이고, 처리량은 3배, GPU 요구 사항은 40% 줄였습니다.

### [Vibravox: A Dataset of French Speech Captured with Body-conduction Audio Sensors](https://arxiv.org/abs/2407.11828)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11828.png)

Vote: 4

Authors: Malo Olivier, Julien Hauret, Sarah Poirée, Christophe Langrenne, Éric Bavu, Véronique Zimpfer, Thomas Joubaud

- **What's New**: 전통적인 마이크와 달리, 신체의 진동을 통해 음성을 포착하는 바디 컨덕션 마이크(BCM)가 등장했습니다. 이 기술은 주변 소음의 영향을 크게 줄여 소음이 많은 환경에서도 유리합니다. 최근 피지컬 및 전자 공학 연구의 발전과 새로운 스킨 어태치블 센서(skin-attachable sensors)의 등장으로 이러한 기술의 잠재적 사용이 확장되고 있습니다.
- **Technical Details**: BCM은 피부 진동을 탐지하여 음성을 포착하며, 이는 음성의 신호 대 잡음비(signal-to-noise ratio)를 향상시키고, 곡면 피부에 쉽게 부착되는 특성을 가지고 있습니다. 하지만 이미 언급된 센서는 여전히 음성 신호의 전체 대역폭을 포착하지 못하고 있습니다. 이 문제를 해결하기 위해, 심층 학습(deep learning) 방법이 적용되고 있으며, 특정한 저주파 오디오 콘텐츠에서 중고주파를 재생성하는 오디오 슈퍼 해상도(audio super-resolution) 문제에 적응하는 연구가 진행되고 있습니다.
- **Performance Highlights**: Vibravox 데이터셋은 대역폭 확장, 음성 인식, 화자 검증의 세 가지 주요 작업에서 심층 학습 연구를 발전시킬 수 있는 잠재력을 가지고 있습니다. Whisper 및 Canary1B와 같은 모델은 신호 인식의 한계를 뛰어넘고 있으며, TitaNet, WavLM, Pyannote 및 ECAPA2는 신뢰성을 높여 다양한 신호를 사용할 수 있도록 하고 있습니다.
- **Dataset Importance**: 대규모 데이터셋의 availability는 BCM 연구 및 개발의 중요한 요소입니다. 기존의 데이터셋들은 규모와 다양성에서 한계가 있으며, 이는 대부분의 실제 응용 시나리오를 포괄하지 못합니다. Vibravox 데이터셋은 이러한 공백을 메우고 비전통적인 오디오 센서를 사용한 음성 캡처 연구를 촉진하기 위해 제공됩니다.

### [Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development](https://arxiv.org/abs/2407.11784)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11784.png)

Vote: 4

Authors: Daoyuan Chen, Haibin Wang, Yaliang Li, Yilun Huang, Jingren Zhou, Bolin Ding, Ce Ge

- **What's New**: Data-Juicer Sandbox라는 새로운 도구셋이 도입되었습니다. 이는 멀티모달 데이터와 생성 모델의 공동 개발을 촉진하는 종합적인 세트로, 실험적인 탐구를 더 통찰력 있고 체계적으로, 편리하게 재사용할 수 있도록 합니다.
- **Technical Details**: Data-Juicer Sandbox는 오픈 소스 데이터 처리 시스템인 Data-Juicer를 기반으로 하며, 멀티모달 생성 모델을 위해 최적화된 다양한 구성 요소를 통합합니다. 사용자는 다양한 데이터 파이프라인과 모델 구성들을 신속하게 탐색할 수 있으며, 비용 효율적인 데이터셋과 모델에서 실행할 수 있습니다.
- **Performance Highlights**: ["멀티모달 데이터와 생성 모델 간의 상호작용을 탐구하기 위해 '탐색-분석-정제' 워크플로우를 제안합니다.", '예시 모델로 Mini-Gemini(이미지에서 텍스트 생성)와 EasyAnimate(텍스트에서 비디오 생성)를 적용하여, VBench 리더보드에서 상위 성과를 달성하였습니다.', '데이터 처리 연산자 31개와 모델 메트릭 35개를 연결하는 통찰력 있는 분석을 통해 데이터 다양성과 모델 성능 간의 균형을 최적화하였습니다.']

### [Click-Gaussian: Interactive Segmentation to Any 3D Gaussians](https://arxiv.org/abs/2407.11793)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11793.png)

Vote: 2

Authors: Hyeonseop Song, Hoseok Do, Jaechul Kim, Seokhun Choi, Taehyeong Kim

- **What's New**: 최근 Neural Radiance Fields(NeRF) 및 3D Gaussian Splatting(3DGS)와 같은 신경 렌더링 기술의 발전이 복잡한 3D 환경에서 사진 실감 이미지를 합성하는 데 큰 영향을 미쳤습니다. 이러한 기술은 가상 및 증강 현실, 디지털 콘텐츠 생성, 실시간 렌더링 등 다양한 분야에서의 응용을 가능하게 합니다.
- **Technical Details**: 본 연구에서는 Click-Gaussian이라는 새로운 방법을 제안합니다. 이 방법은 SAM에서 추출한 2D 분할 마스크를 사용하여 3D Gaussians의 augmented feature로 승격시킵니다. 이 접근법은 두 가지 수준의 세분성을 통해 미세한 분할을 가능하게 합니다. 또한, 다른 뷰 간의 2D 마스크 불일치 문제를 해결하기 위해 Global Feature-guided Learning(GFL) 전략을 도입하여 일관된 3D feature field를 개발합니다.
- **Performance Highlights**: 복잡한 실제 장면에서 Click-Gaussian의 효능을 입증하기 위해 다양한 실험을 수행한 결과, 이 접근법은 정확성과 처리 시간면에서 상당한 개선을 보였습니다. 이는 3D 장면 편집 및 다양한 애플리케이션에서 실시간 상호작용을 가능하게 합니다.

### [Grasping Diverse Objects with Simulated Humanoids](https://arxiv.org/abs/2407.11385)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11385.png)

Vote: 2

Authors: Alexander Winkler, Zhengyi Luo, Jinkun Cao, Kris Kitani, Sammy Christen, Weipeng Xu

- **What's New**: Omnigrasp라는 새로운 풀바디와 손재주 있는 휴머노이드 컨트롤러를 도입했습니다. 이 컨트롤러는 강화 학습(Reinforcement Learning, RL)을 통해 다양한 객체의 궤적을 따르면서 객체를 픽업하는 능력을 가지고 있습니다. 'Omni'는 모든 방향에서 다양한 궤적을 따르고 다양한 객체를 잡는 것을 의미합니다. Omnigrasp는 미리 학습된 보편적인 손재주 있는 모션 표현을 사용하여 새로운 객체 형태와 궤적에도 일반화를 이룹니다.
- **Technical Details**: Omnigrasp는 객체와 궤적 정보를 입력으로 사용하며, 별도의 그립이나 레퍼런스 바디 모션 없이 학습을 진행합니다. 학습에는 무작위로 생성된 궤적만을 사용하여 페어링된 휴머노이드-객체 모션 데이터는 필요하지 않습니다. 키네틱 체인을 통해 몸통의 탐색 노이즈가 팔과 손목의 위치에 큰 변동을 일으키는 문제를 해결하기 위해, 보편적인 손재주 있는 모션 표현을 활용합니다. 이 모션 표현은 대규모 인간 모션 데이터베이스에서 학습되었으며, 기존의 몸통 모션 데이터세트에 관절형 손 동작을 포함하도록 확장되었습니다.
- **Performance Highlights**: Omnigrasp는 새로운 객체를 추가 처리 없이 즉각적으로 운송할 수 있으며, 다양한 객체 궤적을 따라가면서 최고 수준의 성공률을 달성합니다. 이 컨트롤러는 단순하면서도 효과적인 상태 및 보상 설계를 통해 더 높은 학습 샘플 효율성을 발휘하며, 다양한 트레이닝 객체까지 스케일링하고 보지 못한 객체에도 일반화를 이룹니다. 또한, 인간-객체 상호작용 데이터 없이도 높은 그립 성공률을 보여줍니다.

### [Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models](https://arxiv.org/abs/2407.11282)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11282.png)

Vote: 1

Authors: Qifan Wang, Wenyue Hua, Felix Juefei-Xu, Ruixiang Tang, Guangyan Sun, Shiqing Ma, Qingcheng Zeng, Yanda Meng, Mingyu Jin, Zhenting Wang, Fan Yang, Kaize Ding, Zihao Zhou, Qinkai Yu, Yongfeng Zhang

- **What's New**: 이번 연구에서는 대형 언어 모델(LLM, Large Language Model)의 불확실성(uncertainty)을 악의적으로 조작할 수 있는 백도어(backdoor) 공격 방법을 제안합니다. 이는 기존 연구들이 주로 LLM의 불확실성 추정 향상에 집중했던 것과는 달리, 불확실성 추정의 로버스트니스(robustness)를 탐구하는 첫 사례입니다.
- **Technical Details**: 제안된 백도어 공격은 KL발산(KL divergence)을 이용하여 모델의 불확실성을 조작합니다. 백도어 함수를 입력에 삽입하여 특정 트리거(trigger)가 있을 때 불확실성을 높이고, 정상적인 입력에서는 모델 출력을 변경하지 않습니다. 이는 다수의 공격 기법들(gradient-based attacks, red-teaming, jailbreak attacks 등)을 포함한 다양한 어택 시나리오에서 적용될 수 있습니다.
- **Performance Highlights**: 실험 결과, 제안된 백도어 공격은 다중 선택 문제(multiple-choice questions)와 다양한 도메인 데이터셋에서 높은 성능을 보여주었습니다. 2000개의 일반 다중 선택 질문을 사용하여 일반적인 불확실성 패턴을 효과적으로 변화시킬 수 있음을 입증했습니다. 또한, 다수의 모델과 데이터셋을 대상으로 한 광범위한 실험을 통해 백도어 공격의 일반화 성능을 확인했습니다.

