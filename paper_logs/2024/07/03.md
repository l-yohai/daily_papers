## Daily Papers (2024-07-03)

### [Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems](https://arxiv.org/abs/2407.01370)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.01370.png)

Vote: 42

Authors: Chien-Sheng Wu, Caiming Xiong, Philippe Laban, Alexander R. Fabbri

- **What's New**: 이 논문에서는 SummHay라는 새로운 테스트베드를 도입하여 long-context 모델과 RAG(Retrieval Augmented Generation) 시스템의 성능을 평가하고자 합니다. SummHay는 대량의 문서에서 특정 정보를 추출하고 요약하는 과제를 통해 모델의 실제 능력을 평가합니다.
- **Technical Details**: SummHay 테스트베드는 주제별로 문서 모음을 생성하는 데이터 합성 프로그램을 통해 구축됩니다. 각 문서 모음은 'Haystack'이라고 불리며, 특정 검색 쿼리에 관련된 정보를 반복적으로 포함하도록 설계되었습니다. 평가 프로토콜은 참고 인사이트의 적용 범위와 인용의 품질을 중심으로 진행됩니다.
- **Performance Highlights**: 실험 결과에 따르면 SummHay가 평가한 모든 시스템은 인간 성능에 비해 상당히 낮은 성능을 보였습니다. RAG 파이프라인과 long-context LLM을 선택할 때 비인용 품질과 인사이트 커버리지 간의 비직관적인 트레이드오프가 존재합니다. 또한, RAG 컴포넌트를 사용하여 성능이 향상되었음을 확인했습니다. 마지막으로, SummHay는 'lost in the middle phenomenon'이라는 현상을 통해 대부분의 LLM이 컨텍스트 윈도우의 상단이나 하단에 있는 정보에 편향된다는 것을 확인했습니다.

### [OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation](https://arxiv.org/abs/2407.02371)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.02371.png)

Vote: 25

Authors: Tiehan Fan, Zhenheng Yang, Zhijie Chen, Kepan Nan, Rui Xie, Ying Tai, Penghao Zhou, Jian Yang, Xiang Li

- **What's New**: 텍스트-비디오(T2V) 생성이라는 새로운 시각적 이해 과제가 집중 조명을 받고 있습니다. 이는 텍스트를 바탕으로 비디오 시퀀스를 생성하는 기술입니다. 이번 연구에서는 대규모 다중 모달리티 모델인 Sora를 기반으로 한 텍스트-비디오 데이터 세트를 활용하여 새로운 고품질 데이터 세트 OpenVid-1M을 소개합니다.
- **Technical Details**: 이 논문에서는 두 가지 주요 문제를 지적합니다: 첫째, 기존의 인기 비디오 데이터 셋(WebVid-10M, Panda-70M)은 품질이 낮거나 너무 커서 대부분의 연구 기관에서 사용하기 어렵습니다. 둘째, 텍스트 정보를 충분히 활용하지 못하는 문제입니다. 이를 해결하기 위해, 새로운 다중 모달 비디오 확산 트랜스포머(MVDiT)가 제안되었습니다. 이는 시각적 토큰과 텍스트 토큰 간의 상호작용을 촉진하여 비디오의 품질을 개선합니다.
- **Performance Highlights**: 제안된 OpenVid-1M은 100만 개 이상의 엄선된 비디오 클립과 512×512 이상의 고해상도를 자랑합니다. 또한, STDiT와 MVDiT 모델을 사용한 실험을 통해 OpenVid-1M 데이터 셋의 우수성과 MVDiT의 효과가 입증되었습니다. 이로써 T2V 생성의 품질을 크게 향상시킬 수 있었습니다.

### [Agentless: Demystifying LLM-based Software Engineering Agents](https://arxiv.org/abs/2407.01489)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.01489.png)

Vote: 19

Authors: Lingming Zhang, Chunqiu Steven Xia, Yinlin Deng, Soren Dunn

- **What's New**: 이번 논문에서는 LLMs(Large Language Models)을 이용한 기존의 에이전트 기반(Agent-based) 접근 방식의 한계를 극복하기 위해 'Agentless'라는 새로운 접근 방식을 소개합니다. Agentless는 복잡한 도구나 에이전트를 사용하지 않고, 소프트웨어 개발 문제를 자동으로 해결하는 단순한 방법론을 제안합니다.
- **Technical Details**: Agentless는 두 단계(Phases)로 구성됩니다: 로컬라이제이션(Localization)과 수리(Repair). 첫 번째 단계에서는 파일 수준, 클래스/함수 수준, 그리고 세부 코드 위치까지 결함을 찾아냅니다. 두 번째 단계에서는 확인된 위치에서 여러 후보 패치를 생성하고, 간단한 필터링 과정을 통해 최종 패치를 선택합니다.
- **Performance Highlights**: Agentless는 SWE-bench Lite 벤치마크에서 27.33%의 성능을 달성하며, 이는 모든 오픈 소스 접근 방식 중 최고 수준입니다. 또한, 손쉬운 설계 덕분에 비용도 절감할 수 있었습니다.

### [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](https://arxiv.org/abs/2407.02490)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg)

Vote: 12

Authors: Dongsheng Li, Amir H. Abdi, Qianhui Wu, Surin Ahn, Chin-Yew Lin, Huiqiang Jiang, Xufang Luo, Chengruidong Zhang, Yucheng Li, Lili Qiu, Zhenhua Han, Yuqing Yang

- **What's New**: MInference라는 새로운 기술이 소개되었습니다. 이는 Long-context LLMs의 결과 생성 시간을 최대 10배까지 줄일 수 있으며, 정확도를 유지하거나 개선할 수 있다고 합니다.
- **Technical Details**: 기존의 정적 희소 메소드 대신, MInference는 동적 희소 주의를 효율적으로 예측하여 주의 계산의 95% FLOP를 줄입니다. 세 가지 일반적인 희소 패턴(A-shape, Vertical-Slash, Block-Sparse)을 식별하여 최적의 주의 패턴을 할당하고, 이를 기반으로 실시간으로 동적 희소 마스크를 작성합니다.
- **Performance Highlights**: MInference는 다양한 Long-context LLM들(LLaMA-3-8B, GLM-4-9B, Yi-9B)에서 사전 채우기 단계를 30분에서 3분으로 단축했습니다. 이는 특히 1백만 개의 토큰 맥락에서 큰 성능 향상을 보여줍니다.

### [Consistency Flow Matching: Defining Straight Flows with Velocity Consistency](https://arxiv.org/abs/2407.02398)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.02398.png)

Vote: 10

Authors: Zhilong Zhang, Bin Cui, Ling Yang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, Zixiang Zhang

- **What's New**: 최근 몇 년 동안 딥 생성 모델(Deep Generative Models)은 데이터 분포를 모델링하여 고품질 샘플을 생성하는 매력적인 패러다임을 제공해왔습니다. 본 논문에서는 새로운 기본 FM 방법인 일관성 흐름 일치(Consistency Flow Matching, Consistency-FM)를 제안합니다. 이 방법은 속도 필드에서 자가 일관성(self-consistency) 속성을 명시적으로 적용하여 흐름을 직선화하는 것을 목표로 합니다.
- **Technical Details**: Consistency-FM은 각기 다른 시간에서 동일한 끝점으로 시작하는 직선 흐름을 정의하고, 그 속도 값을 제약합니다. 복잡한 분포 간의 이동을 더 잘 가능하게 하고 모델 표현력을 강화하기 위해, 우리는 Consistency-FM을 다중 구간 접근 방식으로 학습합니다. 이는 조각별 선형 궤적을 구성하며, 유연한 시간 간 점프를 통해 사전 학습된 FM 모델을 증류(distillation)하여 샘플링 속도와 품질 간의 보다 나은 균형을 이룰 수 있습니다.
- **Performance Highlights**: 세 가지 클래식 이미지 데이터셋에서의 예비 실험 결과, Consistency-FM은 기존 모델보다 4.4배 및 1.7배 더 빠른 학습 효율성을 보이며 우수한 생성 품질을 입증했습니다. 이는 특히 속도 필드에서의 일관성 속성을 적용함으로써 ODE 궤적을 직선화하는 고수준의 정규화를 통해 달성되었습니다.

### [What Matters in Detecting AI-Generated Videos like Sora?](https://arxiv.org/abs/2406.19568)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19568.png)

Vote: 8

Authors: Xiaojuan Qi, Zhengzhe Liu, Xiaoyang Lyu, Chirui Chang

- **What's New**: 최근 영상 확산 모델(video diffusion model)들은 놀라운 시간적 일관성을 가지고 사실적인 영상을 생성할 수 있다는 점을 보여주었습니다. 대표적으로 Stable Video Diffusion(SVD)와 Sora가 있으며, 특히 Sora는 '세계 시뮬레이터(world simulator)'로 불리기까지 합니다. 그러나 이러한 모델들이 실제로 세계 시뮬레이터로 역할할 수 있는지에 대한 의문이 제기되고 있습니다. 본 연구에서는 SVD를 활용하여 생성된 합성 영상과 Pexels에서 수집한 실제 영상을 비교 분석하고, 현재의 영상 생성 기술의 한계를 조사합니다.
- **Technical Details**: 연구에서는 SVD를 사용하여 합성 영상을 생성하고, 실제 영상은 Pexels에서 수집합니다. 영상은 세 가지 주요 차원을 통해 분석됩니다: 외관(appearance), 동작(motion), 기하학(geometry). 이를 위해 원시 영상 프레임과 시각적 기준 모델(visual foundation models)의 특징, RAFT의 옵티컬 플로우(optical flow), Marigold와 UniDepth의 단안 심도(monocular depth)를 이용합니다. 또한, 3D 합성곱 신경망(convolutional neural network)을 기반으로 각 차원별로 영상 분류기를 구성하고 Grad-CAM을 사용하여 모델이 의존하는 단서를 분석합니다.
- **Performance Highlights**: 분석 결과, 분류기들은 놀라운 성능을 보여주었으며, 인-도메인 세팅에서는 외관과 기하학적 단서를 통해 90% 이상의 정확도를 기록했습니다. 이러한 성능은 Sora로 생성된 영상에서도 70%의 정확도로 그대로 적용되었습니다. 주요 발견 사항으로는, 생성된 영상들이 색상 일관성과 질감 왜곡에 어려움을 겪고 있으며, 비자연적인 동작을 보여주고, 실제 세계의 기하학적 규칙을 위반하는 경우가 많다는 점입니다. 이러한 분석을 바탕으로, 우리의 모델은 SVD로 생성된 합성 영상만으로 훈련되었음에도 불구하고, Sora, Pika Labs, Runway-Gen2로 생성된 영상을 80% 이상의 정확도로 판별할 수 있었습니다. 이는 영상 생성 모델 간에 일관된 차이가 존재하며, 세계 시뮬레이터로 나아가기 위해 지속적인 연구가 필요함을 시사합니다.

### [Understanding Alignment in Multimodal LLMs: A Comprehensive Study](https://arxiv.org/abs/2407.02477)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.02477.png)

Vote: 6

Authors: Zirui Wang, Jean-Philippe Fauconnier, Peter Grasch, Rinu Boney, Christian Kerl, Christoph Roesmann, Yinfei Yang, Zhe Gan, Elmira Amirloo, Afshin Dehghan, Yusu Qian

- **What's New**: 이번 논문에서는 새로운 자연어 처리(NLP: Natural Language Processing) 모델을 소개하고 있습니다. 이 모델은 Transformer 기반 구조를 활용하여 기존 대비 성능 향상을 보여줍니다.
- **Technical Details**: 모델은 self-attention 메커니즘을 사용하며, multi-head attention과 feed-forward neural network를 포함합니다. 또한, positional encoding을 통해 시퀀스 데이터의 순서를 반영합니다.
- **Performance Highlights**: 해당 모델은 여러 벤치마크 데이터셋에서 기존 최첨단 모델보다 우수한 성능을 나타냈습니다. 특히, 기계 번역(machine translation), 텍스트 요약(text summarization), 질문 응답(question answering) 등 다양한 작업에서 높은 정확도를 기록하였습니다.

### [Revealing Fine-Grained Values and Opinions in Large Language Models](https://arxiv.org/abs/2406.19238)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.19238.png)

Vote: 6

Authors: Arnav Arora, Serge Belongie, Nadav Borenstein, Dustin Wright, Isabelle Augenstein, Srishti Yadav

- **What's New**: 이 연구는 정치적 가치 및 의견을 탐지하여 사용자 경험 개선과 피해 방지를 목표로 하는 언어 모델(LLM) 평가의 새로운 방법론을 제안합니다. 156,240개의 LLM 응답을 사용하여 다양한 인구통계적 특성을 갖춘 420개의 프롬프트 변형을 통해 분석을 수행합니다.
- **Technical Details**: 연구는 Political Compass Test (PCT)를 기초로 6개의 LLM을 사용하여 대규모 데이터셋을 생성합니다. 프롬프트 템플릿은 '인구통계학적 특성(demographic)'과 '지시사항(instruction)'을 포함하도록 구성되었습니다. 인구통계학적 특성은 나이, 성별, 국가, 정치적 성향, 계층 등을 포함합니다. 모델은 오픈 엔디드(open-ended)와 클로즈드 폼(closed form) 생성 설정에서 평가되었습니다.
- **Performance Highlights**: 주요 발견 사항으로는 인구통계적 특성이 추가된 프롬프트가 PCT 결과에 상당한 영향을 미쳐 편향(bias)을 반영한다는 점입니다. 오픈 엔디드 응답과 클로즈드 폼 응답 간의 차이가 인구통계적 특성에 따라 달라질 수 있으며, 텍스트 응답에서 반복적으로 나타나는 유사한 패턴을 통해 LLM이 다양한 설정에서 생성하는 정당화를 밝혀냈습니다.

### [FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds](https://arxiv.org/abs/2407.01494)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.01494.png)

Vote: 6

Authors: Zhening Xing, Yanhong Zeng, Yuancheng Wang, Kai Chen, Yiming Zhang, Zhizheng Wu, Yicheng Gu

- **What's New**: FoleyCrafter는 기존 텍스트-투-오디오 (text-to-audio) 모델을 활용하여 비디오에 맞춘 고품질의 효과음을 생성하는 새로운 Neural Foley 프레임워크입니다. 이 모델은 비디오와의 의미적 및 시간적 정렬을 보장하는 동시에 텍스트 프롬프트(text prompt)를 통해 세밀한 조정이 가능합니다.
- **Technical Details**: FoleyCrafter는 두 가지 주요 구성 요소로 이루어져 있습니다. 첫째, 시맨틱 어댑터(semantic adapter)는 병렬 교차 주의 계층(cross-attention layers)을 사용하여 비디오 특징을 기반으로 오디오를 생성합니다. 둘째, 시간 제어기(temporal controller)는 비디오의 시간적 흐름과 맞추어 소리와 침묵을 정렬하기 위해 온셋 감지기(onset detector)와 타임스탬프 기반 어댑터(timestamp-based adapter)를 사용합니다. 훈련 중에는 시맨틱 어댑터와 시간 제어기를 비디오-오디오 대응 데이터로 학습하며, 기준 텍스트-투-오디오 모델은 유지됩니다.
- **Performance Highlights**: FoleyCrafter는 정량적 분석, 질적 비교 및 사용자인 연구를 통해 음질과 비디오 정렬 측면에서 뛰어난 성능을 입증했습니다. 추가적으로, 텍스트 프롬프트를 통해 모델의 제어 가능성을 보여줬습니다. 이 모델은 공통으로 사용되는 벤치마크에서 최첨단 성과를 달성하였습니다.

### [To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models](https://arxiv.org/abs/2407.01920)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.01920.png)

Vote: 5

Authors: Qingbin Liu, Dianbo Sui, Ningyu Zhang, Xi Chen, Siyuan Cheng, Xiaozhuan Liang, Mengru Wang, Huajun Chen, Bozhong Tian

- **What's New**: KnowUnDo: 대형 언어 모델(LLMs)에서 민감한 데이터를 제거하기 위해 제안된 새로운 벤치마크. 이는 저작권과 프라이버시 법령을 바탕으로 데이터의 'Unlearn Scope'와 'Retention Scope'를 구분하고, 이를 통해 더 세밀하고 효율적인 지식 제거를 가능하게 함.
- **Technical Details**: 모델 ℳ(M)의 파라미터 θ를 기준으로 정의되며, 특정 암기 인스턴스 ℐ와 관련된 입력 x와 예측 y로 매핑됨. 기존 방법은 파라미터를 무차별적으로 업데이트해 모든 관련 지식을 잊도록 하나, KnowUnDo는 이를 유연하게 조정함. MemFlex는 Unlearn Scope와 Retention Scope를 명확히 구분하여 민감한 정보를 지워내는 새로운 방법론.
- **Performance Highlights**: MemFlex는 기존 방법 대비 Unlearn Success를 평균 7.97% 향상시키고, 학습 단계별 소요 시간을 11.76% 단축시키는 등 일반 능력에 최소한의 영향을 주면서 뛰어난 성능을 보여줌.

### [Magic Insert: Style-Aware Drag-and-Drop](https://arxiv.org/abs/2407.02489)

![](https://cdn-avatars.huggingface.co/v1/production/uploads/6318c6e6cb116eab31d70a16/yAw5XtvqaPUkPj2B6JOzz.jpeg)

Vote: 4

Authors: Shlomi Fruchter, David E. Jacobs, Neal Wadhwa, Yael Pritch, Michael Rubinstein, Yuanzhen Li, Nataniel Ruiz

- **What's New**: 최근 고품질 이미지 생성에 큰 진전을 보인 대형 텍스트-이미지 모델들의 유용성을 높이기 위해 '컨트롤러빌리티(Controllability)'가 중요하다는 논의가 활발히 진행되고 있습니다. 본 연구에서는 스타일 인식 드래그 앤 드롭(Style-aware Drag-and-Drop)이라는 새로운 응용 문제를 제안하고 'Magic Insert'라는 해결 방법을 소개했습니다. Magic Insert는 현재의 기준을 뛰어넘는 성능을 보여줍니다.
- **Technical Details**: Magic Insert는 두 가지 주요 하위 문제를 해결합니다: 스타일 인식 개인화(Style-aware Personalization)와 스타일화된 이미지 내에서의 현실적인 객체 삽입(Realistic Object Insertion). 스타일 인식 개인화를 위해 확산 모델(Diffusion Model)의 임베딩 및 가중치 공간에서 주제 학습(Subject Learning)을 사용하며, 스타일 주입(Adapter Injection of Style) 기법을 결합합니다. 부트스트랩 도메인 적응(Bootstrap Domain Adaptation)이라는 혁신적인 기법을 도입하여 실제 이미지에서 훈련된 객체 삽입 네트워크를 스타일화된 이미지 도메인으로 점진적으로 전환합니다.
- **Performance Highlights**: Magic Insert는 목표 스타일을 충실히 따르면서 주제의 본질과 정체성을 유지하고, 스타일화된 주제를 생성된 이미지에 현실적으로 삽입할 수 있습니다. 이 방법은 원하는 스타일화의 정도와 원래 주제의 세부사항과 포즈를 얼마나 충실히 보존할지에 대한 유연성을 제공합니다.

### [μ-Bench: A Vision-Language Benchmark for Microscopy Understanding](https://arxiv.org/abs/2407.01791)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.01791.png)

Vote: 2

Authors: Alejandro Lozano, Alyssa Unell, James Burgess, Sanket Rajan Gupte, Yuhui Zhang, Serena Yeung-Levy, Jeffrey Nirschl

- **What's New**: 새로운 연구는 다양한 미지의 및 출판된 데이터셋에서 전문가 주석이 추가된 17,235개의 현미경 이미지로 구성된 고품질 비전-언어 벤치마크(benchmark)인 'μ-Bench'를 도입합니다. 이는 22개의 인지 및 지각 작업(perception and cognition tasks)을 포함하며, 8가지 현미경 서브 모달리티와 24가지 염색 기술을 다룸으로써 12개의 과학 분야를 대표합니다.
- **Technical Details**: 연구의 기술적 세부 사항으로는 1) 다양한 과학 분야에서 현미경 이미지를 수집하고 2) 웹 어플리케이션을 사용해 여러 미세형 작업에 대한 질문을 대중적으로 모집합니다. 선택된 데이터셋은 per-image json 메타데이터와 함께 무손실 PNG 형식으로 표준화되었으며, Apache Arrow 파일 형식으로 배포됩니다. 또한, 전문가 검토를 통해 생물학 개념 및 관계 지식 그래프와 연결될 수 있는 다중 bio-ontology 식별자로 주석이 추가됩니다.
- **Performance Highlights**: μ-Bench를 이용해 최신 일반 및 도메인별 생물의료 비전-언어 모델(VLMs)을 특성화한 결과, 최고 성능의 VLM조차도 현미경 작업 전반에 걸쳐 높은 오류율을 보였으며 일반 VLM 모델이 특화된 모델보다 때때로 더 나은 성능을 보였습니다. 특정 도메인에서의 특화된 fine-tuning은 기본 모델에 있던 생물학적 지식의 '망각(catastrophic forgetting)'을 일으킬 수 있음이 관찰되었습니다. 이를 해결하기 위해 가중치 앙상블 전략이 제안되었습니다.

