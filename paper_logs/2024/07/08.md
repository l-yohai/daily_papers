## Daily Papers (2024-07-08)

### [Unveiling Encoder-Free Vision-Language Models](https://arxiv.org/abs/2406.11832)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11832.png)

Vote: 26

Authors: Haiwen Diao, Xiaotong Li, Yueze Wang, Xinlong Wang, Huchuan Lu, Yufeng Cui

- **What's New**: 최근 대규모 언어 모델(LLMs)의 발전으로 비전-언어 모델(VLMs)이 등장하면서 강력한 시각적 인지능력을 발휘하고 있습니다. 이러한 모델들은 시각적 질문 응답, 이미지 캡셔닝, 세계 지식 이해 등 다양한 작업에서 그 능력을 보여주고 있습니다. 본 논문에서는 Vicuna-7B에서 발전된 EVE-7B라는 인코더가 없는 새로운 VLM을 소개하며, 이는 고해상도 이미지를 자연스럽게 처리할 수 있습니다.
- **Technical Details**: 기존 VLMs는 정사각형 고정 크기 이미지로 사전 훈련되었지만, 본 연구에서는 이러한 제한을 극복하기 위해 인코더 없는 (encoder-free) VLM을 제안합니다. EVE-7B는 Vicuna-7B를 바탕으로 하여 두 개의 8-A100(40G) 노드에서 약 9일 동안 훈련되었습니다. 이 모델은 이미지 데이터를 거의 손실 없이 전달하여 자율적으로 필요한 정보를 얻을 수 있게 하며, LLM 중심의 접근 방식을 사용해 시각적 인식을 발전시킵니다. 또한, 기존의 다운스트림 작업에서 고성능을 발휘하도록 설계되었습니다.
- **Performance Highlights**: EVE-7B는 35M 공개 데이터만을 사용하여, 비슷한 용량의 인코더 기반 VLM과 견줄만한 성능을 보입니다. 이는 Fuyu-8B와 같은 다른 인코더 없는 네트워크보다 성능이 뛰어나며, 특히 높은 해상도 이미지를 처리하는 데서 큰 장점을 가집니다. 결과적으로, EVE-7B는 기존의 비전-언어 모델과 비교하여 투명성, 효율성, 실용성 면에서 혁신적인 연구 방향을 제시합니다.

### [AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents](https://arxiv.org/abs/2407.04363)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04363.png)

Vote: 16

Authors: Artyom Sorokin, Dmitry Evseev, Nikita Semenov, Mikhail Burtsev, Evgeny Burnaev, Petr Anokhin

- **What's New**: 본 논문은 새로운 머신 러닝(machine learning) 모델을 제안하고 있습니다. 이 모델은 기존의 방법론에 비해 높은 정확도와 효율성을 자랑합니다. 특히 자연어 처리(NLP) 분야에서의 응용 가능성을 검토하였습니다.
- **Technical Details**: 제안된 모델은 Transformer 구조를 기반으로 하며, Attention 메커니즘을 활용하고 있습니다. 이 모델은 데이터 전처리(pre-processing) 및 후처리(post-processing) 단계를 포함하여 종단 간(end-to-end) 학습이 가능합니다. 또한, Hyperparameter Tuning과 같은 튜닝 기법도 적용되었습니다.
- **Performance Highlights**: 실험 결과, 본 모델은 다양한 벤치마크 데이터셋(benchmark datasets)에서 최고 성능을 달성하였습니다. 특히, BLEU Score와 같은 평가 지표에서 탁월한 성과를 보였습니다. 이는 기존 모델 대비 최대 15%의 성능 향상을 의미합니다.

### [FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs](https://arxiv.org/abs/2407.04051)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04051.png)

Vote: 16

Authors: Tongyi SpeechTeam

- **What's New**: 최근 몇 년간 인공지능(AI)의 발전은 GPT-4o(OpenAI, 2023) 및 Gemini-1.5(Reid et al., 2024)와 같은 모델을 통해 인간과 기계 간의 상호 작용 방식을 크게 변화시켰습니다. 특히 음성 처리가 주목받고 있으며, 고정밀 음성 인식(Radford et al., 2023), 감정 인식(Ma et al., 2024b), 및 음성 생성(Wang et al., 2023a; Du et al., 2024a) 등의 기술이 직관적이고 인간 같은 상호 작용을 가능하게 하고 있습니다. 이 보고서에서는 인간과 대형 언어 모델(LLMs) 간의 자연스러운 음성 상호 작용을 촉진하기 위한 혁신적인 프레임워크인 FunAudioLLM을 소개합니다.
- **Technical Details**: FunAudioLLM은 음성 이해를 위한 모델 SenseVoice와 음성 생성을 위한 모델 CosyVoice로 구성되어 있습니다. SenseVoice는 다국어 음성 인식 및 감정 인식, 오디오 이벤트 감지 기능을 제공합니다. SenseVoice-Small과 SenseVoice-Large 두 가지 버전이 있으며, SenseVoice-Small은 중국어, 영어, 광둥어, 일본어, 한국어에 대한 다언어 인식을 지원하며 매우 낮은 추론 지연을 제공합니다. SenseVoice-Large는 50개 이상의 언어를 지원하며, 특히 중국어와 광둥어 인식에서 뛰어납니다. CosyVoice는 다국어 음성 생성을 위한 모델로, 구체적 맥락 학습(zero-shot learning), 교차 언어 음성 복제 및 감정 표현 가능 음성 생성을 지원합니다. CosyVoice는 5개 언어(중국어, 영어, 일본어, 광둥어, 한국어)를 지원하며, 세 가지 오픈 소스 모델 (CosyVoice-base-300M, CosyVoice-instruct-300M, CosyVoice-sft-300M)을 포함하고 있습니다.
- **Performance Highlights**: SenseVoice는 Whisper-small보다 5배 이상, Whisper-large보다 15배 이상 빠른 성능을 제공합니다. 대형 모델인 SenseVoice-Large는 50개 이상의 언어에서 고정밀 음성 인식 기능을 제공하며, 특히 중국어와 광둥어 인식에서 뛰어난 성능을 발휘합니다. CosyVoice는 사람과 유사한 수준의 품질로 음성을 생성할 수 있으며, 단 3초의 발화 샘플로 구체적 맥락 학습(zero-shot learning)을 통해 새로운 음성을 복제할 수 있습니다. CosyVoice-Small의 경우 사용자 정의 명령어를 통해 음성의 세부 특성까지 제어할 수 있습니다.

### [ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild](https://arxiv.org/abs/2407.04172)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04172.png)

Vote: 8

Authors: Ahmed Masry, Aayush Bajaj, Megh Thakkar, Enamul Hoque, Shafiq Joty, Aaryaman Kartha

- **What's New**: 새로운 모델인 ChartGemma는 차트 이해와 추론을 위한 첫 번째 멀티모달 모델로, 차트 이미지에서 직접 생성된 데이터를 통해 인스트럭션 튜닝(instruction-tuning)되었습니다. 이 모델은 PaliGemma 백본을 사용하며, 기존 모델보다 작지만 더 효과적으로 차트 이해 작업을 수행할 수 있습니다.
- **Technical Details**: ChartGemma는 차트에서 직접 시각적 인스트럭션 튜닝 데이터를 생성합니다. 차트를 이해하고 대응하는데 중요한 다양한 시각적 요소와 복잡한 추세를 캡처할 수 있는 데이터를 사용합니다. 인스트럭션 튜닝 데이터는 기본 과제와 오픈엔디드 과제로 나뉘어 생성됩니다. 기본 과제는 체인 오브 쏘트(Chain-of-thought), 요약(Summarization), 사실 체크(Fact Checking), 차트-투-마크다운(Chart-to-Markdown), 프로그램 지원 설계(Program Aided Design) 등을 포함합니다.
- **Performance Highlights**: ChartGemma는 차트 요약, 질문 응답, 사실 체크 등 5개의 기준 테스트에서 기존 방법에 비해 최첨단 결과를 얻었습니다. 또한, 정성적 연구에서 ChartGemma는 다른 방법들보다 신뢰성과 사실성이 높은 차트 요약을 생성하는 것으로 나타났습니다. 예를 들어, ChartGemma의 인스트럭션 정확도는 82%로 이전의 ChartInstruct 데이터셋의 61% 대비 상당히 개선된 것으로 나타났습니다.

### [Stark: Social Long-Term Multi-Modal Conversation with Persona Commonsense Knowledge](https://arxiv.org/abs/2407.03958)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03958.png)

Vote: 7

Authors: Ho-Jin Choi, Byungsoo Ko, Jonghwan Hyeon, Dokyong Lee, Kyeongjin Oh, Junyoung Youn, Young-Jun Lee

- **What's New**: 이 논문에서는 새로운 딥러닝 모델(Deep Learning Model)을 소개했습니다. 이 모델은 대규모 데이터셋(Dataset)에서 학습하여 고도로 복잡한 문제를 해결하는 데 사용됩니다.
- **Technical Details**: 연구진은 Transformer 아키텍처(Transformer Architecture)를 사용하여 이 모델을 개발하였습니다. 특히 멀티-헤드 어텐션(Multi-Head Attention) 메커니즘을 적용하여 성능을 크게 향상시켰습니다. 데이터 전처리(Data Preprocessing) 단계에서 여러 기법을 사용하여 모델의 정밀도를 높였습니다. 또한, GPU 클러스터를 이용한 병렬 처리로 학습 시간을 단축시켰습니다.
- **Performance Highlights**: 새 모델은 기존의 최고 성능 모델들을 뛰어넘는 결과를 보여주었습니다. 특히 이미지 인식(Image Recognition), 자연어 처리(Natural Language Processing) 등에서 우수한 성능을 기록하였습니다. 모델의 정확도(Accuracy)는 기존 모델 대비 5% 향상되었으며, 속도(Speed)는 약 30% 증가하였습니다.

### [Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://arxiv.org/abs/2407.04620)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04620.png)

Vote: 7

Authors: Yu Sun, Xinhao Li, Yann Dubois, Carlos Guestrin, Sanmi Koyejo, Xiaolong Wang, Karan Dalal, Arjun Vikram, Tatsunori Hashimoto, Genghan Zhang, Jiarui Xu, Xinlei Chen

- **What's New**: 2020년에 발표된 OpenAI의 스케일링 법칙 논문(Kaplan et. al [40])에서는 LSTMs(RNN의 종류)이 Transformers처럼 효과적으로 확장되지 않으며 긴 문맥을 잘 활용하지 못한다고 평가했습니다. 그러나 현대의 RNNs와 최신 최적 실행 관행을 반영해 이 같은 평가를 재검토한 결과, 현재 가장 인기 있는 RNN 중 하나인 Mamba는 강력한 Transformer와 유사하게 확장되는 것을 확인했습니다.
- **Technical Details**: Mamba가 2020년 LSTM과 동일한 문제를 공유하지만, Transformer는 긴 문맥에서 평균 Perplexity가 감소하는 반면, Mamba는 16k 이후에 Plateau(정체기)에 도달합니다. 이는 RNN 층의 특성상 고정된 크기의 숨겨진 상태에 문맥을 압축해야 하기 때문입니다. 이를 해결하기 위해 새로운 시퀀스 모델링 층인 Test-Time Training(TTT) 층을 도입했습니다. TTT 층은 숨겨진 상태가 모델이고 업데이트 규칙이 자기 지도 학습 단계인 새로운 클래스의 모델링 층입니다. TTT-Linear와 TTT-MLP는 각각 선형 모델과 2층 MLP인 TTT의 간단한 구현들입니다.
- **Performance Highlights**: TTT-Linear는 8k 문맥에서 Transformer보다 빠르고 Mamba와 유사한 성능을 보입니다. TTT 층은 미니배치 TTT와 Dual form을 통해 하드웨어 효율성을 향상시켜 실시간으로 적용 가능한 형태로 만들었습니다. TTT-Linear는 125M에서 1.3B 파라미터 사이의 다양한 평가에서 Transformer와 Mamba를 능가했습니다.

### [Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks](https://arxiv.org/abs/2407.02855)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.02855.png)

Vote: 6

Authors: Pei Ke, Minlie Huang, Hongning Wang, Junxiao Yang, Zhexin Zhang, Shiyao Cui, Chujie Zheng

- **What's New**: 새로운 논문에서는 최근 인공지능(AI) 및 머신러닝(ML) 분야에서의 혁신적인 발전을 소개합니다. 특히, 높은 정밀도와 효율성을 자랑하는 새로운 알고리즘(algorithm)이 발표되었습니다.
- **Technical Details**: 이 논문에서는 새로운 학습 알고리즘, 즉 강화 학습(Reinforcement Learning)과 심층 학습(Deep Learning)의 하이브리드(hybrid) 접근 방식을 사용했습니다. 주목할 만한 기술적 요소로는 트랜스포머(Transformers) 아키텍처와 그래프 신경망(Graph Neural Networks, GNN)의 결합을 포함합니다.
- **Performance Highlights**: 제안된 모델은 기존 최고 수준의 모델 대비 다양한 벤치마크 데이터셋에서 우수한 성능을 보여주었습니다. 특히, 학습 속도와 예측 정확도 측면에서 의미 있는 개선이 있었습니다.

### [DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning](https://arxiv.org/abs/2407.04078)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04078.png)

Vote: 6

Authors: Ru Peng, Xiang Wang, Mingfeng Xue, Guanting Dong, Chengpeng Li, Dayiheng Liu

- **What's New**: 최근 업로드된 논문은 새로운 딥러닝 모델을 소개하고 있으며, 이는 특히 자연어 처리(NLP) 작업에서 혁신적인 성능을 입증하고 있습니다.
- **Technical Details**: 논문에서 제안된 모델은 트랜스포머(transformer) 아키텍처를 기반으로 하며, 기존 BERT(Bidirectional Encoder Representations from Transformers) 모델을 개선하는 방식으로 설계되었습니다. 이 모델은 특히 문장 간 상호 정보를 더욱 효과적으로 학습할 수 있도록 각종 self-attention 메커니즘을 활용하고 있습니다. 또한, multi-head attention 기법을 통해 다양한 문맥 정보를 동시 처리할 수 있도록 합니다.
- **Performance Highlights**: 제안된 모델은 여러 벤치마크 데이터셋에서 기존 SOTA(State-Of-The-Art) 모델들을 능가하는 성능을 보였으며, 특히 GLUE(General Language Understanding Evaluation) 및 SQuAD(Stanford Question Answering Dataset)와 같은 주요 NLP 프로젝트에서 뛰어난 결과를 도출했습니다.

### [On scalable oversight with weak LLMs judging strong LLMs](https://arxiv.org/abs/2407.04622)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04622.png)

Vote: 6

Authors: Rishabh Agarwal, Noah D. Goodman, David Lindner, Noah Y. Siegel, János Kramár, Yunhao Tang, Zachary Kenton, Samuel Albanie, Jonah Brown-Cohen, Rohin Shah, Jannis Bulian

- **What's New**: 이 논문은 AI 연구 분야에서 확장 가능한 감독(scalable oversight) 메커니즘을 탐구하며, 특히 강력한 AI 모델들이 인간 전문가를 능가할 때도 정확한 피드백을 제공할 수 있는 방안을 제시합니다. 주된 초점은 '토론(debate)' 프로토콜을 통해, 덜 능력 있는 인간 심판이 더 강력한 AI 에이전트를 정확히 감독할 수 있는지 여부를 조사하는 것입니다.
- **Technical Details**: 이 연구에서는 다양한 능력 격차(cability gaps)가 존재하는 9개의 이진 선택형 질문 답변 데이터셋을 사용하여 AI 모델들 간의 토론과 컨설턴트 프로토콜을 평가했습니다. 주요 실험은 Gemini Pro 1.5 모델을 사용하여 진행되었고, 심판 역할을 하는 모델의 능력을 조절하여 여러 시나리오에서의 성능을 비교했습니다. 두 가지 확장 가능한 감독 프로토콜을 테스트하였는데, '컨설턴트(consultancy)'는 심판 모델이 단일 '컨설턴트' LLM에게 질문하는 형식이며, '토론'은 두 '토론자' LLM들이 반대의 답변을 제시하고 논쟁하는 형식입니다.
- **Performance Highlights**: 연구 결과, 토론 프로토콜이 모든 작업에서 컨설턴트 프로토콜을 안정적으로 능가하는 것으로 나타났습니다. 그러나, 정보 비대칭(information asymmetry)이 존재하는 상황에서는 토론이 문서 없이 직접 질문 답변(QA without article)보다 우수하지만 문서가 제공된 질문 답변(QA with article)과 비교했을 때는 특별한 장점을 보이지 않았습니다. 또한, 토론 프로토콜은 잘못된 답변을 증폭시키는 위험이 적어 더 안전한 훈련 신호를 제공합니다. 강력한 AI 에이전트를 통한 토론이 심판의 정확성을 높인다는 점에서 확장 가능한 감독의 핵심 목표를 충족할 가능성을 보여줍니다.

### [Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2406.08085)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08085.png)

Vote: 5

Authors: Jifeng Dai, Yansong Tang, Jiashi Feng, Yong Liu, Yiqin Wang, Xiaojie Jin, Haoji Zhang

- **What's New**: Flash-VStream은 온라인 비디오 스트림의 실시간 처리를 가능하게 하는 새로운 비디오-언어 모델을 소개합니다. 이 모델은 오래 지속되는 비디오 스트림을 실시간으로 처리하고 동시에 사용자 질문에 답변할 수 있습니다. 또한, 새로운 질문 응답 벤치마크인 VStream-QA를 제안하여 모델의 성능을 평가할 수 있습니다.
- **Technical Details**: Flash-VStream은 세 가지 주요 구성 요소로 구성됩니다: 스트리밍 비주얼 인코더, STAR(Spatial-Temporal-Abstract-Retrieved) 메모리 메커니즘, 실시간 응답이 가능한 LLM 디코더입니다. 이 모델은 두 개의 비동기 프로세스로 배포되어 하나는 프레임 핸들러 프로세스(STAR 메모리 통합 포함)를, 다른 하나는 질문 핸들러 프로세스(LLM 디코더, STAR 메모리 읽기, 사용자 인터랙션 관리)를 관리합니다. 공유 메모리를 통해 이 두 프로세스가 연결됩니다.
- **Performance Highlights**: 새로운 벤치마크 VStream-QA에서 Flash-VStream은 최첨단 성능을 달성하면서도 추론 지연 시간(Inference Latency)과 GPU 메모리(VRAM) 소비를 크게 줄였습니다. 또한, 기존의 오프라인 및 단일 길이 비디오 QA 벤치마크에 대한 제로-샷 비디오 질문 응답 실험에서도 뛰어난 일반화 능력을 보여주었습니다.

### [LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs](https://arxiv.org/abs/2407.03963)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03963.png)

Vote: 4

Authors: Fei Cheng, Kensuke Fukumoto, Keisuke Kamata, Hiroyuki Deguchi, Yuto Harada, Hiroshi Kataoka, Bowen Chen, LLM-jp, Tatsuya Hiraoka, Akiko Aizawa, Hiroki Kanezashi, Takuya Fukushima, Kazuki Fujii, Sosuke Hosokawa, Rintaro Enomoto, +, Namgi Han, Teruhito Kanazawa, Chikara Hashimoto, Lu Jie, Eiji Aramaki, Shohei Hisada, Satoru Katsumata

- **What's New**: LLM-jp 프로젝트는 대규모 언어 모델(LLM)의 일본어 버전을 개발하기 위해 2023년 5월에 시작되었습니다. 2023년 10월에 첫 번째 모델 스위트(LLM-jp model suite v1.0)를 공개한 후, 2024년 4월에 두 번째 모델 스위트(LLM-jp model suite v2.0)를 공개했습니다.
- **Technical Details**: LLM-jp 프로젝트는 크게 네 개의 작업 그룹(Migration of Working Groups)으로 구성되었습니다: 코퍼스 구축 WG(Corpus Building WG), 모델 구축 WG(Model Building WG), 파인 튜닝 및 평가 WG(Fine-tuning and Evaluation WG), 그리고 컴퓨팅 인프라 WG(Computational Infrastructure WG). 첫 번째 모델 스위트(v1.0)를 구축하기 위해, 코퍼스는 약 270억 토큰으로 구성되었으며, 일본어, 영어 및 코드가 포함되었습니다. 또한, 각 언어에 대해 독립적으로 SentencePiece 기반 unigram 토크나이저를 구축하고, 이를 병합하여 최종 토크나이저를 만들었습니다.
- **Performance Highlights**: 최초로 공개된 LLM-jp 모델 스위트는 130억 파라미터를 가진 모델로 구성되었으며, 다양한 사전 학습 및 파인튜닝 데이터셋을 포함하고 있습니다. 두 번째 모델 스위트에서는 더 큰 규모와 높은 품질의 코퍼스로 개발된 모델이 포함되어 있습니다. 프로젝트는 높은 품질의 코퍼스를 생성하기 위해 Common Crawl 데이터를 활용하여 일본어 문서를 추출하고, 엄격한 필터링 방법을 적용했습니다.

### [HEMM: Holistic Evaluation of Multimodal Foundation Models](https://arxiv.org/abs/2407.03418)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03418.png)

Vote: 3

Authors: Akshay Goindani, Ruslan Salakhutdinov, Louis-Philippe Morency, Paul Pu Liang, Haofei Yu, Talha Chafekar, Leena Mathur

- **What's New**: 최근 대규모 언어 및 비전 사전학습의 급진적 발전을 기반으로, 새로운 세대의 멀티모달 기초 모델들이 여러 모달리티 간의 상호작용을 배우는 데 더욱 능숙해졌습니다. 이런 기술은 정적인 예측뿐만 아니라 동적 상호작용도 가능하게 하며, 사전학습 코퍼스에서는 전혀 나타나지 않았던 새로운 특성들도 보여주고 있습니다. 기존의 모달리티 및 작업 특정 데이터셋 모음에 기반한 벤치마킹 기준은 이제 이러한 넓은 범주의 기능을 평가하는 데 점차 충분하지 않게 되었습니다. 이를 해결하기 위해, 연구진은 멀티모달 모델의 전체적인 평가 체계인 HEMM(Holistic Evaluation of Multimodal Models)을 제안합니다.
- **Technical Details**: HEMM(HEMM)은 세 가지 수준에서 멀티모달 모델을 벤치마킹합니다. 첫 번째 수준은 중복된, 고유한, 시너지를 내는 특징 간의 상호작용, 정교한 정보와 대량 정보의 정렬, 복합 특징에 걸친 추론, 그리고 외부 지식의 통합 등 기본적인 멀티모달 기술을 평가합니다. 두 번째 수준은 querying, translation, editing, fusion과 같은 과제에서 멀티모달 정보가 어떻게 변환되는지를 평가합니다. 세 번째 수준은 멀티미디어, 감성 컴퓨팅, 자연 과학, 헬스케어, 인간-컴퓨터 상호작용(HCI) 등 도메인별 실제 응용 사례를 평가합니다. HEMM은 또한 모델의 크기, 모달리티 처리(e.g., interleaved inputs), 사전학습 목표 등 주요 모델링 결정을 아우르는 새로운 모델 분류 방법을 제공합니다.
- **Performance Highlights**: HEMM은 30개의 주요 데이터셋을 포함하며, 각 데이터셋은 문제 해결에 필요한 다양한 멀티모달 기술, 정보 흐름 유형, 도메인별 실제 응용 사례를 다룹니다. 모델링 결정과 학습 결정이 다운스트림 작업 성능에 미치는 영향을 분석하며, 다음 연구 방향을 위한 구체적인 지침을 제공합니다. 아울러 HEMM은 공개적으로 이용 가능하며, 공동체 참여를 통해 데이터셋, 주석, 모델 및 평가 메트릭스의 확장을 장려합니다.

### [CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](https://arxiv.org/abs/2407.03923)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03923.png)

Vote: 2

Authors: Donghyeong Kim, Suhwan Cho, Dogyoon Lee, Sangyoun Lee, Junghe Lee

- **What's New**: 본 논문에서는 NeRF(Neural Radiance Fields) 기반의 최신 기술로, 카메라 모션 블러(camera motion blur)가 포함된 이미지에서 정확한 3D 장면을 재구성하는 새로운 접근 방식을 제안합니다. 제안된 모델은 CRiM-GS(continuous rigid motion-aware gaussian splatting)로, 리얼타임 렌더링 속도를 유지하면서도 3D 공간에서의 연속적인 카메라 움직임을 정밀하게 모델링합니다. 이를 위해 SE(3) 필드의 왜곡 가능한 3D 변환과 뉴럴 미분 방정식(Neural Ordinary Differential Equations, ODEs)을 사용하여 카메라의 연속적인 움직임을 구현합니다.
- **Technical Details**: CRiM-GS는 세 가지 주요 기여를 포함합니다. 첫째, NeRFs와 유사하게 주제의 형태와 크기를 유지하기 위해 강체 변환(rigid body transformations)을 모델링합니다. 둘째, SE(3) 필드에 학습 가능한 변형 가능한(deformable) 3D 변환을 도입하여 현실 세계의 복잡하고 비선형적인 카메라 움직임을 처리합니다. 셋째, 뉴럴 미분 방정식(ODEs)을 적용하여 노출 시간 동안 카메라 움직임을 연속적으로 모델링합니다. 이러한 방법은 기존의 2D 평면에서의 카메라 움직임 모델링을 넘어, 3D 공간에서의 카메라 궤적을 정밀하게 모델링할 수 있습니다. 또한, CRiM-GS는 3D-GS(3D Gaussian Splatting) 기반의 차별 가능한 스플래팅 방법을 사용하여 리얼타임 렌더링 성능을 보장합니다.
- **Performance Highlights**: 본 접근 방식을 평가하기 위해 Deblur-NeRF의 합성 및 실제 장면 데이터셋에서 CRiM-GS를 테스트하였으며, 모든 벤치마크에서 최첨단 성능을 달성하였습니다. 다양한 기여 요소에 대한 실험을 통해 CRiM-GS의 효과를 입증하는 결과를 도출했습니다.

