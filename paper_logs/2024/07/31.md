## Daily Papers (2024-07-31)

### [Meltemi: The first open Large Language Model for Greek](https://arxiv.org/abs/2407.20743)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.20743.png)

Vote: 53

Authors: Vassilis Papavasileiou, Leon Voukoutis, Prokopis Prokopidis, Sokratis Sofianopoulos, Dimitris Roussis, Stelios Piperidis, Athanasios Katsamanis, Vassilis Katsouros, Georgios Paraskevopoulos

- **What's New**: 최근 발표된 연구는 그리스어 전용 대규모 언어 모델(LLM)인 Meltemi 7B와 Meltemi 7B Instruct의 개발을 소개합니다. 이는 그리스어로서 처음 공개된 대규모 언어 모델이며, 그리스어 LLM 평가를 위한 종합적인 벤치마크도 함께 개발되었습니다.
- **Technical Details**: Meltemi 7B와 Meltemi 7B Instruct 모델은 기존의 Mistral 7B 기반으로 구축되었으며, 새로운 언어 모델에 적합하도록 그리스어, 영어 및 병렬(영어-그리스어) 데이터를 혼합하여 추가 학습을 거쳤습니다. 다양한 그리스어 소스에서 데이터를 수집, 필터링 및 전처리하여 대규모 코퍼스를 구축하였으며, 또한 ORPO 알고리즘과 번역 선호 데이터를 사용하여 챗 기능을 강화했습니다.
- **Performance Highlights**: 모델의 성능을 평가하기 위해 번역, 대화, 추론 등의 다양한 작업을 포함한 그리스어 벤치마크를 사용하였습니다. Meltemi 7B와 Meltemi 7B Instruct는 그리스어 데이터셋에서 우수한 성능을 보여 주었으며, 이는 고품질 데이터와 지속적인 미세 조정 덕분입니다.

### [Adapting Safe-for-Work Classifier for Malaysian Language Text: Enhancing Alignment in LLM-Ops Framework](https://arxiv.org/abs/2407.20729)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.20729.png)

Vote: 23

Authors: Wan Adzhar Faiq Adzlan, Ammar Azman, Aisyah Razak, Kamarul Adha, Ariff Nazhan, Mas Aisyah Ahmad

- **What's New**: 이번 연구에서는 인공지능 대화 시스템에서 안전하지 않은 콘텐츠(NSFW)을 식별하고 필터링하기 위한 말레이어 데이터셋 개발에 중점을 두고 있습니다. 이 작업은 특정 유해 콘텐츠 범주를 측정하고 분류하는 새로운 데이터셋을 만드는 것을 목표로 하여, 말레이어에서 NSFW 콘텐츠 필터링을 개선하려는 최초의 시도입니다.
- **Technical Details**: 연구진은 다양한 소셜 미디어 플랫폼(Twitter, Facebook) 및 공개된 데이터셋(Kaggle, Reddit)을 활용해 데이터를 수집했습니다. 수집된 데이터는 Label Studio를 통해 수동으로 라벨링하고, 대형 언어 모델(LLM)을 사용하여 추가 라벨링 작업을 진행했습니다. 대형 언어 모델로는 mistral-7b와 MaLLaM을 사용하여 각각 영어 및 말레이어 컨텍스트에서 라벨을 생성했습니다.
- **Performance Highlights**: 이 연구의 데이터 셋은 포르노, 괴롭힘, 성차별, 인종차별, 종교적 모독, 자해, 정신질환 관련 콘텐츠와 일반적으로 안전한 콘텐츠(SFW)를 포함하는 다양한 카테고리로 구성되어 있습니다. 이로 인해 모델은 다양한 유해 텍스트를 효과적으로 식별하고 필터링할 수 있게 되었습니다. 마지막으로, 고품질의 라벨 데이터를 제공하여 분류기의 성능을 최적화했습니다.

### [ThinK: Thinner Key Cache by Query-Driven Pruning](https://arxiv.org/abs/2407.21018)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21018.png)

Vote: 20

Authors: Xudong Lu, Doyen Sahoo, Amrita Saha, Lei Wang, Zhanming Jie, Hanze Dong, Caiming Xiong, Yuhui Xu, Aojun Zhou

- **What's New**: 이 논문에서는 긴 시퀀스의 문맥 관리를 개선하기 위해 KV 캐시의 채널을 가지치기하는 새로운 방법인 ThinK를 제안합니다. 이 방법은 특히 채널 차원 D의 불균형을 해결하여 메모리 소비를 줄이는 것을 목표로 합니다.
- **Technical Details**: 논문에서는 KV 캐시의 중요도가 단어별로 희소하다는 점을 발견하고, 이에 따라 KV 캐시의 채널 가지치기 및 최적화를 수행하는 방법론을 제시합니다. 저자들은 이를 최적화 문제로 정의하고, 각 채널의 중요도를 평가하는 새로운 쿼리 의존적인 기준을 소개합니다. 그런 다음 그리디 방식으로 가장 중요한 채널을 선택합니다.
- **Performance Highlights**: ThinK는 LLaMA3와 Mistral 모델을 사용하여 다양한 긴 시퀀스 데이터셋에서 평가되었으며, 토큰 제거 방법과 결합할 때 20% 이상의 KV 캐시 메모리 비용 절감과 비슷하거나 더 높은 정확성을 달성했습니다. 이 연구는 ThinK를 통해 채널의 희소성 구조를 탐구하고, 메모리와 계산 요구사항의 선형 감소를 가져오는 것을 입증했습니다.

### [Knesset-DictaBERT: A Hebrew Language Model for Parliamentary Proceedings](https://arxiv.org/abs/2407.20581)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.20581.png)

Vote: 19

Authors: Gili Goldin, Shuly Wintner

- **What's New**: 자연어 처리(NLP) 분야는 최근 몇 년 동안 큰 발전을 이뤄냈습니다. 그러나 대부분의 NLP 모델이 영어에 집중되어 있고, 히브리어와 같은 저자원 언어에 대한 연구는 부족했습니다. 우리는 이러한 격차를 해소하기 위해 Knesset-DictaBERT라는 히브리어 의회 텍스트에 특화된 모델을 개발했습니다.
- **Technical Details**: 우리는 사전 훈련된 DictaBERT 모델을 기반으로 Knesset Corpus(이스라엘 국회 발의록)를 사용해 모델을 미세 조정(fine-tune)했습니다. Knesset Corpus는 3200만 개 이상의 문장과 3.84억 개의 토큰으로 구성된 방대한 히브리어 데이터셋입니다. 텍스트 샤딩(text shards)으로 효율적 로딩 및 처리를 진행하였고, 각각 80%, 10%, 10%의 비율로 학습, 검증, 테스트 세트로 나누었습니다. MLM(masked language modeling) 작업을 위해 Hugging Face의 AutoTokenizer와 DataCollatorForLanguageModeling을 사용했습니다. 모델은 SLURM 환경에서 다중 GPU를 활용한 분산 학습 설정으로 훈련되었습니다.
- **Performance Highlights**: Knesset-DictaBERT 모델은 테스트 세트에서 6.60의 perplexity를 기록해 기존 DictaBERT 모델의 22.87을 크게 능가했습니다. 또한 마스크 처리된 토큰 예측 정확도에서도 Knesset-DictaBERT 모델은 top-1 정확도에서 52.55%, top-2 정확도에서 63.07%, top-5 정확도에서 73.59%를 기록해 기존 모델보다 높은 성능을 보였습니다. 이는 의회 텍스트 예측에서 Knesset-DictaBERT 모델의 우수성을 입증합니다.
- **Conclusion and Future Work**: 우리는 Knesset-DictaBERT 모델의 성공적인 미세 조정을 통해 히브리어 의회 언어를 이해하고 생성하는 데 탁월한 성능을 보이는 모델을 만들었습니다. 향후 연구에서는 모델의 일반화 능력을 향상시키기 위해 추가적인 히브리어 데이터셋 평가와 다른 언어 모델의 Knesset Corpus에 대한 미세 조정을 시도할 계획입니다.
- **Limitations**: 모델이 의회 발의록 데이터를 사용해 미세 조정되었기 때문에 일반적인 히브리어 텍스트나 다른 도메인에서는 성능이 떨어질 수 있습니다. 그러나 원래의 DictaBERT 모델이 다양한 히브리어 리소스를 기반으로 훈련되었기 때문에 Knesset-DictaBERT는 여전히 폭넓은 언어 패턴과 어휘의 이점을 가지고 있을 것으로 보입니다.
- **Ethical Considerations**: Knesset Corpus는 의회 토론에 나타나는 정치적, 사회적 편향을 반영할 수 있으며, Knesset-DictaBERT는 이러한 편향을 상속받을 가능성이 있습니다.

### [A Large Encoder-Decoder Family of Foundation Models For Chemical Language](https://arxiv.org/abs/2407.20267)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.20267.png)

Vote: 18

Authors: Kristin Schmidt, Eduardo Soares, Renato Cerqueira, Victor Shirasuna, Dmitry Zubarev, Emilio Vital Brazil

- **What's New**: 이 논문은 SMI-TED289M이라는 새로운 분자 인코더-디코더 기본 모델을 소개한다. 이 모델은 PubChem에서 수집된 9,100만 개의 분자 데이터를 사용하여 사전 훈련되었다. SMILES 표현 방식을 사용하여 분자 구조를 모델링하며, 자가 지도 학습(self-supervised learning) 방법론을 적용한 것이 특징이다.
- **Technical Details**: SMI-TED289M 모델은 트랜스포머(encoders) 기반의 분자 토큰 인코더와 인코더-디코더(encoder-decoder) 메커니즘을 결합하여 구현된다. 사전 훈련 데이터는 PubChem 데이터베이스에서 수집된 11,300만 개의 SMILES 문자열을 중복 제거 및 정형화하여 처리한 후, 9,100만 개의 유일하고 유효한 분자를 확보했다. 각각의 분자를 토큰화하는 과정에서 4억 개의 분자 토큰을 생성했으며, 이 과정에서 2,988개의 고유 토큰과 5개의 특수 토큰이 도출되었다. SMI-TED289M 모델의 기본형은 2억 8,900만 개의 매개변수를 가지며, Mixture-of-SMI-TED-Experts 라는 확장형은 8x289M 매개변수를 가진다.
- **Performance Highlights**: SMI-TED289M 모델은 11개의 벤치마크 데이터셋에서의 분자 특성 예측, 분자 재구성 및 효율적인 잠재 공간 측면에서 최첨단 성능을 보여준다. 모델의 잠재 공간 내 조합성은 화학적 추론 작업에도 강력한 잠재력을 시사한다. 또한, few-shot learning을 통한 학습 대표성 평가 역시 포함되어 있다.

### [Futga: Towards Fine-grained Music Understanding through Temporally-enhanced Generative Augmentation](https://arxiv.org/abs/2407.20445)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.20445.png)

Vote: 18

Authors: Amit Namburi, Jiaheng Dai, Carol Chen, Julian McAuley, Hao-Wen Dong, Junda Wu, Zhouhang Xie, Zachary Novack

- **What's New**: 이 논문은 새로운 머신러닝(Machine Learning) 모델인 Hybrid Attention-based GAN을 제안합니다. 이 모델은 음성 합성(Speech Synthesis) 및 텍스트-음성 변환(Text-to-Speech, TTS) System의 성능을 대폭 향상시키는 데 목적이 있습니다.
- **Technical Details**: 제안된 모델은 GAN(Generative Adversarial Network) 아키텍처와 Attention Mechanism을 결합하여 생성 모델의 품질을 높였습니다. 특히, Self-Attention Layer와 Dual Attention Layer를 도입하여, 더 정확한 음성 데이터를 생성할 수 있도록 설계되었습니다. 또한, 이 모델은 Residual Network 구조를 채택해 파라미터(Parameter)의 효율성을 최적화했습니다.
- **Performance Highlights**: 실험 결과, Hybrid Attention-based GAN은 기존의 최신 모델들보다 더 높은 MOS(Mean Opinion Score)를 기록하며, 특히 음성의 자연스러움과 발음의 정확성 측면에서 우수한 성능을 보였습니다. 또한, 학습 시간과 자원 소모 측면에서도 효율적인 것으로 나타났습니다.

### [Harvesting Textual and Structured Data from the HAL Publication Repository](https://arxiv.org/abs/2407.20595)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.20595.png)

Vote: 17

Authors: Guillaume Vimont, Francis Kulumba, Wissam Antoun, Laurent Romary

- **What's New**: 최근 다중모드 딥러닝(multimodal deep learning)의 연구가 활발해지면서 텍스트와 구조화된 데이터를 활용하는 여러 아키텍처가 생겨났습니다. 이러한 배경에서 저자 식별 문제를 해결하기 위해 HAL로부터 데이터를 공개하고 새로운 기법을 구현 및 검증할 수 있게 해주는 HALvest를 소개합니다. HALvest는 구조화된 데이터셋과 텍스트 데이터셋이 결합된 형태로, 학술 인용 네트워크(HALvest-Geometric)와 16.5억 개 이상의 토큰을 포함한 텍스트를 제공합니다.
- **Technical Details**: HALvest는 다음과 같은 특징을 지닙니다: 17억 개 이상의 토큰을 포함하는 56개의 언어와 13개의 도메인에서 텍스트 데이터셋을 구성했고, 238,397명의 저자와 18,662,037개의 학술 논문을 포함하는 학술 인용 네트워크를 제공합니다. 또한, 이를 활용하여 언어 모델과 그래프 표현 학습 아키텍처를 학습할 수 있는 실험을 수행하였습니다. 데이터셋과 이를 구축하는 데 사용된 코드는 온라인에서 제공되며, 정기적으로 업데이트될 예정입니다.
- **Performance Highlights**: 본 연구에서는 다양한 최첨단 그래프 신경망(GNN, Graph Neural Networks) 아키텍처를 테스트하여 폐쇄형 설정에서 저자 식별 성능을 검증하였고, HALvest의 그래프 형식의 유효성을 입증하였습니다.

### [Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning](https://arxiv.org/abs/2407.20798)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.20798.png)

Vote: 17

Authors: Norman Di Palo, Jan Humplik, Leonard Hasenclever, Arunkumar Byravan

- **What's New**: 이 논문에서는 인터넷 규모 데이터셋으로 사전 훈련된 근본 모델(foundation models)을 사용하는 새로운 접근법을 소개합니다. 이 연구는 시각, 언어, 확산 모델(diffusion models)의 상호작용을 통해 에이전트가 환경을 해석하고 수집된 데이터를 새로운 작업과 목표에 재활용하는 방법을 제안합니다. 특히 Diffusion Augmented Agent (DAAG)라는 프레임워크를 통해 자율적으로 보상 설정 및 하위 목표 달성이 가능하도록 설계되었습니다.
- **Technical Details**: DAAG 프레임워크는 큰 언어 모델(LLM)을 주 제어기로 사용하여 비전 언어 모델(VLM) 및 확산 모델(DM)을 활용합니다. 이 프레임워크는 텍스트로 조건화된 확산 모델을 사용하여 시각적 관찰을 수정하고 증강된 샘플을 통해 VLM을 미세 조정하는 방식으로 보상을 자동으로 계산합니다. 또한 확산 모델을 사용하여 기록된 관찰을 수정하고 새로운 작업을 위한 유용한 하위 목표를 디자인하고 인식하는 등 에이전트의 학습 효율성을 높입니다.
- **Performance Highlights**: 다양한 환경에서 실시한 실험을 통해 DAAG 프레임워크는 로봇 조작 및 내비게이션 등의 분야에서 에이전트의 성능을 향상시키는 것으로 나타났습니다. 특히, DAAG는 학습 데이터의 재사용 및 새로운 작업으로의 효과적인 전이를 가능하게 하여 에이전트의 학습 속도를 높이는 데 기여합니다. 적합하지 않은 궤적을 달리 수정하는 방법으로 데이터 효율성을 높이는 데 큰 성과를 보였습니다.

### [JaColBERTv2.5: Optimising Multi-Vector Retrievers to Create State-of-the-Art Japanese Retrievers with Constrained Resources](https://arxiv.org/abs/2407.20750)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.20750.png)

Vote: 17

Authors: Benjamin Clavié

- **What's New**: 이 연구에서는 Japanese ColBERT와 ColBERTv2 모델을 개선한 JaColBERTv2.5 모델을 도입했습니다. 이 모델은 기존의 mono-lingual Japanese retrieval 모델을 능가하는 성능을 보이며, 다중 언어 모델인 BGE-M3보다도 높은 성능을 달성했습니다.
- **Technical Details**: JaColBERTv2.5는 다중 벡터 기반의 retrieval 모델로, 각 문서를 단일 벡터가 아닌 여러 작은 벡터로 표현합니다. 또한, 동적 쿼리 길이(dynamic query-length)를 사용하는 등 몇 가지 최적화된 훈련 기법을 적용했습니다. 훈련 과정에서는 knowledge distillation 및 체크포인트 앙상블(checkpoint averaging) 기법을 사용하여 성능을 향상시켰습니다.
- **Performance Highlights**: JaColBERTv2.5 모델은 기존 모델들에 비해 평균 4.5% 개선된 스코어 0.752를 달성했으며, GLuCoSE 모델보다 60%, BGE-M3 모델보다 5.32% 높은 성능을 보였습니다. 특히, 대규모 retrieval 벤치마크인 MIRACL에서 6.87% 향상된 결과를 보여주었습니다.

### [Matting by Generation](https://arxiv.org/abs/2407.21017)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21017.png)

Vote: 16

Authors: Baiang Li, Jinwei Gu, Yu-Lun Liu, Jian Wang, Shin'ichi Satoh, Zhixiang Wang, Yung-Yu Chuang

- **What's New**: 이 논문에서는 전통적인 회귀 문제를 조건부 생성 모델링 문제로 변환하는 간단하고 효과적인 새로운 매팅(matting) 기술을 제안합니다. 이를 위해 생성(diffusion) 모델을 활용하여 이미지 의미론 및 매트 세부사항에 대한 사전 학습된 지식을 통합했습니다.
- **Technical Details**: 기존의 매팅 방법들은 추가 입력 자료를 통해 문제의 비정형성을 완화하려고 했지만, 이 방법은 불완전한 초기 분할(segmentation) 마스크에 의존해야 하는 단점이 있습니다. 이에 비해, 논문에서 제안하는 방법은 생성 모델의 장점을 활용하여 데이터의 불확실성을 효과적으로 처리하고, 불완전한 라벨의 부정적 영향을 줄이며, 고해상도 입력을 효율적으로 처리할 수 있는 특징을 가지고 있습니다.
- **Performance Highlights**: 제안된 방법은 생성 모델의 풍부한 데이터베이스를 활용하여 보다 포괄적인 이미지 분포를 학습하고, 트레이닝 과정을 규제하는 데 도움을 줍니다. 이로 인해 이미지 가시성이 제한된 상황에서도 우수한 성능을 발휘하며, 사용자가 원하는 매트를 추출하기 위해 추가 지침 없이도 높은 정확도의 매팅이 가능합니다. 또한, 각종 힌트를 포함한 다양한 시나리오를 처리할 수 있는 유연성을 지니고 있습니다.

