## Daily Papers (2024-07-25)

### [OpenDevin: An Open Platform for AI Software Developers as Generalist Agents](https://arxiv.org/abs/2407.16741)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.16741.png)

Vote: 36

Authors: Xingyao Wang, Boxuan Li, Hao Peng, Yanjun Shao, Fuqiang Li, Mingchen Zhuge, Bill Qian, Frank F. Xu, +, Bowen Li, Niklas Muennighoff, Binyuan Hui, Yueqi Song, Yufan Song, Jiayi Pan, Ren Ma, Junyang Lin, Yizhe Zhang, Hoang H. Tran, Jaskirat Singh, Xiangru Tang, Robert Brennan, Mingzhang Zheng

- **What's New**: 이 논문은 효율적인 Deep Learning 모델을 훈련하기 위한 새로운 방법론을 제안합니다. 특히, Multi-Task Learning(MTL)을 통해 여러 작업을 동시에 학습할 수 있도록 하는 접근 방식을 탐색합니다.
- **Technical Details**: 제안된 방법에서는 Cross-Task Regularization을 사용하여 훈련 중 다양한 작업 간의 정보 공유를 촉진합니다. 또한, Adaptive Learning Rate Scheduler를 도입하여 네트워크의 학습 속도를 조정합니다.
- **Performance Highlights**: 실험 결과, 제안한 모델은 Standard Benchmarks에서 기존의 Single-Task 모델에 비해 평균 15% 이상의 성능 개선을 보여주었으며, 다양한 데이터셋에서도 일관된 성능을 발휘하는 것으로 나타났습니다.

### [$VILA^2$: VILA Augmented VILA](https://arxiv.org/abs/2407.17453)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.17453.png)

Vote: 24

Authors: Pavlo Molchanov, Jang Hyun Cho, Marco Pavone, Yunhao Fang, Yan Wang, Hongxu Yin, Yao Lu, Song Han, Ligeng Zhu

- **What's New**: 이번 연구에서는 VLM (Visual Language Model) 자체를 활용하여 데이터셋의 결함을 해결하고 훈련을 개선하는 방법을 제안합니다. 기존의 큰 언어 모델(LLMs)와의 정렬을 통해, VLM의 사전 학습 데이터를 정제 및 증강하는 새로운 훈련 방식이 도입되었습니다.
- **Technical Details**: VILA라는 이름의 새로운 모델 패밀리를 제공하며, 이는 기존 모델을 자가 부트스트랩 방법으로 증강하여 훈련하는 과정에서 생성됩니다. 연구에서는 자가 증강 루프(bootstrapped loop)와 전문가 증강 루프(specialist-augment step)를 포함하는 두 가지 주요 단계로 구성됩니다. VLM은 다양한 프롬프트 선택을 통해 이미지 토큰과 텍스트 토큰을 연결하여 멀티모달 입력 처리에 유연성을 제공합니다.
- **Performance Highlights**: VILA2 모델은 기존의 최첨단 방법들을 초월하며, MMMU 기준에서 새로운 최첨단 성능을 기록하였습니다. 자가 증강 데이터의 질이 향상되고 환각 (hallucination)이 줄어들며 VLM의 성능을 직접적으로 증가시키는 결과를 보였습니다. 자가 증강 과정을 통해 생성된 합성 캡션은 예전의 인간 작성 텍스트보다 길고 정보량이 많았습니다.

### [HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation](https://arxiv.org/abs/2407.17438)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.17438.png)

Vote: 11

Authors: Tianfan Xue, Dahua Lin, Zhenzhi Wang, Yuwei Guo, Wenran Liu, Yanhong Zeng, Bo Dai, Youqing Fang, Kai Chen, Jing Tan, Yixuan Li

- **What's New**: 본 연구에서는 인간 이미지 애니메이션을 위한 고품질의 공개 데이터셋을 제시하고, 정밀한 인간 및 카메라 모션 주석을 보유한 대규모 데이터셋을 통해 애니메이션의 품질과 제어 능력을 향상시키는 방법을 소개합니다.
- **Technical Details**: 연구팀은 Unreal Engine 5 (UE5)와 Blender를 활용하여 다양한 카메라 궤적을 사용하는 3D 씬에서 움직이는 캐릭터를 포함한 합성 비디오 데이터를 생성합니다. 중요하게, SLAM 기반 방법과 정확한 포즈 추정기를 사용해 인간 포즈 시퀀스 및 카메라 경로를 정확하게 추출합니다.
- **Performance Highlights**: 수집된 데이터셋으로 훈련된 모델은 기존의 최첨단 성능을 초월하며, 실험 결과는 고품질 및 변화가 많은 인간 비디오 애니메이션을 성공적으로 생성하는 데 기여하는 것을 보여 줍니다.

### [PERSONA: A Reproducible Testbed for Pluralistic Alignment](https://arxiv.org/abs/2407.17387)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.17387.png)

Vote: 8

Authors: Rafael Rafailov, Nathan Lile, Louis Castricato, Jan-Philipp Fränken, Chelsea Finn

- **What's New**: 이 논문에서는 다원적(Pluralistic) 정렬(pluralistic alignment)을 위한 평가 문제를 해결하기 위해 현실적인 사용자 프로필을 포함한 합성 페르소나(synthetic personas)를 모델링하는 접근 방식을 제안하고 있습니다. 이는 다양한 사용자 그룹의 의견과 선호를 반영하는 데 중점을 두고 있습니다.
- **Technical Details**: 논문에서는 1,586개의 합성 페르소나를 생성하여 다양한 가치 기반, 다양한, 논란이 있는 주제에 대한 피드백을 제공하는 데이터를 생성합니다. 이 페르소나는 미국 인구를 반영하여 인구통계적 정보와 개별적인 배경을 포함하고 있으며, 이는 역할 플레이링 언어 모델(Role-Playing LMs)을 설정하는 데 사용됩니다. 특이한 개인적 속성을 포함하는 다양한 사용자 프로필을 생성하여 다원적 정렬 접근 방식을 평가하기 위한 테스트베드(test-bed)를 제공합니다.
- **Performance Highlights**: PRISM 데이터 세트를 기반으로 한 새로운 접근 방식이 기존의 한정된 평가 환경을 극복하고, 보다 다양한 사용자의 선호를 반영한 데이터와 평가 프레임워크를 제공하여 LM의 개인화(personalization) 및 선호 elicitation을 위한 인프라를 마련하는 데 기여할 것으로 예상됩니다.

### [SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency](https://arxiv.org/abs/2407.17470)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.17470.png)

Vote: 8

Authors: Yiming Xie, Huaizu Jiang, Vikram Voleti, Chun-Han Yao, Varun Jampani

- **What's New**: 논문에서는 동적 3D 객체를 단일 모노큘러 비디오에서 생성할 수 있는 Stable Video 4D (SV4D) 모델을 제안합니다. 기존 기술들과 달리, SV4D는 뷰(view)와 모션(motion) 축을 동시에 고려하여 새로운 비디오를 생성합니다.
- **Technical Details**: SV4D는 비디오 프레임 수 F(italic_F)와 카메라 뷰 수 V(italic_V)에 따라 V×F(italic_V × italic_F) 이미지 그리드를 출력합니다. 이 모델은 Stable Video Diffusion (SVD) 기반으로 구축되며, 뷰 어텐션(view attention)과 프레임 어텐션(frame attention) 블록을 추가하여 동적 및 다중 뷰 일관성을 크게 향상시킵니다. 또한, 긴 입력 비디오의 경우 메모리 문제를 해결하기 위해 입력 프레임의 상호 섞인 부분을 순차적으로 처리하는 기술을 제안합니다.
- **Performance Highlights**: SV4D는 여러 벤치마크 데이터 세트에서 동적 3D 자산에 대한 4D 생성 및 새로운 뷰 비디오 합성에서 최첨단 결과를 제공합니다. 이 모델은 Objaverse와 같은 대규모 데이터 세트를 기반으로 훈련되어, 효율적이고 일관된 4D 출력을 생성할 수 있습니다.

### [Scalify: scale propagation for efficient low-precision LLM training](https://arxiv.org/abs/2407.17353)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.17353.png)

Vote: 7

Authors: Carlo Luschi, Paul Balança, Andrew Fitzgibbon, Sam Hosegood

- **What's New**: 본 논문에서는 저정밀도 훈련을 단순화하기 위한 'scalify transform'을 소개합니다. 이는 계산 그래프의 전체를 자동으로 텐서 스케일링하는 방법으로 기존의 방법들을 일반화하고 통합하여 FP8 및 FP16 기법을 같은 패러다임 하에 한 데 묶습니다.
- **Technical Details**: scalify transform은 매트릭스 곱셈과 스케일링을 분리하여 저정밀도 훈련의 효율성을 높입니다. FP8 포맷인 E4M3와 E5M2는 각각 활성화 및 가중치, 그리고 그래디언트의 동적 범위를 나타내기 위해 사용됩니다. 이 문서에서는 ScaledArray와 같은 구조체를 정의하여 텐서를 저정밀도로 표현합니다.
- **Performance Highlights**: scalify transform은 메모리 사용량을 줄이며 훈련의 Robustness와 정확도를 FP32 및 BF16과 유사하게 유지할 수 있습니다. 또한, 다양한 ML 프레임워크에서 지원할 수 있는 오픈소스 구현을 제공하여, ML 실무자들이 저정밀도 훈련 방법을 쉽게 활용할 수 있도록 합니다.

### [DDK: Distilling Domain Knowledge for Efficient Large Language Models](https://arxiv.org/abs/2407.16154)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.16154.png)

Vote: 6

Authors: Yanan Wu, Jinyang Guo, Wenbo Su, Jie Liu, Lin Qu, Chenchen Zhang, Ge Zhang, Jiakai Wang, Congnan Liu, Jiaheng Liu, Ken Deng, Zhiqi Bai, Yuanxing Zhang, Jiamang Wang, Bo Zheng, Haoran Que

- **What's New**: 본 연구에서는 LLMs(대규모 언어 모델)의 경량화 및 효율적인 지식 증류 지식 증류(Knowledge Distillation, KD) 방법론인 DDK(Distill Domain Knowledge)를 소개합니다. 이 방법론은 다양한 도메인에서 학생 모델과 교사 모델 간의 성능 격차를 최적화하기 위해 데이터 구성을 동적으로 조정하는 것을 목표로 합니다.
- **Technical Details**: DDK 프레임워크는 교사 LLM과 학생 LLM 간의 성능 차이를 정량화하고, 이를 기반으로 도메인 불일치 요소(domain discrepancy factor)를 주기적으로 재계산합니다. DDK는 이 요소를 바탕으로 샘플링 전략을 수립하여 다양한 도메인에서 데이터 샘플링을 수행하며, 페어링된 성능 차이를 줄이고 안정성을 증가시키기 위해 factor smooth updating 기법을 적용합니다.
- **Performance Highlights**: 실험 결과, DDK 접근법을 적용한 학생 LLM은 도메인 전반에 걸쳐 성능 격차를 유의미하게 줄이고, 샘플링 프로세스의 안정성을 효과적으로 유지하면서 균형 잡힌 성능을 발휘하는 것으로 나타났습니다. 이 연구는 LLM을 압축하는 데 있어 도메인별 데이터 혼합의 중요한 영향을 처음으로 탐구하며, 다양한 벤치마크 데이터셋에서 DDK의 효과 및 일반화 가능성을 확인했습니다.

### [ViPer: Visual Personalization of Generative Models via Individual Preference Learning](https://arxiv.org/abs/2407.17365)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.17365.png)

Vote: 6

Authors: Amir Zamir, Roman Bachmann, Mahdi Shafiei, Sogand Salehi, Teresa Yeo

- **What's New**: 이 논문은 개인의 이미지 선호도를 반영한 텍스트-이미지 생성 모델인 ViPer(Visual Personalization of Generative Models via Individual Preference Learning)를 제안합니다. 사용자가 제공한 주석을 활용하여 개인화된 이미지를 생성하며, 기존의 방법보다 더 적은 반복 작업으로 만족스러운 결과를 도출할 수 있음을 보여줍니다.
- **Technical Details**: ViPer는 사용자로부터 받은 주석을 구조화된 표현으로 변환하여 이미지의 시각적 속성을 포착합니다. 이는 사용자로 하여금 좋아하는 이미지의 특징을 더 자세히 설명할 수 있도록 하며, 모델은 Stable Diffusion(안정적 확산)와 같은 기존 생성 모델에 이러한 개인 선호성을 통합할 수 있습니다. 이 과정에서 약 500개의 시각적 속성이 포함된 데이터셋을 활용하여 주석에서 개인의 선호를 추출하고, 이를 통해 개인화된 이미지를 생성하기 위한 방법론을 제시합니다.
- **Performance Highlights**: 사용자 연구 결과에 따르면, 대다수의 사용자는 ViPer 방식이 다른 기존 방법들보다 개인의 선호에 더 잘 부합하는 이미지를 생성하는 것을 선호했습니다. 개인화된 결과가 비 개인화된 결과나 타 사용자의 결과보다 사용자에게 더 매력적이라는 강한 선호가 드러났습니다.

### [Longhorn: State Space Models are Amortized Online Learners](https://arxiv.org/abs/2407.14207)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.14207.png)

Vote: 4

Authors: Peter Stone, Qiang Liu, Rui Wang, Bo Liu, Lemeng Wu, Yihao Feng

- **What's New**: 본 연구에서는 상태 공간 모델(State-Space Models, SSMs)의 설계를 위한 새로운 원칙을 제안하며, Longhorn이라는 효과적인 아키텍처를 제안합니다. Longhorn은 온라인 연상 회상 문제를 해결하기 위해 암묵적인 폐쇄 형식 업데이트에서 파생되었습니다.
- **Technical Details**: SSM의 반복적 업데이트를 온라인 학습 목표를 해결하는 것으로 간주하며, 이로 인해 SSM의 설계가 온라인 학습 목표의 설계로 단순화됩니다. Longhorn은 대다수 기존 SSMs에서 필요한 별도의 매개변수화된 잊기 게이트 없이 안정적인 반복 형식을 통해 작동합니다.
- **Performance Highlights**: Longhorn은 합성 및 대규모 시퀀스 모델링 작업에서 Mamba(Gu & Dao, 2023)와 같은 최첨단 SSMs에 비해 동등하거나 더 나은 성능을 보여줍니다. 특히, 100B 토큰으로 SlimPajama 데이터셋에서 훈련 시 1.3B 파라미터의 Longhorn이 Mamba보다 우수한 성능을 나타냅니다. Longhorn의 샘플링 효율성은 Mamba보다 1.8배 향상되었습니다.

### [MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2407.16312)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.16312.png)

Vote: 4

Authors: Gao Peng, Willem Röpke, Florian Felten, Patrick Mannion, Jordan K. Terry, El-Ghazali Talbi, Grégoire Danoy, Ann Nowé, Roxana Rădulescu, Hicham Azmani, Umut Ucak, Diederik M. Roijers, Hendrik Baier

- **What's New**: 이 논문에서는 Transformer 아키텍처를 개선하기 위한 새로운 접근법을 제안합니다. 특히, 모듈화된 어텐션 메커니즘을 도입하여 텍스트 처리 성능을 향상시키고 있습니다.
- **Technical Details**: 제안된 모델은 Multi-Head Attention과 Feed-Forward Networks를 조합하여 정보 흐름을 최적화하고, Dynamic Routing 기법을 적용하여 더 효율적인 학습을 가능하게 합니다.
- **Performance Highlights**: 실험 결과, 제안된 모델은 기존의 Transformer 기반 모델들에 비해 정확도가 5% 향상되었으며, 특히 긴 문장에서의 성능이 두드러졌습니다.

### [Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning](https://arxiv.org/abs/2407.15815)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.15815.png)

Vote: 2

Authors: Tianming Wei, Huazhe Xu, Gu Zhang, Zhecheng Yuan, Shuiqi Cheng, Yuanpei Chen

- **What's New**: 이 논문에서는 로봇의 비주얼 일반화 능력을 향상시키기 위한 새로운 프레임워크인 Maniwhere를 소개합니다. 이 프레임워크는 강화학습(Reinforcement Learning)에서 시뮬레이션(simulation)과 현실(real-world) 사이의 전이(sim2real)를 보다 효과적으로 수행할 수 있도록 설계되었습니다.
- **Technical Details**: Maniwhere는 다중 시점(multi-view) 표현 학습 목표를 사용하여 다양한 시점에서 공유된 의미 정보를 캡처하며, STN(Spatial Transformer Network) 모듈을 비주얼 인코더에 통합하여 카메라 시점 변화에 대한 로봇의 강인성을 더욱 향상시킵니다. 논문에서는 카메라의 무작위화(randomization)를 통해 훈련 과정을 안정화하고 오버피팅을 방지하는 커리큘럼 기반 접근법을 적용합니다.
- **Performance Highlights**: 실험 결과, Maniwhere는 시뮬레이션 및 실제 환경에서 기존의 선행 연구들을 상회하는 성능을 보였습니다. 다양한 로봇 팔과 손으로 구성된 8개 임무에 대한 평가에서, 이 프레임워크는 full real-world 적용을 위한 비주얼 일반화 능력을 성공적으로 입증했습니다.

### [DistilDIRE: A Small, Fast, Cheap and Lightweight Diffusion Synthesized Deepfake Detection](https://arxiv.org/abs/2406.00856)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.00856.png)

Vote: 1

Authors: Oren Etzioni, Changyeon Lee, Yewon Lim, Aerin Kim

- **What's New**: 이 논문은 새로운 'DistilDIRE' 모델을 제안합니다. 이는 프리트레인된 diffusion 모델을 활용하여 딥페이크를 보다 효과적으로 탐지하는 경량화된 방법입니다. 기존의 DIRE (Wang et al., 2023) 접근법의 느린 속도를 개선하기 위해, 사전 훈련된 ResNet-50 모델을 teacher로 사용하고, 이를 통해 학습하는 student 모델을 도입했습니다.
- **Technical Details**: DistilDIRE 프레임워크는 원본 이미지와 해당 이미지의 첫 번째 시점에서 생성된 노이즈를 결합하여 학습 효과를 높입니다. 여기서 노이즈는 Ablated Diffusion Model (ADM)에서 추출된 것으로, 이러한 접근방식은 복잡한 reconstruction 과정을 생략하고도 효과적인 딥페이크 탐지가 가능하도록 합니다. 학생 모델은 Binary Cross-Entropy Loss (BCE)를 사용하여 실제 이미지와 합성 이미지를 구분하도록 학습됩니다.
- **Performance Highlights**: DistilDIRE는 매우 빠르고 저렴하게 딥페이크를 탐지할 수 있는 모델로, 실제 응용에 적합합니다. DiffusionForensic 데이터셋에서 딥페이크 탐지 성능이 최신 기술에 근접한 결과를 보여주었으며, 이로 인해 상업적 활용 가능성도 크게 높아졌습니다.

### [DreamCar: Leveraging Car-specific Prior for in-the-wild 3D Car Reconstruction](https://arxiv.org/abs/2407.16988)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.16988.png)

Vote: 1

Authors: Haiyang Sun, Tianqing Zhu, Xiaobiao Du, Ming Lu, Xin Yu

- **What's New**: 이 논문에서는 자율주행 차량의 3D 모델을 재구성하기 위한 새로운 방법인 DreamCar를 제안합니다. 이 방법은 교통 시나리오에서 자율주행 차량이 수집한 실제 센서 데이터를 활용하여 고품질의 3D 자산 라이브러리를 생성합니다.
- **Technical Details**: DreamCar는 도로 위에서의 물체 인식을 위한 방법으로, 차량의 거울 대칭(mirror symmetry)과 자세 최적화(pose optimization) 기법을 통합합니다. 특히, PoseMLP라는 다층 퍼셉트론(Multilayer Perceptron)을 설계하여 원래의 자세를 수정할 수 있는 오프셋을 예측합니다. 또한, Car360이라는 고품질 차량 데이터셋을 수집하여 3D-aware diffusion 모델의 일반화 성능을 향상시킵니다.
- **Performance Highlights**: 제안된 DreamCar 방법은 저해상도 및 노이즈가 많은 4장의 참조 이미지만으로도 정밀한 기하학 및 복잡한 질감을 가진 완전한 3D 객체를 정확하게 재구성할 수 있습니다. Car360 데이터셋을 활용하여 기존 방법보다 더 높은 성능을 보여주며, 자율주행 데이터셋에서 대규모 3D 차량 객체 재구성 작업에 효과적입니다.

