## Daily Papers (2024-07-09)

### [MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?](https://arxiv.org/abs/2407.04842)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04842.png)

Vote: 29

Authors: Zhuokai Zhao, Chenhang Cui, Qinghao Ye, Haoqin Tu, Yichao Du, Zhihong Zhu, Yuqing Zhang, Zichen Wen, Rafael Rafailov, Yiyang Zhou, Chaoqi Wang, Qinglan Huang, Huaxiu Yao, Zhenzhen Weng, Canyu Chen, Zhaorun Chen, Jiawei Zhou, Chelsea Finn, Zhengwei Tong

- **What's New**: 멀티모달(FMs) 기초 모델의 최근 발전으로, DALLE-3, Stable Diffusion 등 이미지 생성 모델들이 등장했습니다. 그러나 텍스트-이미지 모델은 텍스트-이미지 불일치(hallucination), 부적절한 콘텐츠 생성, 저품질 생성, 편향된 출력 등 다양한 문제로 고통받고 있습니다. 이러한 문제를 해결하고 모델의 신뢰성을 높이기 위해, 모델의 생성 결과에 대한 피드백을 제공해야 합니다. 이를 위해 MJ-Bench라는 새로운 벤치마크가 제안되었습니다.
- **Technical Details**: MJ-Bench는 텍스트-이미지 정렬, 안전성, 이미지 품질, 생성 편향 등 네 가지 주요 관점에서 멀티모달 판사의 성능을 평가하기 위한 종합적인 데이터셋을 포함합니다. 각 데이터포인트는 지시문, 선택된 이미지, 거부된 이미지로 구성됩니다. 평가 메트릭은 자동 메트릭(예: 승률)과 인간 평가(예: 순위)를 결합하여 신뢰할 수 있는 결론을 도출합니다.
- **Performance Highlights**: 평가 결과, 비공개 VLMs가 다양한 척도에서 더 나은 피드백을 제공하는 것으로 나타났습니다. 특히 GPT-4o가 평균적으로 다른 판사를 능가했습니다. VLMs는 여러 이미지를 동시에 제공할 때 더 나은 피드백을 제공하며, CLIP 기반 스코링 모델은 텍스트-이미지 정렬과 이미지 품질에서 더 나은 피드백을 제공합니다. 반면, VLMs는 안전성과 편향 측면에서 더 정확한 피드백을 제공합니다. 인간 평가 결과는 자동 메트릭과 약간의 차이가 있지만 전반적인 추세는 일치합니다.

### [LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages](https://arxiv.org/abs/2407.05975)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.05975.png)

Vote: 23

Authors: Yinquan Lu, Fei Yuan, Wenhao Zhu, Lei Li, Yu Qiao

- **What's New**: 최근 연구에서는 대형 언어 모델(LLMs)이 고자원 언어 번역에서는 우수한 성능을 보이지만, 저자원 언어의 번역 성능은 여전히 부족하다는 문제점이 있습니다. 이를 해결하기 위해 102개의 언어를 다루는 대규모 다언어 지속 학습(multilingual continual pre-training)을 수행하여 LLAMAX 시리즈 모델을 개발했습니다.
- **Technical Details**: 이번 연구에서는 어휘 확장(vocabulary extension)과 데이터 증강(data augmentation)을 포함한 여러 중요한 기술 설계를 종합적으로 분석합니다. 이 분석을 바탕으로 데이터 증강 전략을 설계하고, 병렬 데이터와 단일언어 데이터를 이용해 지속 학습을 수행하여, 저자원 언어 번역 성능을 개선했습니다.
- **Performance Highlights**: LLAMAX2 모델은 60일간 24개의 A100 GPU를 사용하여 학습되었으며, M2M-100-12B 모델과 유사한 번역 성능을 달성했습니다. 특히, 저자원 언어 번역에서 기존의 베이스라인 모델에 비해 평균 10 spBLEU 이상의 성능 향상을 보였습니다. 또한, 일반 작업 성능을 저해하지 않으면서 번역 성능을 크게 향상시켰습니다.

### [Learning Action and Reasoning-Centric Image Editing from Videos and Simulations](https://arxiv.org/abs/2407.03471)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03471.png)

Vote: 15

Authors: Eva Portelance, Christopher Pal, Benno Krojer, Luis Lara, Varun Jampani, Siva Reddy, Dheeraj Vattikonda

- **What's New**: 이 논문에서는 새로운 딥러닝 기반의 이미지 분류 모델을 제안합니다. 이 모델은 전통적인 Convolutional Neural Networks (CNNs)와 Transformer 구조를 결합하여 더 높은 성능을 달성하고자 합니다.
- **Technical Details**: 새로운 모델은 ResNet과 ViT(Vision Transformer)의 하이브리드 구조를 채택하고 있습니다. 주목해야 할 점은 이 모델이 두 가지 주요 구성 요소로 나뉜다는 것입니다: Feature Extractor와 Classification Head. Feature Extractor는 ResNet을 기반으로 하며, Classification Head는 Transformer 기반입니다. 이 모델은 PyTorch로 구현되었으며, 다양한 데이터셋에서 학습되었습니다.
- **Performance Highlights**: 제안된 모델은 ImageNet 데이터셋에서 88.5%의 Top-1 정확도를 기록하며, 기존의 최고 성능 모델보다 성능이 향상되었습니다. 또한, CIFAR-10과 CIFAR-100 데이터셋에서 각각 98.2%, 89.9%의 Top-1 정확도를 달성하였습니다. 이는 현존하는 가장 뛰어난 이미지 분류 모델 중 하나임을 시사합니다.

### [Associative Recurrent Memory Transformer](https://arxiv.org/abs/2407.04841)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04841.png)

Vote: 14

Authors: Ivan Rodkin, Aydar Bulatov, Yuri Kuratov, Mikhail Burtsev

- **What's New**: 이번 연구에서는 근접 메모리를 확장하여 연관 메모리를 통합한 Associative Recurrent Memory Transformer (ARMT)를 제안합니다. 이는 기존의 segment-level recurrent model인 RMT (Bulatov et al., 2022)의 확장판입니다.
- **Technical Details**: ARMT는 segment-level recurrence와 layerwise 연관 메모리를 통합한 새로운 아키텍처를 사용합니다. 메커니즘은 선형 변환을 통해 키와 값을 생성하여 quasi-linear key-value memory에 저장합니다. 이 과정은 특수 메모리 토큰에만 주의(attend)하며 비선형성(ϕ)을 사용하여 키, 값, 중요도 스칼라를 계산합니다. 기억된 정보를 갱신하기 위해 연관을 호출하고 새로운 값을 추가하거나 이전 값을 삭제합니다.
- **Performance Highlights**: ARMT는 BABILong 벤치마크를 통해 메모리 용량과 연관 검색 작업에서 높은 성능을 기록했습니다. 특히, 5천만 토큰 이상의 입력 크기에서도 80%의 정확도로 단일 사실 질문 응답(single fact QA)을 수행하며, 기존의 RMT와 Mamba 모델을 능가하는 성능을 보였습니다. ARMT는 연관 검색 및 긴 문맥(processing long context) 작업에서 뛰어난 성능을 입증했으며, 내부 연관 메모리를 효과적으로 활용하여 메모리 용량 평가에서도 우월함을 나타냈습니다.

### [ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation](https://arxiv.org/abs/2407.06135)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06135.png)

Vote: 11

Authors: Jiadi Su, Yan Ma, Ethan Chern, Pengfei Liu

- **What's New**: 새로운 논문에서는 다차원 AI 모델 개발에 대한 중요한 진전을 소개합니다. Meta AI의 Chameleon 모델을 기반으로 구축된 Anole은 텍스트 생성 및 다차원 이해의 강점을 유지하면서 비전 생성 및 다차원 생성 기능을 추가하였습니다. Anole은 오픈 소스로 제공되어 연구자들과 개발자들이 쉽게 이용하고 발전시킬 수 있습니다.
- **Technical Details**: Anole은 Chameleon 모델의 초기 융합, 토큰 기반 자동 회귀 접근 방식을 채택하여 다차원 시퀀스를 생성합니다. Chameleon 모델의 대부분의 파라미터는 동결되었으며, 변환기 출력 머리 층의 이미지 토큰 ID에 해당하는 로그만을 미세 조정하였습니다. Anole-7b-v0.1 버전은 LAION-5B 아트 데이터셋에서 5,859개의 이미지를 사용하여 소량의 파라미터를 미세 조정하여 개발되었습니다.
- **Performance Highlights**: Anole은 Chameleon의 비전 및 다차원 생성 역량을 향상시키면서도 모델의 단순성과 효율성을 유지합니다. 이는 보다 높은 성능과 함께 트레이닝 및 추론 시 효율성을 제공합니다. 실험 결과, Anole은 텍스트 이해, 텍스트 생성 및 다차원 이해에서 강력한 성능을 유지하면서 고품질의 이미지를 생성할 수 있습니다.

### [Evaluating Language Model Context Windows: A "Working Memory" Test and Inference-time Correction](https://arxiv.org/abs/2407.03651)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03651.png)

Vote: 10

Authors: Changho Shin, Christopher Glaze, Frederic Sala, Amanda Dsouza

- **What's New**: 대규모 언어 모델(LLMs)은 실제 응용 프로그램에서 긴 컨텍스트 입력을 효과적으로 활용할 수 있을지에 대한 평가가 필요한 상황입니다. 논문에서는 이러한 모델들의 긴 컨텍스트 활용 능력을 보다 현실적으로 평가할 수 있는 새로운 평가 프레임워크인 SWiM(Snorkel Working Memory)을 제안합니다.
- **Technical Details**: SWiM 프레임워크는 작업 생성, 작업 검증, 작업 완료, 작업 평가의 네 단계로 구성됩니다. 이 프레임워크는 현실적인 문서와 작업 쌍에 대해 언어 모델의 긴 컨텍스트 능력을 측정합니다. 또한, Medoid Voting 알고리즘을 제안하여 문서 위치 효과를 완화합니다.
- **Performance Highlights**: 실험 결과, 대부분의 긴 컨텍스트 모델들은 컨텍스트 창의 중간 부분에서 정보 검색에 비효율적입니다('lost-in-the-middle' 효과). Medoid Voting 알고리즘을 통해 이 문제를 개선할 수 있습니다. 테스트에서 모델들은 컨텍스트 창에 방해 문서가 추가될수록 성능이 저하되었지만, 모델별 성능 차이가 존재했습니다. 특히 Gemini-1.5-Pro는 긴 컨텍스트에서도 노이즈에 강한 성능을 보였습니다.

### [Tailor3D: Customized 3D Assets Editing and Generation with Dual-Side Images](https://arxiv.org/abs/2407.06191)

![](/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg)

Vote: 5

Authors: Jiaqi Wang, Xiaoyang Wu, Long Xing, Tong Wu, Zhangyang Qi, Mengchen Zhang, Dahua Lin, Xihui Liu, Yunhan Yang, Hengshuang Zhao

- **What's New**: 최근 몇 년간 Stable Diffusion과 ControlNet 기술은 2D AI 생성 콘텐츠를 혁신하며 텍스트-이미지 변환, 이미지 편집 및 스타일 전환을 보다 쉽게 만들었습니다. 반면, 3D AI 생성 콘텐츠의 잠재력도 인식되면서 텍스트와 이미지를 결합한 직접적인 3D 객체 생성이 가능해졌습니다. Tailor3D는 이러한 기술을 바탕으로 사용자 친화적인 3D 빠른 편집 프레임워크를 제안합니다.
- **Technical Details**: Tailor3D 프레임워크는 고급 2D 이미지 편집 기술을 활용하여 3D 객체 생성을 체계적으로 수행합니다. 각 단계에서 2D 편집과 3D 재구성을 반복하며 더욱 정밀한 객체 편집이 가능합니다. 이 과정에서 Dual-sided LRM 모델은 LoRA Triplane Transformer를 통해 최소한의 메모리로 전전과 후방 뷰를 처리하여 고품질의 3D 객체를 생성합니다.
- **Performance Highlights**: Tailor3D는 다양한 3D 편집 작업에서 성능을 입증했으며, 특히 3D 형태 채우기, 텍스처 합성 및 스타일 전환에서 높은 품질을 구현합니다. 이 시스템은 사용자에게 실시간 상호작용을 통해 빠르고 정밀한 3D 객체 편집을 가능하게 합니다.

### [Training Task Experts through Retrieval Based Distillation](https://arxiv.org/abs/2407.05463)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.05463.png)

Vote: 4

Authors: Graham Neubig, Hongyin Luo, Vijay Viswanathan, Jiaxin Ge, Xueying Jia

- **What's New**: ReBase는 대량의 레이블링된 데이터 소스에서 데이터를 검색한 후, 이를 사용자 작업에 필요한 콘텐츠와 형식으로 변환하여 도메인별 모델을 훈련하는 새로운 프레임워크입니다. 기존의 데이터 생성을 통한 미세 조정 방식과 달리 ReBase는 데이터의 다각성을 높이며, 다양한 데이터 소스에서 관련 데이터를 효과적으로 검색합니다. 그리고 이 데이터를 사용해 특정 작업에 맞춘 모델을 훈련합니다.
- **Technical Details**: ReBase의 방법론은 크게 세 단계로 나눌 수 있습니다: 데이터 저장소 구축(datastore construction), 데이터 저장소 검색(datastore retrieval), 그리고 데이터셋 변환(dataset transformation)입니다. 첫째, Hugging Face Datasets에서 75,000개 이상의 데이터를 수집하여 큰 데이터 저장소를 만듭니다. 각 데이터셋의 열을 별도로 인코딩하고 고유한 식별자와 함께 저장소에 추가합니다. 둘째, 사용자가 제공한 지침과 예제를 인코딩하여 저장소에서 가장 관련성이 높은 항목을 검색합니다. 이후 이 데이터를 대형 언어 모델을 사용해 특정 작업에 필요한 형식과 콘텐츠로 변환합니다. 마지막으로, 데이터 포인트를 쿼리와 답변 필드를 포함한 형식으로 변환하여 작은 모델이 대형 모델의 추론 생성을 기반으로 학습할 수 있도록 합니다. 특히 ReBase는 Chain-of-Thought 변환 단계를 추가하여 단계별 추론을 가능하게 합니다.
- **Performance Highlights**: ReBase는 BBH, MNLI, SQuAD, 그리고 MCoNaLa 코드 생성 벤치마크에서 테스트했습니다. 결과적으로, BBH 벤치마크에서 1.94%, SQuAD 벤치마크에서 7.8%, MNLI 벤치마크에서 1.37% 향상된 성능을 보였습니다. 이는 다양한 소스에서 데이터를 검색해 특정 모델을 훈련시키는 방법의 유용성을 시사합니다.

### [InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with Inverse-Instruct](https://arxiv.org/abs/2407.05700)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.05700.png)

Vote: 4

Authors: Lingzhe Gao, Yewen Pu, Yutong Wu, Yunji Chen, Di Huang, Kaizhao Yuan, Shihao Liu, Ziyuan Nan, Rui Zhang, Xishan Zhang, Wenxuan Shi, Dawei Yin, Wei Wang, Qi Guo, Zidong Du, Xing Hu

- **What's New**: 이 연구는 기존의 폐쇄형 대규모 언어 모델(LLMs)을 이용해 데이터를 생성하는 대신, 모델 자체를 활용하여 추가적인 고품질 명령어 데이터를 생성하고 이를 통해 코드 생성 능력을 향상시키는 새로운 접근법 'inverse-instruct'을 제안합니다.
- **Technical Details**: Inverse-instruct는 코드를 자연 언어로 번역하는 것이 자연 언어를 코드로 번역하는 것보다 더 쉽다는 점과 동일한 코드가 다양한 명령어로 해석될 수 있다는 점을 활용합니다. 이 과정은 코드 전처리, 코드 요약, 자가 평가 및 데이터 선택의 세 단계로 구성됩니다.
- **Performance Highlights**: Inverse-instruct를 적용한 InverseCoder 시리즈는 HumanEval(+), MBPP(+), MultiPL-E, DS-1000 등의 다양한 벤치마크에서 뛰어난 성능을 기록했습니다. 예를 들어 InverseCoder-DS-6.7B 모델은 HumanEval+에서 76.8%, MBPP+에서 69.0%, MultiPL-E에서 62.6%, DS-1000에서 44.2%를 달성하여 각각의 벤치마크에서 최고 성능(State-of-the-Art)을 기록했습니다.

### [UltraEdit: Instruction-based Fine-Grained Image Editing at Scale](https://arxiv.org/abs/2407.05282)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.05282.png)

Vote: 3

Authors: Haozhe Zhao, Peiyu Yu, Kaikai An, Shuzheng Si, Liang Chen, Qing Li, Baobao Chang, Xiaojian Ma, Minjia Zhang, Rujie Wu

- **What's New**: 새로운 데이터셋 UltraEdit을 발표합니다. 이는 대규모로 지시(instruction)에 기반한 이미지 편집을 위한 것입니다. 기존 연구와 비교하여 UltraEdit은 여러 단점들을 극복하고자 합니다. 예를 들어, 지시의 다양성, 이미지의 편향성, 영역 기반 편집의 부족 문제를 다루고 있습니다.
- **Technical Details**: UltraEdit의 생성 파이프라인은 다음과 같습니다: LLM (대형 언어 모델)과 사람의 예제를 사용하여 다양한 편집 지시를 생성합니다. 그런 다음 P2P (prompt-to-prompt) 제어와 기존 T2I(diffusion) 모델을 사용하여 캡션과 지시로부터 원본 및 편집된 이미지를 생성합니다. 또한, COCO 같은 다양한 실제 이미지 데이터셋에서 고품질 이미지-캡션 쌍을 수집하여 T2I 모델의 편향을 줄입니다. 마지막으로, 자동 영역 생성 방식을 사용하여 지시에서 편집 영역을 추출하고, 이를 수정된 인페인팅(diffusion) 파이프라인에서 활용합니다.
- **Performance Highlights**: UltraEdit 데이터셋은 약 400만 개의 편집 샘플과 약 75만 개의 고유한 지시, 9가지 이상의 편집 유형을 포함합니다. 이는 공개된 가장 큰 지시 기반 이미지 편집 데이터셋으로, 초보적인 diffusion 기반 편집 모델 기준에서 MagicBrush 및 Emu-Edit 벤치마크에서 새로운 기록을 세웠습니다. 분석 결과, 실제 이미지 앵커와 영역 기반 편집 데이터의 중요한 역할이 확인되었습니다.

### [Multi-Object Hallucination in Vision-Language Models](https://arxiv.org/abs/2407.06192)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06192.png)

Vote: 3

Authors: Xuweiyi Chen, David F. Fouhey, Shengyi Qian, Sihan Xu, Joyce Chai, Jianing Yang, Ziqiao Ma, Xuejun Zhang

- **What's New**: 최근 대형 언어 모델(LLMs)의 발전은 시각적 의미 이해를 위한 적용 노력 증가를 자극하면서 대형 비전 언어 모델(LVLMs)의 출현을 이끌었습니다. 이 논문은 LVLMs가 여러 객체에 초점을 맞추면 객체 환각(object hallucination)이 발생하는 상황을 다루고, 이를 평가하기 위한 Recognition-based Object Probing Evaluation (ROPE)를 소개합니다.
- **Technical Details**: ROPE는 다중 객체 환각을 평가하는 자동화된 프로토콜로, 블랙박스 신경 모델이나 인간 평가자를 사용하지 않고 이미지를 통해 고유하게 객체를 지칭하는 비주얼 프롬프트를 사용합니다. ROPE는 테스트 시간에 이미지 내 객체 클래스 분포를 고려하여 4개의 하위 집합(In-the-Wild, Homogeneous, Heterogeneous, Adversarial)으로 나뉩니다. 이를 통해 모델의 환각 동작을 심층 분석합니다.
- **Performance Highlights**: ['(1) LVLMs는 단일 객체에 초점을 맞출 때보다 여러 객체에 초점을 맞출 때 더 많은 환각이 발생함.', '(2) 테스트된 객체 클래스 분포는 환각 행동에 영향을 미쳐, LVLMs가 단축 경로(shortcuts)와 가짜 상관관계(spurious correlations)를 따를 수 있음을 드러냄.', '(3) LVLMs의 환각 행동은 데이터 특정 요소, 두드러짐(salience)과 빈도(frequency), 모델 자체의 행동에 영향을 받음.', '이러한 발견은 더 균형 잡힌 객체 분포, 다양한 주석 및 향상된 다중 객체 지시가 포함된 LVLMs 개발 및 적용에 대한 중요한 통찰력을 제공합니다.']

### [PAS: Data-Efficient Plug-and-Play Prompt Augmentation System](https://arxiv.org/abs/2407.06027)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06027.png)

Vote: 3

Authors: Haoze Sun, Fan Yang, Kun Fang, Bin Cui, Kun Li, Yozhen Wu, Tao Zhang, Hao Liang, Tianpeng Li, Weipeng Chen, Miao Zheng, Yanjun Sheng, Yan Zhang, Zenan Zhou, Wentao Zhang, Yujing Qiao, Mingan Lin, Lingchu Xiong, Guosheng Dong

- **What's New**: 최근 몇 년 간, 대형 언어 모델(LLMs: Large Language Models)의 급속한 발전은 데이터 관리와 AI 시스템의 중요성을 강조하고 있습니다. 특히, 확장성과 효율성을 갖춘 플러그 앤 플레이(plug-and-play) 시스템의 필요성이 증가하고 있습니다. 주요 기술 중 하나로 프롬프트 엔지니어링(prompt engineering)이 부각되고 있으며, 이는 비용이 매우 낮고 성능 향상이 두드러집니다. 본 논문에서 제안하는 PAS(Prompt-enhancing Automatic System)는 프롬프트 자동화 기법으로, 인간의 개입 없이 고품질의 프롬프트를 생성하고 이를 통해 LLM을 미세 조정(fine-tuning)할 수 있는 시스템입니다.
- **Technical Details**: PAS 시스템은 두 가지 주요 단계로 구성됩니다. 첫 번째 단계는 고품질 프롬프트 선별입니다. 이를 위해 임베딩 모델을 사용하여 프롬프트 데이터를 특징으로 추출하고, 클러스터링 알고리즘을 적용해 유사한 프롬프트를 그룹화 및 중복 제거합니다. 이후 LLM를 사용해 고품질 프롬프트를 선택하고 이를 다양한 카테고리로 분류합니다. 두 번째 단계는 자동 보완 프롬프트 생성입니다. 몇 샷 학습(few-shot learning) 기술을 활용해 새로운 프롬프트를 생성하고, 이 생성된 데이터는 엄격한 검증 과정을 거쳐 선별됩니다. 선별된 고품질 데이터는 LLM을 미세 조정하는 데 사용됩니다.
- **Performance Highlights**: PAS 모델은 고효율성과 확장성을 자랑합니다. 단 9000 쌍의 프롬프트 보완 데이터로 LLM을 미세 조정할 수 있으며, 데이터 생성 과정은 전적으로 자동화되어 있어 인간의 노동력을 필요로 하지 않습니다. 또한 PAS 모델은 임의의 LLM과 통합될 수 있고, 다양한 작업에서 State-of-the-Art(SoTA) 성능을 달성합니다. 다중 벤치마크에서 뛰어난 성능을 보이며, 이전 최고 성능 모델인 BPO보다 더 높은 성능을 기록합니다. 인간 평가 메트릭에서도 탁월한 성능을 보여줍니다.

### [Compositional Video Generation as Flow Equalization](https://arxiv.org/abs/2407.06182)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06182.png)

Vote: 2

Authors: Xinchao Wang, Xingyi Yang

- **What's New**: Vico는 새로운 합성 비디오 생성 프레임워크로, 모든 개념이 동일하게 표현되도록 보장합니다. 텍스트 토큰 각자가 최종 비디오 출력에 동등한 영향을 미치도록 최적화합니다. 이를 위해, 비디오 확산 모델의 각 역방향 시간 단계에서 각 토큰의 영향을 평가하고 조정하는 테스트 시간 최적화를 사용합니다. ST-Flow (Spatial-Temporal Attention Flow)라는 새로운 속성 할당 방법을 개발하여, 각 입력 토큰의 비디오 토큰에 대한 영향을 평가합니다.
- **Technical Details**: Vico는 텍스트-비디오(T2V) 모델에서 공정한 영향을 보장하기 위해 각 텍스트 토큰의 영향을 평가하고 조정합니다. 최초의 평가 및 조정은 테스트 시간 최적화 택해져, 각 토큰의 영향을 모든 역방향 시간 단계에서 점검합니다. ST-Flow는 주어진 입력 토큰에서 비디오 토큰까지의 플로우 값을 계산하면서 새로운 속성 할당 방법을 통해 공간적 및 시간적 요소를 모두 고려합니다. 이 나이브(max-flow) 계산은 비디오 모델에 있어 매우 비싼 계산 비용을 초래할 수 있으므로, 이를 효율적으로 구현하기 위해 서브그래프의 플로우 값을 벡터화된 방식으로 계산하여 최대 100배 더 빨리 계산하도록 하였습니다.
- **Performance Highlights**: Vico는 텍스트-비디오(T2V) 생성 및 비디오 편집 등 다양한 비디오 애플리케이션에서 구현되었습니다. 기존 모델과 비교하여 생성된 비디오의 정확도와 의미론적 정확도가 크게 향상되었습니다. Vico의 광범위한 평가 결과, 비디오 품질과 의미론적 정확성에서 상당한 개선을 증명하였습니다.

### [PartCraft: Crafting Creative Objects by Parts](https://arxiv.org/abs/2407.04604)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04604.png)

Vote: 2

Authors: Xiatian Zhu, Tao Xiang, Yi-Zhe Song, Kam Woh Ng

- **What's New**: 최근 연구는 텍스트-투-이미지(Text-to-Image, T2I) 생성 모델에서 '제어'의 문제를 해결하기 위한 새로운 접근법을 제안합니다. 사용자가 텍스트나 스케치 대신 시각적 개념을 선택하여 창의적인 객체를 생성하는 것을 허용합니다. 이를 통해 사용자는 모델의 생성 프로세스에 세부적으로 개입할 수 있습니다.
- **Technical Details**: 이 논문은 컴퓨터 비전 기술인 DINOv2 feature maps와 비감독(Unsupervised) 클러스터링을 사용하여 객체의 부분을 식별하고, 엔트로피 기반의 attention loss를 도입하여 선택된 부분을 정밀하게 배치합니다. 이 방식은 객체 부분의 정확한 분리와 종합 구조를 학습하는데 중점을 둡니다. 추가로, bottleneck encoder를 사용하여 텍스트 토큰을 프로젝트(Project)함으로써 학습 효율을 강화합니다.
- **Performance Highlights**: 제안된 PartCraft 기술은 CUB-200-2011 (새)와 Stanford Dogs 데이터셋에서 뛰어난 성능을 보였으며, 정성적(qualitative) 및 정량적(quantitative) 평가에서 기존 방법들을 능가했습니다.

### [LLMAEL: Large Language Models are Good Context Augmenters for Entity Linking](https://arxiv.org/abs/2407.04020)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04020.png)

Vote: 1

Authors: Zijun Yao, Kaisheng Zeng, Lei Hou, Xu Bin, Amy Xin, Yunjia Qi, Juanzi Li, Fangwei Zhu

- **What's New**: LLM-Augmented Entity Linking (LLMaEL) 라는 새로운 방법이 소개되었습니다. 이 방법은 전통적인 entity linking (EL) 모델과 대형 언어 모델(LLMs)의 강점을 결합하여 EL의 성능을 향상시키는 것을 목표로 합니다.
- **Technical Details**: LLMaEL은 세 가지 주요 단계를 포함합니다: (1) 컨텍스트 증강(context augmentation), 여기서 LLMs를 사용하여 동일한 언급(mention)과 관련된 추가 설명을 생성합니다. (2) 데이터 융합(data fusion), 여기서 LLM이 증강한 컨텍스트를 기존의 EL 모델과 결합합니다. (3) EL 실행(EL execution), 여기서 EL 모델을 사용하여 목표 엔티티를 찾습니다.
- **Performance Highlights**: 3333개의 선택된 EL 모델에 대해 LLMaEL은 5555개의 데이터셋 중 6666개의 데이터셋에서 새로운 State-of-the-Art (SOTA) 성능을 달성했습니다. 또한, LLMaEL은 모든 6666개의 데이터셋에서 평균 1.21%의 정확도 향상을 기록했습니다.

### [ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models](https://arxiv.org/abs/2407.04693)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04693.png)

Vote: 1

Authors: Wenwei Zhang, Yuzhe Gu, Chengqi Lyu, Ziwei Ji, Dahua Lin, Kai Chen

- **What's New**: 대형 언어 모델(LLM, Large Language Models)은 다양한 작업에서 우수한 성능을 보여주고 있으나, 그들은 자주 있을 법한 이야기 같지만 실제로는 신빙성이 없거나 의미가 없는 정보를 생성하는 '환각(hallucination)' 문제를 안고 있습니다. 이러한 문제를 해결하기 위해, 이 논문은 환각 주석 데이터셋을 확장하고 주석의 정확성을 동시에 향상시키기 위한 반복적 셀프 트레이닝 프레임워크를 제안합니다.
- **Technical Details**: 이 프레임워크는 기대 최대화(EM) 알고리즘의 관점에서 설명될 수 있습니다. 기대 단계(E-step)에서는 기존의 가장 우수한 환각 어노테이터를 사용하여 확장된 데이터셋의 실제 환각 주석을 추정합니다. 최대화 단계(M-step)에서는 기존 주석과 이전 E단계에서 파생된 확장 데이터 주석을 결합하여 새로운 환각 어노테이터를 훈련합니다. 이러한 단계들은 다차원적 데이터 확장을 포함하며, 세 가지 주요 단계를 통해 주석자의 성능을 점진적으로 향상시킵니다.
- **Performance Highlights**: 우리가 제안한 어노테이터는 GPT-4를 포함한 기존 모델들을 놀라운 정확도로 능가합니다. 특정 데이터셋에서는 89.24%의 정확도를 보였고, 새로운 상태의 최신(SOTA) 기록은 81.54% 및 94.44%로 이를 뛰어넘었습니다. 또한 이 어노테이터는 환각 평가를 자동화하여 앞으로 연구 커뮤니티가 다양한 오픈소스 모델들의 환각 수준을 평가하는 데 있어 중요한 기준을 제공합니다.

### [Understanding Visual Feature Reliance through the Lens of Complexity](https://arxiv.org/abs/2407.06076)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06076.png)

Vote: -

Authors: Louis Bethune, Andrew Kyle Lampinen, Katherine Hermann, Thomas Fel, Thomas Serre

- **What's New**: 최근 연구에서는 대형 비전 모델이 훈련 작업에서 성능을 지원하기 위해 다양한 특징(features)을 학습한다는 사실을 밝혀냈습니다. 이 모델들이 특정 특징(예: 텍스처)보다 다른 특징(예: 형태)에 대한 선호도를 나타내는 경향이 있으며, 이는 모델의 일반화 능력을 저해하는 지름길(shortcut) 사용과 관련이 있을 수 있습니다. 또한 최근 연구에서는 자동 컨셉 추출(Automated Concept Extraction)을 통해 모델이 학습한 다양한 개념을 식별하고, 이를 통해 더 포괄적인 특징 분석이 가능하다고 주장하고 있습니다.
- **Technical Details**: 특징 분석에서 대규모 비전 모델은 텍스처보다 형태를 더 선호하는 경향이 있으며, 이는 더 단순한 특징을 추출하는 '단순성 편향(simplicity bias)' 때문입니다. 모델의 설명 가능성을 높이기 위해 특성 시각화(Feature Visualization) 기법이 발전했으며, 자동 컨셉 추출 기법 또한 많이 사용됩니다. 복잡성 측면에서는 심화 학습 모델의 함수 복잡성을 측정하는 다양한 방법이 제안되어 왔습니다. 예를 들어, 국소 복잡성을 측정하는 스플라인 분할 기법이나 비선형성 전파 점수 등이 있습니다.
- **Performance Highlights**: ImageNet 데이터셋으로 학습된 ResNet50 모델에 대해 특징 복잡성을 연구했습니다. 이 모델은 90 에폭 동안 학습되었으며, 최종적으로 78.9%의 정확도를 달성했습니다. 이를 통해 모델의 특징 복잡성을 분석하기 위한 통제된 환경을 제공했습니다. 최근의 설명 가능성 방법인 Craft를 사용하여 뉴런 수보다 많은 특징을 추출하고, 이를 통해 특징 붕괴 문제를 극복했습니다.

