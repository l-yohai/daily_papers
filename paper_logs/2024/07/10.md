## Daily Papers (2024-07-10)

### [Vision language models are blind](https://arxiv.org/abs/2407.06581)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06581.png)

Vote: 39

Authors: Mohammad Reza Taesiri, Anh Totti Nguyen, Logan Bolton, Pooyan Rahmanzadehgervi

- **What's New**: 지난 8개월 동안 GPT-4V(ision) [32]의 등장을 시작으로 많은 새로운 이미지-텍스트 처리 응용 프로그램이 가능해졌습니다. VLM은 장면에서 객체를 정확하게 식별하고, 발견된 객체를 기반으로 복잡한 작업을 수행할 수 있습니다. 그러나 기존 VLM 벤치마크는 인간과 LLM 사이의 전반적인 차이만을 측정할 뿐, 미래의 비전 연구를 위한 특정 한계를 지적하지 않습니다. 이 논문에서는 시력 테스트에서 영감을 받은 저수준 시각 작업에서 VLM의 시각 능력을 테스트합니다.
- **Technical Details**: 우리는 GPT-4o, Gemini-1.5 Pro, Claude-3 Sonnet, Claude-3.5 Sonnet 4개의 최첨단 VLM을 테스트했습니다. 벤치마크는 단순한 2D 기하학적 도형(선, 원, 사각형 등)을 포함하는 매우 간단한 8개의 시각 작업으로 구성되었습니다. 실험 결과, VLM은 마이오피아(근시)를 가진 사람처럼 세부 사항을 흐릿하게 인식하는 것으로 나타났습니다.
- **Performance Highlights**: VLM은 차트와 다이어그램 벤치마크에서 뛰어난 성능을 보였지만, 두 선이 교차하는지 여부를 신뢰할 수 없게 판단합니다. 원을 인식하는 작업에서는 73-93%의 정확도를 보였지만 여전히 100% 기대에는 미치지 못했습니다. 또한, 겹쳐진 도형을 세는 작업에서 어려움을 겪었습니다. 지하철 노선도를 추적하는 작업에서도 23-50%의 낮은 정확도를 보였습니다. GPT-4o는 복잡한 VLM 벤치마크에서는 뛰어났지만, 우리의 저수준 작업에서는 성능이 저조했습니다.

### [Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision](https://arxiv.org/abs/2407.06189)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06189.png)

Vote: 16

Authors: Idan Szpektor, Serena Yeung-Levy, Orr Zohar, Xiaohan Wang, Yonatan Bitton

- **What's New**: 이번 논문에서는 새로운 딥러닝 모델이 소개되었습니다. 이 모델은 자연어 처리(NLP) 분야에서 더 나은 성능을 발휘하도록 설계되었습니다.
- **Technical Details**: 논문에서 제안된 모델은 Transformer 구조를 기반으로 하며, Attention 메커니즘을 개선하여 더 효율적인 학습이 가능하도록 했습니다. 또한, 데이터 전처리 단계에서 BERT(Bidirectional Encoder Representations from Transformers)를 활용하여 입력 데이터의 품질을 높였습니다.
- **Performance Highlights**: 실험 결과, 제안된 모델은 여러 벤치마크 데이터셋에서 기존 모델들보다 높은 정확도와 효율성을 보였습니다. 특히, 텍스트 분류와 번역 작업에서 뛰어난 성능을 기록했습니다.

### [Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence](https://arxiv.org/abs/2407.07061)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07061.png)

Vote: 14

Authors: Weize Chen, Maosong Sun, Chen Qian, Ruobing Xie, Cheng Yang, Ran Li, Ziming You, Yitong Guan, Zhiyuan Liu, Chenyang Zhao

- **What's New**: IoA(Internet of Agents)은 인터넷의 개념을 자율 에이전트(autonomous agents)들의 협업에 적용하여, 다양한 에이전트들 간의 원활한 소통과 협력을 가능하게 하는 플랫폼입니다. 이러한 접근법은 기존의 멀티 에이전트 시스템에서 발생하는 주요 문제들을 해결하고, 에이전트들이 여러 장치에 분산되어 있더라도 효과적으로 협력할 수 있도록 지원합니다.
- **Technical Details**: IoA는 두 가지 주요 구성 요소로 이루어집니다: 서버와 클라이언트. 서버는 에이전트 등록, 검색, 메시지 라우팅 등을 관리하는 중앙 허브 역할을 하며, 클라이언트는 개별 에이전트를 래핑하여 필요한 통신 기능을 제공하고 프로토콜에 맞게 조정합니다. IoA는 상호작용 계층(Interaction Layer), 데이터 계층(Data Layer), 기초 계층(Foundation Layer)으로 이루어진 계층형 구조를 가지며, 이는 시스템의 확장성과 유연성을 높입니다.
- **Performance Highlights**: IoA는 AutoGPT와 Open Interpreter를 통합하여 개별 에이전트들과 비교했을 때 개방형 도메인 과제 평가에서 66-76%의 승률을 기록했습니다. 또한 몇 가지 간단한 ReAct 에이전트를 통합하여 GAIA 벤치마크에서 이전 작업들보다 우수한 성능을 나타냈습니다. RAG(정보 검색 기반 생성) 질문-답변 도메인에서는 GPT-3.5 기반 구현이 기존 방법을 능가하는 성과를 보여주었으며, 이는 GPT-4와 비슷하거나 더 나은 성능을 보였습니다.

### [RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models](https://arxiv.org/abs/2407.06938)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06938.png)

Vote: 13

Authors: Bowen Zhang, Yansong Tang, Dong Chen, Feng Zhao, Jiaolong Yang, Yiji Cheng, Ting Zhang, Chunyu Wang, Baining Guo

- **What's New**: 이번 연구에서는 RodinHD라는 새로운 방법을 도입하여, 3D 아바타의 높은 충실도 (high-fidelity)를 자동적으로 생성하는 기술을 소개합니다. 기존 방법들이 세부적인 디테일을 생성하는 데 어려움을 겪는 문제를 해결하기 위해, 이 연구는 새로운 데이터 스케줄링 전략인 'task replay'와 가중치 통합 규제 (weight consolidation regularization) 방법을 제안합니다.
- **Technical Details**: RodinHD는 고해상도의 트라이플레인(triplane)을 순차적으로 여러 아바타에 맞추는 과정에서 발생하는 망각 문제를 해결합니다. 제안된 'task replay' 전략은 아바타를 더 자주 교체하여 디코더가 과적합(overfitting)되지 않도록 하며, 가중치 통합 규제는 학습된 지식을 유지하면서 중요한 가중치가 멀리 벗어나지 않도록 합니다. 또한, 생성된 트라이플레인을 조건으로 3D 리프레젠테이션을 생성하는 캐스케이드 변환 모델을 훈련합니다.
- **Performance Highlights**: 4646K개의 다양한 아바타 데이터셋에서 훈련된 모델은 간단한 diffusion model만으로도 높은 해상도와 명확한 옷 등의 디테일을 포함한 고품질 아바타를 생성할 수 있습니다. 이 방법은 아바타 생성에 한정되지 않고, 다른 3D 생성 작업에도 적용될 수 있습니다.

### [Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary and Instruction Capabilities](https://arxiv.org/abs/2407.07080)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07080.png)

Vote: 12

Authors: Amir DN Cohen, Shaltiel Shmidman, Moshe Koppel, Avi Shmidman

- **What's New**: 새로운 논문에서는 히브리어에 최적화된 두 개의 생성 언어 모델인 DictaLM2.0 및 DictaLM2.0-Instruct를 소개했습니다. 이 모델들은 약 1,000억 개의 히브리어 및 영어 데이터 토큰으로 훈련되었으며, 히브리어의 특수한 요구를 충족하도록 설계되었습니다.
- **Technical Details**: 이 모델들은 Mistral 모델을 기반으로 하며, 학습 과정에서 히브리어에 특화된 토큰을 추가하고, embedding distillation을 통해 효과적인 학습을 보장합니다. 또한, DictaLM2.0-Instruct는 다양한 작업 지침에 대한 이해도를 높이기 위해 추가 데이터셋으로 미세 조정(fine-tuning)되었습니다.
- **Performance Highlights**: 제안된 모델들은 히브리어 언어 모델 평가를 위한 새로운 벤치마크 스위트에서 최첨단 성능을 보여줍니다. 여기에는 질문 응답, 감정 분석, Winograd Schema Challenge, 번역 및 요약과 같은 다양한 작업이 포함됩니다. 이를 통해 히브리어 NLP 분야에서 새로운 표준을 설정합니다.

### [AgentInstruct: Toward Generative Teaching with Agentic Flows](https://arxiv.org/abs/2407.03502)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03502.png)

Vote: 8

Authors: Fillipe Silva, Yadong Lu, Luciano Del Corro, Dany Rouhana, Hamed Khanpour, Guoqing Zheng, Ahmed Awadallah, Andres Codas, Arindam Mitra, Corby Rosset, Shweti Mahajan, Olga Vrousgos, Wei-ge Chen, Yash Lara

- **What's New**: 작년 동안 대형 언어 모델(LLMs) 훈련에 합성 데이터의 사용이 크게 증가했습니다. 특히, 미리 훈련(Pre-training), 지시 조정(Instruction-tuning) 및 강화 학습 인간 피드백(RLHF) 단계에서 합성 데이터가 모델 훈련 속도를 가속화하는 데 사용되었습니다. 그러나 고품질 합성 데이터를 생성하는 것은 어려운 작업으로, 이는 모델 붕괴나 스타일리틱 특성만을 배우는 등 여러 문제를 야기합니다. 이러한 문제를 해결하기 위해 Agentic 워크플로우가 도입되어 다양한 도구 및 반복적 피드백 메커니즘을 통해 데이터 품질을 개선할 수 있습니다.
- **Technical Details**: AgentInstruct는 '생성 교육(Generative Teaching)'이라는 새로운 접근 방식을 통해 합성 데이터를 생성합니다. 이 접근 방식은 원시 문서만을 입력으로 사용하여 다량의 도전적이고 고품질 데이터를 생성할 수 있습니다. 세 가지 흐름(플로우)은 다음과 같습니다. 첫째, '콘텐츠 변환 흐름'은 원시 데이터를 중간 표현으로 변환하여 특정 목표에 맞는 지침 생성을 단순화합니다. 둘째, '시드 지침 생성 흐름'은 변환된 시드를 다양하고 복잡한 지침으로 바꿉니다. 마지막으로, '지침 개선 흐름'은 생성된 지침의 복잡성과 품질을 단계적으로 향상시킵니다.
- **Performance Highlights**: AgentInstruct를 사용해 2500만 개의 프롬프트와 응답 쌍을 포함하는 종합적인 합성 후 훈련 데이터 세트를 생성하였습니다. 이를 Mistral-7B 모델의 미세 조정에 사용한 결과, AGIEval에서 40%, MMLU에서 19%, GSM8K에서 54%, BBH에서 38%, AlpacaEval에서 45% 개선되었으며, 여러 요약 벤치마크에서 환각 감소 31.34%를 달성했습니다. 또한, LLAMA-8B-instruct 및 GPT-3.5와 비교할 때 다수의 벤치마크에서 성능이 우수한 것으로 나타났습니다.

### [MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions](https://arxiv.org/abs/2407.06358)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06358.png)

Vote: 7

Authors: Zhaoyang Zhang, Yu Xiong, Ziyang Yuan, Ailing Zeng, Xuan Ju, Xintao Wang, Yiming Gao, Qiang Xu, Ying Shan

- **What's New**: 이번 arxiv 논문은 MiraData라는 새로운 비디오 생성 데이터셋을 소개합니다. MiraData는 향상된 모션 강도와 상세한 캡션을 특징으로 하며, 이를 통해 비디오 생성 모델의 성능을 향상시키는 것을 목표로 하고 있습니다.
- **Technical Details**: MiraData는 여러 장르의 비디오를 포함하고 있으며, GPT-4V를 사용하여 구조화된 캡션 및 자세한 설명을 자동으로 생성합니다. 또한, 신규 평가 지표인 'Temporal Consistency'와 'Motion Strength'를 도입하여 비디오 생성의 품질을 더욱 정확하게 평가합니다.
- **Performance Highlights**: MiraData를 활용한 실험에서는 DiT 기반의 비디오 생성 모델인 MiraDiT의 성능이 크게 향상됨을 확인했습니다. 이 데이터셋은 긴 비디오도 포함하고 있어, 기존 데이터셋에 비해 더 복잡하고 현실적인 비디오 콘텐츠를 다룰 수 있게 됩니다.
- **Ethical Considerations**: 비디오 생성 기술의 강화는 고품질의 가짜 비디오를 생성할 수 있는 가능성을 제공하며, 이는 허위 정보의 확산이나 프라이버시 문제 등을 야기할 수 있습니다. 또한, 저작권 침해나 윤리적 문제 등 여러 사회적 이슈들도 존재합니다. 따라서, 견고한 가이드라인과 규정, 윤리적 프레임워크가 필요합니다.
- **Limitations**: MiraData는 여전히 여러 제약이 있습니다. 데이터셋의 다양성에 한계가 있을 수 있고, 자동 주석 생성 과정에서 오류가 발생할 수 있습니다. 또한, 특정 애플리케이션에 더 적합한 데이터 특성을 가질 수 있어 다른 모델이나 아키텍처에의 일반화가 필요합니다.

### [Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps](https://arxiv.org/abs/2407.07071)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07071.png)

Vote: 7

Authors: Cheng-Yu Hsieh, Linlu Qiu, James Glass, Yoon Kim, Yung-Sung Chuang, Ranjay Krishna

- **What's New**: 새로운 연구는 대형 언어 모델(LLM)의 문맥적 환각(Contextual hallucination) 문제에 중점을 둡니다. 특히, 모델이 제공된 올바른 사실을 입력 맥락에서 얻었음에도 불구하고 정확하지 않은 출력을 생성하는 현상을 다룹니다.
- **Technical Details**: 이 연구에서는 LLM의 주의(attention) 맵을 활용하여 환각을 탐지하고 완화하는 방법을 제안합니다. 구체적으로, Lookback Ratio라는 간단한 특징을 도입하여 주의 가중치가 제공된 맥락과 새로 생성된 토큰 사이의 비율을 계산합니다. 각 시간 단계마다 이 비율을 계산한 후, Lookback Lens라는 선형 분류기를 학습시켜 문맥적 환각을 탐지합니다.
- **Performance Highlights**: Lookback Lens는 복잡한 특징 기반의 탐지기와 비슷하거나 더 나은 성능을 보이며, XSum 요약 작업에서 LLaMA-2-7B-Chat의 환각을 9.6% 줄였습니다. 또한, 주의 맵의 높은 수준의 특징을 활용하여 모델 간의 탐지기를 재학습 없이 전이할 수 있어, LLaMA2-13B-Chat 모델에서도 환각을 3.2% 감소시켰습니다.

### [Knowledge Composition using Task Vectors with Learned Anisotropic Scaling](https://arxiv.org/abs/2407.02880)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.02880.png)

Vote: 6

Authors: Cristian Rodriguez-Opazo, Frederic Z. Zhang, Anton van den Hengel, Ehsan Abbasnejad, Paul Albert

- **What's New**: 이 arXiv 논문은 새로운 기계 학습 모델 또는 알고리즘에 관한 최신 연구 결과를 발표했습니다. 이 논문에서는 기존 모델의 성능을 향상시키기 위해 새로운 접근 방식을 도입했습니다.
- **Technical Details**: 이 연구에서 제안된 모델은 Transformer 아키텍처를 기반으로 하여 Attention Mechanism을 효율적으로 활용합니다. 추가적으로, 모델의 overfitting 문제를 해결하기 위해 정규화(regularization) 기법과 데이터 증강(data augmentation) 방법을 적용하였습니다. 데이터셋은 공개된 여러 벤치마크 데이터셋을 사용하여 모델의 보편적인 성능을 검증했습니다.
- **Performance Highlights**: 새롭게 제안된 모델은 기존 비교 대상 모델들에 비해 여러 벤치마크 테스크에서 우수한 성능을 보였습니다. 예를 들어, NLP(Natural Language Processing) 태스크에서 BLEU(Bilingual Evaluation Understudy) 점수가 기존 대비 5% 향상되었으며, 이미지 인식 분야에서는 Top-1 Accuracy가 3% 증가했습니다. 이러한 성능 향상은 모델의 일반화(generalization) 능력 또한 크게 향상시켰음을 보여줍니다.

### [TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts](https://arxiv.org/abs/2407.03203)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03203.png)

Vote: 5

Authors: Shizhe Diao, Ruida Wang, Jipeng Zhang, Yizhen Jia, Tong Zhang, Rui Pan, Renjie Pi

- **What's New**: LLMs의 논리적 추론 능력을 확장하기 위해, 새로운 프레임워크 'TheoremLlama'가 제안되었습니다. 이 프레임워크는 자연어(NL)에서 포멀 랭귀지(FL)로의 변환 작업을 자동화하고, Lean4 포멀 랭귀지의 수학적 증명을 위한 데이터를 생성하고 정렬하는 방식을 포함합니다. 이를 통해 LLMs의 Lean4 증명 작성 능력을 크게 향상시키고자 합니다.
- **Technical Details**: TheoremLlama는 크게 세 가지 주요 구성 요소로 이루어져 있습니다: (a) NL-FL 정렬 데이터 생성 - Mathlib4를 변형하여 자연어로 작성된 증명과 Lean4 코드를 통합하는 Open Bootstrapped Theorems (OBT) 데이터셋 생성, (b) Lean4 증명기 교육 - Llama3-8B-Instruct를 OBT 데이터셋으로 파인튜닝하여 Lean4 전문가로 훈련, (c) 반복적 증명 작성 - 이전에 생성된 정확한 Lean4 증명을 활용하여 LLM의 포멀 추론 능력을 향상시키는 방법을 도입했습니다.
- **Performance Highlights**: TheoremLlama는 MiniF2F-Valid에서 36.48%, MiniF2F-Test에서 33.61%의 정확도를 달성하며, 이는 GPT-4의 베이스라인인 25.41%와 22.95%를 크게 상회합니다. 프레임워크의 주요 구성 요소의 효과성을 입증하기 위해 철저한 컷팅 연구도 수행되었습니다. 또한, OBT 데이터셋, 모델 체크포인트 및 코드는 곧 오픈소스로 제공될 예정입니다. 상대적으로 작은 GPU 사용량(8 GPU A6000 기계에서 약 32시간)을 요구하여, 학술 연구자들이 Lean4 증명 작업을 보다 쉽게 접근할 수 있도록 합니다.

### [From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty](https://arxiv.org/abs/2407.06071)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06071.png)

Vote: 4

Authors: Jonathan Berant, Maor Ivgi, Mor Geva, Ori Yoran

- **What's New**: 이 논문에서는 대형 언어 모델(LLM)이 불확실성에 직면했을 때 나타나는 다양한 '폴백(fallback) 행동'을 연구하였습니다. 특히 반복적인 텍스트, 변형된 텍스트, 환각(hallucinations) 등의 문제가 불확실성에 의해 발생하는 일련의 현상으로 제안되었습니다.
- **Technical Details**: LLM이 대처해야 하는 불확실성을 조정하기 위해 여러 설정을 만듭니다. 테스트 대상 모델은 Pythia, Llama 2, Llama 3, OLMo 등이며, 이들을 파라미터 수, 사전 학습 토큰 수, 명령어 학습(training), 디코딩 알고리즘의 요소별로 평가했습니다. 실험 환경으로서는 TriviaFacts, BioGeneration, Qampari, FakeQampari 데이터셋을 사용했으며, 그리즈 디코딩(greedy decoding) 메소드를 주로 사용했습니다.
- **Performance Highlights**: 모델의 강도가 증가하면서 반복적인 텍스트에서 변형된 텍스트, 환각으로 폴백 행동의 복잡성이 증가했습니다. 또한, 모델이 더 길게 응답할수록 바르게 응답하지 못할 때, 높은 확률로 다시 반복적인 텍스트로 돌아가는 현상을 관찰했습니다. 랜덤 샘플링(random sampling)은 텍스트 변형을 줄일 수 있지만, 환각이 증가하여 사용자가 감지하기 어려운 오류를 낼 가능성이 높습니다.

### [VIMI: Grounding Video Generation through Multi-modal Instruction](https://arxiv.org/abs/2407.06304)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06304.png)

Vote: 4

Authors: Kuan-Chien Wang, Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Sergey Tulyakov, Graham Neubig, Tsai-Shien Chen, Yuwei Fang

- **What's New**: 비디오 확산 모델의 최근 발전은 다양한 비디오 생성 작업에서 큰 성과를 이루었습니다. 그러나 대부분의 모델은 텍스트 전용 인코더에만 의존하며, 다양한 멀티모달 프롬프트 데이터를 활용하지 못해 시각적 입력을 효과적으로 통합하지 못하는 한계가 있었습니다. 이를 해결하기 위해 ViMi라는 다중 모달 인스트럭션 사전 학습 프레임워크를 소개합니다. 두 단계로 구성된 이 프레임워크는 텍스트와 시각적 입력을 결합한 동영상 생성을 목표로 합니다.
- **Technical Details**: ViMi의 사전 학습 과정은 크게 두 단계로 나뉩니다. 먼저, 대규모 검색 방식을 사용하여 멀티모달 프롬프트 데이터셋을 구성합니다. 이어서, 멀티모달 인스트럭션 세부 조정을 통해 모델의 다양한 입력을 처리할 수 있는 능력을 더합니다. ViMi는 확산 기반 프레임워크와 고해상도 비디오 생성에 최적화된 모델 설계를 통합합니다.
- **Performance Highlights**: ViMi는 멀티모달 사전 학습을 통해 텍스트뿐만 아니라 시각적 입력도 이해할 수 있는 능력을 갖추게 됩니다. 멀티모달 인스트럭션 세부 조정을 통해 다양한 비디오 생성 작업을 효과적으로 수행할 수 있도록 조정합니다. 그 결과 ViMi는 텍스트와 시각적 프롬프트를 결합하여 사용자가 원하는 내용을 충실히 반영한 비디오를 생성할 수 있는 강력한 모델로 자리 잡습니다.

### [BM25S: Orders of magnitude faster lexical search via eager sparse scoring](https://arxiv.org/abs/2407.03618)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03618.png)

Vote: 3

Authors: Xing Han Lù

- **What's New**: 알렉고리듬에 있어 매우 중요한 발전인 Sparse lexical search 알고리듬에 대한 성능 개선 방법을 제시했습니다. BM25의 변형인 BM25S를 소개하며 기존의 파이썬 기반 구현체보다 크게 성능을 향상시킬 수 있다는 것을 입증했습니다. 주요 혁신은 텍스트를 인덱싱할 때 미리 미래의 모든 쿼리 토큰에 대한 가능한 점수를 계산하고, 이를 Sparse matrices에 저장하는 것입니다.
- **Technical Details**: BM25S는 PyTorch 대신 Scipy의 sparse matrix 구현을 사용하며, 문서 매트릭스와의 bag-of-words 곱셈을 제거하고, 토큰 차원 전반에 걸친 합계를 통해 관련 인덱스를 슬라이싱합니다. 또한 Scikit-Learn의 텍스트 분리와 Elastic의 stopword 리스트, 선택적으로 C 기반 구현의 Snowball stemmer를 통합하는 빠른 파이썬 기반 토크나이저를 도입했습니다. 인덱싱 프로세스 동안 텍스트의 단어들을 토큰화하고, 희소 매트릭스 형식으로 TF와 IDF를 사전에 계산합니다.
- **Performance Highlights**: BM25S는 기존의 파이썬 기반 구현체보다 더욱 빠른 속도로 작동하며, 이는 대규모 데이터셋에 대해 더욱 효율적인 검색 성능을 제공합니다. 분석에 따르면, 평균 O(n) 복잡도로 k개의 가장 관련성 높은 문서를 선택함으로써 빠른 top-k 검색을 구현할 수 있습니다.

### [Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions](https://arxiv.org/abs/2407.06723)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06723.png)

Vote: 3

Authors: Yu-Guan Hsieh, Ranjay Krishna, Marco Cuturi, Pavan Kumar Anasosalu Vasu, Shih-Ying Yeh, Hadi Pour Ansari, Louis Béthune, Oncel Tuzel, Chun-Liang Li, Cheng-Yu Hsieh

- **What's New**: 이번 연구에서는 그래프 기반 캡셔닝(GBC)이라는 새로운 비전-언어 데이터 형식을 제안합니다. 이 형식은 일반 텍스트 설명의 직관성과 유연성을 유지하면서 장면 그래프와 유사한 구조를 통해 이미지를 캡셔닝합니다. 이 접근법은 이미지 전체를 설명하는 이미지 노드, 개별 객체를 설명하는 엔티티 노드, 같은 유형의 객체를 연결하는 구성 노드 및 객체 간의 공간적 ('나무가 탑의 왼쪽에 있다') 또는 의미적 ('가지에 눈이 덮여 있다') 관계를 설명하는 관계 노드의 네 가지 노드 유형을 포함합니다.
- **Technical Details**: GBC 주석을 대규모로 생성하기 위해 OSS LLaVA-1.6을 사용한 워크플로우를 설계했습니다. LLaVA는 이미지의 짧고 긴 캡션을 생성하여 엔티티를 추출하고, 객체 탐지 모델인 YOLO-World를 사용해 각 엔티티의 바운딩 박스를 찾아냅니다. 이후, 같은 절차를 반복적으로 실행하여 각 제안에 대해 GBC를 생성하고, LLaVA-1.6이 여러 엔티티 노드를 연결하는 구성 및 관계 캡션을 생성합니다.
- **Performance Highlights**: 새로 제안된 GBC 데이터 형식은 CLIP 모델의 성능을 이미지-텍스트 검색, 텍스트-이미지 검색, 구문론 및 의미적 분할 과업에서 향상시킵니다. GBC의 구조적인 특성 자체가 성능 향상에 기여하며, GBC 워크플로우를 통해 얻은 구성 및 관계 노드가 성능을 크게 향상시킨다는 점을 실험적으로 입증했습니다. 무작위 텍스트 주석에 비해, GBC 형식의 주석이 CLIP 모델에서 더 나은 성능을 보여줍니다.

### [How do you know that? Teaching Generative Language Models to Reference Answers to Biomedical Questions](https://arxiv.org/abs/2407.05015)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.05015.png)

Vote: 3

Authors: Nikola Milošević, Adela Ljajić, Darija Medvecki, Bojana Bašaragin, Lorenzo Cassano, Miloš Košprdić

- **What's New**: 이 논문에서는 PubMed 기반의 하이브리드 검색 및 미세 조정된 생성 모델을 통한 바이오메디컬 도메인에서의 참조된 질문-응답(QA)을 수행하는 RAG 시스템을 제안합니다. 본 논문은 생성 모델과 이를 미세 조정한 데 사용된 데이터를 공개합니다.
- **Technical Details**: RAG 시스템은 두 개의 주요 구성 요소로 이루어져 있습니다. 첫 번째는 하이브리드 의미 및 어휘 검색을 기반으로 한 정보 검색(IR) 구성 요소로, PubMed 초록을 검색해 생성 LLM에 문맥을 제공합니다. 두 번째는 생성 모델로 Mistral-7B를 미세 조정했습니다. 이 시스템은 BM25를 사용하여 단어 기반 검색(OpenSearch)과 조밀 벡터를 사용한 의미 기반 검색(Qdrant)을 통합하며, 결합된 검색 점수를 통해 정확한 결과를 도출합니다.
- **Performance Highlights**: 제안된 시스템은 현존하는 다른 큰 모델들(Llama 2 13B, Llama 1 34B)보다 우수한 성능을 보이며, 특히 맥락 창을 확장하고 다양한 조정을 통해 보다 정확하고 일관된 출력 결과를 제공합니다. 또한, Mistral-7B v0.1과 v0.2 두 버전을 비교 테스트하였습니다.

