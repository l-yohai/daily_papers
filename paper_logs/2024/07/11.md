## Daily Papers (2024-07-11)

### [PaliGemma: A versatile 3B VLM for transfer](https://arxiv.org/abs/2407.07726)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07726.png)

Vote: 35

Authors: Lucas Beyer, Xiao Wang, Neil Houlsby, Thomas Unterthiner, Emanuele Bugliarello, Alexey Gritsenko, Matthias Bauer, Alexander Kolesnikov, André Susano Pinto, Daniel Keysers, Keran Rong, Daniel Salz, Manoj Kumar, Fangyu Liu, Adam Grycner, Rishabh Kabra, Andreas Steiner, +, Skanda Koppula, Julian Eisenschlos, Michael Tschannen, Ibrahim Alabdulmohsin, Maxim Neumann

- **What's New**: PaliGemma는 최신 PaLI 비전-언어 모델과 Gemma 언어 모델 계열을 결합한 새로운 개방형 모델입니다. 이 모델은 400만 SigLIP 모델과 20억 개의 사전에 학습된 Gemma 모델을 합쳐서 Sub-3B VLM을 구성합니다. 목표는 다양한 비전-언어 작업에서 최첨단 성능을 달성하는 것입니다.
- **Technical Details**: PaliGemma는 이미지 인코더로 SigLIP, 언어 모델로 Gemma-2B를 사용하며, 이들을 결합하여 일련의 입력 토큰을 생성합니다. 모델의 훈련은 기존 PaLI 모델의 단계를 따르며, 다단계 훈련 프로세스를 통해 고해상도로 짧게 계속 훈련되고, 최종적으로 작업 특화 모델로 전이됩니다. 모델은 다양한 입력 이미지와 텍스트 설명을 받아 텍스트 스트링 형태로 예측을 생성합니다. 이미지와 텍스트 토큰은 Gemma의 SentencePiece 토크나이저를 사용하여 변환되고, 전체 입력에 대해 완전한 주의(attention)를 가집니다.
- **Performance Highlights**: PaliGemma는 COCO 캡션, VQAv2, InfographicVQA와 같은 표준 과제뿐만 아니라 Remote-Sensing VQA, TallyVQA 및 여러 비디오 캡션 및 질문-응답 과제에서도 뛰어난 성능을 보여줍니다. 이미지를 고정된 크기로 리사이즈하며, 다양한 변종의 모델에 대해 고정된 수의 이미지 토큰을 사용합니다. 모델의 작은 규모에도 불구하고 대규모 언어 모델을 사용하는 VLM보다 성능이 뛰어납니다.

### [LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models](https://arxiv.org/abs/2407.07895)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07895.png)

Vote: 24

Authors: Renrui Zhang, Yuanhan Zhang, Wei Li, Feng Li, Zejun Ma, Chunyuan Li, Hao Zhang, Bo Li

- **What's New**: 최근 대규모 멀티모달 모델 (Large Multimodal Models, LMMs)의 발전은 시각-언어 데이터의 통합을 통해 인공지능 일반 지능(AGI)을 향한 중요한 진전을 이루어냈습니다. 하지만 대부분의 오픈소스 LMM들은 단일 이미지 시나리오의 성능을 개선하는 데 중점을 두었으며, 다중 이미지 시나리오에 대해서는 덜 탐구되었습니다. 본 논문에서는 다양한 실제 응용 프로그램에서 다중 이미지, 동영상, 3D 시나리오 등을 처리할 수 있는 LLaVA-NeXT-Interleave라는 새로운 LMM을 소개합니다.
- **Technical Details**: LLaVA-NeXT-Interleave 모델은 이미지-텍스트 인터리브(Interleave) 형식을 일반 데이터 템플릿으로 사용하여 다양한 시나리오를 통합합니다. 이를 통해 단일 이미지, 다중 이미지, 동영상(다중 프레임), 3D(다중 뷰) 등 다양한 시나리오들을 통합할 수 있게 됩니다. 'M4'로 명명된 네 가지 설정(다중 이미지, 다중 프레임, 다중 뷰, 단일 이미지)을 통해 모델의 능력을 확장합니다.
- **Performance Highlights**: 기존의 특정 과제 전용 모델에 비해 LLaVA-NeXT-Interleave는 단일 모델로 여러 다중 이미지 과제에서 선도적인 결과를 달성하며, 단일 이미지 성능도 유지합니다. 또한, 다양한 과제를 공동으로 학습함으로써 새로운 멀티 이미지 컨텍스트에서의 제로샷 태스크 구성 능력을 보여줍니다.
- **What's New - Related Work**: 이전 연구들은 주로 특정 과제에 중점을 두었으며, 특히 상호교차 학습(ICL) 능력 및 실제 다중 이미지 응용 시나리오에서의 지시 수행 능력 향상에 기여했습니다. 그러나 실제 다중 이미지 응용에서 명령 조정(instruction-tuning)을 사용하는 것은 덜 탐구된 반면, LLaVA-NeXT-Interleave는 다양한 시나리오에 대한 일반화를 통해 이를 개선했습니다.
- **Technical Details - Related Work**: LLaVA-NeXT-Interleave는 다양한 시나리오를 테스트하기 위해 9개의 신규 데이터셋과 13개의 기존 데이터셋을 포함한 LLaVA-Interleave Bench를 구성하여 평가합니다. 또한 다양한 공공 데이터셋을 활용하여 대규모 멀티이미지 연구에 기초를 마련했습니다.
- **Emerging Capabilities**: LLaVA-NeXT-Interleave는 다양한 태스크를 공동 훈련함으로써 새로운 미지의 멀티 이미지 컨텍스트에서 태스크를 구성하는 제로샷 능력을 보여줍니다. 이는 이미지 간 차이점을 찾는 것에서부터 동영상 분석에 이르기까지 다양한 설정과 모달리티에서의 태스크 전이를 가능케 합니다.

### [Controlling Space and Time with Diffusion Models](https://arxiv.org/abs/2407.07860)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07860.png)

Vote: 9

Authors: Saurabh Saxena, Lala Li, Daniel Watson, David J. Fleet, Andrea Tagliasacchi

- **What's New**: 이번 연구에서는 새로운 시점 합성(Novel View Synthesis, NVS)과 3D 생성 모델에 대해 소개하고 있습니다. 3DiM과 Zero-1-to-3와 같은 기존 모델들보다 더 진보된 기능을 제공하는 4DiM 모델을 제안합니다. 이 모델은 객체가 아닌 장면에 적용될 수 있으며, 자유로운 카메라 포즈와 시점 조절을 실현합니다.
- **Technical Details**: 4DiM은 다중 시점 합성을 위한 연속 시간 확산 모델로, 여러 뷰 사이의 공동 분포를 학습합니다. 학습 데이터는 실내외 장면의 위치지정 3D 이미지/비디오와 비위치 지정 비디오로 구성되어 있으며, 무포즈 데이터 및 시간 주석이 없는 데이터로부터 학습이 가능합니다. 새로운 기술적 요소를 도입하여 임의의 시각 및 포즈로 데이터를 생성할 수 있는 모델을 개발하였습니다.
- **Performance Highlights**: 4DiM 모델은 기존의 COLMAP을 사용한 데이터와 비교하여 메트릭 수준의 정확도로 포즈를 제어할 수 있습니다. 이는 실내외 장면의 다양한 동적 장면을 3D 일관되게 생성할 수 있게 합니다. 또한, 영상 간 번역, 파노라마 스티칭, 점수 증류 샘플링(SDS) 등을 가능하게 합니다.

### [Video-to-Audio Generation with Hidden Alignment](https://arxiv.org/abs/2407.07464)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07464.png)

Vote: 7

Authors: Yu Gu, Yong Ren, Chenxing Li, Wei Liang, Rilin Chen, Dong Yu, Manjie Xu

- **What's New**: 본 연구는 기존의 영상 생성 모델이 주로 텍스트 프롬프트를 기반으로 시각적 콘텐츠를 생성하는 데 집중하는 반면, 음성 정보를 통합하지 못해 현실감에서 떨어진다는 문제를 해결하고자 한다. 이에 따라, 무음 비디오에 대해 의미적 및 시간적으로 정렬된 오디오 콘텐츠를 생성하는 것을 목표로 한다.
- **Technical Details**: 주요 기술적 세부 사항은 세 가지 요소에 중점을 둔다: 비전 인코더(vision encoder), 보조 임베딩(auxiliary embedding), 데이터 증강(data augmentation). 비전 인코더는 비디오에서 복잡한 시각적 패턴을 추출하고 해석하며, 보조 임베딩은 모델에 추가적인 맥락 정보를 제공한다. 데이터 증강은 다양한 변형을 도입하여 모델의 일반화 능력을 향상시킨다. 특히, 이번 연구에서는 클립 기반 비전 인코더를 사용하여 비디오 프레임 수준의 특징을 시간적으로 이음새 없이 연결하고, 생성 조건으로 매핑한다.
- **Performance Highlights**: 기초적인 VTA-LDM 모델은 TTA 작업에서 최첨단(SOTA) 성과를 달성했으며, 시각적 입력을 기반으로 의미적 및 부분적으로 시간적으로 정렬된 오디오 콘텐츠를 생성한다. 또한, 추가적 기능을 통합함으로써 생성 품질과 오디오 및 시각 요소 간의 동기화를 크게 향상시킬 수 있음을 확인했다. 이 연구는 앞으로의 VTA 분야 연구에 탄탄한 기초를 제공한다.

### [VEnhancer: Generative Space-Time Enhancement for Video Generation](https://arxiv.org/abs/2407.07667)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07667.png)

Vote: 6

Authors: Yu Qiao, Tianfan Xue, Dongyang Liu, Ziwei Liu, Wanli Ouyang, Jingwen He, Xinqi Lin, Dahua Lin, Peng Gao

- **What's New**: 최근 VEnhancer를 소개합니다. 이는 통합된 생성 공간-시간(super-resolution)을 제공하는 프레임워크로, 상업적 텍스트-비디오(text-to-video, T2V) 모델 생성을 가능하게 합니다. VEnhancer는 공간 및 시간 해상도를 지원하며, 영상 아티팩트(artifacts)와 깜빡임(flickering)을 제거하며 일관성을 유지합니다.
- **Technical Details**: VEnhancer는 사전 훈련된 동영상 확산 모델(video diffusion model)을 기반으로 구축되었습니다. 이를 통해 세밀한 복원이 가능하며, 다양한 다운샘플링 요소 및 노이즈 수준을 사용한 데이터 증강(data augmentation)을 통해 훈련 데이터를 구성합니다. 또한, VEnhancer는 ControlNet의 영감을 받아 멀티 프레임 조건 삽입을 위해 비디오 ControlNet을 설계하였습니다.
- **Performance Highlights**: VEnhancer는 기존의 최첨단 영상 초해상도(video super-resolution) 방법들을 능가하여 생성 동영상 향상에서 최고의 성능을 달성하였습니다. 특히, VideoCrafter-2와 결합하여 VBench 벤치마크에서 1위를 차지하며, 품질과 의미론적 만족도에서 최고 성능을 나타냈습니다.

### [Inference Performance Optimization for Large Language Models on CPUs](https://arxiv.org/abs/2407.07304)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07304.png)

Vote: 5

Authors: Duyi Wang, Changqing Li, Chen Meng, Bin Guo, Weifei Yu, Sheng Gui, Pujiang He, Shan Zhou, Wenhuan Huang, Yi Xie

- **What's New**: 대규모 언어 모델(LLMs)의 실제 배포를 위한 새로운 최적화 솔루션을 제안합니다. 이 솔루션은 주로 CPU에서의 효과적인 배포 및 추론 성능 최적화를 중심으로 합니다. 제안된 솔루션은 Qwen, Llama, ChatGLM, Baichuan, Opt 시리즈와 같은 널리 사용되는 LLM을 지원하며, KV 캐시 크기를 줄이고 정확성을 보장하는 방법도 구현합니다.
- **Technical Details**: LLM의 개별 운영 및 계층에 대한 최적화 솔루션을 제공합니다. SlimAttention이라는 새로운 접근 방식을 도입하여 메모리 사용량을 줄이고 계산 효율성을 높입니다. 또한, 기존의 FlashAttention과 비교시 슬림어텐션은 중간 버퍼의 크기가 커야하지만 불필요한 계산이 없습니다. INT8 KV 캐시 접근 방식을 채택하여 메모리 효율성을 극대화하며 Fused Multiply-Add(FMA) 명령어를 활용한 커스텀 커널을 설계했습니다. 분산 추론 최적화를 위해 oneAPI Collective Communications Library(oneCCL)를 활용하여 데이터 복사를 줄이는 공격적인 최적화 접근 방식도 제안했습니다.
- **Performance Highlights**: Intel® Xeon® CPU 8563C에서 실험을 진행한 결과, 제안된 분산 솔루션이 4대의 베어 메탈 머신(8 소켓)에서 Llama2-70B 모델의 성능을 단일 베어 메탈 머신(2 소켓)과 비교해 2.85배 향상시킵니다. SlimAttention 접근 방식이 FlashAttention보다 더 나은 성능을 보임을 확인했습니다. 각 연구 및 실험 결과는 Llama2-7B 모델 구성에서 첫 번째 토큰을 생성하는 동안 평균 소요 시간(ms)을 보여줍니다.

### [On Leakage of Code Generation Evaluation Datasets](https://arxiv.org/abs/2407.07565)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07565.png)

Vote: 3

Authors: Elena Tommasone, Raymond Ma, Alexandre Matton, Tom Sherborne, Maxime Voisin, Matthias Gallé, Dennis Aumiller, Milad Alizadeh, Jingyi He, Ellen Gilsenan-McMahon

- **What's New**: 본 논문에서는 기존 코드 생성 벤치마크 데이터셋의 오염 문제를 제기하고, 이러한 오염이 모델의 일반화 능력 평가를 왜곡시키는지에 대해 논의합니다. HumanEval(Chen et al., 2021)과 MBPP(Austin et al., 2021)은 현재까지 코드 생성 능력을 평가하는 주요 벤치마크로 사용되고 있지만, 데이터셋의 공공 복제본이 확산되면서 데이터 누출 및 오염 문제가 발생하고 있습니다.
- **Technical Details**: 논문에서는 코드 생성 모델이 훈련 중에 HumanEval과 MBPP 데이터셋을 포함한 경우, 벤치마크 점수가 일반화 능력을 나타내지 못하게 된다고 주장합니다. 본래 평가 범위를 벗어난 데이터 오염이 문제입니다. 이를 해결하기 위해, 새로운 벤치마크인 Less Basic Python Problems(LBPP)을 제안합니다. LBPP는 기존 벤치마크와 유사하지만 더 어렵고, 현재 코드 훈련 데이터에 누출되지 않도록 설계되었습니다.
- **Performance Highlights**: 논문에 따르면 데이터 오염이 모델 성능에 미치는 영향을 측정한 결과, HumanEval 및 MBPP와 유사한 스타일이지만 독립적으로 유지된 LBPP을 통해 현 코드 생성 능력을 측정할 수 있습니다. 데이터 오염이 존재하는 경우 성능이 과도하게 높게 측정될 수 있다고 밝혔습니다. 새로운 벤치마크 LBPP 도입으로 모델의 실제 일반화 능력을 더 정확하게 평가할 수 있게 되었습니다.

### [CosmoCLIP: Generalizing Large Vision-Language Models for Astronomical Imaging](https://arxiv.org/abs/2407.07315)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07315.png)

Vote: 3

Authors: Raza Imam, Fakhri Karray, Mohammed Talha Alam, Mohsen Guizani, Umaima Rahman

- **What's New**: 우주 관측 기술의 발전과 대규모 하늘 조사(Sky Survey)의 데이터 폭증 시대에 대응하기 위해, 이 논문은 비전 언어 모델(Vision Language Model, VLM)을 천문학 응용 분야에 적용하고자 하는 새로운 접근 방식을 제안합니다. 특히, CLIP 모델을 적용하여 천문학 데이터의 해석과 분류를 혁신하려고 합니다.
- **Technical Details**: ['우리가 제안하는 CosmoCLIP 모델은 세 가지 주요 구성 요소로 구성됩니다: (1) 비전 및 텍스트 인코더, (2) 지식 추출, 및 (3) 컨텍스트 유사도 훈련.', '비전 인코더(visual encoder)는 시각적 입력을 고정 길이 임베딩으로 매핑하고, 텍스트 인코더(text encoder)는 텍스트 입력을 처리하여 임베딩을 생성합니다.', '지식 추출(knowledge extraction)은 BLIP와 같은 대규모 캡션 모델을 사용하여 이미지 텍스트 쌍을 생성하고, 이를 통해 컨텍스트 유사도 행렬(context similarity matrix)을 구성합니다.', '컨텍스트 유사도 훈련(context similarity training)은 이미지와 텍스트 임베딩을 공유 공간으로 정렬하여 상호 이해를 증진시키며, 이 과정에서 교차 엔트로피 손실(cross-entropy loss)을 계산합니다.']
- **Performance Highlights**: ['제안된 CosmoCLIP 모델은 CLIP 모델을 천문학 데이터셋에 맞춰 미세 조정하게 되면, 이미지와 텍스트 임베딩 사이의 관련성을 더 정확하게 구축할 수 있음을 보였습니다.', '각종 실험 결과에서 CosmoCLIP은 갤럭시 모폴로지 분류, 이미지 복원, 객체 탐지, 파라미터 추출 등 다양한 작업에서 우수한 성능을 보였습니다.']

### [This&That: Language-Gesture Controlled Video Generation for Robot Planning](https://arxiv.org/abs/2407.05530)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.05530.png)

Vote: 2

Authors: Adam Fishman, Nikhil Sridhar, Nima Fazeli, Chao Feng, Jeong Joon Park, Mark Van der Merwe, Boyang Wang

- **What's New**: 새로운 연구는 로봇의 제어를 간단한 언어-제스처 명령으로 수행할 수 있는 This&That 프레임워크를 제안합니다. 이 프레임워크는 사용자의 의도를 잘 반영하는 비디오 생성을 통해 로봇이 다양한 작업을 수행할 수 있도록 합니다.
- **Technical Details**: This&That는 언어-제스처 조건부의 비디오 생성 모듈과 비디오 기반 로봇 실행 모듈로 구성됩니다. 비디오 생성을 위해 대규모, 다범주 텍스트-비디오 Diffusion 모델을 기반으로 하며, 이를 로봇의 설정에 맞게 세부 튜닝합니다. 영상 Diffusion 모델(VDM)은 작업을 설명하는 두 가지 조건들, 즉 디아텍틱 언어(예: 'this', 'there')와 첫 프레임 이미지 내에서 해당 제스처를 나타내는 2D 위치에 따라 구동됩니다.
- **Performance Highlights**: 실험 결과, This&That 프레임워크는 사용자 의도를 잘 반영하는 고품질 비디오를 생성하며, 기존 연구보다 뛰어난 성능을 보였습니다. 시뮬레이션 환경에서의 행동 클로닝(behavioral cloning) 실험에서도 다수의 작업 정책 학습에 유리한 조건부 비디오 예측의 효과가 입증되었습니다.

### [An accurate detection is not all you need to combat label noise in web-noisy datasets](https://arxiv.org/abs/2407.05528)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.05528.png)

Vote: 2

Authors: Eric Arazo, Noel E. O'Connor, Paul Albert, Jack Valmadre, Kevin McGuinness, Tarun Krishna

- **What's New**: 이 논문은 웹 잡음이 포함된 데이터셋(web-noisy datasets)에서 이미지 분류를 위한 노이즈 검출 방법을 제안합니다. 기존의 SNCF 연구를 확장하여 ID(in-distribution)와 OOD(out-of-distribution) 이미지 간의 선형 분리를 통해 노이즈를 탐지하는 전략을 사용합니다. 웹 잡음이 있는 데이터셋에서는 기존의 synthetically corrupted 데이터셋에 비해 분리가 더 어려운 것으로 나타났으나, 네트워크의 중간 표현(intermediate representations)에서는 분리가 가능함을 발견했습니다.
- **Technical Details**: 본 연구에서는 선형 분리(linear separation)를 사용하여 웹 잡음 데이터셋에서 OOD 샘플을 정확히 검출하고 이를 통한 로그 회귀(logistic regression)를 통해 웹 잡음을 탐지합니다. 기존의 클러스터링 기반 접근법을 대신하여 직접 선형 분리를 추정합니다. 이후 SOTA(state-of-the-art) 노이즈 강인 알고리즘을 사용하여 불완전한 깨끗한/잡음 샘플 검출을 수행하고, 이를 기반으로 노이즈 강인한 알고리즘 PLS와 결합하여 PLS-LSA 알고리즘을 제안합니다.
- **Performance Highlights**: 새로운 노이즈 탐지 방법(LSA)은 여러 이미지 분류 작업에서 기존 노이즈 강인 알고리즘보다 뛰어난 성능을 보였습니다. 특히 PLS-LSA+는 투표 공동 학습 전략(voting co-training strategy)을 도입하여 두 모델을 동시에 훈련 시켜 그 효능을 입증했습니다. 실사용 웹 잡음 데이터셋과 통제된 환경에서 실험한 결과, 제안된 알고리즘의 효과를 입증했습니다.

### [Do Vision and Language Models Share Concepts? A Vector Space Alignment Study](https://arxiv.org/abs/2302.06555)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2302.06555.png)

Vote: 1

Authors: Anders Søgaard, Jiaang Li, Constanza Fierro, Yova Kementchedjhieva

- **What's New**: 이번 연구는 언어 모델(LMs)과 컴퓨터 비전 모델(VMs)의 학습 대상을 비교하고, 두 모델이 얼마나 유사한지를 측정합니다. 기존에는 LMs가 의미를 이해하지 못한다고 주장하는 연구가 많았으나, 본 연구는 LMs와 VMs가 실제로 세계를 모델링하고 있다는 증거를 제시합니다. 구체적으로, 더 성능이 좋은 LMs일수록 VMs의 표현과 유사한 구조적 특징을 나타낸다는 점을 발견했습니다.
- **Technical Details**: 연구에는 SegFormer, MAE, ResNet 등 세 가지 종류의 컴퓨터 비전 모델과 BERT, GPT-2, OPT, LLaMA-2 등 네 가지 종류의 언어 모델이 포함됩니다. 각각 14개의 VMs와 14개의 LMs의 벡터 공간을 분석하여, 이미지와 언어의 표현 유사성을 평가했습니다. 특히, SegFormer 모델은 객체 분류와 장면 분할에 특화되어 있고, MAE 모델은 Transformer 기반의 인코더-디코더 구조로 마스크된 패치를 재구성하는 방식으로 학습됩니다.
- **Performance Highlights**: LMs가 더 커질수록 (BERTBase -> BERTLarge) 컴퓨터 비전 모델과의 유사도가 증가하는 경향을 발견했습니다. 이 유사성 덕분에, 소량의 병렬 예제를 통해 VMs의 표현을 언어 공간으로 선형 변환함으로써 높은 정확도의 캡션을 얻을 수 있었습니다. 이미지와 언어의 분산, 다의성, 빈도에 따라 검색 정확도가 달라지지만, LMs의 크기가 커질수록 일관되게 향상되었습니다.

### [BiGym: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark](https://arxiv.org/abs/2407.07788)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07788.png)

Vote: 1

Authors: Younggyo Seo, Xiao Ma, Yunfan Lu, Nikita Chernyadev, Stephen James, Nicholas Backshall

- **What's New**: BiGym은 휴머노이드 로봇을 위한 데모 중심의 모바일 양손 조작 벤치마크로, 총 40개의 시각적 모바일 조작 과제를 다룹니다. 이 벤치마크는 RLBench와 달리, 인간이 수집한 좀 더 현실적인 데모를 제공합니다. 이 데모들은 실제 로봇 데이터를 더 잘 반영하며, 전신 모드와 양손 모드를 전환할 수 있는 기능을 제공합니다. 이를 통해 알고리즘의 이동 제어와 모바일 양손 조작 능력을 보다 명확하게 평가할 수 있습니다.
- **Technical Details**: BiGym은 부분적으로 관측 가능한 마르코프 결정 프로세스 (POMDP)로서, 시각적 관찰과 로봇의 저수준 상태를 포함한 하이브리드 관찰, 연속적인 행동, 그리고 희소한 보상 등을 제공합니다. 에이전트는 과거의 부분 관찰과 행동을 기반으로 환경 상태의 분포인 믿음 (belief)을 학습해야 합니다. 또한 복잡한 과제 공간과 긴 시간 범위의 희소한 보상을 특징으로 합니다. 이를 통해 복잡한 양손 조작 및 이동성 과제를 해결할 수 있는지 평가합니다.
- **Performance Highlights**: BiGym은 40개의 다양한 모바일 양손 조작 과제를 제공하며, 인간이 수집한 다중 모달리티 데모를 기반으로 현실적인 평가를 가능하게 합니다. 이러한 데모들은 플래너가 생성한 데모보다 더 넓은 데이터 분포를 다루며, 현실 세계의 알고리즘 성능을 보다 잘 반영합니다. 또한 전신 모드와 양손 모드를 전환하는 기능을 통해, 다양한 알고리즘의 역량을 보다 세분화된 측면에서 평가할 수 있습니다.

