## Daily Papers (2024-07-18)

### [Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models](https://arxiv.org/abs/2407.12327)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12327.png)

Vote: 50

Authors: Irina Rish, Tejas Pandey, Tejas Vaidhya, Ayush Kaushal, Aaryan Bhagat

- **What's New**: 이 논문에서는 새로운 AI (인공지능) 모델을 소개합니다. 이 모델은 데이터 처리 능력과 학습 속도에서 혁신적인 개선을 이루었으며, 여러 분야에 걸쳐 적용할 수 있습니다. 새로운 알고리즘과 최적화 기술을 도입하여 효율성을 높였습니다.
- **Technical Details**: 새로운 모델은 Transformer 아키텍처를 기반으로 하여, Self-Attention 메커니즘을 통해 문맥(Context) 이해도를 높였습니다. 또한, 대규모 데이터 세트(Benchmark datasets)에서의 성능을 개선하기 위해 Hyperparameter 튜닝 및 Gradient clipping 기법을 사용하였습니다.
- **Performance Highlights**: 실험 결과, 새로운 모델은 기존 모델들과 비교했을 때 여러 기준에서 성능이 뛰어났습니다. 특히, 자연어 처리(Natural Language Processing)와 컴퓨터 비전(Computer Vision) 분야에서 유의미한 성능 향상을 보였습니다. 정밀도(Precision)와 재현율(Recall) 지표에서 최고 점수를 기록하였습니다.

### [AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases](https://arxiv.org/abs/2407.12784)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12784.png)

Vote: 35

Authors: Dawn Song, Chaowei Xiao, Zhen Xiang, Zhaorun Chen, Bo Li

- **What's New**: 최근 대형 언어 모델(LLM)의 발전으로 인해 금융, 의료, 자율 주행과 같은 안전 필수 애플리케이션을 포함한 다양한 응용 분야에서 LLM 에이전트의 광범위한 배포가 가능해졌습니다. 그러나 이러한 모델들이 신뢰성을 보장하는 데는 한계가 있으며, 정확하지 않은 지식 기반을 통합하는 경우 신뢰성 문제가 크게 대두됩니다. 이에 본 논문에서는 AgentPoison이라는 새로운 백도어 공격 기법을 제안하여 기억 모듈 또는 회귀 증강 생성(RAG) 지식 기반을 악성 시연으로 오염시키는 공격을 수행합니다.
- **Technical Details**: AgentPoison은 희생 에이전트의 장기 기억 또는 지식 기반을 소수의 악성 시연으로 오염시킴으로써 작동합니다. 각 시연에는 유효한 쿼리, 최적화된 트리거 및 일부 처방된 적대적 목표가 포함됩니다. 트리거 최적화는 악성 시연의 회수와 유도되었을 때 공격의 효과성을 최대화하는 목표를 갖고 있으며, 이는 RAG 임베딩 공간 내에서 악성 인스턴스를 고유한 영역에 매핑하도록 설계되었습니다.
- **Performance Highlights**: AgentPoison은 자동 운전, 대화, 의료와 같은 세 가지 유형의 LLM 에이전트에서 테스트되었으며, 82%의 회수 성공률과 63%의 공격 성공률을 달성했습니다. 이 과정에서 정상적인 성능 저하는 1% 미만이며, 오염 비율은 0.1% 미만입니다. 또한, 트리거는 다양한 RAG 임베더 간의 효과적인 공격으로 전이되며, 다양한 교란에도 강인함을 입증했습니다.

### [GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression](https://arxiv.org/abs/2407.12077)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12077.png)

Vote: 23

Authors: Fares Obeid, Daniel Goldstein, Eric Alcaide, Guangyu Song, Eugene Cheah

- **What's New**: 최근 연구에서 전통적인 다중 헤드 스케일드 닷 프로덕트 어텐션(MHA)의 성능에 근접하면서도 추론 비용을 줄이는 다양한 선형 어텐션 변형들이 제안되었습니다. 본 논문에서는 새로운 GoldFinch 아키텍처를 도입하여 고성능의 초장문(Large Language Model) 언어 모델에서 메모리 비용을 줄이고 초기 컨텍스트 처리 속도를 향상시킵니다.
- **Technical Details**: GoldFinch 아키텍처는 여러 혁신적인 기법들을 결합합니다. 특히, Finch-C2와 'TokenCat'이라는 새로운 메커니즘을 통해 글로벌 키 캐시 크기를 획기적으로 줄이며, GPTAlpha라는 새로운 트랜스포머 아키텍처를 사용하여 값(Value) 캐시 없이 키(Key) 캐시만으로도 높은 성능을 유지할 수 있습니다. 이 새로운 모델은 Finch-C2 레이어를 통해 생성된 결과를 활용하며, 'Low Rank Adaptation(LoRA)'를 통해 키 캐시를 압축하고 이를 다시 확장하여 토큰 임베딩과 연결합니다.
- **Performance Highlights**: GoldFinch 모델은 1.45억 파라미터로 1.5조 토큰을 학습한 결과, Finch 및 Llama 모델보다 나은 성능을 보였으며, 파라미터 수는 더 적고 캐시 크기는 훨씬 작습니다. Llama 모델 대비 캐시 크기를 모든 레이어 수 만큼 줄이고, 값 캐시를 제거하여 키 캐시만으로 메모리 효율을 극대화했습니다. 또한, 성능은 유지하면서 계산 복잡성을 획기적으로 줄였습니다.

### [LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models](https://arxiv.org/abs/2407.12772)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12772.png)

Vote: 21

Authors: Kaichen Zhang, Yuanhan Zhang, Ziwei Liu, Joshua Adrian Cahyono, Fanyi Pu, Shuai Liu, Chunyuan Li, Jingkang Yang, Bo Li, Kairui Hu, Peiyuan Zhang

- **What's New**: 최근 GPT-4, Gemini, Claude와 같은 대형 기초 모델들이 인간 수준의 성능을 보이며, 모델간의 차이를 구별하고 약점을 진단하는 것이 중요한 과제로 떠올랐습니다. 이를 위해 LMMs-Eval이라는 통합 평가 프레임워크를 도입하여 50여 가지의 다양한 과제를 포함한 더 많은 멀티모달 모델을 체계적으로 평가합니다.
- **Technical Details**: 기존의 평가 방식은 모델별, 데이터셋별로 큰 오버헤드를 초래하며 비표준화된 문제를 가지고 있습니다. LMMs-Eval은 lm-eval-harness의 프레임워크 디자인을 따라 여러 모델과 데이터셋을 단일 명령으로 평가할 수 있도록 하여 표준화된 평가를 제공합니다. 또한 커스텀 평가 스크립트가 초래하는 스코어의 비교 불가능성을 해결하기 위해, 통합된 평가 프레임워크를 설계했습니다.
- **Performance Highlights**: LMMs-Eval은 다양한 측면에서 모델의 성능을 종합적으로 비교할 수 있게 하여 특정 과제에 훌륭한 성능을 보이는 모델인지, 전반적으로 뛰어난 모델인지를 파악할 수 있게 합니다. 또한, 대규모 평가를 통해 모델 아키텍처와 훈련 데이터에 대한 이해를 높이는 데 기여합니다. LMMs-Eval Lite는 평가 비용을 줄이면서도 신뢰성 있는 결과를 제공하며, LiveBench는 최신 정보와 온라인 포럼에서 수집된 데이터를 바탕으로 모델의 제로샷 일반화 능력을 평가합니다.

### [E5-V: Universal Embeddings with Multimodal Large Language Models](https://arxiv.org/abs/2407.12580)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12580.png)

Vote: 18

Authors: Fuzhen Zhuang, Weiwei Deng, Minghui Song, Ting Jiang, Zihan Zhang, Qi Zhang, Haizhen Huang, Feng Sun, Deqing Wang

- **What's New**: 최신 연구에서는 MLLMs(Multi-Lingual Large Language Models)를 활용한 다중모달 임베딩(multimodal embeddings) 방법을 제안합니다. 이 프레임워크는 기존의 CLIP와 같은 모델 대신 MLLMs를 직접적으로 적응시킴으로써, 보다 유니버설한(multiversal) 다중모달 표현을 달성할 수 있습니다. 이를 통해 복잡한 텍스트 이해와 추론 능력이 향상되며, 단일 모달리티(single modality) 인풋으로도 효과적인 다중모달 임베딩 수행이 가능합니다.
- **Technical Details**: 제안된 프레임워크, E5-V는 프롬프트(prompt) 기반의 표현 방법을 사용하여 MLLMs를 다중모달 임베딩에 적응시킵니다. 이 방법은 텍스트 데이터 쌍만을 사용한 단일 모달리티 훈련으로 다중모달 임베딩을 통합하여 비용을 절감할 뿐만 아니라, 이미지 텍스트 쌍보다 더 효과적인 표현을 가능하게 합니다. 이는 MLLM을 시각 인코더(visual encoder) 없이 텍스트 쌍만으로 훈련할 수 있도록 하여, 훈련 비용을 크게 감소시킵니다.
- **Performance Highlights**: 다양한 작업(text-image retrieval, composed image retrieval, sentence embeddings, image-image retrieval)에서 E5-V의 효과를 평가한 결과, text-image retrieval에서의 경쟁력 있는 성능을 입증했습니다. E5-V는 프롬프트 기반의 접근을 통해 훈련 데이터에 포함되지 않은 작업별 프롬프트를 따를 수 있도록 하였으며, 이를 통해 단일 모달리티 표현 기능을 다중모달 임베딩으로 성공적으로 전환했습니다.

### [Patch-Level Training for Large Language Models](https://arxiv.org/abs/2407.12665)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12665.png)

Vote: 10

Authors: Jie Zhou, Fandong Meng, Chenze Shao

- **What's New**: 본 논문에서는 LLM(Large Language Models)의 훈련 효율을 개선하기 위해 새로운 방법론인 patch-level training을 도입했습니다. 이 접근법은 다수의 토큰을 하나의 패치로 압축해 시퀀스 길이를 줄이는 것을 핵심으로 하며, 최종 모델이 패치 단위로 작동할 필요가 없습니다. 패치 레벨 훈련 단계에서 모델을 더 짧은 시퀀스로 훈련시키고, 이를 통해 얻은 지식을 토큰 레벨 모델로 이전하는 두 단계의 접근법을 제안합니다.
- **Technical Details**: 패치 레벨 훈련은 K개의 연속 토큰을 하나의 패치로 압축하며 시작됩니다. 이 패치 시퀀스는 모델에 입력되고, 모델은 다음 패치의 토큰을 예측하도록 훈련됩니다. 패치 레벨 훈련 중 얻어진 모델 파라미터는 토큰 레벨 모델을 초기화하는 데 사용됩니다. 패치 레벨와 토큰 레벨 모델 사이의 차이를 최소화하기 위해, 문맥 길이 T 및 패치 시퀀스를 설정합니다. 패치 임베딩은 토큰 임베딩의 평균값으로 표현되며, 다음 패치 예측을 통해 훈련이 진행됩니다.
- **Performance Highlights**: 설정 값(K=4, λ=2/3)으로 다양한 크기의 LLM들을 Pile 데이터셋에서 훈련했습니다. 초기화 후, 모델은 토큰 레벨 훈련을 통해 손실이 빠르게 감소하며 훈련 비용이 절반으로 줄었습니다. 하이퍼파라미터 설정을 추가로 조정하여 더 높은 가속률을 달성할 수 있었으며, 모델 성능 저하는 최소화되었습니다.

### [VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control](https://arxiv.org/abs/2407.12781)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12781.png)

Vote: 8

Authors: Chaoyang Wang, Willi Menapace, Guocheng Qian, Aliaksandr Siarohin, Michael Vasilkovsky, Hsin-Ying Lee, Andrea Tagliasacchi, David B. Lindell, Ivan Skorokhodov, Jiaxu Zou, Sherwin Bahmani, Sergey Tulyakov

- **What's New**: 최근 text-to-video 모델들은 전례없는 시각적 품질을 달성했지만, 3D 제어 기능이 부족해 사용자는 느리고 번거로운 프롬프트 엔지니어링으로 결과를 조작해야 했습니다. 이 논문에서는 대형 비디오 변환기 모델을 위한 카메라 제어 방법을 제안하고, SnapVideo 모델을 기반으로 한 새로운 스키마를 개발했습니다.
- **Technical Details**: 기존 방법들은 U-Net 기반 구조에 의존하여 카메라 제어를 구현했으나, 최근의 transformer-based 모델인 Sora, SnapVideo, Lumina-T2X에서는 적용이 어렵습니다. 우리는 Plucker 좌표와 cross-attention 레이어를 사용하여 spatiotemporal 카메라 임베딩을 구현했습니다. 또한, ControlNet 스타일의 조건부 메커니즘을 적용하여 이 문제를 해결했습니다.
- **Performance Highlights**: 제안된 방법은 기존 방법들과 비교할 때 비디오 품질과 카메라 제어 측면에서 최첨단 성능을 보여줍니다. 특히, 복잡한 환경에서 multi-view, text-to-video 생성과 같은 다운스트림 응용 프로그램에서도 효과적으로 작동합니다. 우리의 접근 방식은 기존의 object-centric 생성과 다르게 실제 입력 이미지에 대한 새로운 뷰를 합성할 수 있습니다.

### [Case2Code: Learning Inductive Reasoning with Synthetic Data](https://arxiv.org/abs/2407.12504)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12504.png)

Vote: 5

Authors: Hang Yan, Xiaonan Li, Yichuan Ma, Xuanjing Huang, Qinyuan Cheng, Pengyu Wang, Demin Song, Qipeng Guo, Dahua Lin, Shimin Li, Yunfan Shao, Peiji Li, Xipeng Qiu, Linyang Li

- **What's New**: 이번 연구는 대형 언어 모델(LLMs)의 귀납적 추론 능력을 높이기 위해 새로운 과제 'Case2Code'를 도입했습니다. 이는 LLMs가 여러 입력-출력 예시를 기반으로 프로그램을 추론하는 능력을 배양하기 위한 것입니다.
- **Technical Details**: Case2Code는 실제 세계의 프로그램에서 생성된 입력-출력 변환 사례를 사용하여 귀납적 추론(Inductive Reasoning)을 연습하게 합니다. 데이터 수집을 위해 실행 가능한 코드 텍스트를 다량으로 모으고, LLM과 코드 인터프리터를 활용해 입력-출력 사례를 생성합니다. 이를 통해 강력한 LLM 없이도 대규모 Case2Code 데이터를 효과적으로 구축할 수 있습니다.
- **Performance Highlights**: 실험 결과, Case2Code는 LLaMA3-70B, GPT-3.5, GPT-4와 같은 강력한 LLM에도 도전적인 과제임을 보여줍니다. 또한, Case2Code 데이터를 통해 학습한 LLM은 일반적 추론 과제에서도 성능이 향상되는 결과를 나타냈습니다. 이는 HumanEval과 MBPP와 같은 코드 생성 과제에서도 긍정적인 효과를 발휘했습니다.

### [IMAGDressing-v1: Customizable Virtual Dressing](https://arxiv.org/abs/2407.12705)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12705.png)

Vote: 5

Authors: Xin He, Jinghui Tang, Hu Ye, Fei Shen, Xin Jiang, Zechao Li, Cong Wang, Xiaoyu Du

- **What's New**: 이번 논문에서는 상인들을 위한 새로운 Virtual Dressing (VD) 작업을 소개하며, 생성된 이미지와 참조 의상 간의 일관성을 평가하기 위한 포괄적인 친화성 측정 지수(CAMI)를 설계했습니다.
- **Technical Details**: 이 논문은 주로 다음 두 가지 주요 모듈로 구성된 IMAGDressing-v1을 제안합니다: 의상 UNet과 하이브리드 주의(attention) 모듈이 포함된 노이즈 제거 UNet입니다. IMAGDressing-v1은 기체주파수변환(가변주의) 모듈을 통해 텍스트 프롬프트 제어와 의상 특징을 균형 있게 통합합니다. 더 나아가, ControlNet과 IP-Adapter 같은 확장을 통해 생성된 이미지의 다양성과 통제성을 향상시킬 수 있습니다.
- **Performance Highlights**: 텍스처 정보를 잘 보존하고 인풋 의상의 세밀한 특징을 캡처하며, 사용자가 다양한 씬을 텍스트 프롬프트를 통해 제어할 수 있도록 합니다. 또한, 데이터 부족 문제를 해결하기 위해 300,000쌍 이상의 의상 및 착용 이미지가 포함된 대규모 인터랙티브 의상 매칭(IGPair) 데이터셋을 수집하여 공개했습니다.

### [Goldfish: Vision-Language Understanding of Arbitrarily Long Videos](https://arxiv.org/abs/2407.12679)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12679.png)

Vote: 5

Authors: Jürgen Schmidhuber, Jian Ding, Kirolos Ataallah, Deyao Zhu, Xiaoqian Shen, Essam Sleiman, Eslam Abdelrahman, Mingchen Zhuge, Mohamed Elhoseiny

- **What's New**: 이 논문에서는 Goldfish라는 이름의 새로운 프레임워크를 제안합니다. Goldfish는 임의 길이의 비디오를 이해할 수 있는 시스템으로, 비디오를 짧은 클립으로 분할하고, 관련있는 클립만 선택하여 질문에 답변하는 메커니즘을 도입했습니다. 이는 특히 긴 비디오에서 발생하는 소음 및 중복 문제를 해결하고자 하는 접근법입니다.
- **Technical Details**: Goldfish는 세 가지 주요 모듈로 구성됩니다: (1) MiniGPT4-Video 모델과 텍스트 인코더를 사용한 '비디오 디스크립터' 모듈, (2) 텍스트 임베딩을 통해 유사성을 비교하는 '검색 모듈', (3) 최종 응답을 생성하는 '답변 모듈'입니다. MiniGPT4-Video는 기존 MiniGPT-v2 모델을 확장하여 단일 이미지가 아닌 여러 프레임을 처리할 수 있습니다. 각 프레임의 토큰을 언어 토큰으로 변환하여 학습하며, 텍스트와 시각적 데이터를 모두 이해하게 됩니다.
- **Performance Highlights**: Goldfish는 TVQA, LLama-Vid, MovieChat, Movie QA와 같은 기존의 비디오 벤치마크에서 뛰어난 성능을 보였습니다. 특히 LLAMA-VID 모델 대비 약 15% 더 높은 정확도를 기록했으며, MSVD, MSRVTT, TGIF, TVQA 등의 짧은 비디오 벤치마크에서도 기존 최첨단(SOTA) 방법들보다 더 나은 성능을 보였습니다. 이는 Goldfish의 검색 메커니즘 덕분에 긴 비디오에서도 효율적으로 작동할 수 있는 성능을 입증합니다.

### [Audio Conditioning for Music Generation via Discrete Bottleneck Features](https://arxiv.org/abs/2407.12563)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12563.png)

Vote: 5

Authors: Alexandre Défossez, Yossi Adi, Jade Copet, Axel Roebel, Simon Rouard

- **What's New**: 이 논문에서는 텍스트에 기반한 음악 생성 모델인 MusicGen을 활용하여 새로운 '스타일 조건자(style conditioner)' 모듈을 설계했습니다. 이 모듈은 짧은 오디오 세그먼트를 입력으로 받아들여, 이를 텍스트와 결합하여 새로운 음악을 생성합니다. 이는 기존에 텍스트만을 조건으로 사용하는 모델의 한계를 넘어서며, 오디오의 다양한 정보(멜로디, 리듬 등)를 더욱 효과적으로 활용할 수 있게 합니다.
- **Technical Details**: 논문은 텍스트 기반 모델 MusicGen을 활용합니다. 텍스트-이미지 모델에서 사용하는 '텍스트 인버전(textual inversion)' 방법을 적용하여 오디오 조건자를 학습합니다. 제안된 스타일 조건자 모듈은 Encodec, MERT, MusicFM과 같은 오디오 특징 추출기와 변형 인코더(transformer encoder), Residual Vector Quantizer(RVQ), 시간 다운샘플링을 결합하여 구성됩니다. 이를 통해 높은 레벨의 특징을 추출하면서도 오디오 복사나 반복을 방지합니다.
- **Performance Highlights**: 새로운 더블 클래스 파이어 프리 가이던스(double classifier free guidance) 방법을 제안하여 텍스트와 오디오 조건을 균형 있게 병합합니다. 스타일 조건을 기반으로 하는 새로운 객관적 메트릭을 도입하였으며, 인간 평가로 검증했습니다. 제안된 방법은 텍스트 기반 조건 및 오디오 조건 두 가지 모두를 활용한 음악 생성을 가능하게 합니다. 비교 실험을 통해 기존 모델들과 비교하여 제안된 방법의 실용성과 음악 생성의 품질을 입증했습니다.

### [Splatfacto-W: A Nerfstudio Implementation of Gaussian Splatting for Unconstrained Photo Collections](https://arxiv.org/abs/2407.12306)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12306.png)

Vote: 5

Authors: Justin Kerr, Congrong Xu, Angjoo Kanazawa

- **What's New**: 최근 2D 이미지 모음에서 새로운 시각(view)을 생성하는 기술이 주목받고 있습니다. 전통적인 방법인 Structure-from-Motion(SFM)와 Multi-View Stereo(MVS), 그리고 Neural Radiance Fields(NeRF)와 그 확장판들이 3D 장면을 재구성하는 기반을 마련해 왔습니다. 하지만 이러한 방법들은 시간대나 날씨 변화로 인해 이미지 컬렉션들이 형태적 변화를 겪을 때 어려움을 겪습니다. 이에 대응하여 NeRF-W와 같은 확장판이 등장했지만, 최적화와 렌더링 속도가 느리다는 문제가 있습니다. 이에 비해 3D Gaussian Splatting(3DGS)은 빠른 훈련과 실시간 렌더링 능력을 제공하며, 이를 기반으로 개선된 Splatfacto-W를 제안합니다. Splatfacto-W는 NeRF-W보다 5.3 dB 높은 PSNR 성능을 제공하면서도 실시간 성능을 유지합니다.
- **Technical Details**: Splatfacto-W는 Nerfstudio 환경에서 구현되었으며, 이미지별 색상 변화를 효과적으로 처리하는 Latent Appearance Modeling, 효율적인 Heuristic 기반의 Transient Object Handling, 및 구면 조화 기반의 Background Modeling을 특징으로 합니다. 이를 통해 기존 3DGS의 한계를 극복하고 일관된 장면 재구성 성능을 제공합니다. 특히, Splatfacto-W는 Gaussian마다 appearance feature를 부여하여 색상 변화를 조절하며, 실시간 렌더링 속도를 보장합니다.
- **Performance Highlights**: Splatfacto-W는 PSNR 성능이 NeRF-W보다 17% 높으며, RTX 2080Ti와 같은 상용 GPU에서도 실시간 상호작용이 가능합니다. SWAG와 GS-W 같은 기존 접근법보다 빠르며, 다중 시각 일관성을 보장하는 배경 모델링이 향상되었습니다. 트랜지언트 객체를 처리하는 데 있어 2D 사전 학습된 모델 없이 효율적인 마스킹 방법을 사용하여 최적화 속도를 높였습니다.

### [AUITestAgent: Automatic Requirements Oriented GUI Function Testing](https://arxiv.org/abs/2407.09018)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09018.png)

Vote: 3

Authors: Shiyu Guo, Xin Wang, Xuan Wang, Yu Zhang, Yongxiang Hu, Yingchuan Wang, Chaoyi Chen, Yangfan Zhou

- **What's New**: 모바일 앱의 GUI 테스트를 자연 언어로 자동화하는 AUITestAgent 도구가 소개되었습니다. 이 도구는 개발자들이 자연 언어 요구 사항을 통해 GUI 테스트를 수행하도록 설계된 최초의 자동화 도구입니다.
- **Technical Details**: AUITestAgent는 두 가지 주요 모듈로 구성됩니다: GUI 상호작용과 기능 검증. 먼저, 테스트 요구 사항을 분석하여 상호작용 명령과 기능 검증을 위한 오라클을 추출합니다. 상호작용 명령의 복잡성에 따라 Executor가 직접 수행하거나 Planner가 명령을 분해하는 조직 패턴을 동적으로 선택합니다. 기능 검증 모듈은 상호작용 로그를 테스트 오라클에 기반하여 분석하고 검증 결과와 이유를 출력합니다.
- **Performance Highlights**: 실험 결과에 따르면, AUITestAgent는 AppAgent와 MobileAgent와 비교하여 GUI 상호작용에서 높은 성능을 보였습니다. 구체적으로, AUITestAgent는 GUI 기능 버그를 90% 이상 회상하고, 5% 미만의 false positive rate를 유지하면서 합리적인 설명을 제공할 수 있습니다. 또한, AUITestAgent는 Meituan에서 여러 비즈니스 라인에 걸쳐 xx개의 새로운 기능 버그를 발견하는 데 성공했습니다.

### [The Art of Saying No: Contextual Noncompliance in Language Models](https://arxiv.org/abs/2407.12043)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12043.png)

Vote: 3

Authors: Vidhisha Balachandran, Khyathi Chandu, Pradeep Dasigi, Nouha Dziri, Jack Hessel, Noah A. Smith, Faeze Brahman, Yulia Tsvetkov, Sarah Wiegreffe, Abhilasha Ravichander, Hannaneh Hajishirzi, Yejin Choi, Sachin Kumar, Valentina Pyatkin

- **What's New**: 이번 연구에서는 언어 모델이 모든 사용자 요청에 따르지 않아야 할 필요성을 강조하며, 기존 AI 안전 연구에서 다루던 거부(compliance) 개념의 범위를 확장하고자 합니다. 이를 통해 잘못된 전제나 정보가 포함된 요청에 직접적으로 응답하는 대신, 모델이 이를 올바르게 인지하고 처리하는 방식을 제안합니다.
- **Technical Details**: 이번 연구에서는 요청에 대한 비준수(noncompliance)의 맥락을 다루는 분류 체계(taxonomy)를 개발했습니다. 이 체계는 요청(submissions)을 다섯 가지 주요 카테고리로 나누며, 각각의 경우에 모델이 올바르게 대응해야 하는 방법을 정의합니다. 또한, 높은 품질의 평가 세트와 이와 대비되는 준수(compliance) 요청 세트를 기반으로 한 CoCoNot 데이터셋을 구성하였습니다.
- **Performance Highlights**: 여러 최신 언어 모델을 평가한 결과, GPT-4와 Llama-3와 같은 능력이 뛰어난 모델들도 '불완전'하거나 '지원되지 않는' 요청의 최대 30%에 대해 잘못 응답하는 경우가 발견되었습니다. 추가로, 지시 튜닝(instruction tuning)과 파라미터 효율적 방법(예: low rank adapters)을 이용한 지속적인 미세 조정이 모델의 원래 능력을 유지하면서 비준수를 유도하는 효과적인 방법으로 나타났습니다.

### [NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models](https://arxiv.org/abs/2407.12366)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.12366.png)

Vote: 2

Authors: Xin Eric Wang, Qi Wu, Yicong Hong, Zun Wang, Gengze Zhou

- **What's New**: 새로운 NavGPT-2 시스템이 도입되었습니다. 이 시스템은 로봇 내비게이션에서 발생하는 문제를 해결하기 위해 개발되었으며, 기존의 Vision-Language Navigation (VLN) 접근 방법과 대형 언어 모델(Large Language Model, LLM)을 통합한 것입니다.
- **Technical Details**: NavGPT-2는 InstructBLIP 아키텍처를 기반으로 다중 이미지 인식 기능을 추가하여 VLN 과업에 적응하도록 했습니다. 이 시스템은 GPT-4V와 시각적 지시 튜닝을 통해 단계별 내비게이션 추론 데이터를 구축합니다. 또한, 언어 디코딩과 액션 디코딩을 위해 LLM의 잠재적인 시각-언어 표현을 사용하여 장기 내비게이션 이력을 추적하고 효율적으로 되돌아갈 수 있도록 했습니다.
- **Performance Highlights**: NavGPT-2는 해석 가능한 행동을 언어로 생성할 수 있으며, 사용자가 피드백을 받고 로봇과 상호작용할 수 있는 능력을 제공합니다. 이는 실용적이고 상호작용 가능한 VLN 에이전트를 구축하는 데 필수적인 능력입니다. 주요 기여는 다음과 같습니다: 1) LLM 훈련을 필요로 하지 않는 VLN 전문가를 VLM과 통합하는 파이프라인을 제안, 2) 사전 훈련된 VLM의 강력한 기능 향상을 활용하여 LLM 기반 에이전트와 SOTA VLN 전문가 사이의 격차 해소, 3) 내비게이션 결정의 이유를 명확히 설명하는 모델의 의사소통 능력 보유.

### [ThinkGrasp: A Vision-Language System for Strategic Part Grasping in Clutter](https://arxiv.org/abs/2407.11298)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11298.png)

Vote: 2

Authors: Shuo Jiang, Ondrej Biza, Yu Qi, Linfeng Zhao, Yaoyao Qian, Haojie Huang, Robert Platt, Xupeng Zhu

- **What's New**: 로봇 그레이핑(Robotic Grasping) 분야에서 중요한 진전이 있었습니다. ThinkGrasp라는 새로운 시스템이 제안되었는데, 이는 대규모 사전학습된 비전-언어 모델(Vision-Language Models)과 차폐 처리 시스템을 결합하여 복잡한 환경에서의 로봇 그레이핑을 크게 향상시켰습니다. ThinkGrasp는 GPT-4o 모델의 고급 추론 능력을 사용하여 환경과 물체의 특성을 이해하고, 안전하고 효율적인 그레이핑을 수행하는 데 중점을 둡니다.
- **Technical Details**: ThinkGrasp는 Occlusion Handling System을 포함한 플러그 앤 플레이 시스템으로, 시각 및 언어 정보를 효율적으로 사용하여 로봇 그레이핑을 돕습니다. 이 시스템은 LangSAM과 VLPart를 통해 분할 작업을 처리하고, GPT-4o가 제공하는 목표 물체 이름 기반으로 동작합니다. 이 기능 분담을 통해 언어 모델의 오류가 분할 작업에 영향을 주지 않도록 합니다. 제안된 접근법은 'Imaginative Segmentation'이라는 과정을 통해 시각 장면과 자연어 지시를 입력으로 받아 가상의 세분화 및 파악 위치를 제안합니다.
- **Performance Highlights**: ThinkGrasp는 다양한 테스트와 비교 실험을 통해 우수한 성능을 입증했습니다. 도전적인 RefCOCO 데이터셋에서 98.0%의 성공률을 기록하며, 기존의 OVGNet (43.8%) 및 VLG (75.3%)보다 뛰어난 성과를 보였습니다. 복잡한 환경에서도 높은 일반화 능력을 보여 목표 물체가 거의 보이지 않거나 보이지 않는 경우에도 78.9%의 성공률을 유지합니다. 실제 환경에서도 빠르고 효율적으로 적응하여 높은 성공률을 달성했습니다. 6-자유도(6-DoF)를 가진 이지미 손가락 그리퍼와 호환되는 모듈형 설계로, 새로운 언어 목표 및 새로운 물체에 빠르게 적응하는 높은 유연성을 자랑합니다.

### [Practical Unlearning for Large Language Models](https://arxiv.org/abs/2407.10223)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10223.png)

Vote: 2

Authors: Xiao Wang, Qi Zhu, Chenkai Weng, Chongyang Gao, Lixu Wang

- **What's New**: 최근의 수많은 연구에 따르면, 확장 법칙(scaling laws)에 기반하여 언어 모델(LLM)의 크기가 급증하면서 다양한 작업에서 우수한 성능을 보여주고 있습니다. 그러나 이러한 대형 언어 모델들에 대한 안전성, 헛소리 출력(hallucination output), 그리고 프라이버시 침해에 관한 우려도 증가하고 있습니다. 이를 해결하기 위해 LLM에서 위험한 행동을 완화하고 부정확한 지식을 제거하며, 개인 정보, 유해한 데이터 또는 불법 데이터를 삭제하는 다양한 기술들이 연구되고 있습니다.
- **Technical Details**: 대표적인 접근 방식 중 하나는 머신 언러닝(machine unlearning)입니다. 현재의 LLM 언러닝 방법은 주로 파라미터 최적화(parameter optimization), 파라미터 병합(parameter merging), 그리고 문맥 학습(in-context learning)으로 구분됩니다. 파라미터 최적화 방법은 LLM을 직접 미세 조정하여 필요한 데이터를 언러닝하거나 무작위 라벨 손실을 최소화하는 것을 목표로 합니다. 파라미터 병합은 언러닝 데이터와 관련된 모델 파라미터를 식별하고 수정하는 방식입니다. 문맥 학습 기반 방법은 LLM 입력 프롬프트를 수정하여 언러닝 데이터와 관련된 콘텐츠의 출력을 거부하게 합니다. 하지만 이러한 방법들은 자신의 단점을 가지고 있어 실제로 활용하기 어렵습니다. 이를 해결하기 위해 우리는 O3 프레임워크를 제안하였습니다. O3 프레임워크는 Out-Of-Distribution (OOD) 탐지 모듈과 Orthogonal Low-rank adapter (LoRA)를 포함하여 지속적인 언러닝이 가능하며 보존 데이터 없이도 모델 유효성을 유지할 수 있습니다.
- **Performance Highlights**: O3 프레임워크는 세 가지 벤치마크 작업에서 광범위한 실험을 통해 LLM의 판별, 생성, 그리고 추론 능력을 종합적으로 테스트하였으며, 연속적인 언러닝 요청이 있을 때도 우수한 성능을 보여주었습니다. O3는 어떠한 보존 데이터도 사용하지 않으면서 언러닝 효과와 유틸리티 보존 간의 최적의 균형을 일관되게 달성하며, LoRA 디자인 덕분에 계산 효율성도 개선되었습니다.

### [Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection](https://arxiv.org/abs/2407.11854)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.11854.png)

Vote: 1

Authors: Marc-André Carbonneau, Ben Swanson, Gaetan Lopez Latouche

- **What's New**: 이 연구에서는 문법 오류 탐지(GED)를 인간 주석 없이 수행하는 새로운 방법론을 제안합니다. 제안된 방법론은 언어-무관(back-translation)과 다국어 사전 학습 언어 모델(mPLMs)의 교차 언어 전이(CLT) 기능을 결합하여 인위적인 오류 생성(AEG)을 수행합니다. 이 방법론은 주석이 없는 여러 언어에서의 GED 성능을 향상시키기 위해 두 단계의 파인 튜닝 파이프라인을 사용합니다.
- **Technical Details**: 제안된 방법론은 네 단계로 구성됩니다. 첫째, 소스 언어의 GEC 데이터셋을 사용해 다국어 AEG 모델을 훈련시킵니다. 둘째, 이 AEG 모델을 사용해 소스 언어와 타깃 언어의 GED 데이터셋을 생성합니다. 셋째, 생성된 다국어 GED 데이터셋을 사용하여 GED 모델을 파인 튜닝합니다. 마지막으로, 소스 언어의 인간 주석 GED 데이터를 통해 GED 모델을 추가로 파인 튜닝합니다. 이를 통해 타깃 언어에서도 오류 탐지가 가능한 GED 모델을 만들어냅니다.
- **Performance Highlights**: 본 논문에서는 6개의 소스 언어와 5개의 타깃 언어를 대상으로 실험을 수행하였으며, 제안된 방법론이 이전의 주석 없이 수행되는 최첨단 GED 방법들보다 우수한 성능을 보였음을 입증했습니다. 또한, 다양한 AEG 방법들과의 상세한 오류 분석을 제공합니다. 본 연구의 기여로는 주석이 없는 언어에 대해 새로운 최첨단 GED 방법론을 도입한 점, mPLMs의 CLT 기능을 활용해 성능을 향상시킨 점, 다국어 GED에서 GEC 주석 없는 인위적 데이터 생성 방법을 처음으로 평가한 점, 그리고 11개 언어로 구성된 500만 개 이상의 샘플을 포함하는 합성 GED 코퍼스를 공개한 점이 있습니다.

