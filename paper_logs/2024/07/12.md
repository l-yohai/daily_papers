## Daily Papers (2024-07-12)

### [Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On](https://arxiv.org/abs/2407.08348)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08348.png)

Vote: 26

Authors: Liang Zeng, Tianwen Wei, Yang Liu, Liang Zhao, Han Fang, Rui Hu, Yahui Zhou, Jujie He, Liu Yang, Cheng Cheng, Shuicheng Yan, Liangjun Zhong

- **What's New**: 최근 연구에서는 Skywork-Math 모델 시리즈를 도입하여 기존 7B 파라미터로 학습된 LLM에서 인상적인 수학적 추론 성능을 나타냈습니다. Skywork-Math 7B 모델은 복잡한 정렬 기술 없이도 경쟁 수준의 수학 벤치마크에서 뛰어난 성과를 보였습니다. 특히, Skywork-MathQA 데이터셋을 활용하여 더욱 강화된 성능을 확인했습니다.
- **Technical Details**: Skywork-Math 모델 시리즈는 일반적인 7B 사전 학습 LLM 모델에 대해 감독된 미세 조정(SFT) 기법을 적용했습니다. RLHF (Reinforcement Learning from Human Feedback) 및 DPO (Direct Policy Optimization) 등의 복잡한 정렬 기술은 사용되지 않았습니다. Skywork-MathQA 데이터셋은 250만 개의 사례를 포함하고 있으며, 다양한 파이프라인과 프롬프트를 통해 생성된 합성 데이터도 활용되었습니다.
- **Performance Highlights**: Skywork-Math 7B 모델은 MATH 벤치마크에서 51.2%, GSM8K에서는 83.9%의 정확도를 기록하여 초기 GPT-4 모델을 능가하는 성과를 나타냈습니다. 이 연구는 7B 모델에서도 강력한 수학적 추론 능력이 가능하다는 것을 입증했습니다. 또한, 대량의 합성 SFT 데이터가 성능을 크게 향상시킬 수 있음을 확인했습니다.

### [Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model](https://arxiv.org/abs/2407.07053)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07053.png)

Vote: 21

Authors: Guiyang Hou, Yueting Zhuang, Zhenglin Cheng, Yuanyu He, Yanna Ma, Mingqian He, Weiming Lu, Mengna Wang, Wenqi Zhang, Yongliang Shen, Zeqi Tan

- **What's New**: 최근 대형 언어 모델(LLMs)에서의 돌파구에 힘입어 대형 다중 모달 모델(LMMs)도 빠르게 발전하고 있습니다. LLMs를 이용하여 모든 모달리티를 인코딩하는 것은 LMMs가 일상 환경을 이해하고 복잡한 작업을 수행할 수 있게 하며, 이는 범용 AI 어시스턴트의 잠재력을 크게 확장합니다.
- **Technical Details**: 우리의 다중 모달 셀프-인스트럭트 전략은 다양한 일일 시나리오, 예를 들어 도로 지도, 대시보드, 2D 평면 레이아웃, 차트, 관계 그래프, 흐름도, 시각적 퍼즐 등을 위한 추상 이미지를 생성할 수 있습니다. 먼저, LLMs는 시각적 시나리오를 위한 창의적인 아이디어를 자동으로 제안하고, 이를 시각화하기 위해 상세한 코드를 생성합니다. 그런 후 생성된 이미지를 기반으로 높은 품질의 질문과 답변 쌍을 제안합니다. 이 과정은 LLM에 의해 시연 몇 번만으로 완벽히 수행됩니다.
- **Performance Highlights**: 본 연구는 일상 시나리오에 대한 추상 이미지 벤치마크를 생성하고, 이를 통해 여러 대표적인 LMMs의 성능을 평가했습니다. 예를 들어, 대시보드 씬에서 최고 성능의 LMM(GPT-4o)은 54.7점에 도달했으나, 이는 인간의 수준인 85.3에 비해 상당히 낮은 점수입니다. 또한 우리는 62,476개의 차트 및 지도 작성 지침을 합성하여 Llava-1.5-7B 모델의 성능을 조정했고, 실험 결과 이러한 합성 데이터가 도메인 내 성능을 크게 향상시킬 수 있음을 확인했습니다.

### [Video Diffusion Alignment via Reward Gradients](https://arxiv.org/abs/2407.08737)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08737.png)

Vote: 20

Authors: Mihir Prabhudesai, Russell Mendonca, Katerina Fragkiadaki, Deepak Pathak, Zheyang Qin

- **What's New**: VADER라는 새로운 접근 방식이 도입되어 기존의 비디오 확산 모델을 보강하는 방법을 제시했습니다. 특히, VADER는 Reward Models(보상 모델)의 기울기를 사용하여 다양한 비디오 확산 모델을 정렬합니다. 또한, 단일 GPU로 VADER를 실행할 수 있도록 메모리 사용량을 최적화하는 여러 가지 트릭을 제안합니다.
- **Technical Details**: 기존의 비디오 생성 접근 방식은 대규모 웹 스케일 데이터셋에서 교육된 확산 모델을 사용하여 비디오를 생성하는 것이나, 이 방식은 일반적으로 온라인 콘텐츠를 닮은 비디오를 생산하는 데 그치게 됩니다. VADER는 Reward Gradients(보상 기울기) 기반 접근 방식을 사용하여 비디오 생성 시 더 나은 성능을 발휘할 수 있도록 합니다. 이를 통해 각 시공간 차원에서 고유한 스칼라 피드백을 생성함으로써, 더욱 풍부한 피드백을 모델에 제공할 수 있습니다. VADER는 텍스트-비디오 및 이미지-비디오 확산 모델을 정렬하는 데 사용되며, 미리 학습된 여러 비전 모델을 적용합니다.
- **Performance Highlights**: VADER는 다양한 태스크에서 기본 모델 생성 결과를 현저히 개선하며, 기존의 DPO나 DDPO와 같은 보상 기울기를 사용하지 않는 정렬 방법보다 훨씬 높은 성능을 발휘합니다. 또한, 훈련 중 보지 못한 프롬프트에서도 쉽게 일반화 할 수 있습니다. 자세한 시각적 예제와 고성능 결과가 제공되고, VADER의 코드는 공개되어 있습니다.

### [Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients](https://arxiv.org/abs/2407.08296)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08296.png)

Vote: 16

Authors: Lu Yin, Ajay Jaiswal, Jiawei Zhao, Yuandong Tian, Shiwei Liu, Zhangyang Wang, Zhenyu Zhang

- **What's New**: 2020년대 이후로 대형 언어 모델(Large Language Models, LLMs)은 다양한 분야에서 놀라운 성능을 보여주었으나, 수십억 개의 파라미터를 갖춘 모델을 훈련하고 완전히 파인 튜닝하는 것은 대부분의 연구 그룹에게 엄청난 비용 부담을 안겨줍니다. 이를 해결하기 위해 Q-GaLore라는 새로운 접근 방식을 제안합니다. 이 방식은 저정밀도(저정밀도) 가중치와 저계층(저계층) 그래디언트를 사용하여 메모리 오버헤드와 훈련 지연을 줄입니다.
- **Technical Details**: Q-GaLore는 두 가지 주요 모듈을 도입합니다. 첫째, 저정밀도 저계층 그래디언트 훈련을 통해 가중치와 투영 행렬을 각각 8비트와 4비트로 양자화(quantize)합니다. 둘째, 'Lazy Layer-wise Subspace Exploration'을 통해 훈련 중 특정 레이어에서 SVD 연산의 빈도를 줄여 훈련 시간을 단축합니다.
- **Performance Highlights**: Q-GaLore는 풀-파라미터 훈련과 GaLore에 비해 메모리 요구량을 각각 61%, 30% 줄입니다. 이는 16GB 메모리를 가진 RTX 4060 Ti GPU 단일 유닛으로 LLaMA-7B 모델 훈련이 가능함을 의미하며, 성능은 풀-랭크 모델과 유사합니다. 또한 Q-GaLore는 SOTA 저계층 방법(예: LoRA, QLoRA, GaLore)에 비해 메모리 소비를 최대 50% 줄이며, 동일한 메모리 비용에서 QLoRA보다 일관되게 뛰어난 성능을 보여줍니다.

### [MAVIS: Mathematical Visual Instruction Tuning](https://arxiv.org/abs/2407.08739)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08739.png)

Vote: 16

Authors: Shanghang Zhang, Hongsheng Li, Renrui Zhang, Xinyu Wei, Aojun Zhou, Ziyu Guo, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Bin Wei, Dongzhi Jiang, Peng Gao

- **What's New**: 최근 발표된 논문에서 MAVIS라는 새로운 수학적 시각 모델 튜닝 파라다임을 소개했습니다. 이 모델은 다중 모드의 문제를 해결하는 데 중점을 두고 있으며, 특히 시각적 수학 문제 해결 분야에서 뛰어난 성능을 보여줍니다.
- **Technical Details**: MAVIS는 MAthematical VISual instruction tuning을 기반으로 한 파라다임으로, MLLMs(Multi-modal Large Language Models)을 시각적 수학 문제 해결에 최적화합니다. 논문에서는 두 개의 주요 데이터셋 MAVIS-Caption과 MAVIS-Instruct를 도입하여 수학적 시각 데이터를 폭넓게 다루고 있습니다. 또한, 세 가지 단계의 교육 파이프라인을 통해 모델을 훈련시키며, MathVerse 같은 다양한 평가 벤치마크에서 뛰어난 성능을 입증했습니다.
- **Performance Highlights**: MAVIS-7B 모델은 기존의 오픈소스 MLLMs를 뛰어넘는 성능을 보였으며, 특히 MathVerse 벤치마크에서 다른 7B 모델보다 평균 정확도에서 +11.0% 향상, 두 번째로 높은 성능을 자랑하는 LLaVA-NeXT (110B)보다 +3.0% 높은 성능을 기록했습니다.

### [MambaVision: A Hybrid Mamba-Transformer Vision Backbone](https://arxiv.org/abs/2407.08083)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08083.png)

Vote: 12

Authors: Jan Kautz, Ali Hatamizadeh

- **What's New**: 최근 몇 년간 Transformer가 컴퓨터 비전, 자연어 처리, 음성 처리 및 로봇 공학을 포함한 다양한 도메인에서 표준 아키텍처로 자리잡아 왔습니다. 하지만 주의 메커니즘의 이차 복잡성으로 인해 이들을 훈련하고 배포하는데 높은 계산 비용이 들었습니다. 최근 Mamba는 선형 시간 복잡성을 달성하고 다양한 언어 모델링 작업에서 Transformer를 능가하거나 동등한 성능을 보여주었습니다. 새로운 연구는 Mamba의 SSM 공식의 장점을 활용하여 비전 작업을 위한 하이브리드 아키텍처를 제안했습니다. MambaVision 모델은 ImageNet-1K에서 새로운 SOTA Pareto 전선을 달성했습니다.
- **Technical Details**: MambaVision은 다단계 아키텍처를 가지고 있으며, CNN 기반의 잔차 블록을 사용하여 큰 해상도 특징을 빠르게 추출합니다. 또한, SSM과 Transformer 블록을 결합한 하이브리드 아키텍처입니다. 이미지 일부를 겹치는 패치로 변환한 후 CNN 계층을 사용하여 고해상도 입력에서 빠른 특징 추출을 수행하고, 최종 단계에서 여러 자가 주의(Self-Attention) 블록을 도입하여 전역 문맥과 장거리 공간 의존성을 더욱 효과적으로 캡처합니다.
- **Performance Highlights**: MambaVision 모델은 ImageNet-1K Top-1 정확도 및 이미지 처리량에서 Mamba, CNN 및 ViT 기반 모델을 능가하며, MS COCO 및 ADE20K 데이터셋에서도 비교적 작은 모델을 초과하는 성능을 보여줍니다. 하이브리드 Mamba Transformer 모델이 비전 작업에 뛰어난 효율성과 성능을 제공한다는 것을 검증했습니다.

### [Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist](https://arxiv.org/abs/2407.08733)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08733.png)

Vote: 11

Authors: Shudong Liu, Derek F. Wong, Kaizhu Huang, Zihao Zhou, Wei Liu, Maizhen Ning, Jindong Wang, Qiufeng Wang, Xiaowei Huang

- **What's New**: MathCheck는 최신 수학적 추론 능력 평가 도구로서 LLMs(대규모 언어 모델, Large Language Models)의 수학적 추론 능력과 일반화 및 강건성을 평가하도록 설계된 체크리스트를 소개합니다. GSM8k 및 GeoQA에서 데이터를 생성한 MathCheck-GSM 및 MathCheck-GEO를 활용해 문자 기반 및 다중 모드 기하학 문제에 대한 평가를 제공합니다.
- **Technical Details**: MathCheck는 총 4가지 수학 과제(문제 해결, 정답 가능 여부 판단, 결과 판단, 과정 판단)와 원래 문제에서 의미를 유지하며 다른 문장 구조로 변환하는 문제 이해, 문제와 관련 없는 방해 요소 추가, 시나리오 이해 등 3가지 문제 변형을 통해 모델의 강건성을 테스트합니다. (M)LLMs를 사용하여 데이터를 자동으로 생성하며, GPT-4-Turbo와 같은 모델들이 이 작업에 사용됩니다.
- **Performance Highlights**: 20개의 LLM 및 11개의 MLLM을 대상으로 한 실험 결과, GPT-4o와 같은 첨단 LLM은 MathCheck에서 우수한 성능을 지속적으로 보여주었으나, 다른 모델 계열은 성능이 크게 감소하는 경향을 보였습니다. 이는 MathCheck가 본질적인 수학적 추론 능력을 보다 정확하게 평가할 수 있음을 시사합니다.

### [Self-Recognition in Language Models](https://arxiv.org/abs/2407.06946)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.06946.png)

Vote: 10

Authors: Veniamin Veselovsky, Viacheslav Surkov, Caglar Gulcehre, Robert West, Tim R. Davidson, Giuseppe Russo

- **What's New**: 최근 OpenAI, Anthropic, Gemini, Meta 등의 연구에 따르면, 언어 모델(Language Models, LMs)의 발전이 크게 이뤄지고 있으며, 소비자 응용 프로그램에서의 사용이 급격히 확대되고 있습니다. 특히 '에이전트형' 애플리케이션의 가능성이 많은 주목과 자금을 받고 있습니다. 이는 미래 사회에서 LM 간의 상호작용이 크게 늘어날 가능성을 제시하고 있습니다.
- **Technical Details**: LM의 자기 인식(self-recognition) 능력 이해는 두 가지 관점에서 중요합니다. 첫째, 철학적, 신경과학, 인지과학적 측면에서 비유기체 존재의 자기 인식이 가능하다면 엄청난 의미를 가집니다. 둘째, 실용적인 안전 고려가 필요합니다. 자기 인식이 가능한 LM은 사법 업무에서 민감한 정보 누출과 같은 '미러 리스크(mirror risks)'를 야기할 수 있기 때문입니다. 이를 실험하기 위해 LMs에 보안 질문을 생성하도록 지시하고, 다른 모델에 동일한 질문을 제공하여 응답을 수집한 후, 제시된 질문과 답변을 기반으로 특정 모델이 자신의 답변을 인식할 수 있는지를 테스트합니다.
- **Performance Highlights**: 다양한 최신 LM들을 대상으로 테스트한 결과, 일부 모델은 고도의 자기 인식 능력을 보였으나, 전반적인 일관된 자기 인식 증거는 발견되지 않았습니다. 또한, LM들은 평균적으로 자신보다 '더 강력한' 모델의 답변을 선호하는 경향이 있었으며, 모델들 간의 일관된 선호도가 존재함을 확인했습니다. 이 연구는 LM의 다중 선택 포맷 사용을 고려할 때 위치 편향(position bias) 이슈를 초래할 수 있음을 시사합니다.

### [SEED-Story: Multimodal Long Story Generation with Large Language Model](https://arxiv.org/abs/2407.08683)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08683.png)

Vote: 8

Authors: Yingcong Chen, Yukang Chen, Ying Shan, Yuying Ge, Shuai Yang, Yixiao Ge, Yang Li

- **What's New**: 새로운 논문에서는 SEED-Story라는 혁신적인 방법을 도입하여 시각적 요소와 텍스트가 섞인 멀티모달 스토리를 생성하는 접근법을 제안하고 있습니다. 주요 기여는 세 가지로 요약할 수 있습니다. (1) SEED-Story는 멀티모달 대형 언어 모델(Multimodal Large Language Models, MLLMs)을 활용해 풍부한 내러티브 텍스트와 관련 있는 이미지를 생성합니다. (2) 멀티모달 어텐션 싱크 (multimodal attention sink) 메커니즘을 도입하여 학습된 시퀀스 길이보다 더 긴 스토리도 효율적으로 생성할 수 있습니다. (3) 멀티모달 스토리 생성 훈련 및 평가를 위한 대규모 데이터셋인 StoryStream을 도입하였습니다.
- **Technical Details**: SEED-Story는 사전 학습된 비전 트랜스포머(ViT)를 사용하여 이미지를 토큰화하고, 사전 학습된 확산 모델을 사용하여 이미지를 디코딩합니다. 이 과정에서 SD-XL 모델이 활용됩니다. 학습 시, 시각적 및 텍스트 데이터를 제공하여 다음 단어 예측과 이미지 특징 회귀를 통해 멀티모달 생성을 정규화합니다. 멀티모달 어텐션 싱크 메커니즘은 슬라이딩 윈도우 방식을 활용해 최신 토큰의 Key-Value(KV) 상태를 유지함으로써 긴 시퀀스도 효율적으로 생성할 수 있게 합니다.
- **Performance Highlights**: 제안된 SEED-Story 모델은 이미지 스타일 일관성, 스토리 몰입도, 이미지-텍스트 일관성을 평가하는 지표에서 우수한 성능을 보였습니다. 대규모 데이터셋인 StoryStream은 기존 데이터셋보다 네 배 더 큰 데이터 볼륨, 더 높은 이미지 해상도 및 더 긴 시퀀스 길이를 제공하며, 더 세부적으로 설계된 평가 메트릭을 통해 모델의 우수성을 증명하였습니다.

### [DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception](https://arxiv.org/abs/2407.08303)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08303.png)

Vote: 8

Authors: Haiwen Diao, Ling-Yu Duan, Yueze Wang, Xiaotong Li, Fan Zhang, Xinlong Wang

- **What's New**: 최근 발표된 논문에서는 다양한 시각적 요소에 대한 밀도 높은 설명을 제공하는 고품질의 이미지-텍스트 데이터셋의 부족을 지적하며 이를 해결하기 위한 방법을 제안합니다. 'Perceptual Fusion'이라는 새로운 파이프라인을 고안하여 다양한 비전 전문가들을 통합하고, 저비용 MLLM을 정보를 융합하는 중심 피벗으로 사용합니다. 이를 통해 LAION 데이터셋에 기반한 고품질 데이터셋 'DenseFusion-1M'을 만들어내는 방법을 소개합니다.
- **Technical Details**: 논문에서 제안하는 Perceptual Fusion 파이프라인은 다양한 시각적 요소를 포착하기 위해 여러 시각 전문가들(vision experts)의 지식을 통합합니다. 이는 고급 GPT-4V를 활용해 10만 개의 상세한 설명을 얻고, 이를 기반으로 다중 소스 정보를 통합할 수 있는 강력한 캡션 생성 엔진을 개발하는 방식입니다. 이 과정에서 객체 감지, 이미지 태깅, 텍스트 인식 등의 전문가의 지식을 활용하여 이미지 내용을 철저히 이해합니다. 최종적으로 DenseFusion-1M이라는 풍부한 OCR 정보와 정확한 객체 및 위치 인식, 외부 지식을 포함한 대규모, 고품질 데이터셋을 구축합니다.
- **Performance Highlights**: DenseFusion-1M 데이터셋을 기반으로 훈련된 MLLM은 기존의 최첨단 MLLM들과 비교하여 10개의 비전-언어 벤치마크에서 뛰어난 성능을 보였습니다. 특히 상세한 텍스트 인식과 고해상도 이미지 인지 분야에서 큰 성과를 거두었습니다.

### [Gradient Boosting Reinforcement Learning](https://arxiv.org/abs/2407.08250)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08250.png)

Vote: 7

Authors: Gal Dalal, Chen Tessler, Benjamin Fuhrer

- **What's New**: 이번 논문에서는 RL (Reinforcement Learning) 영역에 적합한 Gradient Boosting Trees (GBT) 프레임워크인 Gradient Boosting Reinforcement Learning (GBRL)을 소개합니다. 이 프레임워크는 구조화된 관찰 데이터와 범주형 데이터를 자연스럽게 취급하며, 해석 가능성이 높다는 강점을 가지고 있습니다. 또한, 경량 구현을 지원하여 리소스가 제한된 엣지 디바이스에서의 훈련 및 배포에 적합합니다.
- **Technical Details**: GBRL의 주요 기여는 다음과 같습니다: 1) PPO, A2C, AWR과 같은 RL 알고리즘에 GBT를 함수 근사 기법으로 사용하여 NN과 경쟁력 있게 작동하는 것을 보여줍니다. 2) 트리 기반 AC (Actor-Critic) 아키텍처를 도입하여 정책과 값에 대한 공통 앙상블 구조를 공유함으로써 메모리와 계산 요구사항을 줄였습니다. 3) CUDA 기반 하드웨어 가속 GBT 프레임워크를 제공하여 RL에서 최적화된 성능을 발휘할 수 있도록 합니다. GBRL 라이브러리는 Stable-baselines3와 같은 인기 있는 리포지토리와 원활히 통합됩니다.
- **Performance Highlights**: GBRL은 범주형 태스크에서는 NN보다 우수한 성능을 발휘하며, 수백만 번의 상호작용이 요구되는 복잡하고 고차원적인 RL 환경에서도 효과적입니다. 이는 기존 GBT 라이브러리의 정적 데이터셋과 사전 정의된 라벨에 대한 한계를 극복한 것입니다.

### [GTA: A Benchmark for General Tool Agents](https://arxiv.org/abs/2407.08713)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08713.png)

Vote: 6

Authors: Xinyi Le, Kai Chen, Songyang Zhang, Jize Wang, Cailian Chen, Yining Li, Zerun Ma

- **What's New**: 통합 도구와 대형 언어 모델(LLMs)을 활용한 종합 AI 어시스턴트 개발이 많은 연구의 관심을 받고 있습니다. 이 새로운 평가 프레임워크는 인간이 설계한 쿼리와 실질적인 멀티모달 입력 multimodal inputs)으로 현실 세계의 시나리오에 맞춘 평가를 가능하게 합니다.
- **Technical Details**: 제안된 평가 프레임워크는 실제 사용자의 쿼리, 배포된 도구, 멀티모달 입력을 포함합니다. 처음으로, 쿼리는 AI가 아닌 인간이 설계했으며, 이는 명확한 목표를 가지고 있지만 도구 사용 단계는 암시적으로 포함되어 있어 LLM이 이 단계를 추론해야 합니다. 두 번째로, 다양한 범주(인지, 운영, 논리, 창의성)에 걸친 실질 도구들이 실행 가능한 플랫폼에 배치되어 있습니다. 마지막으로, 모든 쿼리에는 문제 해결을 위해 필요한 여러 단계의 도구 사용이 포함되어 있습니다.
- **Performance Highlights**: 229개의 실제 과제와 해당 실행 가능한 도구 체인을 수작업으로 설계하여 주요 LLM을 평가했습니다. 총 14개의 도구를 다루는 플랫폼이 구축되었으며 기존 LLM들이 현실 세계의 시나리오에서 도구 사용에 있어 큰 도전 과제를 안고 있음을 발견했습니다. 예를 들어, GPT-4는 과제의 50% 미만을 완료할 수 있었고, 대부분의 LLM은 25%도 달성하지 못했습니다.

### [The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective](https://arxiv.org/abs/2407.08583)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08583.png)

Vote: 6

Authors: Wenhao Zhang, Bolin Ding, Shuiguang Deng, Yaliang Li, Zhen Qin, Yilun Huang, Liuyi Yao, Daoyuan Chen

- **What's New**: 최근 대형 언어 모델(LLMs)의 성능이 다양한 작업에 걸쳐 크게 향상되었고, 이들을 기반으로 하는 기술도 상당한 발전을 이루었습니다. 사람의 감각이 텍스트 모달리티에만 국한되지 않기 때문에, Gemini-1.5와 Sora와 같은 다중 모달 언어 모델(MLLMs)이 등장하게 되었습니다. 이러한 MLLMs는 텍스트 외의 입력 및 출력을 처리할 수 있으며, GPT-4o와 NExT-GPT 같은 모델은 여러 모달리티 간의 상호작용까지 가능합니다. 최근 2년 동안 MLLMs에 대한 연구가 폭발적으로 증가하였습니다.
- **Technical Details**: MLLMs의 뛰어난 성능은 매개변수 수를 확장함으로써 일련의 작업을 해결하는 LLMs의 돌발 능력에서 비롯됩니다. 많은 연구 결과 모델 크기를 확장하면 더 방대한 양의 데이터가 필요하다는 것을 나타내고 있습니다. 멀티모달 모델의 경우, 선형적인 zero-shot 성능 향상을 위해서는 기하급수적으로 더 많은 데이터가 필요합니다. 따라서, 모델 아키텍처와 학습 기술보다 고품질 데이터 큐레이션에 초점을 맞춘 데이터 중심 접근법이 중요하게 대두되고 있습니다. 최근의 리뷰에서는 데이터 중심 접근법을 단일 모달 도메인에서 멀티 모달 도메인으로 확장하여 데이터 파이프라인 단계별로 기존의 데이터를 중심으로 한 접근법을 조직화했습니다.
- **Performance Highlights**: MLLMs의 성능을 한층 더 끌어올리기 위해 데이터와 모델의 상호 개발 패러다임이 제안되었습니다. 데이터의 양과 질이 모델 성능을 향상시키고, 고품질 데이터에서 이익을 얻은 잘 훈련된 모델은 데이터를 더 나은 품질로 향상시킬 수 있습니다. 이를 통해 모델과 데이터셋 모두를 개선하는 반복적인 사이클이 형성됩니다. 예를 들어, Segment Anything 모델(SAM)은 주석이 필요한 분할 마스크를 사용하여 초기에는 사람이 처리하는 작업을 모델이 점차 대체함으로써 데이터와 모델이 상호 증진되는 주기를 보여줍니다.

### [Autoregressive Speech Synthesis without Vector Quantization](https://arxiv.org/abs/2407.08551)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08551.png)

Vote: 6

Authors: Sanyuan Chen, Sheng Zhao, Xixin Wu, Long Zhou, Jinyu Li, Shujie Hu, Bing Han, Shujie Liu, Yanqing Liu, Furu Wei, Lingwei Meng, Helen Meng

- **What's New**: 최신의 autoregressive 대형 언어 모델(LLMs)에서 관찰되는 진전을 통해 음성 합성 분야에서도 autoregressive 언어 모델링 접근방식을 탐구하고 있습니다. 새로운 모델 MELLE는 discrete token을 사용하는 기존 방식의 한계를 극복하고 continuous-valued token을 기반으로 한 autoregressive 음성 합성 모델입니다.
- **Technical Details**: MELLE는 멜-스펙트로그램(mel-spectrogram)을 연속적인 토큰으로 사용하여 텍스트-투-스피치(TTS) 합성에서 autoregressive 예측을 수행합니다. 기존의 cross-entropy 손실 대신 회귀 손실(regression loss)을 사용하며 반복 문제를 해결하기 위해 spectrogram flux loss를 도입했습니다. 또한 변환 추론(variational inference)을 통해 샘플링 모듈을 설계하여 생성된 오디오 샘플의 다양성을 높였습니다.
- **Performance Highlights**: MELLE는 대규모 50K시간 라이브러헤비(Libriheavy) 훈련 데이터셋과 소규모 960시간 라이브러스피치(LibriSpeech) 데이터셋을 통해 평가되었습니다. 실험 결과, MELLE는 VALL-E 2와 동등한 수준의 객관적 지표를 달성했으며, 주관적 지표에서는 VALL-E 2를 능가하는 성능을 보였습니다. 특히, MELLE는 연속 추론 작업에서 WER(말 단어 오류율)에서 VALL-E에 비해 47.9%의 상대적 감소를, VALL-E 2에 비해 8.1%의 감소를 달성했습니다.

### [Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models](https://arxiv.org/abs/2407.08701)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08701.png)

Vote: 5

Authors: Kai Chen, Christian Theobalt, Yanhong Zeng, Zhening Xing, Gereon Fox, Mohamed Elgharib, Xingang Pan

- **What's New**: 최근 자연어 처리 분야에서 큰 성공을 거둔 대형 언어 모델(LLMs: Large Language Models)은 실시간 스트리밍 데이터 생성을 가능하게 하는 자동 회귀(next-token prediction) 방식을 사용하지만, 이러한 방식이 영상 처리 분야에서 제대로 탐구되지 않았습니다. 이에 착안하여, 실시간 스트리밍 비디오 생성 및 비디오-비디오 번역을 목표로 하는 새로운 방법론 Live2Diff가 제안되었습니다.
- **Technical Details**: 기존 비디오 확산 모델들은 양방향 자기 주의(temporal self-attention) 모델링을 사용하여 프레임 간 상관성을 캡처하지만, 이는 실시간 처리를 방해합니다. 이를 해결하기 위해, Live2Diff는 단방향 자기 주의(uni-directional self-attention)를 사용하며, warmup 지역을 도입하여 초기 프레임의 제한된 컨텍스트 주의(attention)를 보완합니다. 또한, Live2Diff는 K/V 맵을 캐시하고 재사용하는 기능을 디자인하여 연산 시간을 크게 절약합니다. inference 동안의 효율성을 높이기 위해 배치 노이즈 제거(batch denoising) 전략을 사용합니다.
- **Performance Highlights**: Live2Diff는 RTX 4090 GPU에서 512x512 해상도의 비디오를 16FPS로 처리할 수 있는 높은 효율성과 시간적 일관성을 자랑합니다. 실험 결과, Live2Diff는 시간적 부드러움과 효율성 면에서 뛰어난 성능을 검증받았습니다.

### [Towards Building Specialized Generalist AI with System 1 and System 2 Fusion](https://arxiv.org/abs/2407.08642)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08642.png)

Vote: 3

Authors: Biqing Qi, Kaiyan Zhang, Bowen Zhou

- **What's New**: 최근 AI 연구에서 대형 언어 모델(LLMs)인 GPT-4, Gemini 1.5, Claude, Llama 3의 출현은 주목받고 있습니다. 이러한 모델들은 방대한 데이터로 학습되어 이전의 AI보다 훨씬 높은 지능을 보여주고 있으며, 특히 지침 준수나 프로그래밍, 수학 등 다양한 하위 작업에서 뛰어난 성능을 발휘합니다. 그러나 아직 인간의 추론과 심사숙고가 필요한 대부분의 작업에서는 인간을 능가하지 못합니다.
- **Technical Details**: LLMs은 '스케일링 법칙'에 따라 방대한 매개변수를 가진 모델들이 학습됨으로써 높은 지능을 보입니다. 이러한 모델들은 자율적으로 새로운 지식을 발견하고, 목표를 인간의 가치에 맞춰 최적화할 수 있는 능력을 가지고 있습니다. 이를 통해 AI는 특정 분야에서 인간 전문가를 능가하는 '스페셜리스트 지능(Specialized Generalist Intelligence, SGI)'을 개발하고자 합니다. SGI는 특정 작업에서는 인간 전문가의 90% 이상을 능가하면서, 일반적인 성능에서도 비숙련 인간과 비슷하거나 더 뛰어난 성능을 유지합니다.
- **Performance Highlights**: SGI 시스템은 지속적으로 새로운 작업을 학습하고 적응하며, 자율적으로 새로운 지식을 발견하고, 목표를 최적화할 수 있는 중요한 능력을 가지고 있습니다. 이러한 시스템은 의료, 금융 등 특정 도메인에서 인간 전문가를 능가하는 성과를 보여줍니다.

### [Map It Anywhere (MIA): Empowering Bird's Eye View Mapping using Large-scale Public Data](https://arxiv.org/abs/2407.08726)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08726.png)

Vote: 3

Authors: Chen Wang, Jiaye Zou, Sai Mitheran Jagadesh Kumar, Sebastian Scherer, Benjamin Chiang, Taneesh Gupta, Nikhil Keetha, Katia Sycara, Omar Alama, Cherie Ho

- **What's New**: 이번 논문에서는 Bird’s Eye View (BEV) 맵을 예측하는 새로운 데이터 엔진인 MIA를 소개합니다. 이는 Mapillary와 OpenStreetMap 같은 공개 소스 플랫폼을 활용하여 다양한 상황에서의 일반화 가능한 BEV 맵 예측을 지원합니다.
- **Technical Details**: MIA 데이터 엔진은 다양한 지형, 시간대, 계절 등을 포함한 세계 규모의 높은 품질의 FPV 이미지를 Mapillary에서 수집하고, 이를 OpenStreetMap의 BEV 시맨틱 맵과 연결합니다. 데이터를 자동으로 큐레이팅 및 정렬하여, 시맨틱 BEV 맵 예측을 위한 풍부한 시맨틱 레이블을 제공합니다.
- **Performance Highlights**: MIA를 통해 구축된 데이터셋은 현재 120만 쌍의 고품질 FPV 이미지와 BEV 맵을 포함하고 있으며, 이는 470 km²에 달합니다. 단순 카메라 내부 매개 변수 무시 모델로 학습해도 최신 기술보다 우수한 제로샷 성능을 보입니다. 특히, 도로나 인도와 같은 정적 클래스에서 뛰어난 성능을 입증했습니다.

### [Generalizable Implicit Motion Modeling for Video Frame Interpolation](https://arxiv.org/abs/2407.08680)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08680.png)

Vote: 3

Authors: Wei Li, Chen Change Loy, Zujin Guo

- **What's New**: 비디오 프레임 보간(VFI)은 두 개의 인접 비디오 프레임 사이에 중간 프레임을 생성하는 컴퓨터 비전의 기본 작업입니다. 이번 연구에서는 이러한 움직임 모델링 문제에 대한 더 효과적이고 일반화 가능한 접근 방식을 탐구합니다. 특히, 암묵적인 신경 표현(Implicit Neural Representations)을 활용하여 임의의 시공간 좌표를 입력받아 원하는 시공간 출력을 디코딩할 수 있는 일반화 가능한 암묵적 운동 모델링(GIMM)을 제안합니다.
- **Technical Details**: 기존의 VFI 프레임워크는 주로 흐름 기반 방법(Flow-based Method)으로, 두 단계로 나뉩니다: 입력 프레임을 기반으로 추정된 광학 흐름(Optical Flow)을 변환하고, 왜곡된 프레임을 병합 및 강화하여 중간 프레임을 생성하는 것입니다. 하지만 정확한 흐름 추정은 복잡한 실제 동작을 모델링하는 데 어려움을 겪습니다. 이를 해결하기 위해 제안된 GIMM은 RAFT나 FlowFormer와 같은 사전 학습된 광학 흐름 추정기를 통해 얻은 양방향 흐름을이어 임의의 중간 타임스탬프에서의 흐름을 추정합니다. GIMM은 시공간 좌표와 운동 잠재 변수를 입력받아 임의의 시점에서의 프레임 생성이 가능합니다.
- **Performance Highlights**: 제안된 GIMM 프레임워크는 임의의 타임스탬프에서 두 개의 인접 비디오 프레임 사이의 광학 흐름을 정확히 예측할 수 있습니다. 특히, 기존 VFI 방법들과의 매끄러운 통합을 가능하게 하며, 다양한 기준 벤치마크에서 최고 성능을 달성합니다.

### [Scaling Up Personalized Aesthetic Assessment via Task Vector Customization](https://arxiv.org/abs/2407.07176)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07176.png)

Vote: 2

Authors: Jooyeol Yun, Jaegul Choo

- **What's New**: 개인 맞춤형 이미지 미학 평가(PIAA)의 최신 연구가 중요한 기여를 했습니다. 이 연구는 기존의 일반 이미지 미학 평가(GIAA) 및 이미지 품질 평가(IQA) 데이터베이스를 활용하여 데이터가 풍부한 접근 방식(data-rich approach)을 통해 개인 맞춤형 모델을 개발합니다. 이 접근 방식은 데이터 수집 비용 문제를 해결하고, 널리 이용 가능한 이미지 스코어 회귀 데이터베이스(image score regression databases)를 활용하여 기존의 확장성 문제를 극복합니다.
- **Technical Details**: 이 논문은 메타러닝(meta-learning) 기법을 확장하여 여러 데이터베이스에서 파인튜닝된 모델의 가중치를 직접적으로 더하거나 빼는 'Task Arithmetic' 방법을 활용합니다. 주요 아이디어는 GIAA와 IQA 데이터베이스 각각을 고유한 이미지 스코어 회귀 과제로 간주하는 것입니다. 다양한 데이터베이스에서 훈련된 모델의 Task 벡터(task vectors)를 조합하여, 특정 작업의 특성을 강조하거나 조정함으로써 모델의 맞춤화를 이룹니다. 이 접근 방식은 사용자가 제공한 입력값을 기반으로 학습 가능한 계수(trainable coefficients)를 도입하여 맞춤형 모델을 생성합니다.
- **Performance Highlights**: 이 접근 방식은 단 몇 개의 샘플로 사용자의 미적 선호도를 학습하는 데 실용적이며, 과적합(overfitting)을 방지하는 데 탁월함을 보입니다. 다양한 시나리오에서 맞춤화된 모델의 효능을 입증하는 실험 결과도 포함되어 있습니다. 기존의 메타러닝 기법에 의한 확장성 문제와 높은 비용을 해결하며, 높은 정확도로 개인의 선호도를 반영하는 모델을 만들어내는 데 성공적임을 보여줍니다. 이로써 이전에는 불가능했던 높은 일반화 능력을 가진 모델을 구현했습니다.

### [WildGaussians: 3D Gaussian Splatting in the Wild](https://arxiv.org/abs/2407.08447)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08447.png)

Vote: 2

Authors: Marc Pollefeys, Zuzana Kukelova, Jonas Kulhanek, Torsten Sattler, Songyou Peng

- **What's New**: 이번 연구에서는 WildGaussians라 불리는 새로운 접근법을 소개합니다. 이는 강력한 장면 재구성 기법인 Gaussian Splatting (3DGS)을 확장하여 이미지 캡처 환경이 통제되지 않는 경우에도 작동 가능하게 합니다.
- **Technical Details**: WildGaussians는 두 가지 주요 구성 요소를 포함합니다: (1) Appearance Modeling: 각 훈련 이미지에 대한 appearance embedding을 훈련하여 다양한 조건에서 촬영된 이미지를 모델링합니다. 추가로, 각 Gaussian에 대해 로컬 효과를 모델링하는 appearance embedding을 훈련합니다. 이러한 embedding은 다중층 퍼셉트론(MLP)을 통해 색상 공간에서의 선형 변환을 예측합니다. (2) Uncertainty Modeling: DINO v2 features를 이용하여 훈련 이미지에서 무시해야 할 영역을 결정하는 불확실성 예측기를 도입합니다.
- **Performance Highlights**: 이 접근법은 특히 환경 조건 변화와 가림막(occluder)을 효과적으로 처리함으로써 기존 방법들보다 빠르게 훈련을 최적화합니다. 또한, 최종 훈련된 표현을 3DGS에 '구워 넣는(bake)' 방식으로, 렌더링 속도와 3DGS의 수정 가능성 및 유연성을 유지합니다.

### [OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects](https://arxiv.org/abs/2407.08711)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.08711.png)

Vote: 1

Authors: Matthew Brown, Kevis-Kokitsi Maninis, James Hays, Abhijit Kundu, Akshay Krishnan

- **What's New**: 본 논문에서는 6자유도(6 Degree-of-Freedom, 6DoF) 자세 및 객체의 모양을 단일 이미지에서 예측하는 문제를 다룹니다. 이를 위해 새로운 대규모 데이터셋인 OmniNOCS를 소개하는데, 이 데이터셋은 Normalized Object Coordinate Space (NOCS) 주석을 포함하며 97개 객체 클래스와 380,000개의 이미지를 포함하는 가장 큰 NOCS 데이터셋입니다. 또한, 이 논문은 NOCS 좌표 및 방향성이 있는 3D 경계 상자를 예측하는 새로운 모델 'NOCSformer'를 제안합니다.
- **Technical Details**: OmniNOCS 데이터셋은 97개의 객체 클래스를 포함하며, 이는 기존의 NOCS 데이터셋보다 10배 이상 크고 다양합니다. NOCSformer 모델은 사전 학습된 자가 지도 학습 ViT(Visual Transformer) 백본을 활용하여 2D 박스에서 입력된 객체의 NOCS 좌표, 마스크 및 크기를 예측합니다. 이 모델은 클래스별 매개변수를 사용하지 않아 대규모 단어집에 대한 일반화가 가능하며, 다양한 객체 범주에 대해 3차원 크기와 3D 방향성을 예측할 수 있도록 학습됩니다.
- **Performance Highlights**: NOCSformer는 OmniNOCS로 학습된 모델로, 기존 NOCS 예측 모델이나 3D 검출 모델보다 높은 정확도를 기록했습니다. OmniNOCS 데이터셋과 NOCSformer의 조합은 이전에 보지 못한 데이터셋에 대해서도 높은 일반화 성능을 보이며, 대상 데이터셋에서 학습된 기준 모델들을 능가하는 결과를 보였습니다.

