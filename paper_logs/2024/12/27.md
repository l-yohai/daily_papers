## Daily Papers (2024-12-27)

### [Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://arxiv.org/abs/2412.18176)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18176.png)

Vote: 2

Authors: Hao Zhang, Jie Ouyang, Kefan Wang, Ruiran Yan, Mingyue Cheng, Qitao Qin, Yucong Luo

- ***What's New***: 이 논문은 다중 모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)과 협력 필터링(다중 모드)을 결합한 새로운 프레임워크, Molar를 소개합니다. Molar는 협력 필터링 신호를 ID 정보와 통합하여 다중 모달 순차 추천의 성능을 개선한다는 점에서 기존 모델과 차별화됩니다.
- ***Technical Details***: Molar는 다중 모달 항목 표현 모델(Multimodal Item Representation Model; MIRM)을 활용하여 텍스트 및 비텍스트 데이터를 결합한 항목 임베딩을 생성합니다. 뒤이어 사용자의 과거 상호작용을 기반으로 동적 사용자 임베딩(Dynamic User Embedding)을 생성하는 메커니즘을 도입합니다. 또한, 포스트 정렬 대조 학습 메커니즘을 사용해 ID 기반 모델과 콘텐츠 기반 모델을 정렬하여 사용자 임베딩 간의 의미적 정렬을 보장합니다.
- ***Performance Highlights***: Molar는 Amazon, PixelRec, MovieLens 데이터셋에서 전통적 SR 모델과 최신 LLM 기반 방법보다 일관되게 뛰어난 정확성과 견고함을 보여줍니다. Molar는 다중 모달 콘텐츠 및 협력 필터링 신호를 결합하여 사용자 관심사를 더 포괄적으로 캡처하고, 다양한 시나리오에서 일관된 성능 향상을 이끌어냈습니다.

### [MMFactory: A Universal Solution Search Engine for Vision-Language Tasks](https://arxiv.org/abs/2412.18072)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18072.png)

Vote: 4

Authors: Wan-Cyuan Fan, Leonid Sigal, Tanzila Rahman

- ***What's New***: MMFactory는 다양한 시각-언어 모델(Vision-Language Models; VLM)과 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLM)간의 솔루션 탐색 엔진 역할을 하는 보편적 프레임워크입니다. 사용자 정의된 작업 설명과 입력-출력 쌍에 기반하여 다양한 프로그래밍 솔루션을 제안하며, 사용자 제약 조건에 맞춰 추천할 수 있습니다.
- ***Technical Details***: MMFactory는 모델과 메트릭 라우팅 컴포넌트를 포함하여, 다양한 비전-언어 도구들(e.g., 객체 탐지, 세그멘테이션, VLMs)을 결합하여 사용자에게 맞춤형 솔루션을 제안합니다. 기술적인 측면에서 다중 에이전트 기반의 솔루션 제안자를 도입하여 실행 가능한 범용적이며 견고한 솔루션을 생성합니다. 이러한 프로세스는 여러 에이전트가 협력하여 사용자가 지정한 작업에 적합한 알고리즘을 개발하도록 지원합니다.
- ***Performance Highlights***: 실험 결과, MMFactory는 최첨단 솔루션을 사용자 문제 명세에 맞춰 제공하여 기존 방법보다 우수한 성능을 보였습니다. 특히, BLINK와 Seedbench 벤치마크 테스트에서 GPT-4o와 같은 모델과의 비교에서 높은 정확도를 달성했으며, 일부 작업에서는 최대 15% 이상의 성능 개선을 보여주었습니다.

### [A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/abs/2412.17483)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17483.png)

Vote: 17

Authors: Kelong Mao, Shuaiyi Li, Zhicheng Dou, Xinting Huang, Chenlong Deng, Zhisong Zhang, Dong Yu

- ***What's New***: 이 연구는 긴 문맥 처리를 개선하기 위한 Gist 토큰 기반의 문맥 압축 방법을 포괄적으로 조사합니다. 두 가지 주요 질문에 중점을 두고 있으며, 하나는 이러한 방법을 통해 전체 Attention 모델을 대체할 수 있는지, 다른 하나는 압축으로 인한 잠재적 실패 패턴이 무엇인지에 관한 것입니다. 이 연구를 통해 Gist 기반 압축은 Retrieval-Augmented Generation(RAG) 및 롱 문서 QA와 같은 작업에서 거의 손실 없는 성능을 달성할 수 있지만, Synthetic Recall과 같은 작업에서는 도전에 직면하는 것을 발견했습니다.
- ***Technical Details***: Gist 토큰 기반 문맥 압축은 메모리 위치와 Gist 세분화의 두 가지 차원에서 기존 모델 아키텍처를 분류하는 통합 프레임워크를 제안합니다. Fine-grained KV cache 아키텍처는 RAG, 롱-문서 QA, 요약과 같은 다양한 작업에서 전체 Attention 모델에 비해 거의 손실 없는 압축 성능을 발휘합니다. 그러나 압축 병목현상으로 인해 특정 상황에서는 성능 격차가 존재합니다. 이를 해결하기 위해 Fine-grained Autoencoding과 Segment-wise Token Importance Estimation이라는 전략을 제안하여 압축 능력을 향상합니다.
- ***Performance Highlights***: 실험 결과, 압축된 모델은 높은 Memory Budget을 활용하는 것에 어려움이 있으며, 이는 대규모 Token이 삽입되지 않은 상태에서 문장의 시작 부분에서의 생성 성능이 낮음을 보여줍니다. Fine-grained AE와 Segment-wise TIE 전략은 이러한 병목현상을 효과적으로 완화하여 모델 성능을 크게 향상시켰습니다. 예를 들어, Synthetic Recall 작업에서 성능이 52.7%까지 증가했습니다.

### [VidTwin: Video VAE with Decoupled Structure and Dynamics](https://arxiv.org/abs/2412.17726)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17726.png)

Vote: 4

Authors: Xu Sun, Xinyi Xie, Tianyu He, Yuchi Wang, Jiang Bian, Junliang Guo

- ***What's New***: VidTwin은 비디오 오토인코더(Video VAE)를 구조(Structure)와 동역학(Dynamics)라는 두 개의 구분된 잠재 공간으로 분리하여 비디오를 효과적으로 인코딩할 수 있는 새로운 모델을 제안합니다. 이 모델은 높은 압축률과 높은 복원 품질을 달성하며, 향후 비디오 잠재 표현 및 생성 연구를 촉진할 수 있습니다.
- ***Technical Details***: VidTwin은 스페이셜-템포럴 트랜스포머(Spatial-Temporal Transformer)를 백본으로 사용하며, 두 개의 서브 모듈을 통해 각각 구조 잠재(Structure Latent)와 동역학 잠재(Dynamics Latent)를 추출합니다. 구조 잠재는 Q-포머(Q-Former)를 통해 저주파수 모션 트렌드를 추출하고, 동역학 잠재는 공간 차원에서 다운샘플링 후 평균을 통해 신속한 움직임을 포착하는 방법을 사용합니다. 최종 복원은 두 잠재 공간을 적절히 결합하여 이루어집니다.
- ***Performance Highlights***: VidTwin 모델은 0.20%의 높은 압축률과 MCL-JCV 데이터셋에서 28.14의 PSNR을 기록하며, 자원이 제한된 환경에서도 강력한 성능을 보입니다. 또한 UCF-101 데이터셋에서 클래스 조건부 비디오 생성 작업에서도 높은 적응력을 보여, 기존의 여러 모델들과 견줄 만한 성과를 냅니다.

### [YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/abs/2412.17743)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17743.png)

Vote: 27

Authors: Yutao Zhu, Yiwen Hu, Jie Chen, Huatong Song, Zican Dong, Kun Zhou, Jiapeng Wang, Wayne Xin Zhao, Jia Deng, Jinhao Jiang, Ji-Rong Wen

- ***What's New***: YuLan-Mini는 2.42B 파라미터를 가진 데이터 효율적인 언어 모델로, 유사한 규모의 다른 모델들과 비교해 최상위 성능을 보여줍니다. 이 모델은 공개 데이터와 합성 데이터를 활용해 훈련되었으며, 고품질의 예비 학습 데이터를 통한 합성 데이터 생성을 통해 수학적 추론 능력과 코드 생성 능력을 증대시킵니다. 공공 도메인에서 직접 데이터와 훈련 방법론을 완전 공개하여 재현성을 강화한 것도 주목할 만합니다.
- ***Technical Details***: YuLan-Mini는 1.08T 토큰으로 예비 학습되었으며, 56층의 디코더 전용 변환기 모델 구조로 이루어져 있습니다. 훈련 안정성을 높이기 위해 데이터 파이프라인에 데이터 정리와 스케줄링 전략을 결합하였고, 훈련 불안정성을 줄이기 위한 최적화 방법을 개발하였습니다. 로타리 임베딩(ROPE)을 활용해 위치에 대한 정보를 통합 처리하고, 죄수 자유 기울기 레이어(SwiGLU)를 적용하여 데이터 관계를 효과적으로 캡처합니다. 또, 합성 데이터 생성을 통해 수학적 추론 및 코드 생성 문제에 대응하기 위한 다양한 방안을 탐색하였습니다.
- ***Performance Highlights***: YuLan-Mini는 수학 추론(MATH-500), 코드 생성(HumanEval 및 MBPP)에서 최상위 성능을 기록하며, 28K 문맥 확장을 통해 긴 문맥 처리 능력을 보강하였습니다. 많은 자원을 필요로 하지 않으며, 대학 수준 연구소에서도 복제 가능한 모델입니다. 가장 주목할 점은 적은 자원으로도 뛰어난 데이터 효율성을 보여주며, 일반 능력에서도 강력한 성능을 입증하였다가 있습니다. 다만, 긴 문맥 처리에서는 최신 모델들에게 약간 미치지 못하는 결과를 보였습니다.

