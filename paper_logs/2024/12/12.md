## Daily Papers (2024-12-12)

### [MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation](https://arxiv.org/abs/2412.07147)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07147.png)

Vote: 1

Authors: Bo Li, Lijie Wen, Shaolin Zhu

- ***What's New***: MIT-10M은 다양한 실제 데이터를 기반으로 10M 이상의 이미지-텍스트 쌍을 포함하는 대규모 다국어 이미지 번역 병렬 말뭉치입니다. 이 말뭉치는 기존 데이터셋의 한계를 극복하기 위한 것으로, 철저한 데이터 정제와 다국어 번역 검증을 통해 고품질 데이터를 구성하였습니다.
- ***Technical Details***: MIT-10M 데이터셋은 크기 840K의 이미지와 14개의 언어로 된 이미지-텍스트 쌍을 포함합니다. 데이터셋은 3단계의 주요 과정을 거쳐 구축되었으며, 웹 크롤링을 통한 데이터 수집, OCR 주석 및 정리, 다국어 번역 및 검증을 포함합니다. 데이터셋은 다양한 크기의 높은 해상도의 실제 이미지를 제공합니다.
- ***Performance Highlights***: MIT-10M을 사용한 모델의 성능은 기존 베이스라인 모델에 비해 BLEU, chrF++, METEOR 점수가 각각 230%, 88%, 130% 증가했습니다. 각 언어쌍에서도 MIT-10M은 기존 데이터셋을 능가하며, 특히 어려운 작업에서의 모델 평가에 더 효과적으로 활용될 수 있습니다.

### [I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token](https://arxiv.org/abs/2412.06676)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06676.png)

Vote: 1

Authors: Eden Biran, Roi Cohen, Konstantin Dobler, Gerard de Melo

- ***What's New***: 이 연구는 대형 언어 모델(Large Language Models; LLMs)의 환각(hallucinations)을 줄이기 위해 새로운 보정 방법으로 [IDK] ('모르겠다') 토큰을 도입했습니다. 이 토큰은 모델이 불확실한 경우를 명시적으로 표현할 수 있게 하며, 이를 통해 모델이 잘못된 예측을 할 때 불확실성을 표현하도록 설계되었습니다.
- ***Technical Details***: 기존의 교차 엔트로피 목표를 변경하여, 모델이 금 토큰(gold token)을 예측하지 못할 때 [IDK] 토큰에 확률 질량을 부여합니다. 불확실성 요인(uncertainty factor)은 예측된 로그잇(logits)에 기반하여 계산됩니다. 이 새로운 IDK 목표는 레이블이 없는 데이터를 사용하며, 모델이 사전 학습 단계에서 불확실성을 표현하도록 가르칩니다.
- ***Performance Highlights***: 다양한 기본 모델에서 IDK 튜닝(IDK-tuning)이 정밀도 향상을 보여주었으며, 기존 모델의 사실적 추론 능력을 더욱 정확하게 만들었습니다. 주요 데이터셋에서 IDK 튜닝 모델의 평균 F1 점수가 증가했으며, 모델의 크기가 증가함에 따라 IDK 튜닝의 효과가 더 두드러졌습니다.

### [StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements](https://arxiv.org/abs/2412.08503)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08503.png)

Vote: 2

Authors: Hao Wang, Xue Song, Beier Zhu, Chi Zhang, Mingkun Lei

- ***What's New***: StyleStudio는 텍스트를 기반으로 스타일 요소를 선택적으로 제어할 수 있는 스타일 전이(Style Transfer) 방법을 제안합니다. 이 연구는 스타일의 과적합 문제를 해결하고 텍스트와 스타일을 더 잘 통합하기 위해 새로운 메커니즘과 접근법을 제시했습니다.
- ***Technical Details***: 이 연구에서는 스타일 및 텍스트 특징을 통합하기 위해 적응형 인스턴스 정규화(Adaptive Instance Normalization; AdaIN)를 크로스 모달 방식으로 도입하였으며, 스타일 기반 분류기 프리 가이던스(Style-based Classifier-Free Guidance; SCFG) 접근법을 통해 특정 스타일 요소에 대한 선택적 제어를 가능케 했습니다. 덧붙여 공간적 레이아웃의 안정성을 위해 초기 생성 단계에서 교사 모델을 활용하였습니다.
- ***Performance Highlights***: 제안된 방법은 다양한 스타일과 프롬프트에서 생성된 이미지의 일치도를 유의미하게 개선하였으며, 텍스트 설명과의 정렬성을 더 높였습니다. 또한, 제안된 방법은 기존 스타일 전이 프레임워크에 조정 없이 통합될 수 있어 활용성이 높습니다.

### [Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation](https://arxiv.org/abs/2412.06016)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06016.png)

Vote: 5

Authors: Chun-Hao Paul Huang, Niloy Mitra, Jong Chul Ye, Hyeonho Jeong, Duygu Ceylan

- ***What's New***: Track4Gen은 비디오 생성에서 공간 추적(spatial tracking)을 통합하여 외관 드리프트(appearance drift)를 줄이는 새로운 방법을 제안합니다. 이는 기존의 비디오 생성 아키텍처를 최소한으로 변경하여 비디오 생성과 포인트 추적(point tracking) 작업을 하나의 네트워크로 병합합니다.
- ***Technical Details***: Track4Gen은 최신 Stable Video Diffusion를 백본으로 사용하여, 각 프레임 간의 포인트 커레스폰던스(correspondence)와 함께 오리지널 디퓨전 오브젝티브를 결합한 감독을 받습니다. 이 방법은 세 번째 디코더 블록의 업샘플러 레이어의 특징을 사용하여, 내부 diffusion 특징의 일관성을 높이는 동시에 공간적 일관성과 외관 일관성의 개선 정보를 제공합니다.
- ***Performance Highlights***: Track4Gen은 기존 SVD 대비 주제 일관성(subject consistency) 및 영상 품질 측면에서 최고의 점수를 기록했습니다. 외관 일관성 면에서 사용자 평가에서도 기존 방법들을 뛰어넘는 성능을 보였으며, 자사의 평가 환경에서도 향상된 비디오 생성의 품질을 보였습니다.

### [The BrowserGym Ecosystem for Web Agent Research](https://arxiv.org/abs/2412.05467)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05467.png)

Vote: 9

Authors: Dehan Kong, Frank F. Xu, Sahar Omidi Shayegan, Nicolas Chapados, Maxime Gasse, Megh Thakkar, Rim Assouel, Lawrence Keunho Jang, Ruslan Salakhutdinov, Alexandre Lacoste, Léo Boisvert, Xing Han Lù, Siva Reddy, Quentin Cappart, Alexandre Drouin, Graham Neubig, Massimo Caccia, Ori Yoran, Thibault Le Sellier De Chezelles, Tom Marty

- ***What's New***: 브라우저짐(BrowserGym)은 웹 에이전트 연구를 위한 통합된 환경을 제공하는 새로운 에코시스템입니다. 이 시스템은 자동화 및 대형 언어 모델(LLM)을 활용한 웹 인터랙션 작업의 평가를 지원합니다. 웹 에이전트의 행동을 통합된 방식으로 평가하여 연구자들이 더 신뢰할 수 있는 비교와 심층 분석을 할 수 있도록 돕습니다.
- ***Technical Details***: 브라우저짐 생태계는 웹 작업 자동화 및 LLM 기반 웹 에이전트 평가를 위한 직관적이며 유연한 인터페이스를 제공합니다. AgentLab은 에이전트의 생성, 테스트 및 분석을 지원하며, 다양한 벤치마크를 통합 관리합니다. 이를 통해 새로운 벤치마크와 에이전트를 쉽게 통합할 수 있습니다.
- ***Performance Highlights***: 최신 LLM에 대한 대규모 실험 결과, 클로드-3.5 소넷(Claude-3.5-Sonnet)이 다수 벤치마크에서 우수한 성능을 보였으며, 특히 WorkArena L2에서 39.1%의 높은 성공률을 기록했습니다. 그러나 AssistantBench에서는 낮은 성과를 보여, 브라우저짐 API가 액션 지향 작업에 더 잘 맞음을 시사합니다. GPT-4o도 업데이트 이후 성능이 개선되었습니다.

### [POINTS1.5: Building a Vision-Language Model towards Real World Applications](https://arxiv.org/abs/2412.08443)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08443.png)

Vote: 25

Authors: Jie Zhou, Yuan Liu, Le Tian, Kavio Yu, Xinyu Gao, Xiao Zhou, Yang Yu

- ***What's New***: POINTS1.5는 Vision-Language 모델의 성능을 강화하여 다양한 실제 응용 분야에서 활용할 수 있도록 설계된 새로운 모델입니다. POINTS1.0과 비교하여, 기존 CLIP Vision Encoder를 고해상도를 지원하는 NaViT 스타일 Encoder로 대체하고, 중국어 지원을 추가하여 중국어 이해능력을 크게 향상시켰습니다. 시각적 지시 조정 데이터세트(Visual Instruction Tuning Dataset)를 엄격하게 필터링하여 최적의 결과를 얻어냈습니다.
- ***Technical Details***: POINTS1.5는 LLaVA 스타일 아키텍처를 따르고 있으며, 고성능 NaViT 스타일 Vision Encoder를 사용하여 임의의 해상도 이미지를 타일링 없이 처리할 수 있습니다. 또한, 중국어 데이터 확장을 위해 자동 및 수작업으로 인터넷에서 이미지를 수집 및 주석 처리하였습니다. 시각적 지시 조정 데이터세트 필터링을 통해 문법 오류 및 이미지 참조 없이 답변 가능한 문제를 해결하여 데이터의 품질을 높였습니다.
- ***Performance Highlights***: POINTS1.5는 4억 토큰 이하로 훈련된 POINTS1.0보다 뛰어난 성능을 보이며, OpenCompass 리더보드에서 10억 파라미터 이하의 모델 중 1위를 기록했습니다. 특히 수학적 문제 해결 능력이 뛰어나 MathVista, MATH-Vision, MathVerse 등 다양한 벤치마크에서 우수한 성적을 거두었습니다.

### [3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark](https://arxiv.org/abs/2412.07825)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07825.png)

Vote: 11

Authors: Haoyu Chen, Wufei Ma, Alan Yuille, Jieneng Chen, Guofeng Zhang, Celso M de Melo

- ***What's New***: 3DSRBench는 최초의 포괄적인 3D 공간 추론 벤치마크로, 대형 멀티모달 모델(Large Multimodal Models; LMMs)의 다양하고 자연스러운 이미지에서 3D 공간 추론 능력을 평가합니다. 12가지 질문 유형으로 구성된 2,772개의 시각적 질문-답변 쌍이 포함되어 있으며, 균형 잡힌 데이터 분포와 새로운 FlipEval 전략을 채택하여 철저한 평가를 수행합니다.
- ***Technical Details***: 3DSRBench는 MS-COCO 데이터셋 내 12가지 질문 유형의 2,100개 시각적 질문-답변 쌍과 HSSD 데이터셋의 다중 보기 렌더링을 통한 두 가지 하위 집합으로 구성됩니다. 평가 전략으로는 CircularEval과 새로운 FlipEval 전략을 도입하여 좌/우 편향을 제거하고, 다양한 3D 시점에 대한 모델의 성능을 평가합니다.
- ***Performance Highlights***: 최신의 LMMs(예: GPT-4o, Gemini-Pro 등)는 인간 평가보다 평균적으로 -56% 정도 낮은 성능을 보이며, 특히 '높이', '위치', '방향', '다중 객체'와 같은 공간적 관계에서 약점을 드러냅니다. 많은 모델들이 비표준적 카메라 각도의 이미지에서 더욱 저하된 성능을 나타냅니다.

### [SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints](https://arxiv.org/abs/2412.07760)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07760.png)

Vote: 36

Authors: Ziyang Yuan, Haoji Hu, Jianhong Bai, Di Zhang, Zuozhu Liu, Pengfei Wan, Menghan Xia, Xiao Fu, Xintao Wang

- ***What's New***: SynCamMaster는 다양한 시점에서의 다중 카메라 동기화 비디오 생성을 위한 새로운 접근 방법을 제안합니다. 이 연구는 기존 4D 객체 생성에 초점을 두었던 것과 달리, 자유 시점에서의 개방형 장면 비디오 생성을 목표로 하며, 6 자유도 (6 DoF) 카메라 포즈를 포함하여 보다 일관된 콘텐츠를 보장합니다.
- ***Technical Details***: SynCamMaster는 사전에 학습된 텍스트-비디오 모델을 기반으로 플러그앤플레이 모듈을 활용하여 다중 카메라 비디오 생성을 향상시킵니다. 이 모델은 Transformer 블록에 통합된 다중 뷰 동기화 모듈을 통해 외관 및 기하학적 일관성을 유지합니다. 또한 Unreal Engine으로 렌더링된 다중 카메라 비디오와 단안 비디오 및 다중 카메라 이미지를 결합한 하이브리드 데이터 학습 방식을 설계하여 데이터 부족 문제를 극복합니다.
- ***Performance Highlights***: SynCamMaster는 다양한 시점에서 일관된 콘텐츠를 생성하며, 기존 기법 대비 훨씬 우수한 뷰 동기화를 달성합니다. 정량적 평가에서는 새로운 뷰 비디오 합성 (novel view video synthesis)에서도 뛰어난 성능을 보여주며, 다양한 실험을 통해 제안된 SynCamMaster가 기존 베이스라인보다 상당한 성능 향상을 보임을 확인하였습니다.

### [FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models](https://arxiv.org/abs/2412.08629)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08629.png)

Vote: 8

Authors: Inbar Huberman-Spiegelglas, Tomer Michaeli, Matan Kleiner, Vladimir Kulikov

- ***What's New***: FlowEdit는 사전 훈련된 플로우 모델(pre-trained flow models)을 사용하여 텍스트 기반 이미지 편집을 가능하게 하는 새로운 방법입니다. 이 방법은 인버션(inversion) 없이 최적화가 필요하지 않으며, 모델에 구애받지 않아 다양한 모델 간 전환이 용이합니다.
- ***Technical Details***: FlowEdit는 특정 ODE(ordinary differential equation)을 구축하여 소스로부터 타겟으로 직접적인 경로를 설정하고, 이를 통해 소스 이미지의 구조를 더 잘 보존할 수 있습니다. 이 접근 방식은 FLUX와 Stable Diffusion 3 (SD3)을 활용하여 각종 벤치마크에서 뛰어난 성능을 보여주었습니다.
- ***Performance Highlights***: FlowEdit는 이미지 편집 시 구조 보존 측면에서 인버션 기반 방식에 비해 낮은 전송 비용(transport cost)을 달성하며, 목표 텍스트 프롬프트에 대한 일관성을 유지함으로써 우수한 성능을 입증하였습니다. 다양한 실험을 통해, 기존 방법보다 더 잘 목표 분포를 따르는 이미지를 생성할 수 있음을 확인했습니다.

### [LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations](https://arxiv.org/abs/2412.08580)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08580.png)

Vote: 26

Authors: Guang Yang, Jinxiong Chang, Jiayi Li, Changyuan Yang, Shengyuan Zhang, Zejian Li, Yize Li, Jiarui Ma, Ling Yang, Chenye Meng, Lingyun Sun, Zhiyuan Yang

- ***What's New***: LAION-SG는 구조적 주석(structural annotation)과 함께 복잡한 이미지-텍스트 모델을 훈련할 수 있는 대규모 데이터셋입니다. 이는 텍스트에서 이미지로 변환하는(T2I) 모델이 복잡한 장면을 생성하는 데 있어서 데이터셋의 한계를 극복하기 위해 개발되어, 장면 그래프(scene graphs; SG)를 활용하여 이미지 속 객체들의 속성과 관계를 정밀하게 묘사합니다.
- ***Technical Details***: LAION-SG 데이터셋은 LAION-Aesthetics V2(6.5+)에서 확장된 것으로, 고품질의 복잡한 장면 그래프 주석을 포함하고 있습니다. SDXL-SG 모델은 GNN(graph neural network)을 사용하여 SG에서 구조를 추출하고 이 임베딩을 SDXL 모델의 U-Net에 통합하여 고품질의 복잡한 이미지를 생성합니다. 또한, 우리는 이 데이터셋을 통해 복합적 이미지 생성 평가를 위한 CompSG-Bench 벤치마크를 도입했습니다.
- ***Performance Highlights***: LAION-SG로 훈련된 모델은 기존 데이터셋으로 훈련된 모델보다 복잡한 장면 생성에서 뛰어난 성능을 보여줍니다. FID(Frechet Inception Distance)와 SG-IoU(Intersection over Union) 메트릭을 사용한 테스트 결과, LAION-SG 데이터셋이 기존의 COCO-Stuff 및 Visual Genome 데이터셋보다 더 효과적인 것으로 나타났습니다. 특히, 복잡한 장면 생성을 위한 새로운 기준을 설정하여 이미지 생성 모델의 성능을 향상시켰습니다.

### [Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel](https://arxiv.org/abs/2412.08467)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08467.png)

Vote: 0

Authors: Yali Wang, Limin Wang, Kunchang Li, Yi Wang, Yu Qiao, Yicong Hong, Songze Li, Zun Wang, Mohit Bansal, Jialu Li, Shoubin Yu

- ***What's New***: 이 논문은 자가 정제 데이터 플라이휠(Self-Refining Data Flywheel; SRDF)을 도입하여 고품질의 대규모 네비게이션 지시-경로 쌍을 생성하는 방법을 제시합니다. 인간의 주석 없이 지시 생성기와 네비게이터가 협력하여 데이터 풀을 반복적으로 정제하는 과정을 통해 대규모 언어 안내 네비게이션 학습 데이터를 지속적으로 개선합니다.
- ***Technical Details***: SRDF는 기본 생성기가 초기 데이터 풀을 생성하고, 이 데이터를 통해 트레이닝된 네비게이터를 이용해 데이터 풀을 필터링하여 더 높은 정확도의 데이터를 생성합니다. 이렇게 생성된 데이터는 다음 단계의 네비게이터를 교육하는 데 사용되며, 이는 고급 생성기를 트레이닝하여 더 높은 품질의 데이터를 제공하게 됩니다. 이 반복적인 프로세스를 통해 데이터의 자기정제 과정이 설정되고, 대규모 언어 인도를 위한 강력하고 효과적인 데이터 세트를 생성합니다.
- ***Performance Highlights***: SRDF의 실험 결과, 네비게이터는 몇 차례의 플라이휠 라운드를 통해 R2R 테스트 세트에서 성과 경계치를 70%에서 78% SPL로 향상시키며, 최초로 인간 성능(76%)을 초과했습니다. 또한, 생성기의 SPICE가 23.5에서 26.2로 증가하여 모든 이전 VLN 지시 생성 방법보다 우수한 성과를 보였습니다. 다양한 네비게이션 작업에서 사전 트레이닝된 네비게이터의 일반화 능력을 입증했으며, 모든 경우에서 최첨단 방법을 크게 능가했습니다.

### [Learning Flow Fields in Attention for Controllable Person Image Generation](https://arxiv.org/abs/2412.08486)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08486.png)

Vote: 16

Authors: Aditya Patel, Zijian Zhou, Shikun Liu, Tian Xie, Yuren Cong, Mengmeng Xu, Juan-Manuel Pérez-Rúa, Hang Li, Kam Woh Ng, Xiao Han, Haozhe Liu, Sen He, Tao Xiang, Miaojing Shi

- ***What's New***: 이 논문에서는 제어 가능한 인물 이미지 생성(Controllable Person Image Generation)을 개선하기 위해 주목(attention)에서 플로우 필드(flow fields)를 학습하는 새로운 접근법인 Leffa를 제안합니다. Leffa는 학습 시 주목 레이어에서 정확한 참조 이미지 영역을 올바르게 주목하도록 유도하는 정규화 손실을 도입하여 기존의 세부 디테일의 왜곡을 감소시킵니다. 이 방법은 가상 착용(virtual try-on)과 자세 전환(pose transfer)에서 최첨단 성능을 달성하며, 다양한 확산 기반(diffusion-based) 모델에 적용할 수 있는 모델-독립적인(model-agnostic) 특징을 가집니다.
- ***Technical Details***: Leffa는 확산 기반 모델(diffusion-based model)에 통합되어, 이미지 생성 중 참조 키(reference key)에 해당하는 지역을 정확하게 주목할 수 있도록 지도합니다. 이는 정확한 텍스처 유지와 높은 이미지 품질을 동시에 달성합니다. 사용된 정규화 손실은 학습 시 올바른 시각적 일관성을 보장하며 추가적인 추론 비용을 발생시키지 않습니다. 여러 가지 확산 모델에 Leffa 손실을 통합하여 성능을 향상시킬 수 있으며, 주목 맵(attention map)을 통해 플로우 필드(flow field)로 변환하여 이미지 재배치를 가능하게 합니다.
- ***Performance Highlights***: Leffa는 다양한 데이터셋에서 기존의 방법들에 비해 우수한 성능을 보였습니다. VITON-HD와 DressCode 데이터셋에서 가상 착용 평가에서 돈 넘하는 방법에 비해 더 나은 FID를 기록하였으며, 자세 전환 영역에서는 DeepFashion 데이터셋을 통해 더 낮은 FID와 더 높은 SSIM, LPIPS 성능을 보였습니다. 인물 이미지의 세부 디테일 왜곡을 크게 감소시키면서 시각적인 질적 향상을 달성하였습니다.

### [KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models](https://arxiv.org/abs/2412.06071)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06071.png)

Vote: 4

Authors: Sunghun Kim, Jing Tang, Chansung Park, Fan Wang, Juyong Jiang

- ***What's New***: KaSA(Knowledge-aware Singular-value Adaptation)은 대규모 언어 모델(LLMs)의 효율적인 파인튜닝(PEFT)을 위해 새로운 방법론을 도입합니다. Singual value decomposition(SVD)과 그에 기반한 singular values를 활용하여 특정 작업에 관련된 지식을 동적으로 활성화합니다.
- ***Technical Details***: KaSA는 두 가지 주요 단계를 포함합니다: 먼저, 작거나 덜 중요한 singular values와 관련된 노이즈 지식을 제거하기 위한 지식 기반의 SVD 절단을 수행합니다. 결과적으로 세계 지식을 보존하는 SVD-절단 모델을 얻습니다. 두 번째로, SVD 형식에서 사용자 정의 singular values를 사용하여 작업별 업데이트를 재파라미터화하여 모델의 일관성을 유지합니다.
- ***Performance Highlights***: KaSA는 16개의 벤치마크와 4개의 synthetic datasets에서 기존의 14가지 PEFT 기반 기법보다 일관되게 우수한 성능을 보여주었습니다. 또한 RoBERTa, DeBERTaV3, 그리고 GPT-2와 같은 다양한 LLM 아키텍처에서 NLU, NLG, 명령어 따르기, 상식 추론에서 모두 탁월한 성능을 달성했습니다.

### [Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction](https://arxiv.org/abs/2412.06234)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06234.png)

Vote: 9

Authors: Gyeongjin Kang, Eunbyung Park, Seungjun Oh, Younggeun Lee, Seungtae Nam, Xiangyu Sun

- ***What's New***: 이 논문에서는 새로운 기법인 Generative Densification을 소개합니다. 이는 피드-포워드 Gaussian 모델들이 생성한 구들을 효율적으로 조밀화하여 고품질의 일반화 가능한 3D 재구성을 가능하게 합니다. 이 방법은 대규모 다중뷰 데이터셋에서 배운 사전 지식을 활용하여 조밀화 단계를 한 번의 전방 패스로 수행합니다.
- ***Technical Details***: Generative Densification은 Gaussian 모델에서 대량의 뷰 공간 위치 그래디언트를 가진 상위 K개의 Gausssians를 선택하고 이를 조밀화 레이어 내에서 세분화합니다. 이후 선택되지 않은 Gaussian과 각각의 레이어에서 생성된 Gaussian을 결합하여 최종 구를 생성합니다. 이 방법은 효율적인 점-레벨 transformer[31]를 활용하여 메모리 사용량을 줄이고 계산상의 비효율성을 해결합니다.
- ***Performance Highlights***: 제안된 방법은 객체 수준 및 장면 수준의 재구성 작업에서 최첨단 방법보다 뛰어난 성능을 보이며, 특히 고주파수 세부 묘사에서 눈에 띄는 개선을 달성하였습니다. 또한, 다양한 데이터셋에 걸쳐 이미지 품질을 일관되게 향상시켜 강력한 일반화를 입증했습니다.

### [StyleMaster: Stylize Your Video with Artistic Generation and Translation](https://arxiv.org/abs/2412.07744)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07744.png)

Vote: 12

Authors: Zixuan Ye, Wenhan Luo, Di Zhang, Pengfei Wan, Huijuan Huang, Xintao Wang

- ***What's New***: StyleMaster는 비디오 스타일 전환(video style transfer)과 스타일화된 비디오 생성에서 탁월한 화질과 스타일 일관성을 보여주는 새로운 접근법을 제시합니다. 이 연구는 모델 환상을 통해 생성된 쌍 이미지 데이터셋을 사용하여 강력한 글로벌 스타일 추출기를 개발했으며, 스타일과 콘텐츠 간의 효과적인 분리를 달성하였습니다.
- ***Technical Details***: StyleMaster는 콘텐츠 누출을 막기 위해 텍스트 프롬프트와의 유사성이 낮은 패치를 선택하여 지역 텍스처(local texture)를 캡처하고, 모델 일루전(model illusion)을 통해 쌍 데이터와 대조 학습(contrastive learning)을 활용한 글로벌 스타일 추출기를 훈련합니다. 모션 어댑터(motion adapter)는 비디오의 동적 품질과 스타일 정도를 향상시킵니다.
- ***Performance Highlights***: StyleMaster는 기존 방법보다 스타일 일관성과 텍스트 정렬에서 더 나은 성능을 보여주며, 비디오 스타일 전환뿐만 아니라 텍스트-비디오 생성에서 우수한 비주얼 품질을 보였습니다. 특히 텍스트와 비디오의 정렬 측면에서 가장 높은 UMT 점수(2.329)를 기록했으며, 스타일 유사성은 0.463으로 나타났습니다.

### [StreamChat: Chatting with Streaming Video](https://arxiv.org/abs/2412.08646)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08646.png)

Vote: 12

Authors: Shihao Wang, Hongsheng Li, Rongyao Fang, Zhiding Yu, Shiyi Lan, Jan Kautz, Jihao Liu, Jose M. Alvare

- ***What's New***: StreamChat는 스트리밍 비디오 콘텐츠와의 상호작용을 향상시킨 새로운 접근법을 제안합니다. 기존의 LMMs(Large Multimodal Models)는 질문이 제기된 순간의 시각 정보만을 사용해 지연을 초래했지만, StreamChat은 각 디코딩 단계에서 시각적 컨텍스트를 지속적으로 업데이트하여 최신 비디오 콘텐츠를 반영합니다.
- ***Technical Details***: StreamChat은 크로스-어텐션 아키텍처(cross-attention-based architecture)를 도입하여 텍스트와 시각 토큰을 연결합니다. 이를 통해 스트리밍 시나리오에서 변화하는 비디오 입력을 효과적으로 처리합니다. 또한, 스트리밍 상호작용 모델의 훈련을 위한 촘촘한 인스트럭션 데이터(dense instruction dataset)를 사용하고, 텍스트와 비디오의 상대적 시간 정보를 인코딩하는 병렬 3D-RoPE 메커니즘을 소개합니다.
- ***Performance Highlights***: StreamChat은 기존 이미지와 비디오 벤치마크에서 경쟁력 있는 성능을 보였으며, 스트리밍 상호작용 시나리오에서는 최첨단 비디오 LMMs보다 우수한 성능을 나타냈습니다. 특히, StreamChat-7B 모델은 LLaVA-Video-72B 같은 대규모 LLMs를 능가하는 성능을 보여줍니다.

### [Mogo: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation](https://arxiv.org/abs/2412.07797)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07797.png)

Vote: 7

Authors: Dongjie Fu

- ***What's New***: Mogo는 텍스트 기반의 3D 인간 동작 생성 모델로, Bert 타입과 GPT 타입의 이점을 결합한 새로운 아키텍처입니다. 이 모델은 고품질의 생동감 있는 3D 인간 동작을 생성하며, 긴 시퀀스와 다양한 스타일의 생성이 가능합니다.
- ***Technical Details***: Mogo는 두 가지 주요 컴포넌트로 구성됩니다: 1) RVQ-VAE는 계층적 잔여 벡터 양자화 변분 오토인코더로, 높은 정밀도로 연속적 동작 시퀀스를 양자화합니다; 2) 계층적 Causal Transformer는 자동회귀 방식으로 기본 동작 시퀀스를 생성하며 다양한 계층에서 잔여를 추론합니다. 이는 relative positional encoding을 통해 시퀀스 길이 제한 문제를 해결하고, 자막을 통한 입력 최적화로 제로 샷 및 몇 샷 시나리오에서 성능을 향상합니다.
- ***Performance Highlights***: Mogo는 HumanML3D 테스트 세트에서 GPT 타입 모델인 T2M-GPT (FID=0.116) 및 AttT2M (FID=0.112), 그리고 BERT 타입 모델인 MMM (FID=0.080)을 능가하여 0.079의 Fréchet Inception Distance (FID) 점수를 기록하였습니다. 또한, CMP 테스트 세트의 제로 샷 평가에서 SOTA 수준의 성능을 달성하였습니다. 196 프레임(10초) 제한을 넘어서 최대 260 프레임(13초)의 연속적이고 순환적인 모션 시퀀스를 생성할 수 있습니다.

