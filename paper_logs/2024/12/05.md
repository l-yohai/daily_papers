## Daily Papers (2024-12-05)

### [LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting](https://arxiv.org/abs/2412.00177)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.00177.png)

Vote: 2

Authors: Anand Bhattad, Theo Gevers, Sezer Karaoglu, Xiaoyan Xing, Konrad Groh

- ***What's New***: LUMINET는 실내 장면의 조명을 효과적으로 전이할 수 있는 새로운 아키텍처로, 생성 모델과 잠재 내재 표현(latent intrinsic representations)을 활용합니다. 이는 복잡한 조명 현상을 포함한 다양한 장면 간 조명 전이를 성공적으로 수행하는 최초의 접근법입니다.
- ***Technical Details***: LUMINET는 StyleGAN 기반의 재조명 모델에서의 데이터 큐레이션 전략과, 소스 이미지의 잠재 내재 속성과 타겟 이미지의 잠재 외재 속성을 처리하는 수정된 diffusion 기반 ControlNet을 활용합니다. 조명 전이를 향상시키기 위해 학습된 어댑터(MLP)를 통해 타겟의 잠재 외재 속성을 크로스 어텐션으로 주입하고 세부 조정(fine-tuning)하는 방식입니다.
- ***Performance Highlights***: LUMINET은 MIT Multi-Illumination 데이터셋에서 이전 SOTA보다 20% 이상의 정량적 지표에서 뛰어넘는 성능을 보였습니다. 이는 복잡한 조명 전이에서의 현재 방법들보다 우수한 성능을 발휘하며, 단일 이미지 입력으로 다양한 조명 효과를 성공적으로 생성할 수 있음을 보여줍니다.

### [U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs](https://arxiv.org/abs/2412.03205)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03205.png)

Vote: 13

Authors: Vitaliy Polshkov, Alex Myasnikov, Vlad Stepanov, Ekaterina Artemova, Sergei Tilga, Konstantin Chernyshev, Alexei Miasnikov

- ***What's New***: U-MATH는 LLMs(대형 언어 모델)의 수학적 사고 능력을 대학 수준의 문제를 통해 평가하기 위한 세계 최초의 광범위한 벤치마크입니다. 이 벤치마크는 1,100개의 미공개 대학 수준 문제로 구성되어 있으며, 약 20%의 문제에 멀티모달 요소가 포함되어 있습니다.
- ***Technical Details***: U-MATH는 대학 수준의 6가지 핵심 주제로 균형을 이룹니다: 전미적 미적분학(Precalculus), 대수학(Algebra), 미분 적분학(Differential Calculus), 적분적 미적분학(Integral Calculus), 다변수 미적분학(Multivariable Calculus) 및 수열과 시리즈(Sequences &Series). µ-MATH는 이러한 문제의 메타 평가를 지원하기 위해 개발된 데이터셋으로, LLM이 생성한 솔루션과의 정확한 비교를 위해 제작되었습니다.
- ***Performance Highlights***: 일반 도메인 및 수학 특화 LLM을 평가한 결과, 텍스트 기반 문제에서는 최대 63%의 정확도를 달성했지만 시각적 문제는 45%에 불과했습니다. 솔루션 평가에서 가장 우수한 LLM 판독이 µ-MATH 벤치마크에서 F1-score 80%를 기록했습니다.

### [Imagine360: Immersive 360 Video Generation from Perspective Anchor](https://arxiv.org/abs/2412.03552)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03552.png)

Vote: 23

Authors: Ziwei Liu, Jing Tan, Jingwen He, Dahua Lin, Shuai Yang, Yuwei Guo, Tong Wu

- ***What's New***: Imagine360은 일반적인 관점 비디오를 360도 구 비디오로 변환하는 최초의 프레임워크로, 동적이고 몰입감 있는 영상 경험을 제공합니다. 기존의 텍스트 또는 이미지 기반의 360도 비디오 생성 방법들과 비교하여, Imagine360은 사용자 친화적이고 개인화된 콘텐츠 제작을 가능하게 합니다.
- ***Technical Details***: Imagine360은 듀얼 브랜치 디자인(dual-branch design)과 고유한 안티포달 마스크(antipodal mask), 해발 고도 인식 설계(elevation-aware designs)를 통해 고품질의 구형 비디오를 생성합니다. 이 프레임워크는 관점 비디오와 파노라마 비디오 디노이징 브랜치를 사용하여 다양한 각도에서의 비디오 마스킹을 관리하고, 진화하는 비디오 마스크를 처리하기 위한 고도 인식 설계를 제공합니다.
- ***Performance Highlights***: Extensive experiments demonstrate that Imagine360 generates 360-degree videos with superior visual and motion quality compared to state-of-the-art methods. 특히 프레임 품질과 모션 연속성에서 높은 평가를 받았습니다. 또한, 후처리 과정을 통해 더욱 향상된 파노라마 이미지 아웃페인팅 결과도 제공할 수 있습니다.

### [SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance](https://arxiv.org/abs/2412.02687)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.02687.png)

Vote: 55

Authors: Anh Tran, Viet Nguyen, Cuong Pham, Khoi Nguyen, Trung Dao, Toan Tran, Anh Aengus Nguyen

- ***What's New***: SNOOPI는 텍스트-이미지 변환 다중 단계를 한 단계로 증류하는 최신 프레임워크로, 변동성 개선 및 부정적인 요소 배제를 위한 새로운 방법을 제시합니다. 기존 기술의 문제점을 해결하며 더욱 강력하고 효율적인 증류 작용을 도모합니다.
- ***Technical Details***: SNOOPI는 두 가지 주요 전략을 포함합니다. 첫 번째로, 'Proper Guidance - SwiftBrush (PG-SB)'를 제안하는데, 이는 교사 모델의 출력 분포를 무작위로 유도하여 VSD 손실의 전반적인 안정성을 개선합니다. 두 번째로 'Negative-Away Steer Attention (NASA)'을 소개하여, 교차 주의 메커니즘을 통해 부정명령을 도입하지 않고도 원치 않는 이미지 특징을 효과적으로 억제합니다.
- ***Performance Highlights***: SNOOPI는 다양한 백본에서 강력한 성능 향상을 보여주었습니다. 특히 HPSv2 점수에서 31.08을 기록하여 단일 단계 확산 모델의 새로운 벤치마크를 설정했습니다. 이는 높은 품질의 텍스트-이미지 생성에서 현존하는 최고의 성능으로 평가됩니다.

### [Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion](https://arxiv.org/abs/2412.03515)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03515.png)

Vote: 21

Authors: Shengyuan Zhang, Ling Yang, Lingyun Sun, Tianrun Chen, Haoran Xu, Chenye Meng, AnYang Wei, Zejian Li, An Zhao, Perry Pengyun GU

- ***What's New***: 본 연구에서는 3D LiDAR 신(Scene) 완료를 위한 새로운 증류 기법을 제안합니다. ScoreLiDAR라는 이름의 이 방법은 효율적이면서도 높은 품질의 장면 완성을 가능하게 합니다. 특히 'Structural Loss'라는 새로운 손실 함수를 도입하여 모델이 3D LiDAR 장면의 기하학적 구조를 더욱 잘 반영할 수 있도록 설계되었습니다.
- ***Technical Details***: ScoreLiDAR는 기존의 Diffusion Model을 기반으로 증류된 모델로, Variational Score Distillation(VSD)를 활용하여 학습됩니다. 이 과정에서 구조적 손실(Structural Loss)은 장면 전체에 대한 제약(scene-wise term)과 중요한 랜드마크 포인트의 상대적 구성에 대한 제약(point-wise term)을 포함합니다. 모델은 초기의 시끄러운 데이터(Gt)에 대한 교차 엔트로피를 최소화하여 최적화됩니다.
- ***Performance Highlights***: 제안된 ScoreLiDAR 모델은 SemanticKITTI 데이터셋에서 장면 완성 시간을 30.55초에서 5.37초로 단축하였습니다. 또한 Chamfer Distance(CD)와 Jensen-Shannon Divergence(JSD)와 같은 지표에서 LiDiff 모델보다 우수한 성능을 보였습니다. 이를 통해 ScoreLiDAR는 효율성과 성능 간의 균형을 잘 맞췄음을 나타냅니다.

### [NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training](https://arxiv.org/abs/2412.02030)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.02030.png)

Vote: 11

Authors: Hmrishav Bandyopadhyay, Kai Zou, Dar-Yen Chen, Yi-Zhe Song

- ***What's New***: NitroFusion는 고품질의 이미지를 단일 단계에서 생성할 수 있는 새로운 접근법인 동적 적대적 훈련(Dynamic Adversarial Training; DAT)을 활용한 단일 단계 확산(Single-Step Diffusion) 방법을 제안합니다. 다양한 품질 측면에서 전문성을 갖춘 대규모 판별기 풀(pool of discriminators)을 사용하여 높은 충실도의 이미지를 단일 단계로 생성할 수 있습니다.
- ***Technical Details***: 동적 적대적 프레임워크(Dynamic Adversarial Framework)는 (i) 다양한 잡음을 다루는 전문 판별기 그룹으로 구성된 동적 판별기 풀(dynamic discriminator pool), (ii) 판별기 과적합(overfitting)을 방지하기 위한 전략적 갱신 메커니즘, (iii) 다중 규모의 훈련 전략(multi-scale training strategy)을 사용합니다. 이를 통해 단일 모델을 사용해 1-4단계의 디노이징(denoising)을 수행함으로써 질과 속도를 유동적으로 조정할 수 있습니다.
- ***Performance Highlights***: NitroFusion는 첨단 단일 단계 방법론들을 여러 평가 지표에서 뛰어넘는 성능을 보여주며, 특히 세부 디테일 유지 및 전반적인 일관성 측면에서 뛰어난 성능을 발휘합니다. 인체 평가 결과에서도 얼굴 디테일과 질감 보존 능력에서 우수한 결과를 확인했습니다.

### [AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning](https://arxiv.org/abs/2412.03248)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03248.png)

Vote: 20

Authors: Yiwu Zhong, Zhuoming Liu, Yin Li, Liwei Wang

- ***What's New***: 이 논문에서는 다중 모달 LLM (Multi-Modal LLMs)을 위한 학습이 필요 없는 적응형 추론 방법을 제안합니다. 이 방법은 최소한의 성능 저하로 다양한 효율성 요구 사항에 대응할 수 있습니다.
- ***Technical Details***: 본 연구에서 제안하는 방법은 다음 두 가지로 구성됩니다: a) 임베딩 유사성을 기반으로 한 LLM 이전의 반복적인 토큰 병합 기법, b) 다중 모달 중요성을 기반으로 한 LLM 층 내의 점진적인 토큰 가지치기 기법. 이 간소화된 설계는 이미지와 비디오 LLM 모두에 적용할 수 있습니다. 또한, 이 방법은 시각적 유사성을 기반으로 하는 토큰 병합과 다중 모달 중요성을 기반으로 하는 토큰 가지치기를 활용하여 각 층에서 점진적인 토큰 수 감소를 통해 LLM의 추론 효율성을 최적화합니다.
- ***Performance Highlights***: 본 방법은 다양한 비디오 및 이미지 벤치마크에서 연산 부하를 크게 줄이면서도 성능을 유지합니다. 예를 들어, FLOPs를 7배로 감소시키면서도 비디오 및 이미지 LLM의 성능을 보존합니다. 유사한 연산 비용 하에서, MLVU 벤치마크에서 SOTA 방법보다 4.6 더 나은 성능을 보입니다. 이 결과는 다중 모달 LLM에서 토큰의 중복성과 LLM 계층 동작에 대한 통찰력을 제공하며, 미래 연구에 방향성을 제공합니다.

### [Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding](https://arxiv.org/abs/2412.00493)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.00493.png)

Vote: 14

Authors: Liwei Wang, Shijia Huang, Duo Zheng

- ***What's New***: 이번 논문에서는 Video-3D LLM이라는 새로운 모델을 제안하였습니다. 이 모델은 3D 장면 이해를 위해 비디오 프레임과 3D 공간 좌표를 통합하여 3D 장면을 동적 비디오로 처리합니다. 이를 통해 비디오 표현을 실제 세계의 공간적 맥락과 보다 정확하게 정렬하며, 효율적인 성능을 지원하기 위해 최대 커버리지 샘플링 기술 또한 구현되었습니다.
- ***Technical Details***: Video-3D LLM은 사전 학습된 비디오 LLM을 기반으로 3D 위치 인코딩을 비디오 표현에 주입하여 포지션-어웨어(Position-Aware) 비디오 표현을 학습합니다. 프레임 샘플링에는 최대 커버리지 전략을 사용하여 메모리 사용 효율성을 향상시킵니다. 각 프레임은 깊이 이미지를 전역 좌표계로 변환하고, Sin PE 방식으로 인코딩된 3D-PE를 비디오 표현에 통합하게 됩니다.
- ***Performance Highlights***: Video-3D LLM은 ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, SQA3D 등 여러 3D 장면 이해 벤치마크에서 최첨단 성능을 발휘했습니다. ScanRefer에서는 Acc@0.25가 58.1%로 기존 최고 성능 모델 대비 향상되었고, Scan2Cap에서는 CIDEr 지표에서 83.8을 기록하여, 기존 모델을 능가하는 것을 보여주었습니다.

### [Mimir: Improving Video Diffusion Models for Precise Text Understanding](https://arxiv.org/abs/2412.03085)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03085.png)

Vote: 7

Authors: Dandan Zheng, Shuai Tan, Shuwei Shi, Jingdong Chen, Kecheng Zheng, Yujun Shen, Biao Gong, Ming Yang, Yutong Feng

- ***What's New***: Mimir는 기존의 비디오 확산 모델(Video Diffusion Models)이 텍스트 이해에서의 한계를 극복하기 위해 설계된 새로운 프레임워크입니다. 이 모델은 텍스트 인코더(Text Encoder)와 대규모 언어 모델(Large Language Models; LLMs)의 출력을 조화롭게 하여 텍스트-투-비디오(Text-to-Video; T2V) 생성에서 예측 정확성을 높이려는 목표를 가지고 있습니다.
- ***Technical Details***: Mimir는 비디오와 텍스트 토큰을 통합하기 위한 토큰 파서(Token Fuser)를 도입합니다. 이 토크 파서는 (1) 제로 컨볼루션(Zero-Conv) 계층을 사용하여 인코더 토큰과 디코더 전용 모델의 토큰을 융합하여 학습 초기 단계에서의 원래 의미 공간을 보존하고, (2) 안정적이지 않은 텍스트 기능을 조정하여 텍스트 및 비디오 생성에서의 의미적 안정성을 확보합니다.
- ***Performance Highlights***: VBench에서의 테스트 결과, Mimir는 다양한 지표에서 다른 최신 텍스트-투-비디오 모델을 능가하였습니다. 특히 다중 객체(Multiple Objects)와 공간 관계(Spatial Relationship) 지표에서 큰 개선을 보여 주며, 사용자 연구에서도 지시 따르기, 물리적 시뮬레이션, 비주얼 품질 측면에서 우수한 성능을 확인하였습니다.

### [MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation](https://arxiv.org/abs/2412.03558)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03558.png)

Vote: 10

Authors: Yunhan Yang, Xingqiao An, Lu Sheng, Yan-Pei Cao, Yuan-Chen Guo, Zehuan Huang, Zi-Xin Zou, Ding Liang, Xihui Liu, Yangguang Li

- ***What's New***: MIDI는 단일 이미지로부터 다중 3D 인스턴스를 효과적으로 생성 가능하게 하는 새로운 패러다임을 제공합니다. 이 방식은 기존의 다단계 객체별 생성과 달리 사전 학습된 이미지-to-3D 객체 생성 모델을 확장하여, 여러 3D 인스턴스를 동시에 생성하며 정확한 공간적 관계를 유지합니다.
- ***Technical Details***: MIDI는 다중 인스턴스 주의 메커니즘(Multi-Instance Attention Mechanism)을 도입하여, 생성 과정 중 객체 간의 상호작용과 공간적 일관성을 직접적으로 포착합니다. 이는 개별 객체 이미지와 글로벌 장면의 컨텍스트를 기반으로 객체 완성을 모델링하여 3D 생성을 수행합니다. 해당 메커니즘은 개별 객체의 3D 인스턴스를 학습할 때 교차 인스턴스 상호작용을 모델링하여 일관성 있는 장면 생성을 가능하게 합니다.
- ***Performance Highlights***: MIDI는 합성 데이터, 실제 세계 이미지, 스타일화된 장면 이미지에서 테스트되어 최고 수준의 성능을 입증했습니다. 특히, 기존 방법과 비교할 때 시간 소모가 크지 않으면서도 높은 해상도의 3D 장면을 생성하며, 강력한 일반화 능력을 보였습니다. 객체 간의 정확한 공간적 관계와 고품질 기하학 및 장면 배치를 달성합니다.

### [One Shot, One Talk: Whole-body Talking Avatar from a Single Image](https://arxiv.org/abs/2412.01106)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01106.png)

Vote: 12

Authors: Boyang Guo, Juyong Zhang, Yancheng Yuan, Yudong Guo, Leipeng Hu, Jun Xiang

- ***What's New***: 이번 연구는 단 한 장의 이미지로부터 전신 토킹 아바타(Talking Avatar)를 생성하는 새로운 기술을 제안합니다. 이 기술은 복잡한 동적 모델링과 미숙한 제스처 및 표정 일반화 문제를 해결하고자 합니다. 특히, 포즈 가이드 이미지-비디오 확산 모델(pose-guided image-to-video diffusion model)을 활용하여 초현실적인 비디오 프레임을 생성하는 새로운 파이프라인을 소개합니다.
- ***Technical Details***: 이 연구는 하이브리드 3DGS-메쉬 아바타 표현(coupled 3DGS-mesh avatar representation)을 도입하여 단일 이미지 입력과 불완전한 의사 레이블(pseudo-labels)을 통해 아바타를 훈련시킵니다. 기하학적 추정 및 표면 정규화를 결합하여 아바타의 정확한 모델링을 돕습니다. TED Gesture Dataset에서 대규모 포즈 데이터를 가공하여 다양한 제스처와 표정으로의 일반화를 시도하며, 이러한 움직임을 기반으로 사전 훈련된 전신 비디오 확산 모델과 3D 얼굴 애니메이션 모델을 사용하여 목표 인물의 다양한 비디오 시퀀스를 생성합니다.
- ***Performance Highlights***: 연구 결과는 단 한 장의 이미지에서도 사실적이고 정밀하게 애니메이션 가능한 토킹 아바타를 생성할 수 있음을 보여줍니다. 제안된 방법은 기존의 다중 시점 비디오 입출력을 필요로 하는 최첨단 기술보다 뛰어난 성능을 발휘하며, 특히 손과 얼굴의 디테일을 안정적으로 모델링하는 데 탁월합니다.

### [PaliGemma 2: A Family of Versatile VLMs for Transfer](https://arxiv.org/abs/2412.03555)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03555.png)

Vote: 55

Authors: Xiaohua Zhai, André Susano Pinto, Sahar Kazemzadeh, Lucas Beyer, Andreas Steiner, Alexey Gritsenko, Ibrahim Alabdulmohsin, Siyang Qin, Michael Tschannen, Matthias Minderer, Anthony Sherbondy, Daniel Keysers, Emanuele Bugliarello, Shangbang Long, Xiao Wang, Yonatan Bitton, Thomas Mesnard, Reeve Ingle

- ***What's New***: PaliGemma 2는 새로운 전이 학습을 위한 다재다능한 시각-언어 모델(Vision-Language Model; VLM) 패밀리로, Gemma 2 모델군을 기반으로 합니다. 기존보다 더 많은 전이 작업을 포함하며, OCR 관련 작업과 분자 구조 인식, 음악 악보 인식 등 다양한 새로운 작업에 대해 최첨단 성능을 기록했습니다.
- ***Technical Details***: PaliGemma 2 모델은 SigLIP-So400m 시각 인코더와 3B에서 28B 규모의 Gemma 2 언어 모델을 결합하여, 224px2부터 896px2까지 세 가지 해상도로 훈련되었습니다. 다단계 학습과 세밀한 튜닝을 통해 전이에 대한 성능을 극대화하였으며, 특히 모델 크기와 해상도가 전이 성능에 미치는 영향을 체계적으로 분석할 수 있도록 설계되었습니다.
- ***Performance Highlights***: PaliGemma 2는 다양한 전이 작업에서 PaliGemma를 능가하는 성능을 보였으며, 특히 더 큰 모델 사이즈에서는 상당한 향상을 달성했습니다. 896px2 해상도에서는 ICDAR’15 인지도와 Total-Text 인식에서 최신 기술을 능가하는 결과를 얻었습니다. 또한, 기계 컴퓨팅 환경에서 성능 저하 없이 정밀도를 낮춰 성공적으로 배포할 수 있었습니다.

### [NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images](https://arxiv.org/abs/2412.03517)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03517.png)

Vote: 13

Authors: Tianfan Xue, Jiale Xu, Zhaoyang Zhang, Lingen Li, Ying Shan, Weihao Cheng, Yaowei Li, Wenbo Hu, Xiaoyu Li, Jinwei Gu

- ***What's New***: NVComposer는 외부 정렬 과정 없이 여러 드문드문한 위치가 지정되지 않은 이미지를 사용하여 새로운 뷰를 생성할 수 있는 혁신적인 생성적 새로운 뷰 합성(Novel View Synthesis; NVS) 모델입니다. 이는 이미지 포즈의 이중 스트림 확산 모델(image-pose dual-stream diffusion model)과 사전 학습된 밀집 스테레오 모델에서 기하학적 정보를 추출하는 기하학적 특성 정렬 모듈(geometry-aware feature alignment module)을 도입하여 달성됩니다.
- ***Technical Details***: 이 모델은 두 가지 주요 구성 요소를 포함합니다: 첫째, 이미지 포즈의 이중 스트림 확산 모델은 목표 뷰를 생성하면서도 조건 이미지를 위한 카메라 포즈를 암묵적으로 추정합니다. 둘째, 기하학적 특성 정렬 모듈은 훈련 중 밀집 스테레오 모델에서 기하학적 사전 정보를 추출하여 사용합니다. 이 방법은 복잡한 장면에 대한 외부 정렬 과정을 요구하지 않으며, 드문드문한 뷰와 큰 가림(occlusion)에도 유연하게 NVS를 수행할 수 있습니다.
- ***Performance Highlights***: NVComposer는 RealEstate10K 및 DL3DV 데이터셋에서 여러 최신 모델을 능가하는 성능을 보여주었습니다. 특히, 다양한 관점을 제공받을수록 성능이 향상되며, 이는 모델이 제공된 미지정 뷰에서 가용 정보를 암묵적으로 활용할 수 있는 능력을 가리킵니다. FVD와 KVD 등 분포 평가를 통해도 NVComposer는 더 정확한 새로운 뷰를 생성함을 입증하였습니다.

### [Weighted-Reward Preference Optimization for Implicit Model Fusion](https://arxiv.org/abs/2412.03187)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03187.png)

Vote: 4

Authors: Ziyi Yang, Xiaojun Quan, Tianyuan Shi, Fanqi Wan, Longguang Zhong

- ***What's New***: WRPO(Weighted-Reward Preference Optimization)는 서로 다른 대형 언어 모델(LLMs; Large Language Models) 간의 복잡한 어휘 정렬(vocabulary alignment)이나 분포 행렬 병합의 필요성을 제거하여, 다양한 구조와 크기의 모델을 효과적으로 융합할 수 있는 방법을 제안합니다. 이는 개방형 LLMs에서 얻어진 선호도를 타겟 LLM에 최적화하여 전이하는 방식으로, 기존의 모델 융합 방법보다 효율적입니다.
- ***Technical Details***: WRPO는 타겟 LLM과 출처 LLM 간의 분포적 편차를 해결하기 위한 점진적 적응 전략을 도입합니다. 이는 선호하는 예시를 점진적으로 타겟 LLM으로부터 출처 LLM으로 이전하는 방식으로 이루어집니다. 실험은 MT-Bench, AlpacaEval-2, Arena-Hard 벤치마크에서 진행되었으며, LLaMA3-8B-Instruct를 타겟 모델로 사용해 WRPO가 기존 방법들을 꾸준히 능가한다는 것을 입증했습니다.
- ***Performance Highlights***: WRPO는 AlpacaEval-2에서 GPT-4-Preview-1106와 비교하여 길이 조정된 승률 55.9%, Arena-Hard에서 GPT-4-0314와 비교하여 46.2%의 승률을 기록했습니다. 이는 WRPO가 이질적 LLMs의 다양한 능력에서 학습할 수 있도록 하면서도 분포적 이동 문제를 해결할 수 있음을 강조합니다.

### [Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models](https://arxiv.org/abs/2412.02980)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.02980.png)

Vote: 4

Authors: Vera Zisler, Mohammed Hamdy, Maia Iyer, Andrew Dai, Sharath Chandra Raparthy, Chase Blagden, Koen Oostermeijer, Baber Abbasi, Wen-Ding Li, Duy Phung, Pawan Sasanka Ammanamanchi, Elliot Meyerson, Giovanni Paolini, Laura O'Mahony, Alon Albalak, Dakota Mahan, Srishti Gureja, Alex Havrilla, Fabrizio Milo, Kanishk Gandhi

- ***What's New***: 이번 논문은 대규모 언어모델(LLM; Large Language Models)을 통한 합성 데이터 생성에서 품질, 다양성, 복잡성이 모델 성능에 미치는 영향을 조사한 연구입니다. 이는 합성 데이터를 구성하는 주요 요소의 정의와 측정 방법을 제안하며, 이러한 요소들이 모델의 일반화 능력에 어떻게 기여하는지를 분석합니다.
- ***Technical Details***: 논문에서는 합성 데이터의 품질(Quality), 다양성(Diversity), 복잡성(Complexity; QDC)에 대한 정의를 내리고, 이들의 측정 및 모델 성능에 미치는 영향을 조사합니다. 품질은 데이터가 목표 분포에 얼마나 일치하는지를 나타내고, 다양성은 데이터 셀프 유사성을, 복잡성은 데이터의 난이도와 구성 가능성을 측정합니다. QDC는 합성 데이터 생성 알고리즘을 평가하는 중요한 지표로 제안되었습니다.
- ***Performance Highlights***: 본 연구는 데이터 품질이 모델의 분포 내(in-distribution) 일반화에 중요하며, 다양성이 분포 외(out-of-distribution) 일반화에 본질적 역할을 하고, 복잡성이 양측 모두에 유익하다는 점을 강조합니다. 또한, 학습 데이터의 품질과 다양성 사이의 상충관계(trade-off)는 모델 성능에 직간접적 영향을 미칩니다.

### [VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models](https://arxiv.org/abs/2411.19103)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.19103.png)

Vote: 11

Authors: SunYoung Park, Youngjune Kim, Jeongho Ju, Daeyoung Kim

- ***What's New***: VARCO-VISION은 한국어와 영어 간의 시각-언어 모델(Vision-Language Model; VLM)로서, 공개 코드를 기반으로 한 최초의 한국어-영어 VLM입니다. 이 모델은 단계별 학습 전략을 도입하여 언어적, 시각적 정보를 동시에 학습하면서도 기존 모델의 지식을 유지합니다. VARCO-VISION은 다중모드에서 이미지-텍스트 이해 및 생성을 포함하여 우수한 성능을 발휘하고, 이미지 기반 태스크에서의 질의응답(Visual Question Answering), 객체 지칭(Referring), 광학 문자 인식(OCR) 등의 다양한 응용 가능성을 보여줍니다. 또한 다섯 개의 한국어 평가 데이터셋을 공개하여 연구자들이 VLM을 훈련할 수 있는 기회를 확대했습니다.
- ***Technical Details***: VARCO-VISION-14B는 크게 비전 인코더(Vision Encoder), 프로젝터(Projector), 그리고 대형 언어 모델(LLM)로 구성된 구조를 가지고 있습니다. SigLIP을 비전 인코더로 사용하고, LLaVA-OneVision의 학습 프레임워크를 확장하여 단일 이미지 기반 예제들에 모델을 훈련했습니다. 각 이니셜 문장에서 사용된 특수 토큰은 OCR 태스크에서의 <ocr>, 객체 지칭에서의 <obj>, 깃박스의 <bbox>와 같은 태스크별 용도로 적용되었습니다. 전반적인 학습은 4단계로 진행되었으며, 모델의 언어적, 시각적 능력을 점진적으로 발전시켰습니다.
- ***Performance Highlights***: VARCO-VISION-14B는 한국어와 영어의 문법적 이해 및 체계를 정확히 수행하는데 뛰어난 성과를 보였으며, 한국어 MCQA 벤치마크에서 유사 규모 모델들에 비해 월등한 성과를 기록했습니다. K-DTCBench에서의 우수한 성과는 다른 모델들이 처리하기 어려운 한국어 문서, 표, 차트를 효과적으로 학습했음을 시사합니다. OCR 태스크에서도 VLM들과 비교하여 탁월한 성능을 발휘하는 것으로 나타났으며, 텍스트 평가 벤치마크에서는 EXAONE 3.0 7.8B에 필적하는 성과를 보였습니다.

### [TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation](https://arxiv.org/abs/2412.03069)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03069.png)

Vote: 18

Authors: Xu Wang, Hu Ye, Yiheng Liu, Zehuan Yuan, Xinglong Wu, Yi Jiang, Liao Qu, Yiming Gao, Daniel K. Du, Huichao Zhang

- ***What's New***: TokenFlow는 시각 정보의 멀티모달 이해와 생성 사이의 격차를 메우는 획기적인 이미지 토크나이저입니다. 기존의 단일재구성 지향형 벡터 양자화(VQ) 인코더와 달리, TokenFlow는 이중-코드북 아키텍처를 도입하여 시맨틱 및 픽셀 수준의 특징 학습을 분리하지만, 이들을 공유 맵핑 메커니즘으로 정렬합니다. 이를 통해 이해를 위한 높은 수준의 시맨틱 표현과 생성을 위한 세밀한 시각적 특징 모두에 직접 접근할 수 있습니다.
- ***Technical Details***: TokenFlow는 이중 인코더 아키텍처와 각기 특화된 코드북을 채택합니다. 시맨틱 인코더는 CLIP 스타일의 교사 모델에서 학습한 강력한 시맨틱 기반을 제공하고, 픽셀 인코더는 세밀한 시각 정보를 캡처합니다. 이는 시맨틱 및 픽셀 수준의 거리 계산을 통해 양자화된 특징을 생성하는데, 최적화된 거리의 가중치 합을 최소화하여 양자화 인덱스를 결정합니다. 우리의 아키텍처는 코드북 사용률을 95% 이상 유지하면서 대규모 코드북에서도 뛰어난 확장성을 보입니다.
- ***Performance Highlights***: TokenFlow는 LLaVA-1.5 13B를 멀티모달 이해 성능에서 처음으로 7.2% 개선했으며, 이미지 재구성에서는 384x384 해상도에서 0.63의 FID 점수를 달성했습니다. 또한, TokenFlow는 256x256 해상도의 자가회귀 이미지 생성에서 최첨단 성능을 기록하며 SDXL과 유사한 결과를 보여줄 수 있었습니다. 이 접근 방식은 기존의 오토리그레시브 기반 방법보다 훨씬 적은 샘플링 스텝으로 더 높은 성능을 달성합니다.

### [CleanDIFT: Diffusion Features without Noise](https://arxiv.org/abs/2412.03439)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03439.png)

Vote: 9

Authors: Stefan Andreas Baumann, Nick Stracke, Frank Fundel, Björn Ommer, Kolja Bauer

- ***What's New***: CleanDIFT는 대규모 사전 학습된 확산 모델(Diffusion Models)에서 노이즈가 없는 시맨틱 기능을 추출하는 새로운 경량 비지도 학습 미세 조정 기법입니다. 이 메서드는 확산 백본을 통해 고품질의 노이즈 없는 시맨틱 기능을 제공하여 다양한 다운스트림 작업에서 더 나은 성능을 제공합니다.
- ***Technical Details***: CleanDIFT는 사전 훈련된 확산 모델의 내부 표현을 정렬하여 타임스텝(timestep) 비종속 잡음 없는 전반적 용도의 확산 기능을 생성합니다. 이미지에 노이즈를 추가하지 않고도 기능을 추출할 수 있도록 미세 조정되며, 30분 이내에 단일 A100 GPU에서 훈련이 완료됩니다. 모델은 여러 타임스텝 의존 기능 추출기를 단일 타임스텝 비종속 기능 예측으로 통합하여, 다운스트림 작업별로 타임스텝을 조정할 필요를 없앱니다.
- ***Performance Highlights***: 본 연구에서는 CleanDIFT를 다양한 다운스트림 작업, 예를 들어, 무감독 제로샷 시맨틱 대응, 단안 깊이 추정, 시맨틱 세분화 및 분류 작업에 대해 평가하였고, 기존의 확산 기능 기반 접근 방법보다 성능이 크게 개선되었습니다. 특히, 시맨틱 대응 작업에서 새로운 무감독 기준을 세웠으며, Noise 또는 타임스텝 앙상블링의 필요성을 제거하여 실질적인 속도 향상이 이루어졌습니다.

### [Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning](https://arxiv.org/abs/2412.03565)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03565.png)

Vote: 5

Authors: Tao Gui, Yu-Gang Jiang, Yitong Chen, Yiweng Xie, Yang Liu, Wujian Peng, Xipeng Qiu, Hang Xu, Zuxuan Wu, Lingchen Meng

- ***What's New***: 이 연구는 멀티모달 인스턴스 이해(Multimodal Instance Understanding)를 개선하기 위한 INST-IT라는 새로운 접근 방식을 소개합니다. INST-IT는 인스턴스 지향 벤치마크(Instance-Centric Benchmark), 대규모 인스트럭션 튜닝 데이터셋(Instruction-Tuning Dataset), 그리고 지속적인 인스트럭션 튜닝 훈련 방식을 포함하여, 시공간적 인스턴스 이해를 효과적으로 증진시키는 해결책을 제공합니다.
- ***Technical Details***: INST-IT는 GPT-4o를 사용하여 명시적 시각적 프롬프트(Explicit Visual Prompting)를 통해 이미지와 비디오에서 인스턴스 수준 정보를 추출하는 자동 주석 파이프라인을 도입합니다. 이를 바탕으로 21k개의 비디오와 51k개의 이미지를 포함하는 INST-IT Dataset을 제작했습니다. 이 데이터셋은 총 207k 개의 프레임 수준 주석과 21k 개의 비디오 수준 주석, 335k 개의 자유형 질문 답변 쌍(Open-Ended Question Answer Pairs)을 포함하고 있습니다.
- ***Performance Highlights***: INST-IT를 통한 모델은 INST-IT Bench에서 뛰어난 성능을 보였을 뿐만 아니라 다양한 이미지 및 비디오 이해 벤치마크에서도 상당한 성능 향상을 나타냈습니다. 이는 데이터셋이 인스턴스 수준 이해를 증진시킬 뿐만 아니라 일반적인 이미지 및 비디오 이해 역량도 강화한다는 것을 보여줍니다.

