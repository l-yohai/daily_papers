## Daily Papers (2024-12-25)

### [ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing](https://arxiv.org/abs/2412.14711)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14711.png)

Vote: 7

Authors: Jianfei Chen, Jun Zhu, Ziteng Wang

- ***What's New***: ReMoE는 기존의 비연속적이고 비분화성의 TopK 라우팅을 대체하기 위해 ReLU 함수를 사용하는 완전히 차별 가능한 Mixture-of-Experts(MoE) 아키텍처를 제안합니다. 이는 모델의 성능 및 확장성을 제한했던 기존 라우팅 방식을 개선하여 계산 자원의 효율적 배분 및 도메인 전문화를 가능하게 합니다.
- ***Technical Details***: ReMoE의 핵심은 ReLU 라우팅으로, 각 전문가의 활성 상태를 ReLU 게이트를 통해 직접 제어하며, 이를 통해 연속적이며 완전히 차별 가능하게 만듭니다. 또한, ReMoE는 L1 정규화를 사용하여 라우터의 희소성을 조절하고 전문가 간의 부하를 균형 잡습니다. ReMoE는 다양한 모델 크기, 전문가 수, 그리고 세분화 수준에서 일관되게 성능을 향상시킵니다.
- ***Performance Highlights***: ReMoE는 다양한 실험에서 전통적인 MoE 아키텍처를 능가했으며, 전문가 수가 늘어날수록 더 가파르게 성능이 향상되었습니다. 또한, TopK-routed MoE 모델 대비 거의 동일한 학습 및 추론 속도를 달성하며, 뛰어난 성능과 효율성을 입증하였습니다.

### [Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization](https://arxiv.org/abs/2412.17739)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17739.png)

Vote: 16

Authors: Youbang Sun, Bowen Zhou, Biqing Qi, Kaiyan Zhang, Xue Kai Zhu, Yuchen Fan, Che Jiang, Ermo Hua, Xingtai Lv, Ning Ding

- ***What's New***: Fourier Position Embedding(FoPE)는 Attention의 주기성을 개선하고 Length Generalization을 향상시키기 위해 제안되었습니다. 기존의 Rotary Position Embedding(RoPE)에서 발생하는 문제점을 보완하여, 주파수 영역 속성을 개선합니다.
- ***Technical Details***: FoPE는 각 차원을 다중 주파수 구성 요소로 모델링하여 스펙트럼 손상을 최소화합니다. 이에 비해 RoPE는 단일 주파수로 취급하기 때문에 스펙트럼 손상 문제를 겪습니다. FoPE는 Fourier Series를 구성하고, 부적절하게 학습된 주파수 구성 요소들을 제거함으로써 모델의 스펙트럼 손상에 대한 견고성을 높입니다. 또한, 매우 낮은 주파수의 구성 요소는 0으로 대체하여 주기적 확장을 유지합니다.
- ***Performance Highlights***: FoPE는 RoPE 및 ALiBi와 비교했을 때, Needle-in-haystack 태스크에서 더 안정적이고 일관된 정확도를 보여주는 수많은 실험을 통해 뛰어난 성능을 입증했습니다. 실험 결과, C4 데이터셋에서의 Perplexity와 Passkey Retrieval의 정확도에서 보다 우수한 성과를 냈습니다.
- ***Conclusion***: FoPE는 주파수 영역의 분석을 통해 거의 모든 언어 모델 부품의 부정적 영향을 밝혀내고 이를 능가하는 성능을 보여 길이 일반화를 크게 개선합니다. Spectral Damage에 대한 견고성을 높이고, 주기적 확장을 개선하여 RoPE 기반 모델보다 뛰어난 성능을 입증하였습니다.

### [MotiF: Making Text Count in Image Animation with Motion Focal Loss](https://arxiv.org/abs/2412.16153)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16153.png)

Vote: 1

Authors: Saketh Rambhatla, Chen Sun, Rohit Girdhar, Shijie Wang, Samaneh Azadi, Xi Yin

- ***What's New***: MotiF는 동작 초점 손실(Motion Focal Loss)을 도입하여 모델 학습이 더 많은 움직임이 있는 영역에 집중하도록 하여 텍스트 정렬과 모션 생성 개선을 목표로 하는 접근법을 제안합니다. 이와 함께 다양한 수준의 평가를 위한 320개의 이미지-텍스트 쌍으로 구성된 TI2V Bench 벤치마크를 제공합니다.
- ***Technical Details***: MotiF는 광류(optical flow)를 사용하여 동작 히트맵(motion heatmap)을 생성하고, 손실을 동작 강도에 따라 가중치를 다르게 부여함으로써 모델이 동작이 큰 영역에 집중하여 학습할 수 있도록 합니다. 동영상 생성을 위한 기초 모델로는 사전 학습된 VideoCrafter2를 사용하며, 이미지 조건을 소음이 섞인 동영상과 결합하여 정합성을 높였습니다.
- ***Performance Highlights***: MotiF는 9개의 오픈 소스 모델과의 비교 실험에서 평균 72%의 선호도를 얻으며, 특히 텍스트 정렬과 객체 움직임에서 두드러진 성능 향상을 보여줍니다.

### [Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning](https://arxiv.org/abs/2412.15797)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15797.png)

Vote: 1

Authors: Edward Choi, Yeyun Gong, Xiao Liu, Sungjin Park

- ***What's New***: 새로운 언어 모델 앙상블 방법인 LE-MCTS가 도입되었습니다. 이는 프로세스 보상 기반의 트리 탐색을 통해 복잡한 추론 문제를 더 효과적으로 해결할 수 있도록 설계된 혁신적인 방법입니다.
- ***Technical Details***: LE-MCTS는 마르코프 결정 프로세스(MDP; Markov Decision Process)에 기반하여 문제 해결의 각 단계를 다양한 언어 모델(Language Models; LMs)로 생성합니다. 이 프레임워크에서는 상태가 중간 추론 경로를 나타내고, 각 단계는 사전 정의된 언어 모델 풀에서 선택된 하나의 모델을 사용하여 생성됩니다. 이 과정에서, 프로세스 기반 보상 모델이 MCTS(Monte Carlo Tree Search)를 안내하여, 가장 정확한 추론 체인을 발견하는 데 도움을 줍니다.
- ***Performance Highlights***: 수학적 추론 벤치마크인 MATH와 MQA 데이터셋에서 LE-MCTS는 각각 3.6%, 4.3%의 성능 향상을 보여줍니다. 이는 LE-MCTS가 기존의 단일 언어 모델 해독 알고리즘 및 기존 앙상블 방법보다 복잡한 문제 해결에 더 효과적임을 강조합니다.

### [DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation](https://arxiv.org/abs/2412.18597)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18597.png)

Vote: 10

Authors: Xiaodong Cun, Zhaoyang Zhang, Wenze Liu, Ying Shan, Yong Zhang, Minghong Cai, Xiangyu Yue, Xiaoyu Li

- **What's New**: DiTCtrl는 다중 프롬프트 기반 긴 비디오 생성을 위한 새로운 방식으로 Multi-Modal Diffusion Transformer(MM-DiT) 아키텍처를 활용한 최초의 튜닝 프리 비디오 생성 방법입니다. 이 방법은 추가 학습 없이 매끄러운 장면 전환과 일관된 객체 움직임을 보장합니다.
- **Technical Details**: DiTCtrl은 MM-DiT 아키텍처의 주목 메커니즘을 분석하여, 3D 풀 어텐션이 UNet 유사 확산 모델에서의 교차/자기 주목 블록과 유사한 동작을 보여줍니다. 이를 기반으로 마스크 기반의 정밀한 의미 제어를 가능하게 하며, 여러 프롬프트 간 심리스 전환을 위한 KV-공유 메커니즘과 잠재 혼합 전략을 적용합니다. 또한 MPVBench란 새로운 벤치마크를 제안하여 다중 프롬프트 비디오 생성 성능을 평가합니다.
- **Performance Highlights**: DiTCtrl는 추가 학습 없이 최신 성능을 달성하며, 동일 기술 기반의 다른 모델 대비 다중 시나리오에서의 텍스트-비디오 정렬, 시간 일관성 및 동작 품질에서 우수한 성능을 입증하였습니다. MPVBench에서의 점수와 사용자 연구 결과도 다른 방법보다 높은 선호도로 나타났습니다.

### [PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models](https://arxiv.org/abs/2412.18608)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18608.png)

Vote: 5

Authors: David Novotny, Jianyuan Wang, Roman Shapovalov, Andrea Vedaldi, Minghao Chen, Tom Monnier, Iro Laina

- ***What's New***: PartGen은 텍스트, 이미지 또는 비구조화된 3D 객체에서 출발하여 의미 있는 부분으로 구성된 3D 객체를 생성하는 새로운 접근 방식을 소개합니다. 이 방법은 여러 보기를 통해 3D 객체를 세분화하고, 다른 멀티 뷰 확산 모델이 각 부분을 3D로 완성하고 재구성하는 것을 가능하게 합니다.
- ***Technical Details***: PartGen은 실질적으로 부분적인 3D 세분화와 재구성을 기반으로 합니다. 여러 보기에서 동일한 부분을 색상 코드로 매핑하여 세분화를 수행하며, 그러한 세분화된 부분들을 차례로 3D 재현 네트워크에 공급하여 전체 3D 객체로 재구성합니다. 이때 각 부분이 서로 잘 조화롭게 이루어지도록 전체 객체의 문맥을 고려합니다.
- ***Performance Highlights***: PartGen은 여러 입력 형식에 적용 가능하며, 생성된 3D 자산과 실제 3D 자산에서 전통적인 세분화와 부분 추출 기반선을 크게 능가하는 성능을 보여줍니다. 또한 텍스트 기반의 3D 부분 편집이 가능하여 3D 객체 생성의 유연성과 제어를 향상시킵니다.

### [DepthLab: From Partial to Complete](https://arxiv.org/abs/2412.18153)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18153.png)

Vote: 21

Authors: Ka Leong Cheng, Yujun Shen, Bin Tan, Qiuyu Wang, Kai Zhu, Qifeng Chen, Shuzhe Wang, Hao Ouyang, Ping Luo, Zhiheng Liu

- ***What's New***: DepthLab는 기존의 깊이(Depth) 데이터를 완전하게 보완하기 위한 근본적인 깊이 인페인팅 모델입니다. 이 모델은 3D 재구성, LiDAR 깊이 완성, 텍스트-장면 생성 등 여러 다운스트림 적용 분야에서 우수한 성능을 보입니다.
- ***Technical Details***: DepthLab는 RGB 이미지 및 알려진 깊이를 조건으로 하는 깊이 인페인팅을 위한 듀얼-브랜치(Dual-branch) 디퓨전 프레임워크를 도입합니다. 참조 U-Net은 RGB 특징을 추출하여 깊이 추정 U-Net에 통합하여 인페인팅 과정을 안내합니다. 훈련 시, 여러 형태의 마스킹 전략을 병행 적용하여 다양한 시나리오를 처리할 수 있도록 합니다.
- ***Performance Highlights***: DepthLab는 다양한 데이터셋에서 기존 방법들에 비해 뛰어난 퍼포먼스를 보이며, 절대 다수의 경우에서 최적의 성능 지표를 보였습니다. NYUv2, KITTI, ETH3D 등에서의 실험적 비교에서, DepthLab는 적은 훈련 데이터로도 높은 일관성과 정확도를 유지하며, Zero-shot 설정에서도 우수한 결과를 보여줍니다.

### [3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding](https://arxiv.org/abs/2412.18450)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18450.png)

Vote: 21

Authors: Dmitry Yudin, Tatiana Zemskova

- ***What's New***: 3DGraphLLM은 3D 장면 그래프의 학습 가능한 표현을 만들어 대형 언어 모델(LLM)에 입력하도록 하는 새로운 방법을 제안합니다. 이 방법은 대화형 로봇 에이전트가 자연어로 되어 있는 다양한 질의에 적절히 대응할 수 있도록 설계되었습니다.
- ***Technical Details***: 3DGraphLLM은 학습 가능한 임베딩을 사용하여 장면의 객체와 이웃 간의 관계를 트리플렛으로 표현합니다. 각 객체는 k-최근접 이웃의 부분 그래프로 표현되며, VL-SAT와 같은 최첨단 3D 의미론적 그래프 생성 방법을 활용하여 그래프의 의미론적 관계를 임베딩합니다. 학습 과정은 두 단계로 이루어지며, 먼저 GT 점 구름 장면 분할 데이터로 사전 학습한 후, 예측된 인스턴스 분할 데이터를 사용하여 모델을 미세 조정합니다.
- ***Performance Highlights***: 3DGraphLLM은 Multi3DRefer 벤치마크에서 +5.8% F1@0.5, ScanRefer에서는 +4.4% Acc@0.5로 기존 방법들을 뛰어넘는 성능을 보였습니다. Scan2Cap 데이터셋에서는 CIDEr@0.5에서 5.8% 향상된 결과를 기록하여 우수한 성능을 증명했습니다.

### [Bridging the Data Provenance Gap Across Text, Speech and Video](https://arxiv.org/abs/2412.17847)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17847.png)

Vote: 1

Authors: Nikhil Singh, Campbell S. Lund, Sara Hooker, Kun Qian, Ariel N. Lee, Tobin South, William Brannon, Kevin Klyman, Vivek Sharma, Vipul Gupta, Christopher Klamm, Yizhi Li, Jad Kabbara, Jianguo Zhang, Lester JV Miranda, Mohammed Hamdy, Kushagra Tiwary, Damien Sileo, Niklas Muennighoff, Manuel Cherep, Joanna Materzynska, Shrestha Mohanty, Luis Villa, Ahmad Mustafa Anis, An Dinh, Caiming Xiong, Deividas Mataciunas, Diganta Misra, Nayan Saxena, Seungone Kim, Shayne Longpre, Robert Mahari, Naana Obeng-Marnu, Stella Biderman, Minnie Liang, Emad A. Alghamdi, Manan Dey, Seonghyeon Ye, Da Yin, Enrico Shippole, Vu Minh Chien, Alex Pentland, Xuhui Zhou

- ***What's New***: 이 논문은 데이터의 근원을 조사하여 AI 시각, 음성, 텍스트의 다중 모드 모델의 데이터 출처를 감시하는 최초의 장기적인 연구를 수행했습니다. 저자들은 1990년부터 2024년까지의 다양한 공개 데이터셋들, 608개의 언어 및 67개국을 포함한 659개의 조직에서 제공된 거의 4000개의 데이터셋을 조사했습니다.
- ***Technical Details***: 연구는 웹에서 크롤링한 데이터, 합성된 데이터, 그리고 소셜 미디어 플랫폼이 2019년 이후 대규모 데이터셋의 주류가 됐음을 밝혔습니다. 데이터의 33% 미만이 상업적인 사용에 제한이 있지만, 사용되는 콘텐츠의 80% 이상이 비상업적 제한을 가진 것으로 나타났습니다.
- ***Performance Highlights***: 데이터의 지리적, 언어적 대표성은 2013년 이래로 크게 개선되지 않았으며, 데이터셋의 대부분은 북미 및 유럽 지역이 주도하고 있습니다.

### [In Case You Missed It: ARC 'Challenge' Is Not That Challenging](https://arxiv.org/abs/2412.17758)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17758.png)

Vote: 7

Authors: Łukasz Borchmann

- ***What's New***: 이 연구는 ARC(Allen AI Reasoning Challenge) Challenge가 현대 대형 언어 모델(LLM)에 있어서 실제로는 그리 복잡하지 않다는 것을 밝힙니다. 기존의 평가 방식이 오류를 범하여 난이도를 포장하는 경향이 있으며, 이로 인해 ARC Easy와 Challenge 간의 성능 격차가 왜곡된다는 점을 제시합니다. 새로운 평가는 이러한 격차를 상당히 감소시킬 수 있으며, 일부 벤치마크(예: OpenBookQA)에서는 인간의 성능을 초월하는 결과를 기록합니다.
- ***Technical Details***: 본 연구는 ARC를 포함한 여러 벤치마크에서 다지선다 문제를 평가할 때 모델이 각 후보 답안을 개별적으로 분석하는 대신 모든 옵션을 한 번에 제시하는 방식을 제안합니다. 이 방법은 인간 평가 맥락을 더 잘 시뮬레이트하며, 보다 정확한 모델 능력 평가를 가능케 합니다. 특히 '사실상 개별적으로 답변할 수 없는(hardly answerable in separation)' 문제 유형에 영향을 크게 미칩니다.
- ***Performance Highlights***: 모델 정확도가 단일 옵션 제시와 전체 옵션 제시에 따라 크게 변화하는 것을 보였습니다. 여기서 Llama 3.1 70B 같은 모델이 ARC Challenge에서 정확도가 64%에서 93%로 상승하며, 평소보다 6배 가까이 격차가 줄어드는 효과를 보였습니다. 이러한 결과는 평가 방법의 변화가 실제 과제의 복잡성보다 더 큰 영향을 미친다는 것을 의미합니다.

### [SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval](https://arxiv.org/abs/2412.15443)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15443.png)

Vote: 4

Authors: Aakash Mahalingam, Vinesh Kumar Gande, Vinija Jain, Divya Chaudhary, Aman Chadha

- ***What's New***: SKETCH는 RAG (Retrieval-Augmented Generation) 시스템의 검색 과정을 강화하기 위해 의미적 텍스트 검색을 지식 그래프(Knowledge Graph)와 통합한 새로운 방법론을 소개합니다. 이를 통해 구조화된 데이터와 비구조화된 데이터를 결합하여 더 전체적인 이해를 제공합니다. SKETCH는 QuALITY, QASPER, NarrativeQA 및 Italian Cuisine 등 네 개의 다양한 데이터셋에서 기존 방법보다 성능을 향상시켰으며, 특히 이탈리아 요리 데이터셋에서 답변의 관련성 0.94와 문맥 정밀도 0.99를 달성했습니다.
- ***Technical Details***: SKETCH는 문서를 의미적 청킹(Semantic Chunking)과 지식 그래프를 사용하여 처리합니다. 의미적 청킹은 문장을 의미 있는 단위로 분할하고, 지식 그래프는 엔티티 간의 관계를 그래프로 나타내어 정보를 구조화합니다. 이 접근방식은 복잡한 질문에 대한 다중 호프 추론을 가능하게 하여 여러 문맥에서의 관련 정보를 효율적으로 검색할 수 있게 합니다. SKETCH는 이러한 요소들을 결합하여 대규모 데이터셋에서 검색의 정확성 및 문맥의 일관성을 유지합니다.
- ***Performance Highlights***: SKETCH는 다양한 데이터셋에서 기존 방법을 상회하였으며, 특히 답변의 관련성에서 높은 성취를 보였습니다. 이탈리아 요리 데이터셋에서 SKETCH는 답변의 관련성 0.94를 달성하였고, 문맥의 정밀성과 일관성을 유지할 수 있었습니다. QuALITY, QASPER 및 NarrativeQA 데이터셋에서도 SKETCH는 문맥 정밀도와 답변의 진실성에서 우수한 점수를 기록하였습니다.

