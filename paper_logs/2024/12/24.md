## Daily Papers (2024-12-24)

### [Outcome-Refining Process Supervision for Code Generation](https://arxiv.org/abs/2412.15118)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15118.png)

Vote: 11

Authors: Wei Ye, Zhuohao Yu, Zhengran Zeng, Jindong Wang, Shikun Zhang, Yidong Wang, Weizheng Gu

- ***What's New***: 이 연구에서는 코드 생성(Code Generation)을 향상시키기 위한 새로운 패러다임인 결과-정제 프로세스 감독(Outcome-Refining Process Supervision; ORPS)을 제안합니다. 이는 구조화된 추론 공간을 제공하고 실행 기반 피드백을 통해 보다 다양한 해결 전략을 탐색할 수 있도록 합니다.
- ***Technical Details***: ORPS는 트리 구조의 탐색 공간을 통해 모델이 다양한 추론 경로를 동시에 유지하여 이론적 추론과 실용적 구현을 병행하여 개선합니다. 이 접근 방식은 실행 결과를 단계별 판단의 객관적 기준으로 사용하며, 전통적인 보상 모델 없이 모델 자체의 추론 능력과 실행 피드백 사이의 시너지를 창출합니다.
- ***Performance Highlights***: ORPS는 Pass@1 정확도를 평균 26.9% 향상시키고 실행 시간을 평균 42.2% 줄이는 등 기존의 결과 기반 및 프로세스 기반 감독 방법보다 탁월한 성능 개선을 보여주었습니다. 이는 작은 모델(Qwen-7B)에서도 효과적이며, 충분한 추론 공간이 모델의 크기보다 중요하다는 점을 강조합니다.

### [LearnLM: Improving Gemini for Learning](https://arxiv.org/abs/2412.16429)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16429.png)

Vote: 9

Authors: Shubham Milind Phal, Holger Winnemoeller, Filip Bar, Jennifer She, Miruna Pîslar, Parsa Mahmoudieh, James Cohan, Shaojian Zhu, Stephanie Chan, Nikhil Joshi, Kaiz Alarakyia, Garth Graham, Steve Yadlowsky, Mahvish Nagda, Andrea Huber, Amy Wang, Divya Pandya, Mike Schaekermann, Prateek Kolhar, Sun Jae Lee, Avishkar Bhoopchand, Brian Veprek, Derek Ahmed, Kevin McKee, Dan Wild, Paul Jhun, Lisa Wang, Shakir Mohamed, Ankit Anand, Daniel Gillick, Yannis Assael, Aditya Srikanth Veerubhotla, Aliya Rysbek, Markus Kunesch, Shashank Agarwal, Wei-Jen Ko, Theofilos Strinopoulos, Viknesh Sounderajah, Renee Schneider, Irina Jurenka, LearnLM Team, Daniel Kasenberg, Abhinit Modi, Julia Wilkowski, Brett Wiltshire, Sara Wiltberger

- ***What's New***: LearnLM은 교사를 대신할 수 있는 AI 개인 교사로서 Gemini 모델의 학습 기능을 개선하기 위한 새로운 접근법을 제시합니다. 페다고지(instructive pedagogy)의 방법론을 따르도록 설계되어, 특정 교육 속성을 모델의 향후 대화에 포함할 수 있습니다.
- ***Technical Details***: LearnLM은 Gemini 1.5 Pro를 기초로 하여 훈련된 모델이며, 페다고지 지시(pedagogical instruction following)를 강화하기 위해 인간의 피드백을 이용한 강화 학습(RLHF)을 포함하고 있습니다. 학습 시나리오 기반 평가를 설계하고 다양한 교육 시나리오에서 장애물을 관리합니다.
- ***Performance Highlights***: 학습 시나리오에 따라 여러 AI 튜터와 짝을 이루어 학습 대화를 평가한 결과, LearnLM은 GPT-4o 모델보다 31%, Claude 3.5보다 11%, 그리고 원래 Gemini 1.5 Pro 모델보다 13% 더 높은 선호도를 보였습니다. 이는 LearnLM이 교육적 대화 영역에서의 시행 전략을 개선하고 있음을 보여줍니다.

### [NILE: Internal Consistency Alignment in Large Language Models](https://arxiv.org/abs/2412.16686)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16686.png)

Vote: 6

Authors: Chen Ma, Hongru Wang, Yasheng Wang, Yufei Wang, Jingyan Zhou, Minda Hu, Qiyuan Zhang, Liangyou Li, Bowei He, Irwin King

- ***What's New***: NILE 프레임워크는 대형 언어 모델(LLM)의 내부 지식과 외부 IFT 데이터셋 간의 일관성을 개선하여 데이터셋의 질을 높이는 새로운 접근법입니다. 이를 통해 LLM의 잠재력을 최대한 발휘할 수 있도록 설계되었습니다.
- ***Technical Details***: NILE 프레임워크는 내부 지식 추출(INTERNAL KNOWLEDGE EXTRACTION; IKE), 지식 기반 샘플 수정(KNOWLEDGE-AWARE SAMPLE REVISION; KSR), 내부 일관성 필터링(INTERNAL CONSISTENCY FILTERING; ICF)의 3단계로 구성됩니다. 기본적으로 LLM의 내부 파라미터 지식을 활용하여 IFT 데이터셋을 수정하고, 이후 내부 지식과 외부 지식의 관계를 고려하여 샘플을 선택합니다.
- ***Performance Highlights***: NILE을 통해 최적화된 데이터셋은 Arena-Hard에서 최대 66.6%, Alpaca-Eval V2에서 68.5%의 성능 향상을 보여줍니다. 이는 NILE의 균형 잡힌 세계 지식과 내부 지식의 통합이 LLM의 새로운 작업과 분야에 대한 일반화 능력을 향상시킴을 나타냅니다.

### [DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought](https://arxiv.org/abs/2412.17498)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17498.png)

Vote: 8

Authors: Jiaan Wang, Yunlong Liang, Jie Zhou, Fandong Meng

- ***What's New***: DRT-o1은 긴 사고 사슬(long Chain-of-Thought)을 뉴럴 기계 번역(Neural Machine Translation; MT)에 도입한 모델입니다. 이 모델은 특히 은유나 유사성을 포함하는 문장을 번역할 때 기존 기계 번역이 놓치는 의미 보존 문제를 해결하는 데 중점을 두었습니다.
- ***Technical Details***: DRT-o1은 다중 에이전트 프레임워크(multi-agent framework)를 활용하여 번역 데이터 세트를 생성합니다. 번역가, 조언자, 평가자로 구성된 이 프레임워크는 각 문장의 번역을 반복적으로 개선합니다. 심사 과정에서 GPT-4o를 활용해 번역의 가독성과 유창성을 강화합니다.
- ***Performance Highlights***: DRT-o1-7B는 기존의 Qwen2.5-7B-Instruct에 비해 BLEU 점수가 8.26, CometScore가 3.36점 높은 성능을 보였으며, DRT-o1-14B도 BLEU 점수에서 7.33점의 향상을 보였습니다. 이는 긴 사고 사슬을 통해 번역 품질이 크게 향상될 수 있음을 나타냅니다.

### [Diving into Self-Evolving Training for Multimodal Reasoning](https://arxiv.org/abs/2412.17451)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17451.png)

Vote: 22

Authors: Junxian He, Xiwen Zhang, Yu Cheng, Junlong Li, Wei Liu, Fan Zhou

- **What's New**: 이 연구는 멀티모달 추론(Multimodal Reasoning) 능력을 향상시키기 위한 셀프 에볼빙 트레이닝(Self-Evolving Training)을 심도 있게 분석하고, M-STAR(Multimodal Self-evolving Training for Reasoning)이라는 새로운 프레임워크를 제시합니다. 이 프레임워크는 모델 크기에 관계없이 다양한 벤치마크에서 효과적으로 작동하며, 추가적인 인간 주석 없이도 성능 향상을 이끌어냅니다.
- **Technical Details**: 셀프 에볼빙 트레이닝은 강화 학습(RL)의 일반적 형태로 간주되며, 트레이닝 메소드(Training Method), 보상 모델(Reward Model), 프롬프트 변동(Prompt Variation)이 중요한 요소로 분석됩니다. M-STAR는 이러한 요소들을 최적화하는 방법을 체계적으로 제시하며, 특히 연속적인 셀프 에볼빙 트레이닝을 제안하여 오프라인 학습과 온라인 학습 간의 간격을 줄입니다.
- **Performance Highlights**: 다섯 가지 멀티모달 추론 벤치마크에서 M-STAR는 기존 모델을 크게 능가하였으며, MathVista와 같은 다양한 벤치마크에서도 두드러진 성능 향상을 보였습니다. 적응적 온도 조절 기법을 도입하여 모델의 탐색 능력을 유지하면서도 최고 성능을 달성했습니다.

### [B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners](https://arxiv.org/abs/2412.17256)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17256.png)

Vote: 29

Authors: Junxian He, Yuzhen Huang, Zifei Shan, Lulu Zhao, Weihao Zeng, Yijun Wang

- ***What's New***: B-STAR은 자신이 생성한 데이터를 학습해 성능을 향상시키는 셀프 트레이닝(Self-Improvement) 모델에 대해 탐색과 활용 균형을 자동으로 조정하는 새롭고 혁신적인 프레임워크를 제안합니다. 이 방법은 모델이 다양한 응답을 생성할 수 있는 탐색 능력과 보상을 통해 높은 품질의 솔루션을 구별할 수 있는 활용의 효과성을 모니터링하고 조정하는 데 중점을 두고 있습니다.
- ***Technical Details***: B-STAR는 수학적 문제 해결, 코딩, 상식 추론에 대한 훈련에서 검증된 평균 균형 점수(balance score)를 통해 탐색과 활용을 최적화합니다. 이를 위해 탐색 온도와 보상 임계값을 자동으로 조정하며, 매 반복 단계마다 이 점수를 최대화하는 온도와 임계값을 선택합니다. 이 과정은 총 9번의 반복을 포함하며, 각 반복에서 500번의 훈련 단계를 수행합니다. 이러한 설정은 Mistral-7B 모델을 베이스로 하고 샘플 크기, 온도 등의 파라미터를 조정하여 모델의 탐색 및 활용 역량을 동적으로 조정합니다.
- ***Performance Highlights***: B-STAR는 GSM8K, MATH, APPS와 같은 다양한 데이터셋에서 Pass@1 등의 성능 지표에서 다른 셀프-훈련 개선 방법보다 우수한 결과를 보였습니다. 특히 탐색과 활용의 균형을 잡는 데 성공하며, 이는 학습의 안정성과 효율성을 증대시키는 데 기여했습니다.

### [RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response](https://arxiv.org/abs/2412.14922)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14922.png)

Vote: 63

Authors: Kaize Ding, Junyu Luo, Ming Zhang, Zhiping Xiao, Jingyang Yuan, Xiao Luo

- ***What's New***: ROBUSTFT는 노이즈가 있는 환경에서 대형 언어 모델(Large Language Models; LLMs)의 성능을 향상시키기 위한 견고한 지도 세부 조정 프레임워크를 제안합니다. 이 프레임워크는 노이즈 데이터에 대한 탐지와 다시 라벨링 과정을 통해 다운스트림 작업에서 모델 성능을 극대화합니다.
- ***Technical Details***: ROBUSTFT는 다중 전문가 협력 시스템(Multi-expert Collaborative System)을 사용하여 노이즈 탐지를 수행하며, 추론이 강화된 모델로 노이즈를 효과적으로 식별합니다. 데이터 선택 메커니즘은 응답 엔트로피(Entropy)를 기반으로 하여, 고품질 샘플만 유지합니다. 잡음 제거에서는 문맥 강화 전략(Context-enhanced Strategy)을 사용하여 가장 관련성 있고 확신 있는 지식을 통합하고 평가하여 신뢰할 수 있는 주석을 생성합니다.
- ***Performance Highlights***: ROBUSTFT는 다양한 LLMs와 5개의 데이터셋에 대한 광범위한 실험에서 뛰어난 성능을 보였습니다. 데이터 노이즈가 모델의 정확성에 큰 영향을 미친다는 점을 보여주었으며, 노이즈 30%의 상황에서 8.9% 성능 저하가 확인되었습니다. 이러한 실험은 노이즈가 있는 환경에서 ROBUSTFT의 견고한 성능을 보여줍니다.

### [PC Agent: While You Sleep, AI Works -- A Cognitive Journey into Digital World](https://arxiv.org/abs/2412.17589)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17589.png)

Vote: 6

Authors: Pengfei Liu, Jiadi Su, Runze Fan, Haoyang Zou, Shijie Xia, Jiahe Jin, Xiangkun Hu, Yanheng He

- ***What's New***: PC Agent는 인간 인지 전이(cognitive transfer)을 통해 복잡한 디지털 작업을 수행할 수 있는 AI 시스템을 제안합니다. PC Tracker, 두 단계의 인지 완성 파이프라인(cognition completion pipeline), 플래닝 에이전트(planning agent)와 시각적 기반 에이전트(grounding agent)를 결합한 다중 에이전트 시스템(multi-agent system)을 통해 인간의 인지 과정을 AI에 효율적으로 학습시키고자 합니다.
- ***Technical Details***: PC Agent 프레임워크는 필수적인 인간-컴퓨터 상호작용 궤적(Human-Computer Interaction Trajectories)을 수집하는 경량의 인프라인 PC Tracker와, 원시 상호 작용 데이터를 인지 궤적으로 전환하는 두 단계의 인지 완성 파이프라인을 포함합니다. 추가적으로 플래닝 에이전트는 행동 결정을, 시각 기반 에이전트는 클릭 위치의 시각적 기반을 제공하여 복잡한 작업 수행을 향상시킵니다.
- ***Performance Highlights***: PC Agent는 단 133개의 인지 궤적(trajectories)으로 훈련되어 여러 어플리케이션에서 최대 50단계의 복잡한 작업 시나리오를 처리할 수 있도록 설계되었습니다. 이를 통해 AI 시스템이 인간의 인지 데이터를 학습하여 복잡한 컴퓨터 작업을 수행할 수 있음을 보여줍니다.

### [Agent-SafetyBench: Evaluating the Safety of LLM Agents](https://arxiv.org/abs/2412.14470)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14470.png)

Vote: 5

Authors: Yida Lu, Jingzhuo Zhou, Zhexin Zhang, Junxiao Yang, Hongning Wang, Minlie Huang, Shiyao Cui

- ***What's New***: AGENT-SAFETYBENCH는 LLM 에이전트의 안전성을 평가할 수 있는 포괄적인 벤치마크로, 349개의 상호작용 환경 및 2,000개의 테스트 케이스를 제공합니다. 이는 다양한 안전 위험 카테고리와 일반적인 실패 모드를 평가하여 현재 LLM 에이전트의 안전성을 높이는 데 기여하는 것을 목표로 합니다.
- ***Technical Details***: AGENT-SAFETYBENCH는 8개의 안전 위험 카테고리와 10개의 대표적인 실패 모드를 평가합니다. 모든 테스트 케이스는 최소 두 번 이상의 수동 검토를 거치며, 실제 환경에서 시뮬레이션된 다양한 상호작용 환경을 포함하고 있습니다. 또한, LLM 기반의 평가 모델을 미세 조정하여 기존의 GPT-4o 대비 15% 향상된 평가 정확도를 제공합니다.
- ***Performance Highlights***: 벤치마크에 의해 평가된 16개의 LLM 에이전트 모두에서 60% 이상의 안전 점수를 달성한 에이전트는 없었습니다. 특히, '확산(Spread)' 카테고리에서 평균 점수가 15.6%에 불과하였으며, 이는 에이전트가 도구 사용 과정에서 불안전한 정보를 쉽게 퍼뜨릴 수 있음을 시사합니다.

### [Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage](https://arxiv.org/abs/2412.15484)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15484.png)

Vote: 11

Authors: Seunghyun Yoon, Sungroh Yoon, Saehyung Lee, Jing Shi, Trung Bui

- ***What's New***: 이 연구에서는 멀티에이전트 어프로치(Multiagent Approach)와 사실성(Factuality) 및 커버리지(Coverage)를 평가하기 위한 이중 평가 메트릭스를 도입하여 매우 상세한 이미지 캡셔닝의 견고성을 향상시킨다는 것을 제안합니다. 이는 대형 멀티모달 언어 모델(MLLMs)이 생성하는 불필요한 정보를 줄여주는 방법입니다.
- ***Technical Details***: CapMAS라는 시스템을 제안하여, 초상세 이미지 캡션의 사실성을 향상시킵니다. CapMAS는 LLM과 MLLM의 협업을 통해 캡션을 교정하며, 사전 훈련 없이 동작합니다. MLLM이 이미지 기반으로 각 문장의 사실 여부를 평가하고, 블록별로 나눈 후 LLM이 개선된 캡션을 만듭니다. 각 캡션은 마이크로 유닛(Atomic Proposition)으로 나뉘어 사실성이 검토됩니다.
- ***Performance Highlights***: CapMAS는 다양한 MLLMs에 적용되어 사실성에서 큰 개선을 보였으며, GPT-4V 같은 최신 모델의 경우에도 유의미한 성과를 얻었습니다. 이 방법이 기존의 대화 기반 평가 방법보다 인간 평가와 더 잘 일치함을 입증하였으며, 자세한 이미지 캡셔닝 작업에서 더욱 뛰어난 성과를 냈습니다.

### [Revisiting In-Context Learning with Long Context Language Models](https://arxiv.org/abs/2412.16926)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16926.png)

Vote: 11

Authors: Jinheon Baek, Sun Jae Lee, Siddharth Dalmia, Geunseob, Oh, Prateek Kolhar, Prakhar Gupta

- ***What's New***: 이 논문에서는 In-Context Learning(ICL)을 위한 기존 샘플 선택 기법이 Long Context Language Models(LCLMs)에서 여전히 유효한지를 분석합니다. 이 연구는 여러 샘플 선택 전략의 효과를 재조사하며, 이들의 효과가 랜덤 선택 방식과 비교할 때 통계적으로 유의하지 않음을 발견하였습니다.
- ***Technical Details***: LCLMs는 평균 100만 개 이상 토큰을 수용할 수 있는 긴 문맥을 처리할 수 있으며, 이는 많은 예제를 포함할 수 있게 해줍니다. 연구진은 다양한 데이터셋과 작업(분류, 번역, 요약, 추론)을 통해 LCLMs에서의 ICL을 실험하였습니다. 샘플 선택은 기존의 문맥 윈도우 길이를 고려한 방법으로, 관련성, 다양성, 난이도를 기준으로 하는 방법을 포함했습니다.
- ***Performance Highlights***: 실험 결과, 기존의 세심한 샘플 선택 기법은 ICL 성능에 거의 영향을 미치지 않았으며, 오히려 랜덤 샘플 선택 방법이 더 효율적이라고 밝혀졌습니다. ICL 성능은 예제 수가 증가하면 향상되지만, 너무 긴 문맥에서는 오히려 성능이 저하되는 경향이 있었습니다. 또한 LCLMs은 노이즈가 적은 상황에서는 강인함을 보였지만, 복잡한 작업에서는 노이즈에 더 취약한 것으로 나타났습니다.

### [Large Motion Video Autoencoding with Cross-modal Video VAE](https://arxiv.org/abs/2412.17805)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17805.png)

Vote: 15

Authors: Qifeng Chen, Yang Fei, Jiaxin Xie, Xiaowei Chi, Jingye Chen, Yazhou Xing, Yingqing He

- ***What's New***: 이 연구는 고정밀 비디오 인코딩이 가능한 새로운 강력한 비디오 오토인코더(Video Autoencoder; VAE)를 제안합니다. 텍스트-비디오 데이터셋의 텍스트 정보를 통합하여 텍스트 가이던스를 모델에 포함시킴으로써 세부 보존과 시간적 안정성을 크게 개선하였습니다.
- ***Technical Details***: 이 논문에서는 공간 및 시간 압축을 위한 새로운 아키텍처를 제안합니다. 동시에 공간-시간(ST) 압축은 저수준의 시간적 부드러움과 텍스처 안정성이 좋으며, 순차적 ST 압축은 특히 큰 운동 시나리오에서 운동 회복 능력이 뛰어납니다. 따라서 두 방법의 장점을 통합한 아키텍처를 통해 비디오 세부 사항과 운동 재구성을 가능하게 합니다. 추가적으로, 이미지를 포함한 공동 학습을 통해 모델의 다용성을 높였습니다.
- ***Performance Highlights***: 실험 결과, 제안한 4 채널 및 16 채널 VAE 모델은 대부분의 데이터셋 및 메트릭에서 기존의 최신 기법들에 비해 뛰어난 성능을 보였습니다. 특히 WebVid 테스트 세트에서 높은 정확도를 기록하며 더 나은 세부 보존과 시간적 일관성을 달성했습니다.

### [OpenAI o1 System Card](https://arxiv.org/abs/2412.16720)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16720.png)

Vote: 11

Authors: Shuyuan Zhang, Saachi Jain, Elizabeth Proehl, Leo Liu, Ally Bennett, Vlad Fomenko, Andy Applebaum, Ignasi Clavera Gilaberte, Dan Roberts, Kevin Yu, Reah Miyara, Hessam Bagherinezhad, Mengyuan Xu, Wenda Zhou, Andrew Duberstein, Dimitris Tsipras, Edmund Wong, Irina Kofman, Oleg Boiko, Sam Toizer, Karina Nguyen, Alexander Neitz, Giambattista Parascandolo, Claudia Fischer, Ofir Nachum, Paul Ashbourne, Jonathan Gordon, Zhuohan Li, Nat McAleese, Chak Ming Li, Mikhail Pavlov, Renny Hwang, Sam Toyer, Hunter Lightman, Karan Singhal, Siyuan Fu, Suchir Balaji, Andre Saraiva, Alex Tachard Passos, David Robinson, Jonathan Ward, Lukasz Kaiser, Camillo Lugaresi, Peter Zhokhov, Rachel Dias, Nikolas Tezak, Roshan James, Thomas Dimson, Valerie Qi, Thomas Degry, Trapit Bansal, Alexander Prokofiev, Brandon Houghton, Mengyuan Yan, Julie Wang, Jiayi Weng, Raz Gaon, Chen Shen, Meghan Shah, OpenAI, Jerry Twore, Andrey Mishchenko, Ryan Greene, Ted Sanders, Felipe Petroski Such, Joost Huizinga, Christopher Hesse, Jiahui Yu, Randall Lin, Alex Carney, Rahul Arora, Wes McCabe, Madelaine Boyd, Pavel Izmailov, Scottie Yan, Alex Iftimie, David Farhi, Kevin Lu, Michael Lampe, Lindsay McCallum, Aiden Low, Cary Bassin, Matt Jones, Miles Wang, Greg Brockman, Fan Wang, Alex Beutel, Dragos Oprica, Chris Koch, Fred von Lohmann, Alexander Wei, Hadi Salman, Noam Brown, Marko Tintor, Manas Joglekar, Allison Tam, Yuchen He, Kai Xiao, Yunyun Wang, Wojciech Zaremba, David Dohan, Vineet Kosaraju, Maja Trebacz, Kai Chen, Leyton Ho, Neil Chowdhury, Joe Palermo, Daniel Selsam, Tyna Eloundou, Behrooz Ghorbani, Yann Dubois, Ryan Cheu, Lama Ahmad, Suvansh Sanjeev, Ben Rossen, James Lennon, Mason Meyer, Chelsea Voss, Jonathan Uesato, Timur Garipov, Spencer Papay, Weiyi Zheng, Yining Chen, Yu Bai, Tianhao Zheng, Mianna Chen, Michael Malek, Hart Andrin, Melody Guan, Lukas Kondraciuk, Gildas Chabot, Barret Zoph, Boaz Barak, Haiming Bao, Sam Altman, Tao Wang, Yinghai Lu, Jakub Pachocki, Kendra Rimbach, Filippo Raso, Michele Wang, John Rizzo, Robin Brown, Daniel Kappler, Joel Parish, Alex Karpenko, Tal Broda, Liam Fedus, Olivia Watkins, Geoff Salmon, David Mely, Jean Harb, Ananya Kumar, Francis Song, Borys Minaiev, Taylor Gordon, Linden Li, Mira Murati, Steph Lin, Adam Kalai, Sandhini Agarwal, Johannes Heidecke, Samuel Miserendino, Ilya Kostrikov, Chris Orsinger, Eben Freeman, Daniel Levy, Brydon Eastman, Reimar Leike, Tejal Patwardhan, Sasha Baker, Vitchyr Pong, Jason Wei, Ashvin Nair, Hongyu Ren, Mike McClay, Szymon Sidor, Ian O'Connell, Kevin Liu, Foivos Tsimpourlas, Karl Cobbe, Luke Metz, Mo Bavarian, Florencia Leoni, Brandon McKinzie, Alec Helyar, Tom Stasi, Patrick Chao, Shibani Santurkar, Katy Shi, Lorenz Kuhn, Shraman Ray Chaudhuri, Scott McKinney, Yuchen Zhang, Keren Gu-Lemberg, Guillaume Leclerc, Mehmet Yatbaz, Mingxuan Wang, Shengli Hu, Bowen Baker, Vinnie Monaco, Cary Hudson, Rhythm Garg, Santiago Hernandez, Grace Zhao, Shengjia Zhao, Jiacheng Feng, Mark Chen, Freddie Sulit, Ilge Akkaya, Aidan Clark, Kevin Stone, Leon Maksin, Hyung Won Chung, Kayla Wood, Hao Sheng, Lauren Yang, Botao Hao, Matt Kaufer, Mia Glaese, Mostafa Rohaninejad, Benjamin Sokolowsky, Eric Wallace, Jie Tang, Doug Li, Ian Osband, Bob McGrew, John Hallman, Max Schwarzer, Clive Chan, Young Cha, Chong Zhang, Evan Mays, Troy Peterson, Rapha Gontijo Lopes, Enoch Cheung, Zheng Shao, Nick Ryder, Adam Lerer, Michelle Fradin, Aaron Jaech, Lilian Weng, Angela Jiang, Rui Shu, Aleksander Madry, Joaquin Quiñonero Candela, Adam Richardson, Keren GuLemberg, Jieqi Yu, Erik Ritter, Ilya Sutskever, Charles de Bourcy, Lindsey Held, Oleg Murk, Andrea Vallone, Eddie Zhang, Thibault Sottiaux, Eric Mitchell, Trevor Creech, Ian Kivlichan, Andrew Kondrich, Ahmed El-Kishky

- ***What's New***: OpenAI의 새로운 o1 모델 시리즈는 체인 오브 쏘트(Chain-of-Thought)를 활용한 대규모 강화 학습을 통해 향상된 추론 능력을 제공하며, 위험한 프롬프트에 대한 대응 시 모델의 안전 정책을 상황에 맞게 분석할 수 있는 '심사 숙고 정렬(Deliberative Alignment)' 접근법을 도입했습니다. 이는 GPT-4o보다 여러 내부 벤치마크에서 더 나은 성과를 보여주었습니다.
- ***Technical Details***: o1 모델은 공개 데이터, 파트너십을 통해 획득한 비공개 데이터 및 내부에서 개발한 맞춤형 데이터셋으로 구성된 다양하고 방대한 데이터셋을 기반으로 훈련되었습니다. 또한, 모델은 GPT-4o에 비해 더 정교한 멀티모달 및 정지형 학습 기법을 적용받았습니다. o1 모델의 훈련에는 데이터 품질 유지를 위한 엄격한 필터링 절차가 포함되며, 개인 정보와 유해하거나 민감한 콘텐츠가 포함되지 않도록 조치합니다.
- ***Performance Highlights***: Disallowed content와 같은 안전 평가에서 o1 모델들은 GPT-4o에 비해 월등한 성과를 보였으며, 특히 Challenging Refusal Evaluation과 같은 어려운 테스트에서도 GPT-4o를 능가했습니다. 또한, 새로운 모델은 강력한 재머(Jailbreak) 공격에 더 견고하며 내부 및 외부의 빨간 팀 테스트에서도 우수한 성과를 나타냈습니다. 모델 내성과 편향 평가에서도 GPT-4o 대비 개선된 성과를 보였습니다. o1 모델은 내부적인 종합 평가에서 중간 위험 점수를 받았습니다.

### [ResearchTown: Simulator of Human Research Community](https://arxiv.org/abs/2412.17767)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17767.png)

Vote: 7

Authors: Tao Feng, Kunlun Zhu, Jinwei Yao, Keyang Xuan, Zirui Cheng, Jiaxuan You, Zhaochen Hong, Haofei Yu

- ***What's New***: 이 연구에서는 인간 연구 커뮤니티를 시뮬레이션할 수 있는 다중 에이전트 프레임워크, RESEARCHTOWN을 제안합니다. 크게 협력 연구 활동을 시뮬레이션할 수 있고 다양한 연구자를 유지하면서도 강력한 시뮬레이션을 제공합니다. 또한 학제 간 연구 아이디어를 생성하여 참신한 연구 방향을 영감 줍니다.
- ***Technical Details***: RESEARCHTOWN은 인물과 텍스트로 구성된 에이전트-데이터 그래프(Agent-Data Graph)로 연구 커뮤니티를 모델링하고, 각 에이전트 및 데이터 노드는 그래프 신경망(GNN)의 메시지 전달 프로세스를 바탕으로 다양한 연구 활동을 모델링합니다. TextGNN이라는 텍스트 기반 추론 프레임워크를 도입하여 논문 읽기, 논문 작성, 리뷰 작성 등의 과정을 특별한 메시지 전달 프로세스로 구현했습니다.
- ***Performance Highlights***: RESEARCHTOWN의 연구 시뮬레이션은 종합적으로 0.67의 유사도 점수를 달성하여 논문 작성 시뮬레이션의 정확성을 증명했습니다. 연구 간 연구 활동 내에서도 강력함을 나타내며, 다양한 연구자와 논문을 포함해도 일관된 성능을 유지했습니다. 더불어, NLP, 범죄학, 천문학 등의 분야를 결합한 혁신적인 아이디어를 생성하여 실존하는 연구에서 확립되지 않은 독창적인 아이디어를 도출했습니다.

### [Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding](https://arxiv.org/abs/2412.17295)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17295.png)

Vote: 4

Authors: Qun Liu, Xiaojun Meng, Yuxuan Wang, Dongyan Zhao, Yueqian Wang, Jianxin Liang

- ***What's New***: Friends-MMC는 멀티모달 다중 참여 대화(Multi-modal Multi-party Conversation; MMC)의 이해를 돕기 위해 개발된 데이터셋으로, 유명 TV 시리즈 'Friends'에서 추출한 영상 및 대화 내용을 포함하여 24,000개 이상의 고유 발화를 제공합니다. 이 데이터셋은 시청각적 맥락에서 여러 참여자 사이의 대화에 있어 강력한 캐릭터 중심의 이해를 요하는 새로운 연구 분야를 개척합니다.
- ***Technical Details***: Friends-MMC 데이터셋은 대화를 지원하기 위해 각 발화의 화자, 영상에 나타난 얼굴의 이름 및 경계 박스를 주석으로 추가합니다. 이 데이터셋에서는 두 가지 기본 MMC 과제인 대화 화자 식별(Conversation Speaker Identification) 및 대화 응답 예측(Conversation Response Prediction)을 추진합니다. 화자 식별은 인셉션(Inception) 모델이나 TalkNet과 같은 비주얼 모델과 DeBERTa-v3 같은 텍스트 모델을 결합하여 수행됩니다. 최적화 솔버를 사용하여 모달리티 전반의 문맥을 통합하여 성능을 향상시킵니다.
- ***Performance Highlights***: 비디오 기반의 모델 조합은 테스트 세트에서 83.21%의 정확도를 보이며, 텍스트 기반 모델만을 사용하는 것보다 성능이 상당히 뛰어납니다. 다중모달 사전 학습 모델들의 경우 이 과제에 있어 기대만큼의 성과를 보이지 못했으며, 이는 새로운 목적의 과제에 대한 충분한 이해가 부족함을 시사합니다. 대화 응답 예측에서는 화자 정보가 응답 정확도에 기여하며, 무작위 화자 정보를 사용했을 때보다 그라운드 트루스 화자 정보를 사용했을 때 정확도가 높습니다.

### [Deliberation in Latent Space via Differentiable Cache Augmentation](https://arxiv.org/abs/2412.17747)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17747.png)

Vote: 15

Authors: Jiaxing Wu, Jonas Pfeiffer, Arthur Szlam, Luyang Liu, Jun Xie

- ***What's New***: 이 논문은 고정된 대형 언어 모델(LLM; Large Language Models)에 오프라인 보조처리기(coprocessor)를 추가하여 모델의 kv-cache에 잠재 임베딩(latent embeddings)을 통해 중간 추론 단계를 개선하는 방법을 제안합니다. 이 방법은 기존 모델 아키텍처를 변경 없이 모델의 추론 능력을 향상시키며, 델리브레이션(Deliberation)을 가능하게 합니다.
- ***Technical Details***: 제안된 구조는 kv-cache를 통해 보조처리기가 소프트 토큰을 생성하고, 이를 kv-cache에 추가하여 추론 성능을 향상시킵니다. 보조처리기는 고정된 트랜스포머 모델과 동일한 아키텍처를 사용하며, 한번의 포워드 패스에서 잠재 임베딩을 생성합니다. 이를 통해 LLM은 보조처리기의 출력을 바탕으로 향상된 문맥 정보를 활용하여 응답을 생성할 수 있습니다.
- ***Performance Highlights***: 제안된 방법은 다양한 추론 집약적 작업에서 꾸준히 성능을 향상시킵니다. 예를 들어, Gemma-2 2B 모델에 64개의 잠재 임베딩을 추가했을 때, GSM8K 퀴즈에서 10.05%, MMLU에서 4.70%의 성능 향상을 관찰했습니다. 이는 kv-cache 보조처리가 모델의 추론 능력을 크게 향상시킬 수 있음을 보여줍니다.

### [Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching](https://arxiv.org/abs/2412.17153)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17153.png)

Vote: 24

Authors: Enshu Liu, Zinan Lin, Yu Wang, Xuefei Ning

- ***What's New***: Distilled Decoding(DD)은 기존의 오토리그레시브 모델(Autoregressive Models; AR)이 필요로 하는 단계적인 토큰 생성 과정을 대체하기 위해, 이미지 AR 모델에서 최초로 일-단계 생성(One-step Generation)을 가능하게 한 혁신적인 방법입니다. 이는 AR 모델의 느린 생성 속도를 획기적으로 개선하여, 보다 빠르고 효율적인 생성이 가능하도록 합니다.
- ***Technical Details***: DD는 플로우 매칭(Flow Matching)을 활용해 가우시안 분포(Gaussian Distribution)에서 사전 학습된 AR 모델의 출력 분포로의 결정론적 맵핑을 생성합니다. 이를 통해 노이즈에서 시작해 모든 데이터를 아우르는 AR 경로를 만듭니다. 이 과정은 AR 모델의 학습 데이터를 필요로 하지 않으며, 이는 특히 최신 LLMs(Large Language Models)에서 매우 유용합니다.
- ***Performance Highlights***: VAR 모델의 경우 10단계 생성(680 토큰)을 필요로 하지만, DD를 통해 일-단계 생성으로 가속(6.3배)하면서 FID가 4.19에서 9.96으로 허용 가능한 증가를 보였습니다. LlamaGen에서는 256단계를 1단계로 줄여 217.8배의 속도 향상을 달성했으며, FID는 4.11에서 11.35로 증가했습니다. 텍스트-투-이미지 생성의 경우에도 DD는 2단계로 줄여 FID 25.70에서 28.95로의 소폭 상승을 보이며 높은 효율성을 증명하였습니다.

### [OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning](https://arxiv.org/abs/2412.16849)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.16849.png)

Vote: 3

Authors: Jitao Sang, Yuhang Wang, Jinlin Xiao, Yuxiang Zhang, Jiangming Shu, Yuqi Yang

- ***What's New***: OpenRFT는 일반적인 추론 모델을 특화된 도메인 작업에 맞춰 Reinforcement Fine-Tuning(RFT) 기법으로 세밀하게 조정하는 시도를 소개합니다. 이 연구는 OpenAI의 RFT 방식에 영감을 받아 세 가지 방법으로 문제를 해결합니다: 질문 증강(question augmentation), 추론 과정 데이터 생성, 그리고 소수 샷 학습(few-shot In-Context Learning; ICL)을 통해 제한된 도메인 데이터의 한계를 극복하고자 합니다.
- ***Technical Details***: OpenRFT는 Skywork-o1 시리즈의 추론 기초 모델(reasoning foundation model)과 프로세스 보상 모델(Process Reward Model; PRM)을 사용하여, 제한된 도메인 샘플을 데이터 증강과 프로세스 보상 모델이 감독하는 강화 학습 환경에서 활용합니다. 이때 질문을 재구성하고 옵션을 혼합하여 도메인 데이터 세트를 확장하며, 강력한 모델을 교사(teacher)로 활용해 누락된 추론 데이터를 생성합니다. 생성된 데이터는 정책 모델(policy model)의 SFT를 통해 학습되며, 결국 도메인 작업에 적합한 모델로 발전합니다.
- ***Performance Highlights***: OpenRFT는 SciKnowEval 벤치마크 상에서 제한된 도메인 샘플(각 작업당 100개)만으로 평균적으로 11%의 성능 개선을 보였습니다. 이는 다양하고 복잡한 과학 영역(생물학, 화학, 물리학, 재료과학)에 걸친 8가지 구체적인 작업에서 유효성을 확인하게 되었습니다. 또한, 데이터 증강 및 적절한 액션 스페이스 조정 등의 다양한 방법이 성능 향상에 기여한다는 것을 보였습니다.

