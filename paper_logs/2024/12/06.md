## Daily Papers (2024-12-06)

### [Personalized Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2412.02142)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.02142.png)

Vote: 7

Authors: Junda Wu, Julian McAuley, Tong Yu, Sungchul Kim, Huanrui Yang, Jiebo Luo, Nedim Lipka, Dang Nguyen, Hanjia Lyu, Mehrnoosh Mirtaheri, Zhehao Zhang, Zhengmian Hu, Xiang Chen, Ruiyi Zhang, Ishita Kumar, Subrata Mitra, Hanieh Deilamsalehy, Nesreen K. Ahmed, Hongjie Chen, Yu Xia, Ryan A. Rossi, Yu Wang, Namyong Park, Joe Barrow, Yue Zhao, Franck Dernoncourt, Jiuxiang Gu

- ***What's New***: 이 논문은 개인화된 멀티모달 대형 언어 모델(Personalized Multimodal Large Language Models; MLLMs)에 대한 포괄적인 설문을 제공합니다. 주로 모델의 구조, 교육 방법 및 응용 분야에 초점을 맞추고 있으며, 개인화 기술을 사용자에게 맞춤해 설명하고 이를 위한 직관적인 분류 체계를 제안하며, 개인화 작업 및 벤치마크 데이터셋을 정리하고 있습니다.
- ***Technical Details***: 개인화된 MLLMs는 다양한 모달리티를 처리할 수 있는 아키텍처로, 텍스트, 이미지, 오디오 등 여러 모달 정보를 통합하여 복잡한 작업을 처리합니다. 개인화 기법에는 텍스트 생성, 이미지 생성, 추천, 검색 등이 포함됩니다. 해당 모델들은 사용자의 선호도, 문맥, 요구에 적응할 수 있도록 고안되었으며, 이를 위해 프롬프트 기반 조정(prompt-based tuning), 특화 조정(fine-tuning) 등의 방법이 활용됩니다.
- ***Performance Highlights***: MLLMs는 개인화를 통해 사용자 맞춤형 콘텐츠 생성, 추천 시스템 활용 등 다양한 성능을 보여주고 있습니다. 특히, 평가 기준으로는 적중률(Recall@k), 평균정밀도 MAP 등이 있으며, 이는 추천 정확도를 측정하는데 사용됩니다.

### [SynFinTabs: A Dataset of Synthetic Financial Tables for Information and Table Extraction](https://arxiv.org/abs/2412.04262)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04262.png)

Vote: 1

Authors: Ethan Bradley, Karen Rafferty, Muhammad Roman, Barry Devereux

- ***What's New***: SynFinTabs는 금융 문서 이미지에서 테이블 정보를 추출하기 위한 대규모 합성 데이터셋입니다. 기존의 데이터셋들이 과학적 테이블에 치중되어 있는 점을 보완하여, SynFinTabs는 금융 테이블의 구조와 전시 특성을 포착하여 다양한 정보와 테이블 추출 과제에 적용할 수 있도록 설계되었습니다.
- ***Technical Details***: SynFinTabs는 총 10만 개의 합성 금융 테이블을 포함하며, 각 테이블은 HTML, JSON, CSV 형식으로 표현됩니다. 테이블 생성 시점에 각 테이블 이미지의 구조와 내용을 정확히 알고 있기 때문에, 모든 단어, 셀, 행의 바운딩 박스를 이미지 내 위치와 함께 주석으로 추가하였습니다. 이를 통해 정확한 셀의 위치 정보를 제공합니다. FinTabQA라는 모델을 SynFinTabs를 사용하여 파인트닝(미세 조정)하여 테이블에서의 시각적 질문-응답 분석을 위해 훈련하였습니다.
- ***Performance Highlights***: FinTabQA는 SynFinTabs를 이용해 파인트닝된 결과로 95.87%의 정확도를 기록하였으며, 실제 세상 데이터셋의 금융 문서에서 100개의 질문에 대한 테스트에서는 89%의 정확도를 보였습니다. 이는 신뢰할 수 없는 OCR로 인한 오류 영향을 줄이고, 데이터셋의 높은 품질 덕분에 가능한 성과입니다.

### [Densing Law of LLMs](https://arxiv.org/abs/2412.04315)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04315.png)

Vote: 8

Authors: Weilin Zhao, Maosong Sun, Jie Cai, Xu Han, Chaojun Xiao, Zhiyuan Liu, Guoyang Zeng

- ***What's New***: 이 연구에서는 대규모 언어 모델(LLMs)의 'capacity density'(용량 밀도)를 소개하여 트레이닝 품질을 평가하는 새로운 개념이 제안되었습니다. LLMs의 최대 용량 밀도는 시간이 지남에 따라 지수적으로 증가하는 'Densing Law'(덴싱 법칙)가 있다는 사실을 밝혀냈습니다.
- ***Technical Details***: 'capacity density'는 LLMs의 실제 파라미터 수와 동일한 성능을 얻기 위해 필요한 최소 파라미터 수의 비율로 정의됩니다. 이 연구에서는 2023년 이후 공개된 오픈 소스 기반 LLMs의 용량 밀도가 지수적으로 성장하는 추세를 보여줍니다. 수학적인 정의는 ln(ρmax) = At + B이며, 여기서 ρmax는 최대 용량 밀도, t는 시간입니다. 실험에서는 A ≈ 0.007, R² ≈ 0.93으로 나타났으며, 이는 약 3.3개월마다 용량 밀도가 두 배로 증가함을 의미합니다.
- ***Performance Highlights***: 실험 결과, LLMs의 추론 비용이 지수적으로 감소하고 있으며, 'Moore's Law'(무어의 법칙)와 결합하여 동일한 가격의 칩에서 실행 가능한 LLMs의 유효 파라미터 크기가 빠르게 증가하고 있습니다. 또한, ChatGPT의 출시 이후 LLMs 용량 밀도의 증가율이 50% 증가했습니다.

### [p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay](https://arxiv.org/abs/2412.04449)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04449.png)

Vote: 2

Authors: Tao Wu, Zhenpeng Huang, Limin Wang, Ji Qi, Desen Meng, Jun Zhang

- ***What's New***: p-MoD는 Progressive Ratio Decay(PRD)를 활용하여 비전 토큰의 중복성을 층별로 조절하고, 이를 통해 다중모달 대형 언어 모델(MLLM)의 효율성을 개선하는 새로운 메커니즘을 제안합니다. 기존의 Mixture-of-Depths(MoD) 메커니즘을 업그레이드하여 비전 정보 처리를 최적화합니다.
- ***Technical Details***: p-MoD는 tanh-gated weight normalization(TanhNorm)과 symmetric token reweighting(STRing)을 통해 MoD 모듈을 강화합니다. PRD 전략을 통해 층별로 토큰 유지 비율을 변화시켜 효율성을 극대화하고, 모델이 토큰의 중요성을 정확하게 평가하도록 학습합니다.
- ***Performance Highlights***: p-MoD는 두 개의 기저 모델과 비교하여 14개의 벤치마크에서 동등하거나 더 나은 성능을 보였습니다. 추론 시 TFLOPs는 55.6% 절감되고 KV 캐시 저장소는 53.8% 절감되었으며, 훈련 시간도 77.7% 감소하였습니다.

### [Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension](https://arxiv.org/abs/2412.03704)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03704.png)

Vote: 4

Authors: Lin Kevin, Wang Xiyao, Wang Lijuan, Lu Hongjin, Huang Furong, Lin Chung-Ching Lin, Li Linjie, Xu Yuancheng, Yang Zhengyuan

- ***What's New***: 이 연구는 Vision Value Model (VisVM)을 도입하여 비전-언어 모델(Vision-Language Models; VLMs)의 추론 시간 검색(Inference-Time Search)을 보다 효과적으로 수행하도록 합니다. 이로써 비주얼 이해와 입력 설명의 질을 큰 폭으로 향상시켰습니다.
- ***Technical Details***: VisVM은 템포럴 디퍼런스 학습(Temporal Difference Learning)을 이용하여 각 단계에서 생성된 응답의 장기 가치를 예측합니다. 문장 단위로 단계별로 이미지를 분석하고, 미래에 발생할 수 있는 환상을 방지하기 위해 다음 단계에서 발생할 응답의 가능성 있는 질을 예측함으로써 VLM의 검색 지시자로 작동합니다.
- ***Performance Highlights***: VisVM을 이용하여 생성된 설명문은 그리디 디코딩(Greedy Decoding) 및 CLIP-PRM 기반의 방법과 비교하여 환상을 줄이고 상세도를 높이며 성능이 10.8% 향상되었습니다. 인간 평가에서도 VisVM 기반 설명이 74%의 선호 비율을 보였습니다.

### [KV Shifting Attention Enhances Language Modeling](https://arxiv.org/abs/2411.19574)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.19574.png)

Vote: 4

Authors: Mingyu Xu, Weipeng Chen, Wei Cheng, Bingning Wang

- ***What's New***: KV Shifting 주의(KV Shifting Attention)는 최신 대형 언어 모델(Large Language Model; LLM)의 언어 모델링 성능을 향상시키기 위해 제안된 새로운 접근입니다. 이 방법은 주의 메커니즘에서 키(key)와 값(value)을 분리하여 효율적으로 학습할 수 있도록 설계되었습니다.
- ***Technical Details***: KV Shifting Attention은 트랜스포머의 induction heads 메커니즘의 깊이와 너비 요구 사항을 줄이기 위해 설계되었습니다. 이 방법은 기존의 다층 트랜스포머 대신 단일층으로도 유도(induction) 작업을 효율적으로 수행할 수 있게 합니다. 이는 각 헤드별로 학습 가능한 파라미터(α1, α2, β1, β2)를 추가하여 키와 값의 이동을 통해 구현됩니다.
- ***Performance Highlights***: 실험 결과, KV Shifting Attention은 다양한 크기의 모델에서 기존 벤치마크보다 뛰어난 성능을 보였으며, 특히 Lambda에서는 2.9B 모델이 수렴할 때 더 우수한 성능을 보였습니다. 또한 학습 속도 면에서도 두드러진 개선을 보였습니다. 19B 파라미터 모델에서도 유사한 향상된 성능을 달성했습니다.

### [Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion](https://arxiv.org/abs/2412.04424)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04424.png)

Vote: 26

Authors: Tianyi Zhou, Bin Xiao, Jianfeng Gao, Haiping Wu, Jianwei Yang, Jiuhai Chen, Dianqi Li

- ***What's New***: Florence-VL는 Florence-2라는 생성적 비전 기초 모델을 사용하여 시각적 특성을 강화한 멀티모달 대형 언어 모델(MLLMs)입니다. 이 모델은 Florence-2의 시각적 특징을 기존의 사전 학습된 언어 모델에 통합하기 위해 새로운 특성 융합 아키텍처와 혁신적인 훈련 방식, 깊이-넓이 융합(DBFusion)을 도입하여, 다양한 후속 작업 적용이 가능하도록 설계되었습니다.
- ***Technical Details***: Florence-VL는 Florence-2 모델을 비전 인코더로 사용하며, 다양한 컴퓨터 비전 작업에 대한 프롬프트 기반 표현을 통해 리치한 시각적 표현을 제공합니다. 깊이-넓이 융합(DBFusion)을 통해 다양한 층에서 추출된 개념 수준의 '양질의 깊이'와 다양한 프롬프트로 확장된 시각 정보의 '넓이'를 성공적으로 융합하여 모델 학습에 적극 활용합니다. 훈련은 고품질의 이미지 캡션과 지침 조정 데이터셋을 통해 이루어지며, 채널 통합 방식을 통해 시각적 특징을 LLM의 입력 공간으로 투영합니다.
- ***Performance Highlights***: Florence-VL는 각종 멀티모달 및 비전 중심 벤치마크에서 최첨단 MLLMs보다 뛰어난 성능 향상을 달성했습니다. 특히, 시각 중심(Vision-centric) 및 지식 기반(Knowledge-intensive), OCR 및 차트(Chart) 작업에 걸쳐 25개의 벤치마크를 커버하며 다른 모델에 비해 높은 정렬 품질을 나타냈습니다. 더불어, 이 연구는 Florence-2가 CLIP 및 SigLIP와 같은 일반적인 비전 인코더보다 LLM과의 정렬에 있어서 우수한 장점을 보인다는 것을 시각적으로 분석하여 보여줍니다.

### [A Noise is Worth Diffusion Guidance](https://arxiv.org/abs/2412.03895)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03895.png)

Vote: 21

Authors: Eunju Cha, Wooseok Jang, Hyoungwon Cho, Jiwon Kang, Seungryong Kim, Donghoon Ahn, Sayak Paul, Kyong Hwan Jin, Sanghyun Lee, SeonHwa Kim, Minjae Kim, Jaewon Min

- ***What's New***: NoiseRefine는 새로운 접근 방식으로, 기존의 Guidance 방법 없이도 높은 품질의 이미지를 생성할 수 있게 하며, 이를 통해 추론 속도와 메모리 효율성을 향상시킵니다. 기존 가우시안 노이즈를 'Guidance-Free Noise'로 변환하여 작은 저주파 요소가 이미지의 품질을 크게 향상시킬 수 있음을 발견했습니다.
- ***Technical Details***: NoiseRefine는 학습을 통해 초기 랜덤 노이즈를 가이드 없이도 성능을 발휘할 수 있는 'Guidance-Free Noise'로 매핑하는 방법론입니다. 이 과정은 다중 스텝의 Gradient Propagation을 생략하는 Multistep Score Distillation(MSD) 기법을 사용하여 효율적으로 구현됩니다. 이를 통해 Model Convergence 속도를 높이고, 소수의 텍스트-이미지 페어만으로 강력한 성능을 발휘하는 것이 가능합니다.
- ***Performance Highlights***: NoiseRefine를 사용한 경우, 기존의 가우시안 노이즈를 사용했을 때보다 눈에 띄게 개선된 이미지 품질을 보였으며, Concurrently Free Guidance(CFG)와 동등한 품질을 유지하면서도 약 2배 빠른 속도를 기록했습니다.

### [Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis](https://arxiv.org/abs/2412.04431)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04431.png)

Vote: 11

Authors: Zehuan Yuan, Bin Yan, Yuqi Zhang, Xiaobing Liu, Jinlai Liu, Yi Jiang, Bingyue Peng, Jian Han

- ***What's New***: Infinity는 비트 단위의 Visual AutoRegressive(MVAR) 모델로 고해상도 이미지 생성에서의 성능 향상을 이루었습니다. 특히 무한단어사전(Infinite-Vocabulary) 토크나이저와 분류기(Classifier)를 갖춘 비트 단위 자기 수정 메커니즘을 통해 세밀한 이미지 생성이 가능해졌습니다.
- ***Technical Details***: Infinity는 비트 단위의 비주얼 토크나이저(Visual Tokenizer)와 비트 단위 무한 단어사전 분류기, 그리고 비트 단위 자기 수정(Bitwise Self-Correction)으로 구성됩니다. 본 기술은 2^64 크기의 대규모 단어사전을 지원하며, 이를 통해 메모리 사용을 크게 줄였습니다. 비트 단위 자기 수정 메커니즘은 예측 오류를 시뮬레이션하고 재정량화하여 자기 수정 능력을 부여합니다.
- ***Performance Highlights***: Infinity는 다양한 벤치마크에서 기존 최고 수준의 확산 모델인 SDXL과 PixArt-Sigma를 뛰어넘는 성능을 보여주었습니다. GenEval 벤치마크 점수를 0.62에서 0.73으로, ImageReward 점수를 0.87에서 0.96으로 향상시켰으며, 1024x1024 해상도의 이미지를 생성하는 데 0.8초 소요됨으로써 SD3-Medium보다 2.6배 빠른 속도를 자랑합니다.

### [OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows](https://arxiv.org/abs/2412.01169)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01169.png)

Vote: 6

Authors: Shufan Li, Kazuki Kozuka, Akash Gokul, Aditya Grover, Yusuke Kato, Konstantinos Kallidromitis, Zichun Liao

- ***What's New***: OmniFlow는 새로운 생성 모델로, 다중 모달 정규 흐름(Multi-Modal Rectified Flows; MRF)을 사용하여 텍스트-이미지, 텍스트-오디오 및 오디오-이미지 생성 등의 모든-to-모든 생성 작업을 지원합니다. 이는 기존의 모든-to-모든 모델을 능가하며, 다중 모달의 공동 분포를 처리하는 독창적인 안내 메커니즘을 도입하여 생성된 출력에서 서로 다른 모달 간의 정렬을 유연하게 제어할 수 있도록 합니다.
- ***Technical Details***: OmniFlow는 Stable Diffusion 3의 텍스트-이미지 MMDiT 구조를 확장하여 오디오 및 텍스트 생성을 가능케 하는 새로운 아키텍처를 제안합니다. 이 확장 모듈은 각기 독립적으로 사전 학습이 가능하며, 이를 통해 효율적인 파인튜닝을 지원합니다. 또한, 대규모 오디오 및 텍스트 생성에 최적화된 다중 모달 정류 흐름 변환기를 설계하고 다양한 모달리티에 걸쳐 성능을 최적화하는 설계 선택에 대한 포괄적인 연구를 수행했습니다.
- ***Performance Highlights***: OmniFlow는 MSCOCO-30K 벤치마크에서 텍스트-이미지 생성 작업에서 최첨단 기술을 보유한 모델들을 능가합니다. 또한, 오디오 캡션 생성 작업에서 FAD 및 CLAP 점수를 통해 AudioLDM2보다 우수한 성능을 보여주며, CoDi와 같은 기존의 모든-to-모든 모델에 비해 상당한 성능 향상을 기록하였습니다. 이는 텍스트-이미지 정렬 및 이미지 충실도 면에서 기존 모델들과 비교했을 때 높은 생성 품질을 제공합니다.

### [HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing](https://arxiv.org/abs/2412.04280)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04280.png)

Vote: 10

Authors: Ling Yang, Hanwang Zhang, Xiangtai Li, Wei Chow, Juncheng Li, Shuicheng Yan, Jinbin Bai

- ***What's New***: HumanEdit는 이미지 편집의 인스트럭션 기반 데이터셋으로, 고품질의 인간 피드백을 통해 구성된 새로운 데이터셋입니다. 이전의 대규모 편집 데이터셋들이 인간 피드백을 거의 포함하고 있지 않은 문제를 HumanEdit는 인간 주석 작업자들을 채용하여 해결했습니다. HumanEdit는 6가지의 명령 유형(행위, 추가, 계산, 관계, 제거, 대체)을 포함하며, 다양한 실제 시나리오를 포괄합니다.
- ***Technical Details***: HumanEdit 데이터셋은 총 5,751개의 이미지로 구성되어 있으며, 2,500시간 이상의 인간 작업이 필요했습니다. 모든 이미지는 마스크(masks)를 포함하고 있으며, 일부 데이터는 마스크 없이도 편집이 가능하도록 자세한 인스트럭션이 제공됩니다. 이 데이터셋은 다양한 출처에서 고해상도의 1024 × 1024 콘텐츠를 포함하여, 이미지 편집 벤치마크로 설정되었습니다.
- ***Performance Highlights***: 여섯 가지 유형의 편집 명령에 대한 정량적 결과는 Add 태스크가 Remove 태스크보다 성능이 더 우수하며, 마스크가 제공된 방법이 일반적으로 픽셀 수준의 메트릭보다 더 뛰어난 성능을 보인다는 것을 보여줍니다. 이 데이터셋은 향후 이미지 편집 작업에 대해 더 세밀한 발전을 촉진하기 위한 벤치마크를 수립합니다.

### [4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion](https://arxiv.org/abs/2412.04462)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04462.png)

Vote: 2

Authors: Michael Vasilkovsky, Aliaksandr Siarohin, Peter Wonka, Tuan Duc Ngo, Hsin-Ying Lee, Sergey Tulyakov, Willi Menapace, Ivan Skorokhodov, Chaoyang Wang, Peiye Zhuang

- ***What's New***: 4Real-Video는 일반화 가능한 사진 실감형 4D 비디오 생성(4D Video Generation)을 위한 새로운 프레임워크를 제안합니다. 이 프레임워크는 시간과 시점 축을 갖는 비디오 프레임의 그리드로 구성된 4D 비디오를 생성하며, 각 행은 동일한 시간 스탬프를 공유하고 각 열은 동일한 시점에서의 프레임을 포함합니다. 두 개의 스트림 아키텍처를 제안하여 시점와 시간 일관성을 개선하며, 기존 비디오 모델의 강점을 활용하여 효율적인 4D 비디오 생성을 이끌어냅니다.
- ***Technical Details***: 제안된 모델은 두 개의 독립적인 토큰 스트림을 사용하는 두 방향 아키텍처(Two-Stream Architecture)를 사용해 비디오 프레임 그리드를 처리합니다. 하나의 스트림은 시점 업데이트를 담당하고 다른 하나는 시간 업데이트를 수행합니다. 동기화 레이어(Sync Layer)를 통해 두 스트림 간 정보를 교환하며 하드 혹은 소프트 동기화를 통해 각기 다른 강도로 토큰 상호작용을 조정합니다. 이 아키텍처는 시점과 시간 간 의존성을 더욱 잘 관리하여 세부적이고 시각적으로 일관된 4D 비디오를 생성합니다.
- ***Performance Highlights***: 4Real-Video 모델은 기존의 방법보다 훨씬 빠른 약 1분 내에 다양한 동적인 멀티뷰 비디오를 생성하는 데 성공하였습니다. 픽셀 기반의 확산 모델(Diffusion Model)을 기반으로 고해상도의 4D 비디오를 생성하며, FVD, CLIP, VideoScore 등 다양한 지표에서 탁월한 성능을 보였습니다. 특히, 멀티뷰 및 시간 일관성 측면에서도 높은 Dust3R-Confidence와 GIM-Confidence를 기록했습니다.

### [MV-Adapter: Multi-view Consistent Image Generation Made Easy](https://arxiv.org/abs/2412.03632)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03632.png)

Vote: 14

Authors: Lizhuang Ma, Haoran Wang, Zehuan Huang, Lu Sheng, Yuan-Chen Guo, Yan-Pei Cao, Ran Yi

- ***What's New***: MV-Adapter는 텍스트-이미지(T2I) 확산 모델을 멀티뷰 이미지 생성기로 변환하는 최초의 어댑터 기반 솔루션으로, 원래의 네트워크 구조나 기능 공간을 변경하지 않고 훨씬 적은 매개변수만 업데이트하여 효율적인 학습을 가능하게 합니다.
- ***Technical Details***: MV-Adapter는 중복된 자기주의 레이어와 병렬 주의 아키텍처를 포함하여 기본 T2I 모델의 강력한 선험적 지식을 그대로 상속하도록 설계되었습니다. 또한, 카메라 매개변수와 기하학적 정보를 공간 지도 표현에 원활하게 통합하는 통합 조건 인코더를 도입했습니다.
- ***Performance Highlights***: Stable Diffusion XL(SDXL)을 기반으로 한 MV-Adapter는 768 해상도에서 멀티뷰 생성 성능을 달성하며 다양한 모델 및 조건에서 적응성과 다재다능함을 보여줍니다. 실험 결과는 MV-Adapter가 멀티뷰 이미지 생성의 새로운 품질 표준을 설정하고 텍스트-이미지 모델의 응용 가능성을 확장함을 보여줍니다.

### [MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation](https://arxiv.org/abs/2412.04448)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04448.png)

Vote: 1

Authors: Bo An, Jiahao Lu, Yifan Zhang, Hanzhong Guo, Shuicheng Yan, Zhenxiong Tan, Longtao Zheng, Chuanxin Tang, Jiachun Pan

- ***What's New***: MEMO는 오디오 기반의 실감 나는 토킹 비디오 생성을 위해 제안된 새로운 접근 방식입니다. 이 방법은 오디오-립 동기화, 장기적인 아이덴티티 일관성을 유지하며 자연스럽고 감정적으로 맞춘 표정을 생성하는 데 중점을 둡니다.
- ***Technical Details***: MEMO는 두 가지 주요 모듈로 구성됩니다: (1) 메모리 가이드 시간 모듈(temporal module)은 메모리 상태를 개발하여 장기적인 아이덴티티 일관성과 모션 부드러움을 향상시키고, 선형 주의(attention)를 통해 시간을 모델링합니다. (2) 감정 인식 오디오 모듈은 전통적인 크로스 어텐션을 멀티모달 어텐션으로 대체하여 오디오-비디오 상호작용을 향상시키고, 오디오에서 감정을 탐지하여 감정 적응 층 표준화(emotion adaptive layer norm)를 통해 표정을 조정합니다. 또한, 감정 디커플링(emotion decoupling) 훈련 전략을 도입하여 오디오의 감정적 변화를 고려한 표현을 생성합니다.
- ***Performance Highlights***: MEMO는 다양한 이미지 및 오디오 유형에서 더 현실적인 토킹 비디오를 생성하여, 전반적인 품질, 오디오-립 동기화, 아이덴티티 일관성 및 표현-감정 맞춤의 상태-of-the-art 방법을 능가합니다. 광범위한 정량적 및 정성적 결과는 MEMO의 뛰어난 성능을 입증하며, 노이즈 오류 누적을 줄이고 장기적 아이덴티티 일관성을 향상시키는 데 기여합니다.

### [Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement](https://arxiv.org/abs/2412.04003)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04003.png)

Vote: 4

Authors: Yu Zhao, Yangyang Liu, Lingfeng Ming, Yefeng Liu, Hao Zhou, Xue Yang, Weihua Luo, Chenyang Lyu, Xiaohu Zhao, Heng Liu, Linlong Xu, Longyue Wang, Yiyu Wang, Haijun Li, Kaifu Zhang, Huifeng Yin, Tianqi Shi, Bo Zeng, Hao Wang, Zifu Shang

- ***What's New***: Marco-LLM은 다중 언어 훈련을 통해 저자원 언어 작업에서의 성능 격차를 줄이고, 다개국 언어 모델로서의 성능을 크게 향상시킬 수 있도록 설계된 최초의 모델입니다. 특히 저자원 언어를 포함한 다양한 언어에서 뛰어난 성능을 발휘하며, 기존 고자원 언어와의 성능 차이를 줄이는 데 주력합니다.
- ***Technical Details***: Marco-LLM은 Qwen2 모델을 기반으로 대규모 다국어 데이터를 수집하여 연속적인 다국어 전훈련(Continual Pre-training)과 포스트 훈련(Post-training)을 수행했습니다. 이 프로세스는 대규모 데이터 정제, 병렬 데이터 사용, 고품질 지식 데이터 및 합성 데이터 활용을 포함합니다. 두 단계로 이루어진 전훈련 전략을 통해 다국어 적응을 강화하고 이전 데이터셋 학습의 손실을 방지합니다.
- ***Performance Highlights***: Marco-LLM은 다양한 벤치마크에서 기존 최첨단 모델들을 초과하는 성능을 보여줍니다. MMMLU, Flores, CEVAL, TydiQA 등의 다국어 평가에서 Marco-72B는 70B+ 파라미터 모델 중 최고 성적을 기록했습니다. 특히 저자원 언어에서 뛰어난 개선을 보였으며, CEVAL에서 94.5점, Flores에서 영어-다국어 번역에서 평균 43.8점을 기록하여 강력한 다국어 처리 능력을 입증했습니다.

### [Challenges in Trustworthy Human Evaluation of Chatbots](https://arxiv.org/abs/2412.04363)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04363.png)

Vote: 0

Authors: Wenting Zhao, Alexander M. Rush, Tanya Goyal

- ***What's New***: 이 연구는 Chatbot 평가에서 인간의 신뢰할 수 있는 평가에 대한 도전 과제를 탐구합니다. 특히, 악의적이거나 무관심한 사용자들의 잘못된 투표가 모델 순위에 미치는 영향을 분석하고, 신뢰성 있는 평가를 위한 개방형 플랫폼에서의 품질 보증 메커니즘에 대한 필요성을 강조합니다.
- ***Technical Details***: 이 논문에서는 Chatbot Arena 플랫폼을 사용하여 사례 연구를 진행하였습니다. 주요 문제는 (1) 외부 유인이 없는 사용자들의 무관심한 투표, (2) 악의적인 공격자가 목표 모델의 순위를 인위적으로 높이려는 경우, (3) 주관적인 질의에 대한 임의의 선호 투표입니다. 이러한 투표는 최종 모델의 순위에 중대한 영향을 미칠 수 있으며, 포스트 훅 방법으로도 쉽게 검출되지 않습니다.
- ***Performance Highlights***: 간단한 모델 귀속 알고리즘을 통해 Llama-2, Mistral과 같은 시험 모델들의 순위에 대한 공격을 시뮬레이션한 결과, 10%의 악의적 투표가 모델 순위를 5위 이상 변화시킬 수 있음을 보여주었습니다. 이는 오픈 플랫폼이 신뢰할 수 있는 평가 환경을 유지하기 위한 더욱 강력한 가드레일의 필요성을 시사합니다.

### [Monet: Mixture of Monosemantic Experts for Transformers](https://arxiv.org/abs/2412.04139)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04139.png)

Vote: 5

Authors: Jaewoo Kang, Kee-Eung Kim, Young Jin Ahn, Jungwoo Park

- ***What's New***: 이 논문에서는 대형 언어 모델(LLMs)의 다의성을 해결하기 위한 새로운 방법으로, 트랜스포머에 대한 단의적 전문가 집합(Mixture of Monosemantic Experts; MONET) 아키텍처를 제안합니다. 이는 단일 전문가가 서로 무관한 여러 개념에 반응하는 문제를 해결하여 기계적 해석 가능성을 높이는 데 중점을 두고 있습니다.
- ***Technical Details***: MONET 아키텍처는 희소 딕셔너리 학습을 엔드-투-엔드 전문가 집합 사전 훈련 과정에 통합하여, 다의성 문제를 해결합니다. 전문가의 수를 층당 262,144개까지 확장하고, 전체 파라미터는 전문가 수의 제곱근에 비례하여 증가합니다. 이는 전통적인 전문가 집합 모델의 메모리 제약을 극복할 수 있게 해줍니다.
- ***Performance Highlights***: MONET 아키텍처는 LMMs의 기계적 해석 가능성을 높이고, 종합적 성능 저하 없이 지식 도메인, 언어, 유해성 감소 기능을 효과적으로 조작할 수 있음을 실험을 통해 입증합니다. 이는 기존의 전문가 집합 모델보다 우수한 도메인 특화 지식 조작 성능을 보여줍니다.

### [NVILA: Efficient Frontier Visual Language Models](https://arxiv.org/abs/2412.04468)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04468.png)

Vote: 17

Authors: Jan Kautz, Song Han, Yunhao Fang, Ligeng Zhu, Shang Yang, Cheng-Yu Hsieh, Yuming Lou, Dacheng Li, Yuxian Gu, Sifei Liu, Hongxu Yin, Daguang Xu, Baifeng Shi, Vishwesh Nath, Yao Lu, Xiaolong Wang, Zhijian Liu, Haocheng Xi, Ranjay Krishna, Shiyi Cao, Zhuoyang Zhang, Yukang Chen, Pavlo Molchanov, De-An Huang, Xiuyu Li, Jinyi Hu, An-Chieh Cheng

- ***What's New***: NVILA는 Visual Language Models (VLM)의 효율성을 강조하여 효율성과 정확성을 최적화한 새로운 VLM입니다. VILA를 기반으로 공간 및 시간 해상도를 확장하고 시각적 토큰을 압축하여 고해상도의 이미지와 긴 동영상을 처리할 수 있는 'scale-then-compress' 접근법을 제시합니다.
- ***Technical Details***: NVILA 것은 세 가지 주요 구성 요소, 즉 비주얼 인코더(Visual Encoder), 프로젝터(Projector), 토큰 프로세서(Token Processor)로 구성된 Auto-regressive VLM입니다. SigLIP를 비전 인코더로 사용하고, Qwen2를 다양한 크기의 토큰 프로세서로 특징화합니다. 스파티얼 및 템포럴 해상도를 개선한 뒤, 시각적 토큰을 압축하여 효율성을 강화합니다. FP8와 같은 혼합 정밀도를 활용하여 훈련 속도를 높이고 학습 과정 전반에 걸친 효율성을 최적화합니다.
- ***Performance Highlights***: NVILA는 이미지 및 비디오 벤치마크에서 최상의 성능을 발휘하며, 훈련 비용을 4.5배, 파인튜닝 메모리 사용을 3.4배, 인퍼런스 시간의 지연을 1.6-2.2배 줄입니다. 또한 GPT-4o와 Gemini 같은 기존의 대형 모델에 비해 경쟁력 있는 성능을 보여주며, 의료 이미지, 로봇 내비게이션 등 새로운 기능을 가능하게 합니다.

### [Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction](https://arxiv.org/abs/2412.04454)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04454.png)

Vote: 27

Authors: Zekun Wang, Caiming Xiong, Amrita Saha, Junli Wang, Yiheng Xu, Tao Yu, Tianbao Xie, Dunjie Lu, Doyen Sahoo

- ***What's New***: AGUVIS는 다양한 플랫폼에서 작동하는 순수 비전 기반의 통합적 GUI 에이전트 프레임워크로 처음 소개되었습니다. 이미지 기반의 관찰과 자연어 지시를 비주얼 요소로 연결하며, 일관된 행동 공간을 사용하여 플랫폼 간의 일반화를 보장하는 새로운 접근 방식입니다.
- ***Technical Details***: AGUVIS는 순수 비전 관찰과 플러그 가능(Pluggable) 액션 시스템이 포함된 일관된 액션 공간을 활용하여 다양한 플랫폼에서 일반화와 추론 비용 감소를 달성했습니다. GUI 에이전트의 훈련을 위해 멀티모달 추론과 그라운딩을 통합한 대규모 데이터 세트를 구성하고, GUI 그라운딩 및 계획 및 추론에 중점을 둔 두 단계 훈련 파이프라인을 사용합니다.
- ***Performance Highlights***: AGUVIS는 이전 최첨단 방법들을 뛰어넘어 여러 플랫폼에서 오프라인 및 온라인 테스트에서 우수한 성능을 발휘하였으며, 닫힌 소스 모델에 의존하지 않고도 독립적으로 실제 작업을 완료할 수 있는 최초의 완전 자동화된 순수 비전 기반 GUI 에이전트임을 입증했습니다.

### [AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models](https://arxiv.org/abs/2412.04146)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04146.png)

Vote: 11

Authors: Qichao Sun, Zhichao Liao, Fulong Ye, Qian He, Songtao Zhao, Pengze Zhang, Xinghui Li, Wanquan Feng

- ***What's New***: AnyDressing은 다중 의상을 착용한 가상 캐릭터를 원하는 텍스트 프롬프트에 맞춰 생성할 수 있는 새로운 방법입니다. 이 방법은 의상 세부사항을 상세히 추출하고, 맞춤형 이미지를 생성할 수 있는 두 가지 주요 네트워크 GarmentsNet과 DressingNet으로 구성되어 있습니다.
- ***Technical Details***: GarmentsNet은 Garment-Specific Feature Extractor(GFE) 모듈을 통해 다중 의상의 세부적인 특징을 병렬로 추출하여 네트워크 혼합을 방지하고 효율성을 보장합니다. DressingNet에서는 Dressing-Attention(DA) 메커니즘과 Instance-Level Garment Localization Learning(IGL) 전략을 활용하여 각 의상 특징을 해당하는 이미지 영역에 정확히 주입합니다. 또한, Garment-Enhanced Texture Learning(GTL) 전략으로 의상의 세밀한 텍스처 세부사항을 향상시킵니다.
- ***Performance Highlights***: AnyDressing은 다양한 장면과 스타일에서 사용자 지정 텍스트 프롬프트에 맞춘 높은 품질의 가상 드레싱을 가능하게 합니다. 광범위한 실험에서 AnyDressing이 최신 기술 수준의 결과를 달성했으며, 이는 단일 및 다중 의상 가상 드레싱 작업 모두에서 뛰어난 신뢰성을 보여줍니다.

### [Evaluating Language Models as Synthetic Data Generators](https://arxiv.org/abs/2412.03679)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03679.png)

Vote: 20

Authors: Sean Welleck, Yizhong Wang, Graham Neubig, Vijay Viswanathan, Xiang Yue, Seongyun Lee, Seungone Kim, Juyoung Suk, Carolin Lawrence, Kiril Gashteovski

- ***What's New***: 이 논문은 AGORABENCH라는 새로운 벤치마크를 제안하여 언어 모델(당신 예측기)의 데이터 생성 능력을 평가합니다. AGORABENCH는 고유한 설정과 지표를 제공하여 언어 모델의 데이터 생성 능력을 체계적으로 비교합니다. 이를 통해 6개의 언어 모델을 사용하여 1.26백만 개의 훈련 인스턴스를 합성하고 99개의 학생 모델을 훈련하며, 언어 모델이 데이터 생성에서 나타내는 주요 통찰을 드러냅니다.
- ***Technical Details***: AGORABENCH는 9가지 설정에서 언어 모델의 데이터 생성 능력을 평가하는 벤치마크입니다. 이는 수학, 명령어-팔로우, 코드라는 세 가지 도메인과 인스턴스 생성, 응답 생성, 품질 향상이라는 세 가지 데이터 생성 방법을 결합하여 제공합니다. 각 설정 내에서 데이터 생성기만 변경되고 나머지는 고정되어 비교가 가능합니다. 성능 격차 회복(Performance Gap Recovered; PGR)라는 지표를 통해 학생 모델이 데이터에 의해 얼마나 개선되는지를 측정합니다.
- ***Performance Highlights***: GPT-4o는 새로운 인스턴스 생성에서 46.75%의 PGR 개선을 보이며, 데이터 생성기들 중 두드러진 성과를 보입니다. 반면 Claude-3.5-Sonnet은 기존 인스턴스를 개선하는 품질 향상 부문에서 17.89%로 우수한 성과를 냅니다. 이 실험 결과는 더욱 효과적인 데이터 생성기를 선택하는 데에 도움이 될 수 있습니다. 또한, 데이터 생성 능력은 반드시 문제 해결 능력과 강하게 상관되지 않음을 발견했습니다.

### [VisionZip: Longer is Better but Not Necessary in Vision Language Models](https://arxiv.org/abs/2412.04467)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04467.png)

Vote: 53

Authors: Jingyao Li, Senqiao Yang, Bei Yu, Yukang Chen, Chengyao Wang, Zhuotao Tian, Jiaya Jia

- ***What's New***: VisionZip은 시각적 토큰(visual tokens)의冗장성을 줄이면서 대형 언어 모델(LLM)의 성능을 유지하는 새로운 방법론입니다. CLIP와 SigLIP과 같은 인기 있는 비전 인코더에서 생성된 시각적 토큰의冗장성을 인식하고, 이를 선택적으로 줄임으로써, 효율성을 개선하고 성능을 유지할 수 있습니다.
- ***Technical Details***: VisionZip은 정보가 많은 토큰 세트를 선택하여 언어 모델에 입력합니다. 주어진 시각적 문맥과 정의된 토큰을 기반으로 가장 중요한 토큰(dominant tokens)을 선택하고, 그 외의 토큰은 유사성을 기반으로 병합합니다. 이 방식은 트레이닝 없이 적용할 수 있으며, fine-tuning 모드에서는 최소한의 데이터로 프로젝터 층을 조정하여 토큰 수 감소에 따른 misalignment를 보완합니다.
- ***Performance Highlights***: VisionZip을 적용한 모델은 기존 state-of-the-art 방법들에 비해 최소 5% 이상의 성능 향상을 보였습니다. 특히 LLaVA-NeXT 13B 모델은 같은 크기의 LLaVA-NeXT 7B 모델보다 더 빨리 추론하고, 더 나은 결과를 달성했습니다. VisionZip을 통해 시각적 토큰의 수를 10%까지 줄여도 성능의 95% 이상을 유지할 수 있었습니다.

### [ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality](https://arxiv.org/abs/2412.04062)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04062.png)

Vote: 7

Authors: Yuanyu He, Hong Zhou, Bohan Zhuang, Yefei He, Shaoxuan He, Kaipeng Zhang, Feng Chen

- ***What's New***: ZipAR는 자가 회귀(autoregressive) 시각 생성 모델의 속도를 높이기 위한 새로운 병렬 디코딩 프레임워크로, 시각적 콘텐츠의 공간적 근접성을 활용하여 한 번의 모델 전진 단계에서 여러 인접한 시각적 토큰을 예측합니다. 이를 통해 기존의 다음 토큰 예측(next-token prediction) 패러다임에 비해 생성 효율성을 크게 향상시킵니다.
- ***Technical Details***: ZipAR는 훈련이 필요 없는 플러그 앤 플레이 방식의 병렬 디코딩 프레임워크로, 이미지를 생성할 때 한 번의 모델 전진 단계에서 여러 인접한 시각적 토큰을 디코딩합니다. 이는 각 행의 토큰을 개별적으로 순차적으로 생성하는 대신, 공간적으로 인접한 여러 토큰들이 연결되어 나타나는 강한 상관 관계를 활용합니다. ZipAR는 16의 윈도우 크기를 사용하는 경우 모델 전진 단계를 30.5% 줄이면서 성능 저하를 최소화합니다.
- ***Performance Highlights***: ZipAR는 Emu3-Gen 모델에서 최대 91%까지 모델 전진 단계를 줄여 시각적 생성 속도를 크게 향상시킵니다. 실험 결과 LlamaGen 및 Lumina-mGPT와 같은 최신 자가 회귀 시각 생성 모델과의 결합에서, ZipAR는 고해상도 이미지 생성 시 성능 저하 없이 클립 점수(CLIp Score)의 변화 없이 추론 지연(latency)을 최대 83.3%까지 줄였습니다.

### [Structured 3D Latents for Scalable and Versatile 3D Generation](https://arxiv.org/abs/2412.01506)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01506.png)

Vote: 18

Authors: Zelong Lv, Sicheng Xu, Jianfeng Xiang, Ruicheng Wang, Dong Chen, Jiaolong Yang, Xin Tong, Yu Deng, Bowen Zhang

- ***What's New***: 이 논문은 통합된 구조적 잠재 표현(Structured LATent; 심층 표현)을 통해 다양한 출력 형식으로 디코딩할 수 있는 새로운 3D 생성 방법을 소개합니다. 이 접근법은 다차원 시각적 특징을 통합하여 구조적 및 질감 정보를 포괄적으로 캡쳐하고, 다양한 3D 자산의 고품질 생성을 지원합니다.
- ***Technical Details***: 이 모델은 SLAT(Structured Latents)를 통해 3D 자산을 인코딩합니다. 듬성듬성 배치된 3D 그리드와 강력한 시각 기반 모델에서 추출된 밀도 있는 다각적 시각적 특징을 결합하여 다양한 3D 표현으로 디코딩할 수 있습니다. 랙티파이드 플로우 트랜스포머(rectified flow transformers)를 사용하여 최대 20억 개의 파라미터로 대규모 3D 자산 데이터셋에서 학습되었습니다.
- ***Performance Highlights***: 이 연구는 기존 방법을 능가하는 고품질의 결과를 텍스트 또는 이미지 조건에서 생성할 수 있음을 보여주었습니다. 다양한 형식으로의 유연한 출력 선택 및 국소적인 3D 편집 기능을 제공하여 이전 모델에서는 제공되지 않던 유연성을 강조했습니다.

### [Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection](https://arxiv.org/abs/2412.04455)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04455.png)

Vote: 31

Authors: Zhongyuan Wang, Zhizheng Zhang, Tiejun Huang, Qi Su, Cheng Chi, Lu Sheng, He Wang, Enshen Zhou

- ***What's New***: Code-as-Monitor (CaM)는 비전-언어 모델(Vision-Language Model; VLM)을 활용하여 반응적 및 능동적 로봇 실패 탐지를 동시에 수행할 수 있는 새로운 패러다임을 제안합니다. 이 방법은 공간-시간 제약 만족 문제를 통합하여 실시간 모니터링을 가능케 합니다.
- ***Technical Details***: CaM은 주어진 환경 내에서 로봇과 물체 또는 그 부품들이 필요한 상태를 유지하는지를 검토하는 '모니터 코드'를 VLM이 생성하는 방식으로 제약 인식 시각적 프로그래밍을 수행합니다. 이를 위해 제약과 관련된 엔티티나 그 부품을 기하 요소로 단순화하여 실시간 탐지를 용이하게 합니다.
- ***Performance Highlights***: CaM은 세 가지 시뮬레이터 및 실제 환경에서 약 28.7%의 높은 성공률을 보였으며, 심한 방해 조건에서 평균 실행 시간을 약 31.8% 단축했습니다. 특히, 단일 프레임에서의 VLM 쿼리 대신 코드 기반 평가를 통해 필요할 때에만 계획을 재수립함으로써 효율성이 개선됩니다.

### [Discriminative Fine-tuning of LVLMs](https://arxiv.org/abs/2412.04378)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04378.png)

Vote: 6

Authors: Alexandros Xenos, Yassine Ouali, Georgios Tzimiropoulos, Anestis Zaganidis, Brais Martinez, Ioannis Maniadis Metaxas, Adrian Bulat

- ***What's New***: 이 논문에서는 활성화된 대형 이미지-언어 모델(Large Vision-Language Models; LVLMs)을 판별적(Discriminative)으로 미세 조정하는 새로운 방법론을 제안합니다. 이 방법론은 생성 모델에서 판별적 모델로 변환하여 향상된 이미지-텍스트 판별 및 언어 이해 능력을 제공합니다.
- ***Technical Details***: 새로운 학습/최적화 프레임워크는 이미지-텍스트 쌍을 다양한 길이와 세분화로 활용하며, 대조적(Contrastive) 손실과 다음-토큰 예측 손실을 결합하여 강력한 판별 및 조합 역량을 발휘합니다. 이러한 접근 방식은 소프트 프롬프트(Soft Prompting)와 LoRA 어댑터를 결합한 파라미터 효율적인 적응 기법으로 미세 조정됩니다.
- ***Performance Highlights***: 이 새 방법론은 유사한 규모의 최신 CLIP 유사 모델들과 비교하여 표준 이미지-텍스트 검색 벤치마크에서 +4.7-7.0%의 절대 성능 향상을 보입니다. 또한, 여러 비전-언어 이해 및 조합성 벤치마크에서 최대 +15%의 주목할 만한 성능 향상이 있습니다.

### [Negative Token Merging: Image-based Adversarial Feature Guidance](https://arxiv.org/abs/2412.01339)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01339.png)

Vote: 18

Authors: Ranjay Krishna, Yejin Choi, Pang Wei Koh, Jaskirat Singh, Liang Zheng, Weijia Shi, Lindsey Li, Michael F. Cohen, Stephen Gould, Luke Zettlemoyer

- ***What's New***: 이 논문은 네거티브 토큰 병합(Negative Token Merging; NegToMe)이라는 새로운 방식을 도입하여, 텍스트 기반의 네거티브 프롬프트를 사용하는 대신 이미지 자체를 이용해 직접적으로 시각적 특징을 분리해 내는 훈련이 필요 없는 접근법을 소개합니다. 이 방식은 이미지 생성의 다양성을 높이고 저작권 있는 콘텐츠와의 시각적 유사성을 줄이는 데 활용할 수 있습니다.
- ***Technical Details***: NegToMe는 디퓨전 모델에서 역방향 디퓨전 프로세스 동안 생성된 이미지와 참조 이미지 사이의 시각적 특징을 분리함으로써 적대적 가이던스를 수행합니다. 이 방법은 Transformer 블록의 주의 집중과 MLP 레이어 사이에 네거티브 토큰 병합 모듈을 삽입하여 수행됩니다. 각 소스 토큰을 참조 이미지의 가장 일치하는 목표 토큰과 선형 외삽을 통해 분리합니다.
- ***Performance Highlights***: NegToMe는 특정 사용 사례에서 저작권이 있는 캐릭터와의 시각적 유사성을 34.57%까지 줄이고, 출력 다양성을 크게 향상시킵니다. 또한, SDXL 및 Flux와 같은 다양한 디퓨전 아키텍처와 호환되며, <4%의 약간 증가된 추론 시간으로 작동합니다.

### [MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities](https://arxiv.org/abs/2412.04106)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04106.png)

Vote: 5

Authors: Weidi Xie, Haoning Wu, Ziheng Zhao, Yanfeng Wang, Ya Zhang

- ***What's New***: MRGen은 새로운 패러다임을 도입하여 마스크 주석이 없는 MRI 모달리티를 위한 데이터를 생성하여 세그멘테이션 모델을 훈련할 수 있도록 합니다. 이 데이터 엔진은 다양한 모달리티에 대한 텍스트 프롬프트와 마스크를 조건으로 한 MR 이미지를 합성할 수 있습니다.
- ***Technical Details***: MRGen은 대규모 방사선 이미지-텍스트 데이터셋인 MedGen-1M을 기반으로 개발되었으며, 이 데이터셋은 모달리티 레이블, 속성, 영역 및 장기 정보와 장기 마스크 주석의 하위 집합을 포함합니다. 제안된 MRGen은 텍스트 가이드 프리트레이닝과 마스크 조건 미세 조정의 두 단계 훈련 전략을 채택하여 목표 모달리티에 대한 세그멘테이션 모델 훈련을 지원합니다.
- ***Performance Highlights***: MRGen은 다양한 모달리티에서 고품질 MR 이미지를 제어 가능하게 합성하여 주석이 없는 모달리티에서의 세그멘테이션 성능을 향상시킵니다. 실험 결과, MRGen을 통해 생성된 데이터로 훈련된 세그멘테이션 모델은 주석이 없는 시나리오에서 우수한 성능을 발휘할 수 있음을 입증하였습니다.

### [Towards Universal Soccer Video Understanding](https://arxiv.org/abs/2412.01820)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.01820.png)

Vote: 6

Authors: Weidi Xie, Haoning Wu, Hao Jiang, Yanfeng Wang, Ya Zhang, Jiayuan Rao

- ***What's New***: 이 논문에서는 축구 비디오 이해를 위한 종합적인 멀티모달 프레임워크를 개발했습니다. 특히, 1,988개의 전체 경기 비디오와 상세한 주석을 포함한 최대 규모의 멀티모달 축구 데이터셋 SoccerReplay-1988을 소개하고, 축구 도메인에서 최초로 시각-언어 기반 모델(MatchVision)을 제시하여 다양한 다운스트림 작업에서 뛰어난 성능을 보였습니다.
- ***Technical Details***: SoccerReplay-1988 데이터셋은 주요 유럽 축구 리그의 1,988개 경기를 포함하며 자동화된 주석 파이프라인을 통해 확보된 풍부한 주석을 특징으로 합니다. MatchVision 모델은 시사-언어 모델(SigLIP)을 기반으로 하여 시공간 정보(spatiotemporal information)를 활용하며, 이벤트 분류(event classification), 해설 생성(commentary generation), 다중 뷰 파울 인식 등의 다양한 작업에 적응할 수 있습니다.
- ***Performance Highlights***: MatchVision은 다양한 다운스트림 작업에서 기존 모델들을 능가하며, 주어진 데이터와 통합된 벤치마크에 대해 최첨단 성능을 달성하였습니다. 이로써, 제안된 데이터와 모델의 우수성이 입증되었습니다.

### [Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation](https://arxiv.org/abs/2412.03304)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03304.png)

Vote: 6

Authors: Shayne Longpre, Andre F. T. Martins, Daphne Ippolito, Sara Hooker, Beyza Ermis, Enzo Ferrante, Alice Oh, Wei-Yin Ko, Shivalika Singh, Kelly Marchisio, Jian Gang Ngui, Yosephine Susanto, Marzieh Fadaee, Raymond Ng, Daniel Vila-Suero, Peerat Limkonchotiwat, Antoine Bosselut, Wei Qi Leong, Clémentine Fourrier, Leshem Choshen, Madeline Smith, David I. Adelani, Angelika Romanou

- ***What's New***: Global MMLU은 다국어 평가에서 문화적 및 언어적 편견을 이해하고 해결하는 목표를 가지고 새롭게 출시된 데이터셋입니다. 이 데이터셋은 42개의 언어를 포함하며, 각 언어에 대한 평가 품질을 향상하기 위해 보상 받는 전문 및 커뮤니티 주석자를 통해 번역 품질과 문화적 편향을 엄격하게 평가했습니다.
- ***Technical Details***: Global MMLU는 Massive Multitask Language Understanding (MMLU) 데이터셋을 기반으로 생성되었습니다. 14,000개의 샘플을 영어에서 출발하여, 42개 언어로 번역하고 이를 다국어 평가에 사용합니다. 특히, 문화적으로 민감한(culturally-sensitive; CS) 지식과 문화적으로 무관한(culturally-agnostic; CA) 지식을 구분하여 평가할 수 있도록 각 샘플에 대한 메타데이터를 제공합니다.
- ***Performance Highlights***: 최신 모델 14개를 평가한 결과, CS 및 CA 데이터셋에서의 성능이 상당히 다르게 나타났습니다. 대부분의 모델은 사회 과학 및 인문학 분야에서 더 나은 성능을 보였으며, CS 데이터셋에서 평균 정확도가 CA 데이터셋보다 높았으나 변동성도 높았습니다. 특히, 문화적으로 민감한 문제에서는 모델의 랭킹이 상당히 변화하는 경향이 나타났습니다.

### [VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding](https://arxiv.org/abs/2412.02186)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.02186.png)

Vote: 20

Authors: Woongyeong Yeo, Kangsan Kim, Sung Ju Hwang, Geon Park, Youngwan Lee

- ***What's New***: 이 논문에서는 비디오 대형 멀티모달 모델(Video Large Multimodal Models; Video LMMs)이 학습 데이터에서 적게 나타나는 분포 외 영상(Out-of-Distribution; OOD)에서의 성능 저하 문제를 해결하기 위해 VideoICL이라는 새로운 영상 내 맥락 학습 프레임워크를 제안합니다. 특히 유사도 기반 예제 선택 전략(Similarity-based example selection strategy)과 신뢰도 기반 반복 추론 기법(Confidence-based iterative inference approach)을 도입하여, 적절한 예제들을 선택하고 유사도에 따라 순위를 매겨 추론에 활용하는 방법을 개발했습니다.
- ***Technical Details***: VideoICL은 기존의 비디오 LMMs에서 발생하는 맥락 길이 제한 문제를 극복하기 위해 설계되었습니다. 이를 위해 주어진 쿼리와 테스트 예제의 벡터 표현에서 코사인 유사도를 통해 유사한 예제를 선택하고, 예제 데이터를 사전 처리해 벡터 데이터베이스에 저장하는 방식으로 비용을 최소화합니다. 반복 추론에서는 각 반복 시점마다 상위 유사도 순위의 예제 하위 집합을 활용해 신규 답안을 생성하고 신뢰도 점수를 기반으로 판단하여 충분한 신뢰도를 얻을 때까지 반복합니다.
- ***Performance Highlights***: 약 25.6%p에서 최대 54.6%p까지 OOD 비디오 이해에서 상위 성과를 기록하였으며, 캡션 생성 성능에서는 0.143 BLEU-4 포인트까지 향상되었습니다. VideoICL은 7B 파라미터를 가진 모델로도 72B 모델을 초과하는 성능을 보여주며, LoRA 튜닝 모델도 일부 데이터셋에서 능가합니다. 신뢰도 추정 기법을 통해 성능을 크게 향상시킴으로써 VideoICL의 비훈련 방식이 기존 최첨단 모델보다 확장성과 일반화를 증가시킵니다.

