## Daily Papers (2024-12-09)

### [Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction](https://arxiv.org/abs/2412.04887)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04887.png)

Vote: 10

Authors: Wanhua Li, Jixuan Fan, Yansong Tang, Yifei Han

- ***What's New***: 이 논문에서는 Momentum-GS라는 새로운 접근 방식을 소개합니다. 이는 대규모 장면의 3D 재구성을 위한 모멘텀 기반 자체 증류(Momentum-based Self-Distillation)를 활용한 방법으로, 블록 간의 일관성과 일관성을 높여 새로운 최첨단 성능을 달성합니다.
- ***Technical Details***: Momentum-GS는 하이브리드 표현 방식(Hybrid Representations)을 사용하여 GPU의 수와 블록 수를 분리함으로써 대규모 장면 재구성의 확장을 가능하게 합니다. 모멘텀을 사용한 자체 증류 기법을 통해 각 블록에 글로벌 가이드를 제공하여 일관성을 유지하며, 재구성 품질에 따라 각 블록의 가중치를 동적으로 조정하여 균형 잡힌 훈련을 보장합니다.
- ***Performance Highlights***: Momentum-GS는 CityGaussian 대비 LPIPS를 12.8% 개선하였으며, 훨씬 적은 수의 블록을 사용하며도 더 나은 일관성과 높은 품질을 보여주었습니다. 이는 기존 방법들보다 블록 내 균일성과 전반적인 재구성 품질이 뛰어남을 나타냅니다.

### [SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion](https://arxiv.org/abs/2412.04301)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04301.png)

Vote: 19

Authors: Anh Tran, Trong-Tung Nguyen, Khoi Nguyen, Quang Nguyen, Cuong Pham

- ***What's New***: SwiftEdit는 텍스트를 통해 주도되는 이미지 편집을 순식간에 가능하게 하는 새로운 도구로, 기존의 멀티스텝 방식보다 최소 50배 더 빠른 성능을 보여줍니다. SwiftEdit의 혁신은 원스텝(One-Step) 인버전 프레임워크와 마스크 기반 편집 기법에 기반합니다.
- ***Technical Details***: SwiftEdit는 SwiftBrushv2 기반의 원스텝 텍스트-이미지 생성 모델을 활용하며, 이 모델을 중심으로 하여 인버전 네트워크를 두 단계로 학습시켜 어떤 이미지를 단 한 번의 스텝으로 변환할 수 있도록 합니다. 새로운 관심확대(attention-rescaling) 기술을 도입해 편집 영역과 비편집 영역의 영향을 세부적으로 조절하여 편집 강도를 조절할 수 있습니다.
- ***Performance Highlights***: SwiftEdit는 PieBench 벤치마크에서 다단계(50 스텝) 및 몇 단계(4 스텝) 방법과 비교했을 때 매우 빠른 속도(0.23초)로 경쟁력 있는 편집 결과를 얻었습니다. 또한, SwiftEdit는 다양한 테스트에서 배경 보존(BG Preservation) 점수와 편집 의미 반영(CLIP Semantics)에서 탁월한 성과를 보여줍니다.

### [PanoDreamer: 3D Panorama Synthesis from a Single Image](https://arxiv.org/abs/2412.04827)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04827.png)

Vote: 3

Authors: Xilong Zhou, Nima Khademi Kalantari, Andrii Tsarov, Avinash Paliwal

- ***What's New***: PanoDreamer는 단일 입력 이미지로부터 일관된 360° 3D 장면을 생성하는 혁신적인 방법을 소개합니다. 기존 방식과 달리 PanoDreamer는 단일 이미지 파노라마와 깊이 추정 문제를 최적화 작업으로 구성하여, 일관된 파노라마 이미지와 해당 깊이를 생성한 후 소규모 가려진 영역을 복원하고 이를 3D 공간에 투사함으로써 장면을 재구성합니다.
- ***Technical Details***: 연구에서는 MultiDiffusion에서 영감을 받아, 두 개의 손실 항목을 갖는 최적화 문제로 파노라마 생성 문제를 설정하고, 교대로 최소화하는 전략을 제안하여 최적의 결과를 확보합니다. 파노라마 이미지와 깊이를 주어진 시각적 문맥에 맞춰 LDI(Layered Depth Image)로 변환하여 미싱 영역을 인페인팅하고, 3D Gaussian Splatting(3DGS) 표현을 통해 성능을 극대화합니다.
- ***Performance Highlights***: PanoDreamer는 기존의 LucidDreamer 및 WonderJourney와 비교했을 때, 360° 3D 장면의 일관성과 무결성에서 뛰어난 성능을 보여줍니다. 실험 결과 품질(Q-Align) 및 일관성(CLIP 및 스타일 일관성) 측면에서 두드러진 개선을 나타냈으며, 시각적 일관성이 더 높은 장면을 생성했습니다.

### [EXAONE 3.5: Series of Large Language Models for Real-world Use Cases](https://arxiv.org/abs/2412.04862)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04862.png)

Vote: 29

Authors: Kibong Choi, Haeju Lee, Junwon Hwang, Gerrard Jeongwon Jo, LG AI Research, Kyunghoon Bae, Joonkee Kim, Hyojin Jeon, Seokhee Hong, Youchul Kim, Yongil Kim, Hyosang Kim, Sunkyoung Kim, Sooyoun Park, Hyeongu Yun, Heuiyeen Yeen, Kyungmin Lee, Soyeon Kim, Jinsik Lee, Sangha Park, Edward Hwayoung Lee, Stanley Jungkyu Choi, Eunbi Choi, Jiyeon Jung, Yongmin Park, Yountae Jung, Hyunjik Jo, Seonghwan Kim, Yireun Kim, Sihoon Yang, Woohyung Lim, Honglak Lee, Soyoung An

- ***What's New***: EXAONE 3.5 모델은 LG AI Research에서 실제 사용 사례를 위해 개발된 최신 대형 언어 모델 시리즈입니다. 총 32B, 7.8B, 2.4B의 세 가지 설정으로 제공되며, 7개의 벤치마크에서 최고 성능을 보이는 명령어 준수 능력과 4개의 벤치마크에서 최상위 성능을 달성한 장문의 문맥 이해력을 특징으로 합니다.
- ***Technical Details***: EXAONE 3.5 모델은 최신 디코더 전용 Transformer 아키텍처를 기반으로 하며, 최대 32,768개의 토큰을 지원하는 장문 문맥 프로세싱을 제공합니다. 32B, 7.8B, 2.4B 등 모델 별로 파라미터 수, 레이어 수, 피드포워드 차원 등 다양한 설정을 채택하였으며, 각 모델은 독특한 SwiGLU 비선형성과 GQA 헤드 타입을 사용하여 학습되었습니다.
- ***Performance Highlights***: 32B EXAONE 3.5 모델은 실제 응용 사례와 장문 문맥 카테고리에서 비교 모델들보다 뛰어난 성능을 보였으며, 일반 도메인에서도 경쟁력 있는 결과를 나타냈습니다. EXAONE 3.5의 2.4B 모델은 작은 크기에도 불구하고 다른 비슷한 크기의 모델보다 뛰어난 성능을 보여주어, 특히 소형 대형 언어 모델(sLLM)에 대한 최근 수요에 잘 부합합니다.

### [GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration](https://arxiv.org/abs/2412.04440)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04440.png)

Vote: 13

Authors: Xihui Liu, Xuefei Ning, Kaiyi Huang, Yu Wang, Yukun Huang, Zinan Lin

- ***What's New***: GenMAC은 복합적인 동영상 생성 작업을 멀티 에이전트 협업(Multi-Agent Collaboration)으로 해결하는 새로운 접근법을 제시합니다. 본 연구는 텍스트-동영상(Text-to-Video) 생성에 적합하게 설계된 반복적 프레임워크를 통해 여러 에이전트가 복잡한 목표를 달성하도록 지원합니다.
- ***Technical Details***: GENMAC의 구조적 워크플로우는 DESIGN, GENERATION, REDESIGN 세 가지 단계로 구분되며, 이 중 가장 도전적인 단계인 REDESIGN에서는 비디오의 생성 내용과 텍스트 프롬프트의 일치성을 확인하고 수정 방안을 제안 및 추진합니다. 이 단계는 검증 에이전트(Verification Agent), 제안 에이전트(Suggestion Agent), 수정 에이전트(Correction Agent), 산출물 구조화 에이전트(Output Structuring Agent)로 구성되어 각 역할에 특화된 에이전트가 협력합니다. 또한, 다양한 시나리오에 적응하여 적절한 수정 에이전트를 선택할 수 있는 자기 라우팅(Self-Routing) 메커니즘을 설계하였습니다.
- ***Performance Highlights***: GenMAC은 최신 상태의 텍스트-동영상 생성 성능을 보여주었습니다. 복합적인 텍스트 프롬프트에 대한 구성적 비디오 생성 품질을, 일관성 있는 속성 바인딩, 동적 속성 바인딩, 공간적 관계 및 움직임 바인딩 등에서 두드러지게 향상시켰습니다. 이는 다양한 구성 요구 사항을 효과적으로 처리할 수 있는 다중 에이전트 시스템의 효용성을 입증합니다.

### [Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](https://arxiv.org/abs/2412.05271)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05271.png)

Vote: 56

Authors: Botian Shi, Wenhai Wang, Hao Tian, Jinguo Zhu, Xizhou Zhu, Zhiyong Wu, Yimin Ren, Han Lv, Zhongying Tu, Jifeng Dai, Yi Wang, Xingcheng Zhang, Wenqi Shao, Jiaye Ge, Dahua Lin, Yue Cao, Conghui He, Xuehui Wang, Weiyun Wang, Zixuan Chen, Huipeng Deng, Erfei Cui, Min Dou, Tong Lu, Zhangwei Gao, Yu Qiao, Yangzhou Liu, Tan Jiang, Zhaoyang Liu, Bo Wang, Jiapeng Luo, Jiahao Wang, Lixin Gu, Pei Chu, Lewei Lu, Kai Chen, Zhe Chen, Tong He, Shenglong Ye, Qingyun Li

- ***What's New***: 이 논문은 Open-source 멀티모달 대형 언어 모델(Innovative Multimodal Large Language Models; MLLMs)인 InternVL 2.5를 소개합니다. 이 모델은 기존 InternVL 2.0을 대체하며, 모델 스케일과 성능 간의 관계를 체계적으로 탐구하여, 비전 인코더(vision encoder), 언어 모델(language model), 데이터셋 크기, 테스트 시간 구성(test-time configuration)을 분석합니다.
- ***Technical Details***: InternVL 2.5는 InternVL 2.0의 아키텍처를 기반으로 하되, 훈련 및 테스트 전략, 데이터 품질에서 중요한 향상을 도입하였습니다. MLLM의 다양한 요소 변경이 모델 성능에 미치는 영향을 체계적으로 탐구하여, 모델 확대와 성능 간의 관계를 보여줍니다.
- ***Performance Highlights***: InternVL 2.5는 다양한 벤치마크에서 경쟁력 있는 성능을 입증하며 GPT-4o 및 Claude-3.5-Sonnet과 같은 주요 상용 모델들과 경쟁합니다. 특히, MMMU 벤치마크에서 최초로 70%를 넘었으며, 수학적 추론에서 중요한 개선을 보여줍니다. 이 모델은 멀티모달 AI 연구 발전에 기여할 것입니다.

### [BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks](https://arxiv.org/abs/2412.04626)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04626.png)

Vote: 1

Authors: Pierre-André Noël, Zichao Li, Amirhossein Abaskohi, Akshay Kalkunte, Sai Rajeswar, Siva Reddy, Yoshua Bengio, Issam Laradji, Christopher Pal, Perouz Taslakian, Noah Bolger, Mahsa Massoud, Shubbam Agarwal, Abhay Puri, Rabiul Awal, Siba Smarak Panigrahi, M. Özsu, Ahmed Masry, Juan Rodriguez, Spandanna Gella, Sanket Biswas, Kurt MacDonald, Tianyu Zhang, Sepideh Kharagani, Suyuchen Wang, François Savard, David Vazquez, Krishnamurthy DJ Dvijotham, Ying Zhang, Joao Monteiro, Sean Hughes, Marco Pedersoli, Sara Shanian, Aarash Feizi, Torsten Scholak, Sathwik Tejaswi, Srinivas Sunkara, Xiangru Jian, Simon Fauvel, Shravan Nayak, Nicolas Chapados, Saverio Vadacchino, Mats Leon Richter

- ***What's New***: BigDocs는 750만 개의 멀티모달 도큐먼트를 포함하는 고품질의 개방형 데이터세트로, 멀티모달 모델을 문서 이해 및 코드 작업에 활용하는 것을 목표로 합니다. BigDocs-7.5M는 다양한 문서 유형과 구조화된 출력 작업을 지원하는 지속적인 사전 훈련과 세밀한 튜닝이 가능합니다.
- ***Technical Details***: BigDocs는 133개 공개 데이터세트를 조사하여 사용했으며, 허용 가능한 라이센스(Creative Commons BY 4.0, MIT 등)와 추적 가능한 메타데이터를 강조합니다. BigDocs Toolkit은 데이터 전처리, 필터링, 정리에 유용한 도구를 제공하며, 투명성을 높이기 위한 통합 메타데이터 프레임워크도 포함되어 있습니다. 새로운 벤치마크 세트인 BigDocs-Bench는 GUI 및 이미지에서 코드 생성 작업과 같은 현실적인 활용 사례를 반영하는 10개의 새로운 작업을 포함하고 있습니다.
- ***Performance Highlights***: BigDocs-Bench 실험 결과는 모델이 문서 추론과 구조화된 출력 작업(예: Screenshot2HTML, Image2Latex 생성)에서 25.8%까지 성능이 향상된 것을 보여주었습니다. 또한, 인간 평가 결과 BigDocs에서 훈련된 모델의 출력을 더 선호하는 경향을 보였으며, 이는 BigDocs가 학문적 연구와 오픈 소스 커뮤니티 모두에 멀티모달 역량을 향상시키는 데 도움을 줄 수 있음을 시사합니다.

### [DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling](https://arxiv.org/abs/2412.04905)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04905.png)

Vote: 3

Authors: Haiyang Yu, Kun Chen, Xinghua Zhang, Yongbin Li, Fei Huang, Minzheng Wang, Wenji Mao, Nan Xu

- ***What's New***: DEMO는 대화 상호작용을 미세한 요소로 모델링하는 새로운 연구 과제로, 대화 요소 모델링(Dialogue Element Modeling)을 제안합니다. 이는 대화 요소 인식(Element Awareness)과 대화 에이전트 상호작용(Dialogue Agent Interaction)을 포함하여 포괄적인 대화 모델링 및 평가를 위한 참신한 벤치마크 DEMO를 소개합니다.
- ***Technical Details***: DEMO 벤치마크는 영어와 중국어에서 적용 가능한 다양한 대화 요소를 포괄하며, 목표 인식(Goal Recognition), 페르소나 모델링(Persona Modeling), 장면 재구성(Scene Reconstruction), 발화 발굴(Utterance Mining)의 네 가지 태스크를 포함합니다. 이 벤치마크를 통해 대화 요소 모델링을 향상시키기 위해 모방 학습(Imitation Learning) 기법을 활용하여 DEMO 에이전트를 개발하였습니다.
- ***Performance Highlights***: 다양한 최신 LLM의 평가 결과, DEMO 에이전트는 in-domain 및 out-of-domain 태스크에서 뛰어난 성능을 보였습니다. GPT-4o와 같은 독점 모델은 우수한 성능을 기록하였으며, DEMO 에이전트는 모방 학습을 통해 다양한 상호작용 정책을 학습하여 사회적 지능 태스크에서도 탁월한 성능을 발휘했습니다. 이는 현재 LLM들이 대화 요소 모델링에서 여전히 개선의 여지가 큼을 나타냅니다.

### [Mind the Time: Temporally-Controlled Multi-Event Video Generation](https://arxiv.org/abs/2412.05263)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05263.png)

Vote: 6

Authors: Yuwei Fang, Willi Menapace, Varnith Chordia, Igor Gilitschenski, Ziyi Wu, Aliaksandr Siarohin, Sergey Tulyakov, Ivan Skorokhodov

- ***What's New***: MinT(Mind the Time)는 특정 시간 간격에 각각의 이벤트를 매핑하여 다중 이벤트 비디오를 생성하고 시간 제어 능력을 제공하는 최초의 비디오 생성기입니다. 이 모델은 각 이벤트별로 집중적으로 생성하여, 이벤트들이 부드럽게 연결될 수 있도록 지원합니다.
- ***Technical Details***: 기본적으로 사전 훈련된 비디오 디퓨전 트랜스포머(Latent Diffusion Transformer)를 기반으로 구축된 MinT는 각 DiT 블록에 글로벌 캡션과 이벤트 캡션을 위한 두 개의 크로스 주의(Cross-Attention) 레이어를 채택했습니다. Rescaled Rotary Position Embedding(ReRoPE)이라는 새로운 시간 기반 위치 인코딩을 도입하여 이벤트 캡션이 지정된 시간 범위 내의 프레임에 집중할 수 있도록 했습니다. 이 모델은 또한 장면 전환(scene cuts)을 조건화하여, 영상 씬 컷도 제어할 수 있습니다.
- ***Performance Highlights***: MinT는 시계열적으로 잘 연결된 이벤트를 생성하며, 최신 오픈소스 및 상용 모델의 다중 이벤트 비디오 생성 성능을 상당히 능가합니다. 이는 특히 신규 데이터셋에 대해 테스트했을 때 더욱 명확히 드러나며, 기존 방식의 공통적인 결함인 순차적 이벤트 생성을 성공적으로 개선합니다.

### [2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction](https://arxiv.org/abs/2412.03428)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03428.png)

Vote: 5

Authors: Haodong Xiang, Xiansong Lai, Xinghui Li, Zhichao Liao, Long Zeng, Wanting Zhang

- ***What's New***: 2DGS-Room은 내부 장면 재구성을 위한 새로운 방법으로, 2D Gaussian Splatting 기술을 활용하여 높은 품질의 기하학적 재구성을 제공합니다. 이 방법은 장면 표면과 일치하도록 2D Gaussian의 분포와 밀도를 제어하는 Seed-포인트 안내 메커니즘을 제안합니다. 또한 단안(depth, normal) 프라이어를 도입하여 세부 영역 및 텍스처리스 영역의 기하학적 정확성을 향상시킵니다.
- ***Technical Details***: 2DGS-Room은 3D Gaussian Splatting과 유사한 기술을 기반으로 하되 2D 평면 Gaussian을 사용하여 장면을 표현합니다. Seed-point는 장면 구조를 유지하는 기반을 제공하며, 학습 가능한 오프셋을 통해 최적화 과정에서 Gaussian의 위치를 조정하여 장면과의 정렬을 향상시킵니다. 깊이와 정상 프라이어는 각각 세부 영역 조정과 텍스처리스 영역에서의 표면 추정을 보장합니다. 다중 뷰 일관성 제약은 잔여 아티팩트를 줄이는 데 사용됩니다.
- ***Performance Highlights***: 2DGS-Room은 ScanNet 및 ScanNet++ 데이터셋에서 수행된 광범위한 실험에서 최첨단 성능을 기록했습니다. 특히 정확성(Accuracy), 완성도(Completion), 정밀도(Precision), 재현율(Recall), 및 F-점수(F-score)에서 기존 방법들 대비 우수한 성능을 보였습니다. 특히, ScanNet에서 F-점수가 0.575로, 기존 방법들과 비교해 높은 점수를 기록했습니다.

### [APOLLO: SGD-like Memory, AdamW-level Performance](https://arxiv.org/abs/2412.05270)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05270.png)

Vote: 23

Authors: Zhangyang Wang, Hanqing Zhu, Zhenyu Zhang, Xi Liu, Wenyan Cong, Jinwon Lee, Bo Long, Vikas Chandra, David Z. Pan, Sem Park

- ***What's New***: APOLLO는 새로운 메모리 효율적인 최적화 방법으로, AdamW 만큼의 성능을 유지하면서도 SGD와 유사한 메모리 사용량을 자랑합니다. 이는 고비용의 SVD(Singular Value Decomposition)를 피하고, 순수한 랜덤 프로젝션을 사용하여 낮은 랭크의 옵티마이저 상태로 학습률을 근사화하는 방법을 제안하여, 메모리 사용을 크게 절감합니다.
- ***Technical Details***: APOLLO는 구조적 학습률 업데이트를 통해 채널 별 학습률 스케일링을 근사화하여 옵티마이저 상태를 낮은 랭크의 공간에서 저장합니다. 또한 APOLLO-Mini는 극한의 메모리 효율성을 위해 텐서 별 스케일링 방식으로 랭크-1 부차공간을 사용하여 SGD와 유사한 메모리 비용을 유지하면서 Adam(W)보다 뛰어난 성능을 보여줍니다. 이들은 INT8 양자화를 통해 메모리 사용을 더욱 줄일 수도 있습니다.
- ***Performance Highlights***: APOLLO와 APOLLO-Mini는 다양한 모델 아키텍처 및 작업에서 Adam(W)와 동등하거나 더 나은 성능을 보여주며, 메모리 사용량은 GaLore보다 훨씬 적습니다. 특히 APOLLO는 AdamW 대비 약 3배의 처리량을 8×A100-80GB 설정에서 달성하며, LLaMA-13B 모델을 A100-80GB에서 다른 시스템 최적화 없이 사전 훈련할 수 있게 합니다. APOLLO-Mini는 단일 GPU와 12GB 이하의 메모리로 LLaMA-7B의 훈련을 처음으로 가능하게 했습니다.

### [Moto: Latent Motion Token as the Bridging Language for Robot Manipulation](https://arxiv.org/abs/2412.04445)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04445.png)

Vote: 18

Authors: Yi Chen, Yixiao Ge, Yizhuo Li, Xihui Liu, Yuying Ge, Mingyu Ding, Ying Shan

- ***What's New***: Moto는 로봇 조작을 위한 새로운 프리트레이닝 방법을 제안하며, 비디오 데이터를 통해 자동적으로 '동작 토큰(Latent Motion Tokens)'을 학습하고 이를 활용하여 로봇 조작 성능을 크게 향상시켰습니다. 이는 비디오 데이터를 사용하여 로봇 제어와 관련된 운전 지식을 사전 학습 이후 실제 로봇의 조작에 전이시키는 첫 시도입니다.
- ***Technical Details***: Moto 시스템은 세 단계로 구성됩니다. 첫째, 'Latent Motion Tokenizer'를 비디오 데이터의 두 프레임 간 동작을 추출해 내는 언어로 학습합니다. 둘째, Moto-GPT는 이러한 토큰을 기반으로 자동 회귀 프리 트레이닝을 통해 비디오로부터 동작에 관한 사전 지식을 학습합니다. 마지막으로, 잡일 데이터로 사전 학습된 모션 프라이어를 실제 로봇의 조작으로 전이하기 위해 특수한 '동일 학습(co-fine-tuning)' 전략이 사용됩니다.
- ***Performance Highlights***: Moto-GPT는 SIMPLER 및 CALVIN 벤치마크에서 다른 모델보다 뛰어난 성능을 보입니다. 특히 큰 비전-언어 모델(VLM)과의 비교에서 두드러진 결과를 나타냈으며, 살아있는 비디오 데이터만으로 사전 학습을 수행하여 효과적이고 데이터를 효율적으로 사용하는 성과를 보여줍니다. 이로써 적은 양의 액션 레이블 데이터로도 높은 성능을 발휘할 수 있음을 증명했습니다.

### [LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment](https://arxiv.org/abs/2412.04814)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04814.png)

Vote: 32

Authors: Hao Li, Xiaomeng Yang, Zhiyu Tan, Cheng Jin, Yibin Wang, Junyan Wang

- ***What's New***: LiFT는 텍스트 기반 비디오(T2V) 생성 모델을 인간의 선호도에 맞게 조정하기 위한 새로운 미세 조정 방법입니다. Human Rating Annotation 데이터셋(LIFT-HRA)을 구축하여 약 1만 건의 인간 주석을 포함하고, 이 주석을 바탕으로 보상 모델인 LIFT-CRITIC을 훈련하여 인간 판단을 대리하는 보상 함수를 학습합니다. 이 보상 함수를 T2V 모델의 정렬을 위해 사용하여, 최적화된 비디오 출력을 이루었습니다.
- ***Technical Details***: LiFT는 세 가지 주요 단계로 T2V 모델을 정렬합니다: (1) 인간 피드백 수집을 통해 포괄적인 피드백 데이터셋인 LIFT-HRA를 구축, (2) 데이터셋에서 학습한 보상 모델 LIFT-CRITIC을 사용하여 T2V 모델 정렬, (3) 리워드 가중 가능성 최적화를 통해 T2V 모델을 미세 조정합니다. 인간 선호도를 반영하기 위한 삼 차원(semantic consistency, motion smoothness, video fidelity)에 따라 비디오 평가를 진행하며, 각 차원에 대해 점수와 함께 이유를 포함합니다.
- ***Performance Highlights***: 실험 결과, LIFT는 인간 피드백의 이유 학습을 통해 모델의 인간 선호도에 대한 정렬이 크게 향상됨을 보여주었습니다. CogVideoX-2B 및 CogVideoX-5B 모델을 대상으로 한 실험에서 LIFT로 조정된 모델은 VBench 벤치마크의 모든 16가지 지표에서 향상된 성능을 보였습니다.

### [MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale](https://arxiv.org/abs/2412.05237)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05237.png)

Vote: 30

Authors: Yubo Wang, Bo Li, Jarvis Guo, Yizhi Li, Graham Neubig, Yuelin Bai, Xiang Yue, Tuney Zheng, Wenhu Chen, King Zhu

- ***What's New***: MAmmoTH-VL은 대규모 교육을 통해 체인 오브 소트(Chain-of-Thought; CoT) 추론을 이끌어내는 멀티모달 reasoning 능력을 향상시키기 위해 설계된 새로운 방법론을 소개합니다. 12M개의 instruction-response 쌍을 포함하는 대형 멀티모달 instruction tuning dataset을 구축하여 강력한 중간 reasoning을 포함하도록 합니다.
- ***Technical Details***: 이 연구에서는 CoT 추론을 유도하기 위해 open models만을 사용하여 12M instruction-response 쌍을 생성합니다. 작성 파이프라인은 데이터 수집, open 모델을 통한 태스크 특정 데이터 확장 및 재작성, 자가 필터링의 세 단계로 구성됩니다. 선택된 데이터는 153개의 공개 멀티모달 instruction 데이터셋에서 수집되며 후제할 수 있도록 모델을 훈련합니다.
- ***Performance Highlights***: 실험 결과, 학습된 멀티모달 Large Language Models(MLLMs)는 MathVerse에서 +8.1%, MMMU-Pro에서 +7%, MuirBench에서 +13.3%의 state-of-the-art 성능을 달성했습니다. 또한, 비추론 기반 벤치마크에서도 최대 4%의 성능 향상을 보였습니다. 추가적인 ablation studies는 재작성과 자가 필터링이 데이터셋 구축과정에서의 중요성을 강조합니다.

### [CompCap: Improving Multimodal Large Language Models with Composite Captions](https://arxiv.org/abs/2412.05243)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05243.png)

Vote: 9

Authors: Mahmoud Azab, Xiaohui Chen, Baosheng He, Qifan Wang, Aashu Singh, Xuewen Zhang, Hanchao Yu, Satya Narayan Shukla, David Yang, ShengYun Peng, Shen Yan

- ***What's New***: CompCap은 합성 캡션(Composite Captions)을 활용하여 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)의 성능을 개선하는 새로운 접근 방식입니다. 본 연구는 기존 MLLMs가 합성 이미지(Composite Images; CIs)에 대해 이해하는 데 어려움을 겪고 있다는 점을 밝히고 CompCap 프레임워크를 통해 보다 정확한 캡션을 자동 생성하여 이를 해결하고자 합니다.
- ***Technical Details***: CompCap은 다양한 메타데이터를 사용하여 MLLMs의 학습에 필요한 고품질의 합성 이미지와 캡션 쌍을 생성하는 유연한 프레임워크입니다. 이 연구에서 CompCap-118K라는 데이터셋을 구축하였는데, 이는 6가지 CI 타입에 걸쳐 11만8천 개의 이미지 캡션 쌍으로 구성됩니다. 각 타입별 파이프라인에서 다양한 이미지를 생성하고 이에 대한 상세한 캡션을 생성하도록 설계되었습니다. 이를 통해 시각적 요소와 언어적 요소 사이의 정교한 연계를 강화합니다.
- ***Performance Highlights***: CompCap-118K를 통해 MLLMs를 학습시킨 결과, 11개의 벤치마크에서 평균적으로 1.7%, 2.0%, 2.9%의 성능 향상을 보였습니다. 특히 CI가 포함된 벤치마크에서 두드러진 성능 향상이 나타났습니다. 이러한 결과는 고품질의 캡션 데이터가 MLLMs의 향상된 시각적 이해에 기여하는 중요한 요소로 작용함을 시사합니다.

