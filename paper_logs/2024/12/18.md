## Daily Papers (2024-12-18)

### [When to Speak, When to Abstain: Contrastive Decoding with Abstention](https://arxiv.org/abs/2412.12527)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12527.png)

Vote: 1

Authors: Sang-goo Lee, Youna Kim, Taeuk Kim, Hyuhng Joon Kim

- ***What's New***: 이 논문에서는 대형 언어 모델(LLMs)이 관련 지식이 부족할 경우 응답을 생성하지 않도록 하는 새로운 방식인 부재를 포함한 대조적 디코딩(Contrastive Decoding with Abstention; CDA)을 소개합니다. 이는 훈련 없이도 LLMs가 요청에 대한 답변을 가능한 경우 생성하고, 불가능할 경우 생성을 중단하는 기능을 제공합니다.
- ***Technical Details***: 부재를 포함한 대조적 디코딩(CDA)은 주어진 쿼리에 대한 각 지식의 적합성을 평가하여 어떤 지식을 우선적으로 사용할지 결정합니다. 장애 제도를 통해 필요한 지식이 없을 때는 부처를 하도록 모델을 유도합니다. 이 방법은 다양한 질문-답변(QA) 데이터셋을 기반으로 LLMs의 실험에서 검증되었습니다.
- ***Performance Highlights***: CDA를 통해 LLMs는 관련 지식이 없는 경우에도 잘못된 응답을 발생시키는 것을 방지하고, 신뢰성과 정확성을 개선할 수 있음을 확인했습니다. 이러한 접근 방식은 LLMs의 응용 가능성을 넓히고 사용자 신뢰를 보존하는 데 기여합니다.

### [Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion Models](https://arxiv.org/abs/2412.11423)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11423.png)

Vote: 1

Authors: Daesik Kim, Seung-Hun Nam, KiYoon Yoo, Namhyuk Ahn, Wonhyuk Ahn

- ***What's New***: FastProtect는 이미지 생성의 위험 요소를 방지하는 새로운 보호 프레임워크로, 특히 지연시간(latency)의 문제를 해결하는 것에 중점을 두었습니다. 사전 학습(Pre-training)된 혼합 파생물(Mixture-of-Perturbations)을 사용하여 보호 성능 저하 없이 즉각적인 보호가 가능하도록 설계되었습니다.
- ***Technical Details***: FastProtect는 사전 학습된 혼합 파생물을 통해 이미지의 잠재 코드(latent code)에 따라 적절한 파생물을 선택하여 보호 이미지를 생성합니다. 또한 다중 레이어 보호 손실(Multi-layer Protection Loss)을 통해 성능을 강화하며, 보호의 강도를 LPIPS 거리 기반으로 조정하여 높은 투명성을 유지합니다.
- ***Performance Highlights***: FastProtect는 기존 방법들에 비해 보호 효율성과 보이지 않음을 위한 절충에서 개선을 보여주며, 특히 CPU와 GPU 환경에서 초고속으로 보호 작업을 수행합니다. 512x512 이미지의 경우 FastProtect는 CPU에서 2.9초, GPU에서 0.04초로 동작하여 다른 방법들과 비교했을 때 최대 175배까지 빠른 결과를 냅니다.

### [OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain](https://arxiv.org/abs/2412.13018)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13018.png)

Vote: 28

Authors: Zhicheng Dou, Shuting Wang, Jiejun Tan, Ji-Rong Wen

- ***What's New***: OmniEval은 금융 도메인에서 대규모 언어 모델(LLMs)을 평가하기 위해 탐색-증강 생성(Retrieval-Augmented Generation; RAG) 기술을 활용하는 포괄적이고 자동화된 벤치마크를 제안합니다. 이 벤치마크는 5개의 작업 클래스와 16개의 금융 주제에 걸쳐 다양한 쿼리 시나리오를 구조화하여 평가합니다.
- ***Technical Details***: OmniEval은 매트릭스 기반의 RAG 시나리오 평가 시스템, 다차원 평가 데이터 생성, 단계별 평가 시스템, 그리고 규칙 기반 및 LLM 기반 평가 메트릭을 포함하여 RAG 시스템의 성능을 총체적으로 평가합니다. 데이터를 자동 생성 및 수동 주석을 통해 확장 가능한 평가 데이터셋을 구축하며, 생성된 데이터의 87.47%가 인간 평가에서 수용 가능한 수준임을 확인했습니다.
- ***Performance Highlights***: OmniEval로의 실험 결과, RAG 시스템은 다양한 토픽과 작업에 대해 성능 변동을 보였으며, 금융 도메인 내에서 대폭 개선될 여지가 있음을 시사합니다. GTE-Qwen2-1.5b 리트리버 및 여러 LLM 조합이 평가되었으며, 특히 고급 금융 작업에서 성능 부족이 확인되었습니다.

### [Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation Model Internet Agents](https://arxiv.org/abs/2412.13194)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13194.png)

Vote: 4

Authors: Qianlan Yang, Xiong Zhou, Erran Li, Kaixiang Lin, Sergey Levine, Yu-Xiong Wang, Yifei Zhou, Min Bai

- ***What's New***: Proposer-Agent-Evaluator(PAE)는 파운데이션 모델 인터넷 에이전트(Foundation Model Internet Agents)에게 자율적으로 새로운 기술을 발견하고 연습하게 하는 학습 시스템입니다.
- ***Technical Details***: PAE는 사용자 데모나 웹사이트 이름과 같은 환경의 맥락 정보(contextual information)를 활용하여 에이전트가 연습할 수 있는 목표를 제안하는 컨텍스트 기반 태스크 제안자(context aware task proposer), RL을 통해 에이전트의 정책을 개선하는 자율 평가기(autonomous VLM-based evaluator)로 구성되어 있습니다.
- ***Performance Highlights***: PAE는 WebVoyager와 WebArena에서의 실험을 통해 VLM 인터넷 에이전트의 제로샷 일반화 능력을 크게 향상시켰으며, 10% 절대적 성능 향상(22.6%에서 33.0%)을 달성했음을 보여주었습니다.

### [Seeker: Towards Exception Safety Code Generation with Intermediate Language Agents Framework](https://arxiv.org/abs/2412.11713)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11713.png)

Vote: 1

Authors: Yuan Yuan, Xuanming Zhang, Zhexin Zhang, Yiming Zheng, Minlie Huang, Yuxuan Chen

- ***What's New***: Seeker는 개발자가 놓치기 쉬운 예외 처리 문제를 해결하기 위해 여러 에이전트 프레임워크(Multi-agent Framework)를 사용하여 대형 언어 모델(LLMs)을 활용합니다. 이 연구는 실제 개발 시나리오에서 예외 처리에 대한 연구를 체계적으로 수행한 첫 번째 시도입니다.
- ***Technical Details***: Seeker는 예외 처리 작업을 스캐너(Scanner), 탐지자(Detector), 포식자(Predator), 평가자(Ranker), 핸들러(Handler)라는 다섯 가지 특정 작업으로 나누고 여러 에이전트를 배정하여 다중 에이전트 프레임워크를 기반으로 구성됩니다. 이 프레임워크는 기존 코드의 변화성과 종속성을 평가하여 효율적인 예외 처리 모듈을 생성합니다. 예외 처리 개선을 위해 CEE(Common Exception Enumeration) 문서를 사용하여 긴 리스트의 예외 처리 유형에 대한 표준화를 제공합니다.
- ***Performance Highlights***: Seeker는 전통적인 예외 처리 코드 생성 방법과 비교하여 모든 평가 지표에서 뛰어난 성과를 보여줍니다. 특히 코딩 표준 준수 점수(ACRS)에서 0.85를 기록하며, 감지율(COV)과 처치 정확도(ACC)에서 각각 91%와 79%를 달성하였습니다. 이 프레임워크는 다양한 언어 모델을 통합하여 Java와 Python 등 여러 프로그래밍 언어에 적용할 수 있도록 설계되었습니다.

### [SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video](https://arxiv.org/abs/2412.09982)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09982.png)

Vote: 6

Authors: Jihyong Oh, Jaeho Moon, Munchurl Kim, Juan Luis Gonzalez Bello, Jongmin Park, Minh-Quan Viet Bui

- ***What's New***: SplineGS는 모노큘러 비디오에서 COLMAP 없이 역동적인 3D 가우시언을 실시간으로 생성하기 위한 새로운 프레임워크입니다. 이 연구는 고품질의 역동적인 장면 재구성과 빠른 신경 렌더링을 가능하게 합니다.
- ***Technical Details***: 이 연구는 큐빅 허미트 스플라인(Cubic Hermite Splines)을 사용하여 연속적인 역동적 3D Gaussian의 궤적을 표현하는 새로운 Motion-Adaptive Spline (MAS) 방법을 제시합니다. 또한, 모션에 맞게 제어점을 동적으로 조정하는 Motion-Adaptive Control points Pruning (MACP) 방법을 도입하여 렌더링 품질과 효율성을 최적화합니다. 카메라 파라미터 추정과 3D Gaussian 속성을 광학적 및 기하학적 일관성을 이용하여 공동 최적화합니다.
- ***Performance Highlights***: SplineGS는 NVIDIA 데이터셋에서 기존의 최신 방법들보다 1.1 dB 높은 PSNR 및 8,000배 빠른 렌더링 속도를 달성했습니다. 실험 결과, SplineGS는 전통적인 COLMAP 중심의 방법보다 더 빠르고 정확한 데이터 처리 능력을 보여줍니다.

### [Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models](https://arxiv.org/abs/2412.12606)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12606.png)

Vote: 28

Authors: Ye Tian, Qiuna Tan, Peiqing Yang, Xiaofei Wang, Zhe Wei, Yadong Xue, YiFan Zhang, Shanglin Lei, Runqi Qiao, Honggang Zhang, Xiaoshuai Song, Guanting Dong, Zhuoma GongQue

- ***What's New***: 이 논문에서는 대형 멀티모달 모델(Large Multimodal Models; LMMs)의 실생활 개인화 역량을 평가하기 위한 MDI 벤치마크를 제안합니다. 이 벤치마크는 일상 생활의 6가지 시나리오를 아우르는 500개 이상의 이미지와 1200개의 질문을 포함하며, 다양한 연령대의 요구를 반영할 수 있도록 설계되어 있습니다.
- ***Technical Details***: MDI 벤치마크는 다양한 시나리오, 문제의 복잡도 및 다양한 연령층의 수요를 평가합니다. 질문은 두 가지 복잡성 수준으로 나뉘며, 나이에 따라 청소년, 중년, 노인으로 분류하여 평가가 이루어집니다. 또한, 데이터 수집은 500여 개의 새 이미지와 120명의 자원봉사자가 참여하여 균형 잡힌 데이터 소스를 구성했습니다.
- ***Performance Highlights***: GPT-4o는 모든 지표에서 가장 우수한 성능을 보여주었으며, 다양한 시나리오와 연령대에 걸쳐 성능 격차가 존재했습니다. 특히, LMMs는 나이별로 성능 격차를 보이며, 중년층의 질문에 대한 대응력이 상대적으로 낮았습니다. 이는 향후 연구에서 개선이 필요한 영역임을 시사합니다.

### [Emergence of Abstractions: Concept Encoding and Decoding Mechanism for In-Context Learning in Transformers](https://arxiv.org/abs/2412.12276)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12276.png)

Vote: 6

Authors: Seungwook Han, Jinyeop Song, Jeff Gore, Pulkit Agrawal

- ***What's New***: 이 논문은 in-context learning(ICL)에서 변환기(transformer)가 내부적으로 추상 개념을 형성하고 사용하는 과정을 설명하는 개념 부호화-디코딩 메커니즘을 제안합니다. 이론적 분석과 기계적 개입을 통해 ICL 성능을 예측할 수 있는 개념 디코더빌리티(Concept Decodability; CD)를 도입하였습니다.
- ***Technical Details***: 변형기를 사용하여 다양한 규모와 사전학습 모델군(Gemma-2 2B/9B/27B, Llama-3.1 8B/70B)에서 개념 부호화-디코딩 메커니즘이 일반적임을 검증하고, 간섭 실험과 통제된 미세 튜닝을 통해 개념 디코더빌리티가 실제 ICL 성능을 예측할 수 있음을 보여주었습니다. 개념 부호화는 중간 층에서 피크를 기록하며, 이전 층을 미세 조정하는 것이 개념 부호화에 더 효과적임을 확인했습니다.
- ***Performance Highlights***: 제안된 개념 부호화-디코딩 메커니즘을 통해 변형기가 ICL 작업을 더 잘 수행할 수 있음을 실증했습니다. 특히, 개념 디코더빌리티(CD) 점수가 높은 개념군에 대해 더 높은 ICL 성능을 보였으며, 이는 POS 태깅 및 비트 연산에서 특히 두드러졌습니다. 모델의 해석 가능성과 확장성에 대한 통찰력을 제공하여 향후 연구를 위한 기초를 마련하였습니다.

### [MIVE: New Design and Benchmark for Multi-Instance Video Editing](https://arxiv.org/abs/2412.12877)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12877.png)

Vote: 1

Authors: Jihyong Oh, Munchurl Kim, Agus Gunawan, Samuel Teodoro, Soo Ye Kim

- ***What's New***: MIVE는 멀티 인스턴스 비디오 편집을 위한 제로샷 프레임워크로, 두 가지 주요 모듈을 도입하여 작동합니다: (i) Disentangled Multi-instance Sampling(DMS)로 편집 누출을 방지하고, (ii) Instance-centric Probability Redistribution(IPR)로 정밀한 로컬라이제이션과 신뢰성 있는 편집을 보장합니다.
- ***Technical Details***: MIVE는 기존의 전역 편집 캡션에 의존하지 않고, 개별 인스턴스 캡션을 활용합니다. 제안된 MIVE Dataset은 VIPSeg에서 유래된 200개의 다양한 비디오로 구성되어 있으며, Cross-Instance Accuracy(CIA) Score를 소개하여 멀티-인스턴스 비디오 편집 작업의 편집 누출을 평가합니다.
- ***Performance Highlights***: MIVE는 편집의 신뢰성, 정확성 및 누출 방지 측면에서 최근의 최고 수준의 방법을 능가하여 멀티-인스턴스 비디오 편집을 위한 새로운 기준을 설정합니다. 실험 결과, MIVE는 여러 인스턴스에 대한 정확한 편집과 편집 누출을 최소화함으로써 정량적 및 사용자 연구 평가에서 탁월한 성과를 보였습니다.

### [GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training](https://arxiv.org/abs/2412.11863)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11863.png)

Vote: 2

Authors: Jiakang Yuan, Wenjie Wu, Hongbin Zhou, Conghui He, Bo Zhang, Xiangchao Yan, Botian Shi, Xinyu Cai, Tao Chen, Bin Wang, Renqiu Xia, Hancheng Ye, Tianshuo Peng, Mingsheng Li, Junchi Yan

- ***What's New***: GeoX는 새로운 멀티모달 큰 모델로, 자동 기하 문제 해결(Geometry Problem Solving; GPS)을 위한 기하학적 이해와 추론 태스크에 초점을 맞추고 있습니다. 이는 기하학적 도형-심볼과 자연 이미지-텍스트 간의 큰 차이를 해결하기 위해 단일 모드 사전 훈련을 도입한 것입니다. 기하학적 이미지와 질문을 입력으로 받아 검증 가능한 솔루션을 생성하며, 기하학적 다이어그램과 형식화된 언어 간의 모달리티 간격을 효과적으로 연결합니다.
- ***Technical Details***: GeoX는 단일 모드 사전 훈련, 형식화된 기하-언어 정렬(geometry-language alignment), 시각적 명령어 조정(visual instruction tuning)으로 구성된 형식화된 훈련 계획을 진행합니다. Diagram Encoder와 Symbol Decoder를 통해 기하학 전용 데이터로 기하학적 이미지를 이해하고, 기하학적 코퍼스를 기반으로 100M 토큰 규모의 데이터로 학습합니다. 또, Generator-and-Sampler Transformer를 사용하여 의미없는 표현을 제거하고 형식화된 프로그램 시퀀스를 생성합니다.
- ***Performance Highlights***: GeoX는 GeoQA, UniGeo, Geometry3K, PGPS9K와 같은 공개적으로 인정받는 4개의 벤치마크에서 일반 모델 및 기하 전문 모델을 능가하는 성능을 보였습니다. 대조적으로, 이전의 일반 모델들은 기하 태스크 해결에 한계가 있었던 반면, GeoX는 여러 기하학 데이터셋에서 최첨단 정확도를 달성했습니다. GeoX의 성능은 다양한 시나리오에서 SOTA 성능을 보여주며, 복잡하고 다양한 기하 문제를 해결하는 데 있어서 탁월한 역량을 입증하였습니다.

### [VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2412.10704)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.10704.png)

Vote: 2

Authors: Ryan A. Rossi, Kanika Goswami, Dinesh Manocha, Franck Dernoncourt, Manan Suri, Puneet Mathur

- ***What's New***: VisDoM은 멀티도큐먼트 환경에서 QA 시스템을 평가하기 위한 최초의 포괄적인 벤치마크, VisDoMBench를 소개합니다. 이 벤치마크는 테이블, 차트, 슬라이드쇼와 같은 풍부한 멀티모달 콘텐츠를 포함하며, VisDoMRAG라는 새로운 멀티모달 Retrieval Augmented Generation(RAG) 접근법을 제안합니다. 이는 멀티모달 검색 역량과 정교한 언어적 추론을 결합하여 시각적 및 텍스트 기반 RAG를 동시에 활용하는 기능을 갖추고 있습니다.
- ***Technical Details***: VisDoMRAG는 텍스트와 시각적 요소에 대해 각각 증거 큐레이션(Evidence Curation)과 생각의 체인(Chain-of-Thought) 추론을 포함한 다중 단계 추론 과정으로 이루어진 병렬 RAG 파이프라인을 사용합니다. 또한 모달리티 융합(Modality Fusion) 메커니즘을 통해 최종 답변을 생성할 때 일관성 제약을 두어 텍스트와 시각적 추론 프로세스가 맞춰지도록 합니다.
- ***Performance Highlights***: VisDoMRAG는 최고 수준의 멀티모달 문서 QA 수행을 위해 기존의 단일모달 및 롱 컨텍스트 LLM 기준값을 12-20% 초과하여 성능을 발휘합니다. 다양한 오픈소스 및 독점 대형 언어 모델을 통해 실험을 진행하며, 멀티도큐먼트 쿼리 처리에 있어 이점이 입증되었습니다.

### [Are Your LLMs Capable of Stable Reasoning?](https://arxiv.org/abs/2412.13147)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13147.png)

Vote: 57

Authors: Hongwei Liu, Kai Chen, Junnan Liu, Songyang Gao, Kuikun Liu, Songyang Zhang, Wenwei Zhang, Linchen Xiao, Ziyi Wang

- ***What's New***: 이 논문에서는 대형 언어 모델(LLMs)의 강력한 추론 능력을 평가하기 위한 새로운 평가 기준인 G-Pass@k를 소개합니다. 이는 여러 샘플링 시도를 통한 모델의 성능과 안정성을 연속적으로 평가하여 모델의 최대 성능 잠재력과 운영상의 일관성을 정량화합니다. 또한, 최신 수학 문제를 포함하여 데이터 누출 위험을 최소화하도록 설계된 동적 벤치마크인 LiveMathBench를 제시합니다.
- ***Technical Details***: G-Pass@k는 문제 해결 능력과 성능 일관성을 동시에 평가하는 지표로, 각 샘플이 정확한 해답을 제공할 확률을 평가합니다. 다양한 수학 대회에서 수집된 문제를 포함하여 LLMs에 대한 일반화 능력을 평가하기 위해 다양한 난이도 및 언어 학적 변이를 가진 문제들을 특징으로 하는 LiveMathBench를 구축했습니다. 실험에서는 Greedy Accuracy 및 기존의 Pass@K 수치를 고려하여 다차원적인 비교와 분석을 수행했습니다.
- ***Performance Highlights***: 고난이도 수학 문제를 포함한 LiveMathBench와 AIME2024-45 같은 경쟁 수준의 질문은 여전히 최신 모델들에게 상당한 도전 과제로 남아있습니다. 많은 모델들의 Greedy Accuracy는 평균적으로 10%~45% 사이에 위치하며, O1 같은 모델조차도 60% 이상의 정확도를 기록하지 못하고 있습니다. 대부분의 모델이 Greedy Accuracy 및 Pass@16에서는 높은 성능을 보이지만, G-Pass@k에서 안정성을 요하는 상황에서는 50% 이상의 성능 저하를 보였습니다. 이는 모델들이 다중 샘플에서 일관되게 추론능력을 유지하는 데 어려움을 겪고 있음을 암시합니다.

### [Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration](https://arxiv.org/abs/2412.13180)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13180.png)

Vote: 5

Authors: Serena Yeung-Levy, Xiaohan Wang, Mark Endo

- ***What's New***: FEATHER (Fast and Effective Acceleration wiTH Ensemble cRiteria)은 시각-언어 모델(Vision-Language Models; VLMs)의 빠른 연산을 위한 새로운 방법론으로, 기존의 시각 토큰 가지치기(pruning) 문제를 해결하며 전반적인 성능을 크게 향상시킵니다. 이 연구는 특정 작업에서는 시각 정보를 크게 압축해도 강력한 성능이 유지될 수 있음을 밝혀내었습니다.
- ***Technical Details***: FEATHER는 초기 층에서 비효율적으로 토큰을 가지치는 기존의 방법론적 문제를 분석하여 이를 해결하는 방안을 제시합니다. 본 연구는 '고속 효과적 가속화 집합 기준(Fast and Effective Acceleration wiTH Ensemble cRiteria)'을 도입하여, 이미지의 모든 영역에 걸쳐 균일하게 토큰을 선택하는 방식으로 개선했습니다. 또한, 초기 층에서의 가지치기를 통해 연산에 드는 부하를 줄이고, 후반 층에서는 향상된 기준을 적용하여 보다 효과적으로 토큰을 가지칩니다.
- ***Performance Highlights***: FEATHER는 기존의 가속화 접근법에 비해 현저한 성능 향상을 보여줍니다. 특히 시각 중심의 로컬라이제이션(localization) 작업에서 원래 방법론보다 5배 이상의 성능 향상을 달성했습니다. 이 방법은 64%의 FLOPS 감소를 이루면서도, 모델의 시각적 토큰은 3.3%만을 후반 층에 유지하여 대부분의 작업에 강력한 성능을 유지합니다. 이는 시각 정보 압축 깊이를 조정함으로써 VLMs의 성능을 효과적으로 개선한 사례로, 시각-언어 모델 가속화 연구의 새로운 방향을 제시합니다.

### [Compressed Chain of Thought: Efficient Reasoning Through Dense Representations](https://arxiv.org/abs/2412.13171)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13171.png)

Vote: 15

Authors: Jeffrey Cheng, Benjamin Van Durme

- ***What's New***: 이 페이퍼는 체인-오브-생각(Chain-of-Thought; CoT) 디코딩의 효율성을 높이기 위해 새로운 프레임워크인 압축 체인-오브-생각(Compressed Chain-of-Thought; CCoT)을 제안하고 있습니다. 이는 가변 시퀀스 길이의 연속적인 사색 토큰(Contemplation Tokens)을 생성하여 복잡한 문제에 대한 추론 성능을 향상시키는 방법을 제공합니다.
- ***Technical Details***: CCoT는 미리 훈련된 디코더 전용 언어 모델에 적용할 수 있는 프레임워크입니다. 훈련은 주어진 완전한 추론 체인에 대해 교사 강제(Teacher Forcing)를 이용하여 수행됩니다. 데이터의 효율성을 극대화하기 위해 가변 압축 비율을 통해 성능-효율성의 균형을 조정할 수 있습니다. 이 프레임워크는 LoRA 파인튜닝을 통해 사전 훈련된 LLM에 적응시킬 수 있습니다.
- ***Performance Highlights***: GSM8K 데이터셋을 사용한 실험에서 제안된 CCoT 프레임워크는 r=0.1의 압축 비율로 9%의 정확도 향상을 가져오면서도 디코드 시간을 0.4초밖에 늘리지 않았습니다. 이는 기존의 사색 토큰 기반의 방법보다 뛰어난 성능을 보여주며 연결된, 내용이 풍부한 사색 토큰의 생성이 모델의 추론 성능을 향상시킬 수 있음을 입증합니다.

### [RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning](https://arxiv.org/abs/2412.09858)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09858.png)

Vote: 1

Authors: Sergey Levine, Jianlan Luo, Qiyang Li, Charles Xu

- ***What's New***: RLDG는 로봇 지향적인 정책(Robotic Generalist Policies)을 강화학습(Reinforcement Learning)으로 훈련 데이터를 생성해, 이를 통해 일반적 정책을 고품질로 미세 조정하는 방법을 제안합니다. 이 방법은 인간의 시연보다 최대 40% 높은 성공률을 달성하며, 새로운 작업에 더 잘 일반화되는 것을 입증했습니다.
- ***Technical Details***: RLDG는 강화학습을 통해 자동으로 고품질 데이터를 생성하여 이를 로봇 기반 모델의 미세 조정에 활용합니다. RL 에이전트가 자율적으로 보상을 최적화함으로써 높은 품질의 궤적을 생성하며, 이러한 데이터를 기반으로 일반적인 로봇 정책을 미세 조정합니다. 본 방법은 특정 RL 알고리즘이나 정책 아키텍처에 구애받지 않아 유연성을 제공합니다. 또한, 멀티태스킹 또는 복잡한 다단계 작업에서 인간 시연을 보완하는 방식으로 사용됩니다.
- ***Performance Highlights***: RLDG는 고정밀 작업에서 일반 정책이 인간의 시연보다 30% 이상 높은 성공률을 기록했습니다. 특히, 적은 데이터를 사용하여 크게 향상된 성능을 보여줬으며, 오픈소스 모델인 OpenVLA와 Octo를 이용한 실험에서 RL 데이터로 미세 조정할 때 최대 두 배 이상의 일반화 성능을 달성했습니다.

