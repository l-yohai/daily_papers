## Daily Papers (2024-12-17)

### [The Open Source Advantage in Large Language Models (LLMs)](https://arxiv.org/abs/2412.12004)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12004.png)

Vote: 3

Authors: Laura Boettcher, Jasser Jasser, Jiya Manchanda, Matheus Westphalen

- ***What's New***: 이 논문은 대형 언어 모델(LLMs)의 개발에 있어서 오픈소스 접근법의 이점을 강조합니다. 특히, LLaMA와 BLOOM 같은 오픈소스 모델은 소규모 리소스로도 높은 성능을 발휘할 수 있도록 Low-Rank Adaptation (LoRA)와 같은 새로운 기술들을 사용하여 폐쇄형 모델과의 성능 격차를 줄였습니다.
- ***Technical Details***: 오픈소스 모델은 Transformer 아키텍처를 기반으로, 자원을 효율적으로 활용하기 위해 LoRA와 그룹 쿼리 주의(GQA) 같은 기술을 도입했습니다. 이 기술들은 메모리 요구량을 줄이고 훈련 속도와 에너지 효율성을 높여주며, 모델 디스틸레이션(model distillation)과 양자화(quantization)를 통해 더 효율적인 모델을 만듭니다.
- ***Performance Highlights***: 폐쇄형 모델인 GPT-4는 state-of-the-art 벤치마크에서 우세를 보이며, 특히 HumanEval과 GSM8K와 같은 테스트에서 우수한 성능을 보입니다. 그러나 오픈소스 모델들은 LoRA 등을 통해 적은 자원으로도 경쟁력 있는 성능을 발휘하며, 최근 몇 가지 도메인 특화된 모델이 특정한 과업에서 더 나은 성능을 기록하였습니다. 특히 NVLM 모델은 데이터 처리와 코드 작업에서 뛰어난 성능이 입증되었습니다.

### [Smaller Language Models Are Better Instruction Evolvers](https://arxiv.org/abs/2412.11231)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11231.png)

Vote: 15

Authors: Sen Su, Tingfeng Hui, Yaqi Zhang, Guanting Dong, Lulu Zhao, Hua Zhou

- ***What's New***: 이 연구에서는 SLM(작은 언어 모델)이 LLM(대형 언어 모델)보다 더 효과적이고 복잡한 지시문을 진화시킬 수 있음을 발견했습니다. 실험 결과, SLM은 LLM보다 더 넓은 출력 공간을 가지고 있어 복잡하고 다양한 지시문을 생성할 수 있습니다.
- ***Technical Details***: SLM과 LLM을 비교하여 지시문을 진화시키는 능력을 분석하였습니다. Evol-Instruct, AutoIF, Auto Evol-Instruct 세 가지 시나리오에서 SLM과 LLM의 성능을 비교하였으며, Alpaca, GSM8K Train, Code Alpaca 데이터셋을 사용하여 실험하였습니다. 또한, 복잡성을 고려한 Instruction Complex-Aware IFD(IC-IFD)를 제안하여 지시문 데이터의 효과를 보다 정확하게 평가하였습니다.
- ***Performance Highlights***: SLM이 LLM을 능가하는 성능을 보여주며, Evol-Instruct 시나리오에서는 SLM-INST가 LLM-INST보다 나은 성능을 보였습니다. AutoIF 및 Auto Evol-Instruct 시나리오에서도 SLM의 지시문 데이터가 LLM보다 더 나은 성능을 보여주었습니다. IC-IFD를 통해 고난도 지시문으로 인해 발생하는 성능 저하를 완화할 수 있음을 확인했습니다.

### [Whisper-GPT: A Hybrid Representation Audio Large Language Model](https://arxiv.org/abs/2412.11449)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11449.png)

Vote: 2

Authors: Prateek Verma

- ***What's New***: WHISPER-GPT는 연속 오디오 표현과 이산(Acoustic) 토큰을 동시에 처리할 수 있는 최초의 하이브리드 생성 대형 언어 모델(Generative Large Language Model; LLM)로써, 음성 및 음악 생성을 개선하는 데 중점을 두고 있습니다. 이는 mel-spectrogram과 같은 연속적인 오디오 표현과 ENCODEC 기반의 이산 토큰을 결합하여 더 우수한 예측 성능을 발휘합니다.
- ***Technical Details***: 이 논문에서는 WHISPER-GPT 아키텍처가 Transformer 기반의 디코더만을 사용하는 방식을 채택하였습니다. 이는 기본적인 GPT-2 구조를 하이브리드 연속-이산 표현으로 변형시켜, mel-spectrogram 입력을 통해 ENCODEC 토큰을 예측합니다. Whisper의 비인과적 ASR 모델을 변형하여, 연속적인 mel-spectrogram 입력과 이산 음향 토큰의 접목을 가능하게 했습니다. 이를 통해 모델은 750 토큰의 컨텍스트 길이 내에서 더 나은 토큰 예측을 수행하도록 훈련됩니다.
- ***Performance Highlights***: 실험 결과 WHISPER-GPT는 단순한 토큰 기반의 GPT보다 높은 성능을 보여주었으며, 특히 음악 데이터셋에서 그 차이가 두드러졌습니다. 네거티브 로그 가능성(NLL)과 혼란도(PPL) 측면에서 Baseline GPT-Small(3.7M 파라미터) 대비 하이브리드 LLM(4M 파라미터)이 더 낮은 값(더 나은 성능)을 기록하였습니다. 음악 데이터셋에서의 PPL은 12.43으로 GPT-Small의 16.12에 비해 크게 개선되었습니다.

### [Just a Simple Transformation is Enough for Data Protection in Vertical Federated Learning](https://arxiv.org/abs/2412.11689)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11689.png)

Vote: 1

Authors: Philip Zmushko, Aleksandr Beznosikov, Andrei Semenov, Alexander Pichugin

- ***What's New***: 이 논문은 Vertical Federated Learning (VFL)에서 중요한 데이터 보호 문제를 해결하기 위해 간단한 모델 구조 변환을 사용할 수 있음을 보여줍니다. VFL에서의 특징 복원 공격에 대한 이론적 분석을 기반으로 MLP 모델이 이러한 공격에 대해 더 견고함을 증명합니다.
- ***Technical Details***: 이 연구는 VFL 환경에서 특징 복원 공격이 데이터의 사전 분포에 대한 정보가 없이는 실패한다는 사실을 이론적으로 주장합니다. 이 연구는 MLP 기반 모델이 불법적인 특징 재구성 공격에 강인하다는 실험 결과를 제공합니다. 논문에서는 정렬 학습(SL) 환경을 깊이 있게 설명하며, 실제 데이터는 클라이언트 측에 저장되고 서버는 레이블만을 보유하며, 클라이언트는 최종 계층의 활성화 결과만을 서버에 전송합니다.
- ***Performance Highlights***: 모델 인버전(Model Inversion; MI) 및 특징 공간 하이재킹 공격(Feature-space Hijacking Attack; FSHA)은 MLP 기반 클라이언트 모델에 대해 실패함을 나타냅니다. 이는 기존의 CNN 기반 모델의 결과와 크게 차이가 나며, MLP 기반의 해결책이 데이터 프라이버시 보호에 실질적인 이점을 가짐을 보여줍니다. 실험 결과, MLP 기반 모델 사용 시 공격자에 의한 재구성에서 상당한 품질 악화가 있었으며, 모델의 주된 작업에서의 성능은 유지되었습니다.

### [IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations](https://arxiv.org/abs/2412.12083)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12083.png)

Vote: 10

Authors: Mengchen Zhang, Dahua Lin, Jing Tan, Jiaqi Wang, Tong Wu, Zhibing Li

- ***What's New***: IDArb는 다양한 조명 하에서 임의의 이미지 수에 대해 고유한 성분을 분해할 수 있는 새로운 모델입니다. 이는 cross-view, cross-domain attention module과 조명에 적응하는 학습 전략을 활용하여 표면 법선과 소재 속성의 정확하고 여러 뷰에 일관된 추정을 가능하게 합니다.
- ***Technical Details***: IDArb는 Wonder3D의 cross-view, cross-component attention module을 채택하여, 여러 뷰와 고유 성분 간 정보를 효과적으로 융합합니다. 새로 개발한 ARB-Objaverse 데이터세트는 다양한 조명 조건 하에서 대규모 멀티뷰 본질 데이터와 렌더링을 지원하며, 조명 적응 및 뷰 적응 학습 전략은 복잡한 조명 조건 하에서도 견고한 성능을 보장합니다.
- ***Performance Highlights***: IDArb는 단일 이미지 및 멀티뷰에서 기존의 학습 기반 방법들을 초월하여, 질적 및 양적으로 최첨단 성과를 달성했습니다. 합성 및 실제 데이터에 대한 실험 결과, 특히 복잡한 소재의 모호성을 해결하고 본질적 특성에 관한 보다 정밀한 결과를 제공합니다.

### [TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning](https://arxiv.org/abs/2412.10447)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.10447.png)

Vote: 3

Authors: William Chong, Szymon Rusinkiewicz, Yihuai Gao, Robert Holmberg, Oussama Khatib, Jeannette Bohg, Jimmy Wu, Aaditya Prasad, Shuran Song

- ***What's New***: TidyBot++는 가정용 환경에서 사용할 수 있는 로봇 팔을 위한 저비용, 견고하고 유연한 개방형 소스 홀로노믹 모바일 매니퓰레이터(Holonomic Mobile Manipulator)를 제안합니다. 이 설계는 모든 평면 자유도를 독립적이고 동시에 제어할 수 있는 구동 캐스터를 이용하여 전동 베이스가 완전히 홀로노믹할 수 있도록 합니다.
- ***Technical Details***: TidyBot++는 구동 캐스터(Powered-caster)를 사용하는 모바일 베이스를 기반으로 하며, Kinova Gen3 로봇 팔과 Intel NUC 미니 PC를 장착합니다. 이 베이스는 일반적인 FRC(First Robotics Competition) 부품으로 조립 가능하며, 휴대폰 웹XR(WebXR API)을 사용한 조작 인터페이스를 통해 데이터 수집 효율성을 높였습니다.
- ***Performance Highlights***: 실험 결과, TidyBot++는 다양한 가정용 작업에서 높은 성과를 보였으며, 특히 홀로노믹 드라이브는 비홀로노믹 드라이브 시스템에 비해 더 높은 성과(예: 작업 성공률)와 효율적인 경로를 보여주었습니다. 예를 들어, '카운터 닦기' 작업에서 홀로노믹 드라이브는 비홀로노믹 드라이브보다 작업 성공률이 두 배가량 높았습니다.

### [Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models](https://arxiv.org/abs/2412.09645)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09645.png)

Vote: 24

Authors: Shulin Tian, Ziwei Liu, Fan Zhang, Ziqi Huang, Yu Qiao

- ***What's New***: Evaluation Agent는 시각적 생성 모델(Visual Generative Models)의 평가를 인간과 유사한 방식으로 수행하기 위한 새로운 프레임워크입니다. 이 프레임워크는 사용자의 특정 요구에 맞춘 평가와 이해할 수 있는 설명을 제공하며, 효율적이고 유연하며 확장 가능한 평가를 지원합니다.
- ***Technical Details***: Evaluation Agent는 시각적 생성 모델을 평가하기 위해 LLM 기반 에이전트를 활용합니다. 이 프레임워크는 프로포절(Stage 1)과 실행(Stage 2) 두 단계로 나뉘어져 있으며, 각 단계는 샘플링 및 평가를 수행하여 사용자의 쿼리에 맞춘 동적 평가를 가능하게 합니다. 또한, 스케일링이 용이하여 다양한 모델과 툴을 손쉽게 통합할 수 있는 기능을 제공합니다.
- ***Performance Highlights***: Evaluation Agent는 전통적인 평가 방법에 비해 평가 소요 시간을 90% 이상 줄이면서도 유사한 수준의 정확도를 유지합니다. 다양한 시각적 생성 모델에 대한 실험 결과, 동일한 평가 결과를 더욱 적은 샘플과 시간으로 도출할 수 있었습니다. 또한, 오픈된 사용자 쿼리 데이터를 활용한 실험에서는 프레임워크가 복잡하고 다양한 요청을 처리할 수 있음을 보여주었습니다.

### [Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture](https://arxiv.org/abs/2412.11834)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11834.png)

Vote: 4

Authors: Jingze Shi, Bingheng Wu

- ***What's New***: Wonderful Matrices는 시퀀스 변환(Sequence Transformation)과 상태 변환(State Transformation)을 결합하여 보다 효율적이고 효과적인 기초 모델 아키텍처를 만드는 것을 목표로 합니다. 새로운 구조는 Rotary Position Embedding과 Dynamic Mask Attention, Cross Domain Mixture of Experts를 도입하여 기존 모델의 주요 한계를 극복합니다.
- ***Technical Details***: Rotary Position Embedding을 사용하여 하이브리드 알고리즘에서 위치 인코딩을 일관되게 적용하며, Quadratic Causal Self-Attention과 State Space Duality 알고리즘에 동적으로 적용합니다. Dynamic Mask Attention은 현재 상태에 따른 주의 점수 마스크를 동적으로 조정할 수 있어 정보를 선별적으로 필터링하며, Cross Domain Mixture of Experts는 일반 지식과 분야별 지식을 효율적으로 저장합니다.
- ***Performance Highlights***: Wonderful Matrices는 긴 시퀀스를 처리할 때 Rotary Position Embedding을 통합함으로써 기존 방법보다 낮은 혼란도를 달성했습니다. 특히, 다중 쿼리 연관 회상 실험에서는 Dynamic Mask Attention이 효과적인 회상 성능을 유지하며, 1024개 이상의 전문가가 있는 큰 규모의 모델에서도 Cross Domain Mixture of Experts는 뛰어난 검색 속도를 유지했습니다. CEvalBenchmark에서 모든 영역에서 최고의 정확도를 기록했습니다.

### [Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/abs/2412.09871)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09871.png)

Vote: 30

Authors: Lili Yu, Benjamin Muller, Artidoro Pagnoni, Pedro Rodriguez, Luke Zettlemoyer, Jason Weston, John Nguyen, Margaret Li, Chunting Zhou, Gargi Ghosh, Mike Lewis, Srinivasan Iyer, Ram Pasunuru, Ari Holtzman

- ***What's New***: Byte Latent Transformer(BLT)는 바이트 수준의 학습에서 고정 어휘를 사용하지 않고도 크게 스케일업할 수 있는 최초의 토커나이저 프리 아키텍처입니다. 이는 패치를 통해 바이트를 역동적으로 그룹화하여 컴퓨팅 효율성과 강건성을 향상시킵니다.
- ***Technical Details***: BLT 아키텍처는 바이트를 entropy 기반으로 세그먼트로 나누어 동적인 패치를 생성하고, 이는 컴퓨팅을 효과적으로 할당하여 토큰 기반 모델보다 보다 효율적으로 데이터 복잡성을 처리합니다. 패칭은 3개의 변환기 블록으로 구성되며, 각각이 바이트 수준과 패치 정보의 혼합을 수행합니다. 모델은 최대 8억 개의 매개변수와 4조 개의 학습 바이트까지의 플롭 제어 스케일링을 통해 효율성을 입증합니다.
- ***Performance Highlights***: BLT 모델은 연산 최적화(FLOP-efficient) 설정에서 LLaMA 3의 토큰화 기반 성능을 50% 적은 추론 플롭으로 매칭하는 데 성공했습니다. 또한 긴 문맥 이해와 데이터의 롱테일 일반화에서도 질적 향상을 보이며, 잡음 있는 입력에 대한 강건성과 하위 단어 수준의 이해 능력을 강화합니다.

### [StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors](https://arxiv.org/abs/2412.11586)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11586.png)

Vote: 4

Authors: Jian Yang, Zhenyu Zhang, Ying Tai, Xiaokun Sun, Zeyu Cai

- ***What's New***: StrandHead는 텍스트를 통해 3D 헤드 아바타를 생성하는 혁신적인 프레임워크로, 머리카락의 기하학적 구조를 분리하여 높은 사실성과 다양성을 갖춘 3D 헤드와 머리카락을 만들어냅니다. 2D 생성적 확산 모델을 증류하여 텍스트 설명만으로도 현실감 넘치는 머리카락을 생성할 수 있습니다.
- ***Technical Details***: StrandHead는 차별화된 프리즘화 알고리즘과 머리카락의 기하학적 특성에서 영감을 받은 방향 일관성 손실 및 곡률 정규화 손실을 기반으로 머리카락을 메시로 변환합니다. 이 방법은 메시 기반의 렌더링과 알고리즘을 사용하여 와이어레스 방식으로 머리카락을 모델링할 수 있게 합니다.
- ***Performance Highlights***: 다양한 실험 결과에 따르면, StrandHead는 가장 최신의 기술적 성과를 뛰어넘는 성능을 보여주었으며, 프리즘 기반의 머리카락 생성은 높은 사실성과 물리 기반의 렌더링 및 시뮬레이션 지원을 통한 다양한 응용 가능성을 제시합니다.

### [RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation](https://arxiv.org/abs/2412.11919)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11919.png)

Vote: 23

Authors: Jiajie Jin, Yongkang Wu, Xiaoxi Li, Qi Ye, Zhonghua Li, Yujia Zhou, Zhicheng Dou

- ***What's New***: RetroLLM은 정보 검색과 생성 과정을 하나의 통합된 자동 회귀적 생성 프로세스로 통합하여, 대형 언어 모델(LLM)이 지식 기반으로부터 세밀한 증거를 직접 생성할 수 있도록 합니다. 이는 별도의 정보 검색기를 필요로 하지 않으며, 중복된 입력 토큰을 줄이고 생성 및 정보 검색의 통합 최적화를 가능하게 합니다.
- ***Technical Details***: RetroLLM은 계층적 FM-Index를 사용하여 세밀한 증거 생성을 위한 단서를 생성하고 문서 후보를 식별한 후, 정방향 제약 생성 전략을 통해 새로운 증거로 연결되는 토큰을 생성합니다. 또한, 인코딩과 검색 프로세스를 단일 모델로 통합하여 자동 회귀적 방식으로 증거와 최종 응답을 생성합니다.
- ***Performance Highlights***: RetroLLM은 5개의 오픈 도메인 QA 데이터세트에서 기존의 Retrieval-Augmented Generation(RAG) 방법에 비해 우수한 성능을 보였으며, 토큰 소비를 평균적으로 2.1배, Iter-RetGen 등과 비교하여 6배 이상 줄였습니다. 이는 특히 적은 수의 방문할 패시지로 세밀하고 정밀한 정보를 검색할 수 있는 능력을 보여줍니다.

### [Wonderland: Navigating 3D Scenes from a Single Image](https://arxiv.org/abs/2412.12091)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12091.png)

Vote: 8

Authors: Konstantinos N. Plataniotis, Jian Ren, Demetri Terzopoulos, Sergei Korolev, Junli Cao, Hanwen Liang, Vidit Goel, Guocheng Qian, Sergey Tulyakov

- ***What's New***: Wonderland는 단일 이미지에서 고품질의 광범위한 3D 장면을 효과적으로 생성할 수 있는 새로운 파이프라인을 제안합니다. 이는 비디오 확산 모델(video diffusion model)의 라틴트(latent) 공간을 활용하여 장면의 3D Gaussian Splatting를 피드포워드(feed-forward) 방식으로 예측하는 대규모 재구성 모델을 소개합니다.
- ***Technical Details***: Wonderland는 카메라 안내 비디오 확산 모델(camera-guided video diffusion model)과의 카메라 이동 통제를 통합한 이중-브랜치 조건 메커니즘(dual-branch conditioning mechanism)을 사용하여 단일 이미지로부터 멀티뷰 일관성(multi-view consistency)과 정확한 포즈 제어(pose control)를 실현합니다. 이 시스템은 대규모 3D 재구성 모델(Latent Large Reconstruction Model; LaLRM)을 사용하여 비디오 라틴트에서 직접 3D Gaussian Splatting로 변환하며, 이는 메모리와 시간 소모를 크게 줄입니다.
- ***Performance Highlights***: Wonderland는 다양한 벤치마크 데이터셋에서 단일 뷰 기반 3D 장면 생성에 대한 최첨단 성능을 입증했습니다. RealEstate10K [59], DL3DV [23], Tanks-and-Temples [20] 데이터셋에서 LPIPS, PSNR, SSIM 등 여러 지표에서 기존 방법보다 우수한 결과를 보여줍니다.

### [SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models](https://arxiv.org/abs/2412.11605)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11605.png)

Vote: 9

Authors: Xiao Liu, Xiaotao Gu, Dan Zhang, Yida Lu, Minlie Huang, Cunxiang Wang, Hongning Wang, Jiale Cheng, Yuxiao Dong, Jie Tang

- ***What's New***: 이 연구에서는 대형 언어 모델(Large Language Models; LLMs)의 지시사항을 따르는 능력을 개선하기 위해 자체 플레이(Self-Play)와 트리 탐색(Tree-Search) 정제를 결합한 SPAR라는 새로운 프레임워크를 소개합니다. 기존의 방법론과 달리, SPAR는 필요하지 않은 변이를 최소화하여 지시사항에 따른 미세한 차이를 체계적으로 강조합니다.
- ***Technical Details***: SPAR 프레임워크는 행위자(Actor) 모델과 정제자(Refiner) 모델로 구성되어 있으며, 동일한 기본 모델에서 초기화됩니다. 행위자는 지시사항에 대한 응답을 생성하고, 정제자는 이러한 응답의 정확성을 판단하고 개선합니다. 이 과정에서 트리 탐색 알고리즘을 사용하여 부정확한 응답을 체계적으로 개선하며, 성공적인 비교쌍을 생성하여 모델을 최적화합니다.
- ***Performance Highlights***: 세 번의 반복 학습 후, SPAR는 LLaMA3-8B 모델이 IFEval 벤치마크에서 GPT-4-Turbo를 능가하는 성과를 보였으며, 전반적인 성능 저하 없이 지시사항을 따르는 능력을 크게 개선시켰습니다. 또한, SPAR는 모델 크기에 따라 확장성이 우수하여, LLaMA3-70B 모델에서도 탁월한 성과를 거두었습니다.

### [DynamicScaler: Seamless and Scalable Video Generation for Panoramic Scenes](https://arxiv.org/abs/2412.11100)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11100.png)

Vote: 0

Authors: Shaoheng Lin, Ming-Hsuan Yang, Jinxiu Liu, Yinxiao Li

- ***What's New***: DynamicScaler는 파노라마(dynamic panoramic) 장면 생성을 위한 프레임워크로, 이미지와 텍스트 또는 텍스트만을 기반으로 동적인 파노라마 영상을 생성할 수 있는 기능을 제공합니다. 이는 다양한 규모의 파노라마 장면을 생성하면서도 경계 일관성을 유지하여 몰입형 시각 경험을 가능하게 하며, AR/VR 애플리케이션을 위한 새로운 변화를 제시합니다.
- ***Technical Details***: DynamicScaler는 오프셋 쉬프팅 디노이저(Offset Shifting Denoiser)를 소개하여 고정된 해상도의 diffusion model을 통해 동적 파노라마 장면을 효과적으로 제거함으로써 파노라마 전체에서의 경계 전환과 일관성을 보장합니다. 또한, 글로벌 모션 가이던스(Global Motion Guidance)를 사용하여 국지적 세부사항의 충실도와 전역적인 움직임의 일관성을 보장합니다. 중요한 것은 이 방법은 기존의 모델 학습 없이 VRAM 사용량을 일정하게 유지하면서 고해상도 동적 장면 생성을 가능하게 한다는 점입니다.
- ***Performance Highlights***: DynamicScaler는 기존 방법들과 비교하여 시각 품질과 운동 일관성에서 우수한 성능을 보이며, 몰입형 애플리케이션에 적합한 연속적이고 반복 가능한 동적 장면을 생성합니다. 사용 사례 평가에서 DynamicScaler는 360DVD에 비하여 더 높은 시각적 품질을 제공하며, 장면의 복잡한 동적 요소를 보다 효과적으로 처리합니다.

### [ColorFlow: Retrieval-Augmented Image Sequence Colorization](https://arxiv.org/abs/2412.11815)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11815.png)

Vote: 19

Authors: Junhao Zhuang, Zhaoyang Zhang, Yong Liu, Chun Yuan, Xuan Ju, Shiyi Zhang, Ying Shan

- ***What's New***: ColorFlow는 이미지 시퀀스 컬러화를 위한 최초의 모델로, 이미지를 연속적으로 컬러화하면서 인물이나 객체의 정체성(ID)을 정밀하게 보존합니다. 컨텍스트 정보와 참고 이미지를 활용하여 컬러 데이터를 생성하고, 이는 만화 시리즈나 고전 흑백 영화의 컬러화에 큰 도움이 될 것으로 기대됩니다.
- ***Technical Details***: ColorFlow는 세 가지 단계로 구성됩니다: Retrieval-Augmented Pipeline(RAP), In-context Colorization Pipeline(ICP), Guided Super-Resolution Pipeline(GSRP). RAP는 참고 이미지 풀에서 유의미한 컬러 패치를 추출하고, ICP는 강력한 인-컨텍스트 학습을 통해 컬러 ID를 불러와 Colorization을 수행하며, GSRP는 구조적 왜곡을 줄여 고해상도 컬러 이미지를 생성합니다. 클립(CHIP) 이미지를 활용하여 패치 유사성을 계산하고, Low-Rank Adaptation(LoRA) 방법을 사용하여 diffusion 모델을 미세 조정합니다.
- ***Performance Highlights***: ColorFlow는 ColorFlow-Bench 벤치마크에서 기존 모델을 여러 지표에서 초과 달성하며, 시퀀스 컬러화에서 최고의 성능을 보여주는 것으로 나타났습니다. 일반 주관적 평가에서도 미적 품질, 원본 유사성, 시퀀스 일관성에서 높은 점수를 받았습니다. FID 지표에서는 기존의 컬러화 모델 대비 37% 이상의 감소를 기록하였으며, 여러 실험을 통해 우수한 성능을 입증했습니다.

### [GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs](https://arxiv.org/abs/2412.11258)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11258.png)

Vote: 7

Authors: Wenhang Ge, Haoyu Zhao, Hanfeng Zhao, Zhuoyun Liu, Shunsi Zhang, Ying-Cong Chen, Dicong Qiu, Xinli Xu, Dongyu Yan, ZhiFei Chen, Junwei Liang

- **What's New**: GaussianProperty는 대형 멀티모달 모델(Large Multimodal Models; LMMs)을 이용해 3D 모델에 물리적 특성을 할당하는 새로운 방법을 제안합니다. 이 프레임워크는 코드 학습 없이 3D 가우시안(Gaussian)에 물질의 물리적 특성을 첨부함으로써 다양한 물리 시뮬레이션과 로봇 그리핑(roboic grasping)을 가능하게 합니다.
- **Technical Details**: GaussianProperty는 Segment Anything Model (SAM)과 GPT-4V의 인식을 결합하여 2D 이미지에서 물리적 특성을 추론합니다. 여러 뷰(view)의 2D 이미지에서 얻은 물리적 특성을 3D 가우시안으로 프로젝션함으로써 복수 이미지에서의 정보 일관성을 보장합니다. 이러한 방법은 주어진 물체 부분의 글로벌 및 로컬 정보를 사용하여 보다 정밀한 물리적 특성 추정을 가능하게 합니다.
- **Performance Highlights**: 다양한 실험을 통해 GaussianProperty는 재료 분할, 기능적 물리 시뮬레이션, 현실 세계의 로봇 그리핑 작업에서 우수한 성능을 입증했습니다. 특히, 새로운 주어진 물체의 그리핑 성공률을 높이고, 손상 없이 물체를 들어올리는 무손상률에서도 뛰어난 성능을 보였습니다.

### [MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization](https://arxiv.org/abs/2412.12098)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12098.png)

Vote: 1

Authors: Carmelo Sferrazza, Pieter Abbeel, Andreas Krause, Stelian Coros, Bhavya Sukhija

- ***What's New***: MaxInfoRL은 강화학습(Reinforcement Learning; RL)에서 정보 이득 최대화를 통한 탐색을 증진하는 새로운 프레임워크입니다. 기존의 탐색 방법이 주로 랜덤한 액션 시퀀스를 선택하는 비지도적 탐색에 의존했던 반면, MaxInfoRL은 내재적 보상(intrinsic rewards)인 정보 이득을 최대화함으로써 정보가 풍부한 전환을 목표로 탐색을 유도합니다.
- ***Technical Details***: MaxInfoRL은 볼츠만 탐색(Boltzmann exploration)을 기본으로 하여 가치 함수의 최대화와 상태, 보상, 액션의 엔트로피 최대화의 균형을 자연스럽게 조절합니다. 이 알고리즘은 다중 무장 강도 게임(multi-armed bandits)에서의 하위 선형 후회를 달성하며, 연속 상태-액션 공간을 가진 오프-정책, 모델-프리 강화학습 방식에 일반화된 형태로 강화하여 높은 성능의 새로운 알고리즘을 제공합니다.
- ***Performance Highlights***: MaxInfoRL은 여러 심층 RL 벤치마크에서 탁월한 성능을 보였으며, 특히 시각 제어 과제와 같이 탐색이 어려운 문제에서 우수한 성능을 입증하였습니다. 예를 들어, MaxInfoDrQv2는 인간형 로봇 시각 제어 과제에서 평균 성능이 크게 향상되었습니다.

### [Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning](https://arxiv.org/abs/2412.11974)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11974.png)

Vote: 2

Authors: Pengfei Hong, Qi Sun, U-Xuan Tan, Soujanya Poria, Vernon Toh, Deepanway Ghosal, Tej Deep Pala

- ***What's New***: EMMA-X는 로봇 정책 생성에서 공간 추론과 작업 계획을 향상시키기 위해 설계된 7B-파라미터의 다중 모달 액션 모델입니다. 이 모델은 다층적 구현 데이터셋을 활용하여 점진적인 연관 사고(Chain of Thought; CoT) 추론과 미래 상태로의 공간적 이동 예측을 통해 복잡한 작업을 보다 효과적으로 수행할 수 있도록 합니다.
- ***Technical Details***: EMMA-X는 OpenVLA에서 파인튜닝된 모델로, 60,000개의 로봇 조작 경로가 포함된 BridgeV2 데이터셋을 기반으로 한다. 이 데이터셋은 각 상태에 대한 구체적인 공간 추론과 과제 계획을 포함한다. 특히, 로봇의 움직임 경로와 그리퍼의 상태를 기반으로 궤적을 분할하는 새로운 전략을 통해 세부적인 단계별 계획을 생성하고, 이를 통해 로봇 행동의 정확성을 높입니다.
- ***Performance Highlights***: EMMA-X는 실제 월드 로봇 과제에서 공간 추론이 필요한 작업에서 기존의 기준선을 크게 능가하는 성능 향상을 보여주었습니다. 예를 들어, 공간 관계 과제에서 43%향상의 절반 성공률을 기록했으며, OOD(out-of-domain) 설명 조건에서도 개선된 성과를 보였습니다. 이를 통해 장기적인 계획과 적응성이 요구되는 다양한 실세계 로봇 작업에서의 뛰어난 수행 능력을 입증했습니다.

### [MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes](https://arxiv.org/abs/2412.11457)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11457.png)

Vote: 2

Authors: Yixin Chen, Gang Zeng, Junfeng Ni, Baoxiong Jia, Yu Liu, Diwen Wan, Siyuan Huang, Ruijie Lu

- ***What's New***: MOVIS는 실내 장면에서 다중 객체 소설 보기 합성을 개선하기 위한 새로운 접근 방식을 제안합니다. 이 모델은 구조적 인식을 향상시켜 보기 조건에 따른 확산 모델의 효과를 증가시킵니다. 새로운 구조 인지 기능을 모델 입력에 주입하고, 보조 작업으로 소설 보기 객체 마스크를 예측하도록 하여 모델의 객체 구별 및 배치 능력을 개선하였습니다.
- ***Technical Details***: MOVIS는 심도와 객체 마스크와 같은 구조 인식 기능을 디노이징 U-Net에 주입함으로써 객체 사례와 공간적 관계를 더욱 잘 이해하도록 합니다. 또한, 노이즈 타임스텝 샘플링 스케줄러를 채택하여 학습하는 동안 전역 객체 배치 학습과 세밀한 디테일 회복을 균형 있게 조절합니다. 구조에 기반한 타임스텝 샘플러는 학습 초기 단계에서 큰 타임스텝을 우선시하여 시간이 경과함에 따라 감소합니다.
- ***Performance Highlights***: 대규모 실험 결과, MOVIS는 다양한 합성 및 실제 데이터셋에서 강력한 일반화 능력을 보이며 일관된 소설 보기 합성을 만들어냈습니다. 특히, 새로운 데이터셋에 대한 소설 보기 생성에서 뛰어난 성능을 보여주며, 객체 배치, 모양, 외형 회복의 일관성을 효과적으로 강조하였습니다.

### [VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video Face Swapping](https://arxiv.org/abs/2412.11279)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11279.png)

Vote: 7

Authors: Hao Shao, Yang Zhou, Yu Liu, Hongsheng Li, Shuo Qin, Zhuofan Zong, Shulun Wang, Guanglu Song, Dailan He, Bingqi Ma

- ***What's New***: VividFace는 비디오 페이스 스왑핑(video face swapping)을 위한 최초의 확산 기반 프레임워크로, 정적 이미지 데이터와 시간적 비디오 시퀀스를 활용하여 비디오 트레이닝의 한계를 극복하는 이미지-비디오 하이브리드 트레이닝 프레임워크를 도입했습니다. 새로운 Attribute-Identity Disentanglement Triplet(AIDT) 데이터셋을 활용하여 큰 포즈 변화와 폐색에도 강한 비디오 스왑핑 성능을 자랑합니다.
- ***Technical Details***: VividFace 프레임워크는 이미지와 비디오 데이터를 모두 효과적으로 처리하는 확산 모델(diffusion model)과 VidFaceVAE를 결합합니다. 이러한 모델은 정적 이미지 데이터와 시간적 비디오 시퀀스를 처리하여 시간 일관성을 유지합니다. 또한 3D 재구성 기술을 통합하여 큰 포즈 변화 시에도 자세와 표정을 정확히 처리합니다. 이 모델은 Target Face의 포즈와 감정을 일치시키면서 소스로부터의 동일한 아이덴티티를 유지하도록 설계되었습니다.
- ***Performance Highlights***: VividFace는 기존 방법들과 비교했을 때 ID 유지, 시간 일관성 및 시각 품질에서 우수한 성능을 나타내며, 특히 Fréchet Video Distance(FVD)에서 좋은 결과를 보여줍니다. 다른 방법과 달리 적은 추론 단계로 효율성을 증대시켰으며, 다양한 복잡한 사례에서도 안정적인 생성 결과를 보였습니다.

### [Reliable, Reproducible, and Really Fast Leaderboards with Evalica](https://arxiv.org/abs/2412.11314)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11314.png)

Vote: 1

Authors: Dmitry Ustalov

- ***What's New***: Evalica는 신뢰성과 재현성을 강조한 NLP 모델 리더보드를 자동화하는 오픈 소스 툴킷입니다. 이 툴킷은 현대적인 평가 프로토콜을 지원하며, 사용자 인터페이스를 웹(Web) 인터페이스, 명령줄 인터페이스, Python API 형태로 제공합니다.
- ***Technical Details***: Evalica는 Rust로 작성된 코어 부분을 가지고 있으며, Python과 Rust에서 데이터 처리 및 순위 매김 알고리즘을 독립적으로 구현했습니다. 이 툴킷은 다양한 스코어링 방식을 지원하며, 데이터 재표본화를 위해 부트스트랩 방식으로 신뢰 구간을 계산할 수 있습니다. 이는 특히 NLP의 실용적인 평가 사례를 다루기 위해 설계되었으며, 데이터 처리 시 효율성을 제공하기 위해 최적화된 컴파일드 프로그래밍 언어로 구현되었습니다.
- ***Performance Highlights***: Evalica는 다른 벤치마크 구현에 비해 최대 46배 빠른 성능을 보여주며, Rust 기반 구현은 Python 기반 구현에 비해 더 일관된 성능과 낮은 런타임 변동성을 제공합니다. Chatbot Arena 데이터셋에서 수행된 실험에서는 Evalica의 Elo 및 브래들리-테리(Bradley-Terry) 알고리즘이 Arena-Hard와 Chatbot Arena의 기존 방식보다 효율적으로 성능을 발휘했습니다.

### [Efficient Generative Modeling with Residual Vector Quantization-Based Tokens](https://arxiv.org/abs/2412.10208)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.10208.png)

Vote: 18

Authors: Taehong Moon, Jaehyeon Kim, Keon Lee, Jaewoong Cho

- ***What's New***: 이 논문에서는 고충실도 데이터 생성을 위한 임베디드 토큰의 직접 예측을 통해 효율적인 생성 모델링을 가능하게 하는 RVQ(Residual Vector Quantization)를 기반으로 한 ResGen을 소개합니다. 이 방법은 기존의 순차적 토큰 예측에서 벗어나 여러 토큰을 집단적으로 예측하여 효율성을 높입니다.
- ***Technical Details***: ResGen은 RVQ 기반의 생성 모델로, 마스크된 토큰 예측을 변량적 추론(variational inference)과 결합한 확률론적(discrete diffusion process) 프레임워크 안에서 수행합니다. 여기서는 RVQ 프로세스의 잠재적인 깊이 증대에도 불구하고 효율성을 유지하도록 설계되었으며, 토큰 마스킹과 다중 토큰 예측 기술을 사용합니다.
- ***Performance Highlights***: 실험 결과, ResGen은 ImageNet 256×256에서 조건부 이미지 생성 및 텍스트 투 음성 합성(zero-shot text-to-speech) 작업에서 기존의 자동회귀 모델 대비 우수한 성능을 보였으며, 생성 속도를 저해하지 않고 우월한 성능을 달성했습니다. RVQ의 깊이를 조절함에 따라, 생성 충실도와 샘플링 속도에서 기존 모델들보다 더 나은 성능을 보여주었습니다.

### [BrushEdit: All-In-One Image Inpainting and Editing](https://arxiv.org/abs/2412.10316)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.10316.png)

Vote: 22

Authors: Zhaoyang Zhang, Qiang Xu, Yaowei Li, Xuan Ju, Yuxuan Bian, Ying Shan

- ***What's New***: BrushEdit는 언어 모델(Large Language Models; LLMs)과 인페인팅 모델(Image Inpainting Models)을 결합하여 자연어 기반의 자유로운 이미지 편집을 구현하는 새로운 프레임워크를 제안합니다. 사용자가 단순한 자연어 지시만으로 다양한 이미지 편집 작업을 수행할 수 있도록 설계되었습니다.
- ***Technical Details***: BrushEdit는 멀티 모달 대형 언어 모델(MLLMs)과 이중 분기 인페인팅 모델을 결합하여 편집 범주 분류, 주요 객체 식별, 마스크 획득, 편집 영역 인페인팅을 수행합니다. 이를 통해 사용자 인터페이스를 개선하고, 텍스트 기반 이미지 편집이 가능하여 사람-중심의 조작을 지원합니다. 이 시스템은 무작위 라운드의 상호작용이 가능하여 교정 및 개선이 용이합니다.
- ***Performance Highlights***: BrushNet에 비해 향상된 BrushEdit는 고품질의 인페인팅 마스크 이미지를 사용하여 학습되었고, 다양한 마스크 형식에 대한 처리 능력을 갖춘 통합된 모델로, 모든 인페인팅 및 편집 벤치마크 작업을 수행하며 현존하는 방법들보다 우수한 성능을 보였습니다. 이미지의 배경 유지 및 명확한 이미지-텍스트 정렬면에서 우수성을 입증했습니다.

### [SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator](https://arxiv.org/abs/2412.12094)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12094.png)

Vote: 7

Authors: Jiawei Li, Yimeng Chen, Zhenguo Li, Guoxuan Chen, Weiyang Liu, Xiaozhe Ren, Han Shi, Chao Huang, Yihang Gao, Xin Jiang

- ***What's New***: SepLLM은 대형 언어 모델(LLM)의 추론 속도를 'Separator' 토큰 하나로 여러 세그먼트를 압축하여 가속화하는 혁신적인 플러그 앤 플레이 프레임워크입니다. 이 프레임워크는 불필요한 토큰을 제거하고 세그먼트 정보를 효율적으로 압축하여 정보 손실 없이 처리 속도를 개선합니다.
- ***Technical Details***: SepLLM은 데이터 의존적인 Sparse Attention 메커니즘을 특징으로 하는 효율적인 Transformer 아키텍처입니다. 이 메커니즘은 초기 토큰, 이웃 토큰, 그리고 Separator 토큰만 선택적으로 유지하여 다른 토큰을 제거합니다. FlexAttention 기반의 하드웨어 효율적인 커널을 통합하여 훈련과 추론 간의 불일치를 줄입니다. 또한 네트워크 트래픽을 줄이고 인퍼런스 시간을 단축하기 위해 Seg-Attention 모듈을 포함한 멀티 노드 분산 훈련을 지원합니다.
- ***Performance Highlights***: SepLLM은 Llama-3-8B를 사용한 실험에서 GSM8K-CoT 벤치마크에서 KV 캐시를 50% 이상 줄이면서도 기존 성능을 유지하는 데 성공했습니다. 스트리밍 상황에서는 최대 4백만 개의 토큰을 효율적으로 처리할 수 있으며, 추론 시간이 줄어들고 메모리 사용이 감소합니다. 또한, 다른 여러 LLM 아키텍처 및 규모에서도 유사한 가속 효과가 관찰되었습니다.

### [Causal Diffusion Transformers for Generative Modeling](https://arxiv.org/abs/2412.12095)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.12095.png)

Vote: 15

Authors: Kunchang Li, Shi Guan, Deyao Zh, Haoqi Fan, Chaorui Deng

- ***What's New***: Causal Diffusion Transform, autoregressive(A R) 모델과 기존의 확산 모델을 결합한 새로운 접근 방식으로, 두 모델의 장점을 결합한 'CausalFusion'이라는 decoder-only Transformer 모델을 도입하였습니다. 이 모델은 임의의 길이의 시퀀스를 생성할 수 있는 A R의 장점과 확산 생성 모드의 장점을 동시에 활용합니다.
- ***Technical Details***: CausalFusion 모델은 데이터 시퀀스와 노이즈 수준을 따라 이중 요인화(Dual-Factorization)를 통해 데이터를 처리합니다. 이 접근법은 전통적인 A R 또는 확산 패러다임으로 원활하게 전환할 수 있도록 설계되었으며, 이를 통해 다양한 생성 작업에서 충분한 탐색과 일반화가 가능합니다. 또한, 모델의 손실은 A R 축을 따라 가중치를 조절하여 최적화할 수 있습니다. 이는 디코더 전용 Transformer 구조를 기반으로 하고 있으며, 기존의 A R 모델들과도 호환됩니다.
- ***Performance Highlights***: ImageNet에서 CausalFusion은 가장 우수한 성능을 기록하며, 특히 이미지 생성 벤치마크에서 기존의 DiT 모델을 크게 능가했습니다. 또한, 다중모달 모델로서의 가능성도 실험을 통해 증명되었습니다. 이는 고화질의 이미지 편집과 텍스트-이미지, 이미지-텍스트 생성 작업에서도 뛰어난 성능을 보였습니다.

