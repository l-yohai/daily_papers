## Daily Papers (2024-12-26)

### [Token-Budget-Aware LLM Reasoning](https://arxiv.org/abs/2412.18547)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18547.png)

Vote: 11

Authors: Chunrong Fang, Shiyu Zhao, Zhenyu Chen, Zhenting Wang, Shiqing Ma, Tingxu Han

- ***What's New***: Token-Budget-Aware LLM Reasoning는 대형 언어 모델(LLM)의 불필요하게 긴 추론 과정을 압축하여 토큰 사용량을 줄이는 새로운 프레임워크입니다. 다양한 문제의 복잡성에 기반하여 동적으로 토큰 예산을 추정하고 이를 활용하여 LLM의 추론 과정에서 비용 효율성을 높입니다.
- ***Technical Details***: 이 프레임워크는 문제의 복잡성에 따라 토큰 예산을 추정하며, 이를 Chain-of-Thought(CoT) 추론 과정에서 적용하여 토큰 비용을 절감합니다. 알고리즘1과 알고리즘2를 통해 특정 문제에 대한 최적의 토큰 예산을 검색하고, 이 예산에 기반하여 최종 답변을 끌어내는 방식을 활용합니다. 제안된 방법은 토큰 비용을 줄이면서도 대체로 답변의 정확성을 유지합니다.
- ***Performance Highlights***: 실험 결과 TALE은 CoT 추론에서 평균적으로 68.64%의 토큰 사용량을 줄이며, 정확도 감소폭은 5% 이하로 유지하는 등 매우 효율적인 성능을 입증했습니다. 다양한 LLM 모델(GPT-4o, GPT-4o-mini, Yi-lightning)에서도 일관되게 성능을 발휘하여 비용과 정확성 사이의 균형을 잘 맞췄습니다.

### [How "Real" is Your Real-Time Simultaneous Speech-to-Text Translation System?](https://arxiv.org/abs/2412.18495)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18495.png)

Vote: 1

Authors: Sara Papi, Dominik Macháček, Peter Polak, Ondřej Bojar

- ***What's New***: 이 논문은 동시 음성-텍스트 번역(SimulST) 시스템의 실시간 성능과 관련된 실제적 어려움을 강조하고, 시스템 구축을 위한 표준화된 용어와 분류 체계를 제안합니다. 특히, 주제는 연속적인 오디오 스트림을 처리하는 SimulST 시스템에 대한 연구의 필요성을 강조합니다.
- ***Technical Details***: SimulST 시스템을 구성하는 데 필요한 6단계 과정을 정의하고, 오디오 획득에서 번역 제시까지의 총 과정을 설명합니다. 이 과정에서 오디오 스트림의 세그먼트화를 선택적으로 수행할 수 있으며, ST 모델은 증분적으로 정보를 처리합니다. 용어 혼란을 해결하기 위해 통합 용어와 분류 체계를 도입하며, 순차적 번역과 재번역을 통한 출력 전략에 따라 시스템을 분류합니다.
- ***Performance Highlights***: 현재 동시 음성-텍스트 번역 연구는 대부분 사람이 세그먼트를 구분한 오디오를 전제로 하고 있으며, 이는 실제 활용에서의 복잡성을 단순화시킵니다. 용어의 불일치 때문에 연구 간의 비교가 어렵고, 이는 이 분야의 진보를 저해할 수 있습니다. 따라서, 현실적 비전을 갖춘 SimulST 솔루션 개발을 위한 연구 방향성을 제시하고 있습니다.

### [WavePulse: Real-time Content Analytics of Radio Livestreams](https://arxiv.org/abs/2412.17998)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17998.png)

Vote: 0

Authors: Shruti Wagle, Anthony J DeMattee, Nasir Memon, Chinmay Hegde, Mustaque Ahamad, Govind Mittal, Chirag Chopra, Sarthak Gupta

- ***What's New***: WavePulse는 AM/FM 라디오 방송이 인터넷으로 스트리밍되는 현실에서, 라디오 콘텐츠를 실시간으로 기록하고 문서화하고 분석하는 혁신적인 프레임워크입니다. 특히 2024년 미국 대통령 선거에 초점을 맞추어 정치 과학자들과 협력하여 3개월 동안 396개 뉴스 라디오 방송국의 약 500,000시간의 오디오 스트림을 처리하여, 지역 문제와 전국적 트렌드의 상호작용을 분석하는 데 성공하였습니다.
- ***Technical Details***: WavePulse는 URL을 통해 웹에서 접근 가능한 수백 개의 라디오 라이브 스트림의 실시간 획득, 전사, 화자 분절(diarization), 데이터 큐레이션 및 콘텐츠 분석을 수행합니다. 이 시스템의 대부분은 현대적인 멀티모달 대형 언어 모델(LLMs)을 사용하여 설계 및 배포 속도를 높이고 방송 미디어 분석에서 AI의 잠재력을 보여줍니다. 녹음된 오디오는 WhisperX 모델을 사용하여 시간 스탬프가 포함된 전사본과 화자 정보로 변환됩니다.
- ***Performance Highlights***: 이 프레임워크를 통해 수집된 데이터는 485,090.5시간 이상의 음성 녹음과 약 4.5억 단어, 329백만 텍스트 세그먼트를 포함하는 방대한 데이터셋을 생성하였으며, 이 데이터셋은 미국 전역의 대중 담론, 미디어 내러티브, 여론 형성 및 허위 정보 과학을 연구하는 데 활용될 수 있습니다. LLM 기반 QA-RAG 시스템을 통해 120개의 전사본 중 53개의 일치하는 샘플을 식별하는 데 성공하였습니다.

### [Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search](https://arxiv.org/abs/2412.18319)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18319.png)

Vote: 3

Authors: Dacheng Tao, Yuxin Song, Shunyu Liu, Jingyi Zhang, Yingjie Wang, Huanjin Yao, Wenhao Wu, Jiaxing Huang, Yibo Wang, Li Shen, Haocheng Feng

- ***What's New***: 본 논문에서는 대규모 멀티모달 언어 모델(MLLMs)을 위한 'Collective Monte Carlo Tree Search(CoMCTS)'를 제안하여 효과적이고 효율적인 추론 경로 탐색 및 학습을 가능하게 합니다. 이 새로운 메서드는 모델의 협력적 지식을 활용하여 각 단계의 중간 추론 상태를 생성하고 최종 답변에 도달하기 위해 단계별로 문제를 해결하는 방안을 제시합니다.
- ***Technical Details***: CoMCTS는 여러 모델에서의 집합적 지식을 활용하여 추론 경로를 효과적으로 탐색합니다. 이 과정은 'Expansion', 'Simulation and Error Positioning', 'Backpropagation', 'Selection'의 네 가지 반복적인 작업을 통해 이뤄집니다. 이를 통해 각 질문에 대한 풍부하고 명확하게 정의된 추론 노드를 가진 멀티모달 데이터셋 Mulberry-260k를 구축하고 SFT를 통해 Mulberry 모델을 훈련합니다.
- ***Performance Highlights***: CoMCTS를 사용한 Mulberry 모델은 공유된 데이터와 CoMCTS가 제공한 효과적인 추론 데이터에서 훈련되어 독보적인 단계별 추론 및 반영 능력을 가집니다. 다양한 벤치마크에서 기존의 오픈 소스 및 일부 폐쇄 소스 모델을 능가하는 성능을 보여주었습니다. 또한, CoMCTS는 검색 성공률에서 80.2%로 다른 트리 탐색 방법에 비해 우수한 효과성과 효율성을 입증하였습니다.

### [PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion](https://arxiv.org/abs/2412.17780)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.17780.png)

Vote: 0

Authors: Yinuo Zhang, Sophia Tang, Pranam Chatterjee

- ***What's New***: PepTune는 Multi-Objective-Guided Discrete Diffusion을 사용하여 새로운 치료적 펩타이드를 생성하는 혁신적인 모델입니다. Masked Discrete Language Model(MDLM) 프레임워크를 기반으로 설계된 PepTune은 유효한 펩타이드 구조를 보장하며, Monte Carlo Tree Search(MCTS)-기반 전략을 사용하여 여러 치료 특성의 최적화된 펩타이드를 생성합니다.
- ***Technical Details***: PepTune은 화학적으로 수정된 사이클릭 펩타이드를 생성하기 위해 Simplified Molecular Input Line Entry System(SMILES) 문자열을 사용하며, 비구조적 및 다양한 구조적 타겟에 대해 유연하게 작동합니다. MDLM은 RoFormer 백본을 사용하여 예측된 토큰 확률을 생성하며, state-dependent masking schedule과 global sequence invalidity loss를 도입하여 유효한 펩타이드 생성을 촉진합니다.
- ***Performance Highlights***: 훈련된 PepTune은 다양한 치료적 특성(예: Membrane Permeability, Solubility, Hemolysis)에서 페어 최적화된 펩타이드를 효과적으로 생성하며, 약 45%의 유효한 펩타이드 생성률을 달성했습니다. 멀티오브젝트 MCTS 가이던스는 유사성과 다양성 점수에서 경쟁력을 유지하며, 높은 임상 잠재력을 보이는 펩타이드를 발견해냈습니다.

### [Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models](https://arxiv.org/abs/2412.18609)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.18609.png)

Vote: 1

Authors: Jinhui Yi, Yanan Luo, Muzammal Naseer, Juergen Gall, Syed Talal Wasim

- ***What's New***: Video-Panda는 비디오-언어 모델에 있어서 효율적인 인코더-프리(encoder-free) 접근 방식을 제안했으며, Spatio-Temporal Alignment Block (STAB)를 도입하여 이전의 무거운 인코더 사용을 배제하면서도 유사한 성능을 보여줍니다. 이 방법은 비디오 입력을 직접 처리하여 인코더 없이 시각과 언어 정보를 효과적으로 연결합니다.
- ***Technical Details***: Video-Panda의 핵심 구성 요소는 Spatio-Temporal Alignment Block (STAB)으로, 이는 로컬 시공간 인코딩(Local Spatio-Temporal Encoding)과 프레임 단위 및 비디오 레벨 관계를 모델링할 수 있는 개별 메커니즘을 포함합니다. 이 구조는 비디오 입력을 바로 처리할 수 있으며, 효율적인 시공간 모델링을 통해 비디오-언어 문제를 해결합니다. 이 과정에서 45M 파라미터로 시각 처리에서 큰 파라미터가 사용되었던 기존 모델들과 비교했을 때, 최소 6.5배 이상 절감이 가능합니다.
- ***Performance Highlights***: Video-Panda는 MSVD-QA 같은 비디오-언어 벤치마크에서 64.7%의 정확도를 달성하였고, MSRVTT-QA에서 54.8%의 점수를 기록하여 기존의 인코더 기반 접근법을 압도합니다. STAB는 기존 접근법보다 3-4 배 빠르게 비디오를 처리하여 실용적인 활용에 이점을 제공합니다. 더 나아가, Video-ChatGPT와 Video-LLaVA와 같은 인코더 기반 접근법을 주요 측면에서 능가하며, 특히 정확성과 시간 이해에서 두드러집니다.

