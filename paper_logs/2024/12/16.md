## Daily Papers (2024-12-16)

### [Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on Breast Ultrasound Images](https://arxiv.org/abs/2412.09910)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09910.png)

Vote: 1

Authors: Moein Heidari, Ilker Hacihaliloglu, Clayton Allard, Leonid Sigal, Yasamin Medghalchi

- ***What's New***: Prompt2Perturb (P2P)은 텍스트 기반 지시 사항을 활용하여 유방 초음파 이미지에 대한 의미 있는 공격을 생성하는 새로운 언어 유도 공격 방법으로, 기존 방식과 달리 텍스트 임베딩을 직접 업데이트하여 대규모 사전 학습 모델을 재훈련할 필요가 없습니다.
- ***Technical Details***: P2P는 CLIP 텍스트 인코더를 활용하여 텍스트 임베딩을 최적화함으로써 치명적이지 않으면서 효과적인 공격 이미지를 생성합니다. 초기 반전 전파 단계만 최적화하여 효율성을 높이고 초음파 이미지 품질을 유지하며, Stable Diffusion의 텍스트 임베딩을 활용하여 임상 용어를 정확히 반영합니다.
- ***Performance Highlights***: P2P는 3개의 유방 초음파 데이터 세트에서 최신 공격 기법보다 더 자연스럽고 효과적인 이미지를 생성하며, FID와 LPIPS 측정에서 우수한 결과를 기록했습니다. 이는 공격 예제가 더 자연스럽고 인식하기 어려우며, 성공률은 Diff-PGD와 유사하지만, 구조적 정합성과 시각적 충실도가 높음을 보여줍니다.

### [GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong Prompt Optimizers](https://arxiv.org/abs/2412.09722)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09722.png)

Vote: 1

Authors: Rui Zhang, Ryo Kamoi, Caiming Xiong, Sarkar Snigdha Sarathi Das, Bo Pang, Yusen Zhang

- ***What's New***: GREATER는 소규모의 오픈 소스 언어 모델들이 대형 독점 LLM(Large Language Models)에 의존하지 않고도 'gradient over reasoning' 방법을 통해 프롬프트를 최적화할 수 있도록 지원하는 새로운 기법입니다. 이를 통해 작은 모델들에서도 높은 성능의 프롬프트 최적화가 가능해졌습니다.
- ***Technical Details***: GREATER는 입력 샘플을 사용하여 위치에 대한 유망한 토큰 후보를 생성하고, LLM으로 해결책 추론을 유도한 후, 손실(loss)과 그라디언트를 계산하여 최적의 토큰을 선택합니다. 'gradient over reasoning'은 강력한 신호를 제공하여 프롬프트 최적화를 크게 향상시킵니다.
- ***Performance Highlights***: GREATER는 다양한 추론 업무에서 작고 경량인 언어 모델을 사용하여 기존 최첨단 프롬프트 최적화 방법보다 8.9%까지 도전적 업무 성능을 향상시켰습니다. 또한, GREATER 프롬프트는 대형 모델들에 의해 최적화된 프롬프트와 동등하거나 더 나은 성능을 자주 보여주고 있습니다.

### [GenEx: Generating an Explorable World](https://arxiv.org/abs/2412.09624)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09624.png)

Vote: 60

Authors: Alan Yuille, Cheng Peng, Jieneng Chen, Chen Wei, Daniel Khashabi, Luoxin Ye, Jiahao Wang, Junfei Xiao, Taiming Lu, Tianmin Shu, Rama Chellappa

- ***What's New***: GenEx는 단일 RGB 이미지에서 탐험할 수 있는 3D 일관된 상상 환경을 생성하고, 이 환경을 파노라마 비디오 스트림을 통해 생생하게 제작합니다. GPT 보조 에이전트는 목표 비중 탐색 및 목표 구동 네비게이션을 포함하여 복잡한 구현 작업을 수행할 수 있습니다.
- ***Technical Details***: GenEx 플랫폼은 단일 이미지 입력에서 시작해, 360도 파노라마 환경을 이미지-파노라마 모델과 비디오 생성 모델을 활용해 동적으로 생성합니다. Unreal Engine 등의 물리적 엔진에서 얻은 데이터를 통해 훈련되며 구형 일관적 학습을 사용하여 탐험 중 3D 일관성을 유지합니다.
- ***Performance Highlights***: GenEx는 다양한 FVD, SSIM, LPIPS 및 PSNR 지표에서 우수한 비디오 생성 품질을 보여주며, 장거리 탐험에서도 높은 상상력 탐험 루프 일관성(IELC)을 유지합니다. Multi-agent 상상 보강 정책을 통해 인간-기계 상호작용 시 정확도를 높이며, 상상 비디오를 활용한 경우 인간의 인지 능력을 향상시킵니다.

### [Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation](https://arxiv.org/abs/2412.09428)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09428.png)

Vote: 5

Authors: Si Liu, Chenxi Bao, Zhaokai Wang, Jizhong Han, Xuecheng Nie, Baisen Wang, Yue Liao, Le Zhuo, Wu Chengjing, Jiao Dai

- ***What's New***: 이 논문은 텍스트, 이미지, 비디오 등 다양한 입력 모달리티로부터 음악을 생성하는 멀티모달 음악 생성 방법론을 소개합니다. 특히, 텍스트와 음악을 명시적인 연결선(bridges)으로 사용하여 멀티모달 정렬이 강화되었으며, 데이터 부족과 크로스 모달 정렬의 문제를 해결하기 위해 Visuals Music Bridge (VMB)를 제안했습니다.
- ***Technical Details***: VMB는 Multimodal Music Description Model을 통해 시각적 입력을 상세한 텍스트 설명으로 전환하여 텍스트 연결선을 제공합니다. Dual-track Music Retrieval 모듈은 광범위 및 특정 검색 전략을 결합하여 음악 연결선을 제공하고 사용자 제어를 가능하게 합니다. 최종적으로 두 연결선을 활용한 Explicitly Conditioned Music Generation 프레임워크로 음악을 생성합니다.
- ***Performance Highlights***: VMB는 비디오-음악, 이미지-음악, 텍스트-음악 및 제어 가능 음악 생성 작업에서 기존 방법에 비해 음악 품질, 정렬 및 사용자 맞춤화를 상당히 개선하였습니다. 다양한 멀티미디어 분야에서 응용 가능성이 높으며, 현재 최첨단 멀티모달 음악 생성의 새로운 표준을 설정합니다.

### [FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion](https://arxiv.org/abs/2412.09626)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09626.png)

Vote: 12

Authors: Ziwei Liu, Ruihang Chu, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Yingya Zhang, Yujie Wei, Haonan Qiu

- ***What's New***: 이 논문에서는 FreeScale이라는 고해상도 생성 능력을 발휘할 수 있는 튜닝 프리 방식(tuning-free)의 추론 패러다임을 소개합니다. 기존의 디퓨전 모델(diffusion models)을 활용해 8k 해상도의 이미지 생성까지 가능하게 하며, 스케일 융합(scale fusion)을 통해 다양한 스케일에서 정보 처리 및 융합을 실행합니다.
- ***Technical Details***: FreeScale은 다양한 수용 스케일에서 정보를 처리한 후 원하는 주파수 성분을 추출하여 융합하는 방식을 사용합니다. 이는 기존의 셀프-어텐션 레이어(self-attention layers)에 매끄럽게 통합되며, 최소한의 시간 증가만 초래합니다. 이미지 및 비디오 모델 모두에 대해 실험을 통해 이 모델의 효과를 입증하였습니다.
- ***Performance Highlights***: FreeScale은 FID, KID 같은 주요 품질 관련 메트릭에서 가장 뛰어난 성능을 보여 주었으며, 상대적으로 짧은 추론 시간과 함께 탁월한 성능을 입증합니다. 특히, 이전의 최첨단 방법과 비교할 때, 8k 해상도 이미지 생성을 처음으로 가능하게 했으며, 시각적 품질 측면에서 더 우수한 결과를 나타내었습니다.

### [BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities](https://arxiv.org/abs/2412.07769)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07769.png)

Vote: 23

Authors: Shanavas Cholakkal, Salman Khan, Fahad Khan, Rao Anwer, Hisham Cholakkal, Khaled Aldahmani, Timothy Baldwin, Mohammed Irfan Kurpath, Sahal Shaji Mullappilly, Saeed Yahya Alseiari, Sara Pieri

- ***What's New***: BiMediX2는 바이오-의료 분야에서 영어와 아랍어로 된 다양한 의료 이미지를 처리할 수 있는 대형 멀티모달 모델(LMM)로, 최초로 GPT-4o 기반의 이중언어 의료 벤치마크인 BiMed-MBench를 제안했습니다. 의료 LLM 및 VLM 평가 벤치마크에서 여러 최신 모델을 능가했으며, 영어 평가에서 9% 이상, 아랍어 평가에서 20% 이상의 개선을 보여줍니다.
- ***Technical Details***: BiMediX2는 Llama3.1 아키텍처를 기반으로 하며, 160만 개의 이중언어 건강 데이터셋으로 훈련되었습니다. Vision Encoder, Projector, Tokenizer, 그리고 LoRA 어댑터로 구성되며, 의료 이미지 분석과 아랍어 및 영어 상호작용을 지원합니다. 이 모델은 BiMed-V라는 포괄적인 데이터 셋을 사용하여 강력한 바이오-의료 이미지-텍스트 정렬과 다중채널 이해를 제공합니다.
- ***Performance Highlights***: BiMediX2는 USMLE 벤치마크에서 GPT-4보다 9% 이상 향상된 성능을 보였으며, BiMed-MBench 평가에서 영어 및 아랍어 모두 62.2% 및 50.5%의 높은 점수를 기록했습니다. 또한, Rad-VQA, Slake-VQA, Path-VQA 같은 데이터셋에서 VQA(시각적 질문 응답) 벤치마크에서 최고 평균 점수인 0.611을 기록했습니다.

### [SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs](https://arxiv.org/abs/2412.08347)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08347.png)

Vote: 2

Authors: Sultan Alrashed

- ***What's New***: SmolTulu는 작은 언어 모델(small language models; SLMs)을 위해 AllenAI의 Tulu 3 후처리(post-training) 파이프라인을 Huggingface의 SmolLM2-1.7B 모델에 적용하여 최적화한 모델입니다. SmolTulu는 학습률(learning rate)과 배치 크기(batch size) 비율이 과제별로 모델 성능에 미치는 영향에 대한 연구를 기반으로 개발되어, 새로운 최적화 전략을 제안합니다.
- ***Technical Details***: SmolTulu의 주요 기여는 학습률과 배치 크기 비율이 작은 언어 모델에서 복잡한 추론 과제와 패턴 인식 과제에 서로 다르게 영향을 미친다는 실증적 증거를 제시하는 것입니다. 이를 뒷받침하기 위해, SmolLM2-135M 모델을 사용하여 광범위한 실험을 수행하였으며, 높은 비율은 ARC 및 GSM8K와 같은 복잡한 추론 작업에 유리한 반면, HellaSwag 및 IFEval과 같은 패턴 인식 작업에는 낮은 비율이 최적의 성능을 보임을 발견했습니다.
- ***Performance Highlights***: SmolTulu는 여러 지표에서 두각을 나타냈으며, 2B 파라미터 미만 모델 중 최고의 성능을 달성했습니다. SmolTulu의 DPO-1130 변형은 IFEval에서 67.7%, GSM8K에서 51.6%의 점수를 기록하였으며, 이는 다른 대형 모델에 비해 훌륭한 성과로 해석될 수 있습니다. 반대로, SmolTulu DPO-1207 변형은 더 낮은 비율 설정으로 ARC와 PIQA 패턴 인식 작업에서 더 강한 성능을 보였습니다.

### [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](https://arxiv.org/abs/2412.10319)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.10319.png)

Vote: 4

Authors: Jianfeng Gao, Chengruidong Zhang, Yuqing Yang, Lili Qiu, Huiqiang Jiang, Yucheng Li, Dongsheng Li, Qianhui Wu, Xufang Luo, Surin Ahn, Amir H. Abdi

- ***What's New***: SCBench는 KV 캐시(KV Cache)의 전 과정을 포괄적으로 평가하는 새로운 벤치마크입니다. 이 벤치마크는 긴 컨텍스트 방법론을 KV 캐시 생성, 압축, 검색, 로딩 네 가지 측면에서 분석합니다. 이는 기존 벤치마크에서 제공하지 않았던 다중 요청 및 다중 라운드 상호작용 시나리오에 대한 테스트를 포함하여 보다 현실적인 평가를 제공합니다.
- ***Technical Details***: SCBench는 12개의 테스트 작업과 두 가지 공유 컨텍스트 모드를 포함하며, 문자열 검색, 의미 검색, 글로벌 정보 처리, 멀티태스킹의 네 가지 긴 컨텍스트 능력을 평가합니다. 또한, MInference와 같은 동적인 희소 주의 방법, A-Shape와 같은 희소 주의 방법을 포함하여 다양한 긴 컨텍스트 솔루션을 테스트합니다. 이 벤치마크는 Qwen2.5, Llama-3 등 여러 최신 오픈소스 모델을 사용하여 테스트됩니다.
- ***Performance Highlights***: 실험 결과, 다이내믹한 희소 주의 방법이 여러 요청에서 전체 주의력 정확성을 근접하게 달성하며 다른 방법보다 우수한 성능을 냈음을 보였습니다. 또한, 오직 O(n) 메모리를 사용하는 방법들이 다중 요청 시나리오에서 뛰어난 성능을 발휘했으며, 이는 긴 컨텍스트 방법의 발전에 있어 중요한 인사이트로 작용합니다.

### [FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers](https://arxiv.org/abs/2412.09611)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09611.png)

Vote: 6

Authors: Kavana Venkatesh, Yusuf Dalva, Pinar Yanardag

- ***What's New***: FluxSpace는 교정된 흐름 변환기(Rectified Flow Transformers)에서 텍스트로 안내된 이미지 편집 방법을 제안합니다. 이는 인간, 동물, 자동차 등의 다양한 도메인에서 의미론적 편집이 가능한 방법이며, 기존 이미지의 특정 부분을 타겟으로 하지 않아도 됩니다. 또한, 학습이 필요하지 않고 추론 시간에 원하는 편집을 적용할 수 있습니다.
- ***Technical Details***: 이 논문에서 제안하는 FluxSpace는 흐름 매칭 변환기(Flow-Matching Transformers)의 주의 레이어 출력(Attention Layer Outputs)을 사용하여 고도로 독립적인 의미 정보를 디코딩할 수 있는 새로운 이미징 편집 프레임워크입니다. 사용자는 미소 추가와 같은 세밀한 편집과 변형이나 스타일화와 같은 대규모 수정 모두를 제어할 수 있습니다. 특히 변환기의 주목 레이어에서 선형 편집 방식을 사용하여 이미지 내용과 편집 조건 간의 상호작용을 더욱 강화합니다.
- ***Performance Highlights***: FluxSpace는 고유의 분리된 표현을 사용하여 원본 이미지의 속성을 유지하면서 다양한 속성을 정확히 편집할 수 있습니다. 경쟁 방법들과 비교했을 때, FluxSpace는 편집을 위한 현대적인 접근 방식들인 LEDITS++, TurboEdit, Sliders-FLUX 및 RF-Inversion을 성능 면에서 능가하며, 실습적 실험과 사용자 연구에서도 우수한 결과를 보여주었습니다. 예를 들어 ‘스마일 추가’ 또는 ’선글라스 추가’ 같은 미세 조정 작업에서 더 나은 결과를 보였으며, 원본 이미지의 아이덴티티를 유지하는 데 성공적이었습니다.

### [LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity](https://arxiv.org/abs/2412.09856)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09856.png)

Vote: 4

Authors: Tingbo Hou, Felix Juefei-Xu, Jialiang Wang, Yaqiao Luo, Niraj K. Jha, Chih-Yao Ma, Hongjie Wang, Yen-Cheng Liu, Ji Hou, Peter Vajda, Tao Xu, Peizhao Zhang, Xiaoliang Dai

- ***What's New***: LinGen은 최초로 고해상도의 분 단위 길이(text-to-video generation)를 단일 GPU에서 선형 계산 복잡도로 생성할 수 있는 프레임워크를 제안합니다. 이는 기존의 Diffusion Transformers(DiTs)에 비해 계산 비용이 많지 않으며, Mamba의 문제점을 해결하여 인접성을 보존하면서도 고품질 영상을 생성할 수 있습니다.
- ***Technical Details***: LinGen은 자신만의 MATE 블록을 사용해 DiTs의 자가-어텐션(self-attention) 레이어를 대체합니다. MA-브랜치는 Mamba2 블록과 우리의 Rotary-Major Scan(RMS), 리뷰 토큰으로 구성되며, TE-브랜치는 TEmporal Swin Attention 블록으로 구성됩니다. 이는 짧은, 중간, 긴 범위의 상호작용을 포괄적으로 다룹니다. 추가로, 우리는 짧은 범위의 공간적 인접성과 중간 범위의 시간적 인접성을 중심으로 하는 TESA 모듈을 개발하여 비디오 일관성을 크게 높입니다.
- ***Performance Highlights***: LinGen은 비디오 품질에서 DiT 대비 75.6%의 우수성을 보여주며, 최대 15배의 FLOPs(11.5배의 지연) 감소를 달성합니다. 자동 평가 및 인간 평가 모두 LinGen-4B가 최신 모델들과 비슷한 품질의 비디오를 생성함을 나타냈으며, 이는 미래의 실시간 비디오 생성과 장시간 영화 생성의 길을 열어줍니다.

### [ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation](https://arxiv.org/abs/2412.08645)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08645.png)

Vote: 9

Authors: Yedid Hoshen, Alex Rav-Acha, Yael Pritch, Dana Berman, Asaf Shul, Matan Cohen, Daniel Winter

- ***What's New***: 이 논문은 객체 삽입(Object Insertion)과 주제 기반 생성(Subject-Driven Generation)을 위한 새로운 메소드인 ObjectMate를 소개합니다. 이 방법은 대규모 인터넷 이미지 데이터셋에서 반복적으로 등장하는 객체들을 활용하여 대규모 감독 학습 데이터 셋을 생성하여, 객체와 장면 설명을 합성 이미지로 매핑하는 텍스트-이미지 확산 아키텍처를 훈련시킵니다. ObjectMate는 테스트 타임 튜닝이 필요하지 않으며, 객체의 정체성을 잘 유지하면서도 포토리얼리틱한 합성을 달성합니다.
- ***Technical Details***: ObjectMate는 대규모 이미지 데이터셋에서 객체를 탐지하고 각 객체의 심층 정체성 특징을 추출하여, 유사한 특징을 가진 다른 객체들을 검색하고, 각 객체에 대한 다양한 뷰 셋을 생성합니다. 이를 통해 생성된 데이터셋을 기반으로 단순한 텍스트-이미지 확산 모델을 훈련시킵니다. 이 방법은 Counterfactual Object Removal 모델을 사용하여 장면에서 객체 제거 후 배경 이미지를 확보함으로써 기존의 그림자 및 반사 문제를 해결합니다.
- ***Performance Highlights***: 객체 삽입과 주제 기반 생성 모두에서 ObjectMate는 현재의 최상위 방법들보다 우수한 결과를 보였습니다. 특히, 객체의 정체성을 더 잘 보존하며 포토리얼리틱한 합성을 이룰 수 있는 것으로 나타났습니다. 새로운 평가 데이터셋과 사용자 연구를 통해 제안된 메트릭이 더 나은 평가를 제공함을 검증했습니다.

### [Large Action Models: From Inception to Implementation](https://arxiv.org/abs/2412.10047)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.10047.png)

Vote: 20

Authors: Lu Wang, Pu Zhao, Chaoyun Zhang, Junting Lu, Jian-Guang Lou, Shilin He, Jiaxu Qian, Qingwei Lin, Ray Huang, Si Qin, Qisheng Su, Jiayi Ye, Saravan Rajmohan, Dongmei Zhang, Qi Zhang, Yudi Zhang, Fangkai Yang, Bo Qiao

- ***What's New***: 이 논문은 전통적인 대형 언어 모델(LLM)에서 벗어나, 실제 세계에서 행동을 생성하고 실행할 수 있는 대형 행동 모델(Large Action Models; LAMs)의 개발과 구현을 제안합니다. LAMs는 언어 이해에서 행동 완료로 AI를 전환시켜 인공지능의 일반적 지능(AGI)으로의 진전을 나타냅니다.
- ***Technical Details***: Windows 운영 체제 기반의 에이전트를 사례 연구로 사용하여 LAM 개발의 주요 단계에 대한 자세한 설명을 제공합니다. 데이터 수집, 모델 훈련, 환경 통합, 평가를 포함한 이 일반적인 워크플로우는 다양한 응용 도메인에서 기능적 LAMs를 구축하기 위한 청사진으로 사용할 수 있습니다.
- ***Performance Highlights***: LAMs는 기존의 LLMs에 비해 특정 작업 또는 환경에서 효율성과 효과성을 우선시하며, 작거나 리소스가 제한된 환경에서도 뛰어난 성능을 발휘할 수 있습니다. 실험 결과, LAM의 task success rate는 81.2%로 GPT-4o의 67.2%를 능가했습니다. 이러한 성능 향상은 단계별 훈련 전략을 통해 데이터 효율을 높이고 복합적인 문제 해결 능력을 강화했습니다.

### [TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies](https://arxiv.org/abs/2412.10345)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.10345.png)

Vote: 2

Authors: Jianfeng Gao, Ruijie Zheng, Hal Daumé III, Shuaiyi Huang, Andrey Kolobov, Jianwei Yang, Yongyuan Liang, Furong Huang

- ***What's New***: TraceVLA는 VLA(Vision-Language-Action) 모델의 시간적-공간적 인식을 개선하기 위해 새로운 비주얼 트레이스 프롬프팅(Visual Trace Prompting) 기법을 도입했습니다. 이를 통해 모델의 동작 예측 성능을 향상시키고 다양한 환경과 상황에서 강력한 일반화 능력을 발휘합니다.
- ***Technical Details***: TraceVLA 모델은 오픈VLA(OpenVLA) 를 기반으로 하여 7B 파라미터와 새로운 비주얼 트레이스 프롬프팅 데이터셋을 사용하여 파인튜닝되었습니다. 이 모델은 150K의 로봇 조작 궤적을 포함한 데이터셋으로 학습되었으며, 모델 입력으로 시각적 트레이스를 오버레이한 이미지를 제공합니다. CoTracker를 통해 시계열적 점 궤적을 생성하고, 이를 다양한 조작 작업 시나리오에 적용하여 모델의 공간적 시간적 인식을 강화합니다.
- ***Performance Highlights***: TraceVLA는 SimplerEnv 및 실제 로봇(WidowX) 실험 전반에서 기존의 OpenVLA 모델을 능가하며, SimplerEnv에서는 최대 12.7%의 성능 향상을 기록하였습니다. 이 모델은 다양한 환경 변화에 대한 우수한 처리 능력을 보여주며, 물체 조작 시나리오에서 더욱 효과적인 예측을 수행합니다. 또한, 테스트 중 시계열적 시각 트레이스를 만들어 사용함으로써, 다중 환경에서의 강력한 일반화 능력을 보입니다.

### [InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption](https://arxiv.org/abs/2412.09283)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09283.png)

Vote: 16

Authors: Tiehan Fan, Chaoyou Fu, Rui Xie, Zhenheng Yang, Xiang Li, Jian Yang, Ying Tai, Kepan Nan, Penghao Zhou

- ***What's New***: InstanceCap은 인스턴스 인지 구조 캡션(Instance-aware Structured Caption) 방식을 통해 텍스트-비디오 생성(Text-to-Video Generation)의 성능을 향상시키는 새로운 프레임워크입니다. 이 방법은 비디오를 인스턴스 별로 분할하여 세부적인 캡션을 제공함으로써 비디오와 캡션 간의 높은 충실도를 확보하고 환각을 감소시킵니다.
- ***Technical Details***: InstanceCap은 AMC(Auxiliary Models Cluster)를 사용하여 글로벌 비디오를 개별 인스턴스로 변환하여 인스턴스의 충실도를 향상시킵니다. 인스턴스에 대한 정확한 설명을 위해 MLLMs(Multimodal Large Language Models)의 개선된 Chain-of-Thought(CoT) 프로세스를 도입하여 복잡한 프롬프트를 구조적 문구로 변환합니다. 이를 위해 22K의 InstanceVid 데이터셋을 구축하여 실험적으로 지원합니다.
- ***Performance Highlights***: InstanceCap은 기존의 모델에 비해 높은 캡션과 비디오의 충실도를 보이며, 환각을 줄이는 데 크게 기여합니다. CogVideoX와 같은 고급 텍스트-비디오 모델과의 비교 실험에서, InstanceCap을 통합한 모델이 비디오 재구성 정확도 및 인스턴스 세부사항 묘사에서 우수한 성능을 보였습니다. Open-Sora를 통해 InstanceVid로 미세 조정된 모델은 액션, 색상, 형태 및 질감과 같은 여러 주요 차원에서 향상된 성능을 나타냅니다.

### [SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding](https://arxiv.org/abs/2412.09604)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09604.png)

Vote: 26

Authors: Hao Li, Lewei Lu, Jifeng Dai, Jinguo Zhu, Zhaokai Wang, Jie Shao, Xiaogang Wang, Hongsheng Li, Wenhan Dou, Xizhou Zhu, Changyao Tian

- ***What's New***: SynerGen-VL은 이미지 이해와 생성 모두를 수행할 수 있는 간단하면서도 강력한 인코더-프리(encoder-free) 멀티모달 대형 언어 모델(MLLM)로 제안되었습니다. 기존의 복잡한 모델 구조나 학습 과정을 단순화하면서 높은 해상도의 이미지 이해를 지원하고 학습 복잡성을 줄이는 토큰 폴딩(token folding) 메커니즘과 비전 전문가 주도 시퀀스 정렬 사전 학습 전략을 도입했습니다.
- ***Technical Details***: SynerGen-VL은 이미지 이해와 생성을 한 번에 해결하기 위해 비-시맨틱(discrete) 이미지 토큰을 사용합니다. 이미지의 고해상도 이해를 효과적으로 지원하기 위해 토큰 폴딩 매커니즘을 이용하여 입력 이미지 토큰 시퀀스를 압축하고, 이미지 생성을 위해 생성된 시퀀스를 풀기 위해 얕은 오토 레그레시브 트랜스포머(autoregressive Transformer) 헤드를 사용합니다. 또한, 비전 전문가(vision experts)를 이용한 비언어적인 피드-포워드 네트워크(FFN)를 통해 LLM의 사전 학습된 지식을 보존하면서 이미지를 효과적으로 표현합니다.
- ***Performance Highlights***: SynerGen-VL은 다양한 이미지 이해 및 생성 벤치마크에서 기존의 인코더-프리 통합 MLLM들과 비교가능한 성능을 내며, 더 큰 규모의 모달 특화 최신 모델들과의 격차를 줄였습니다. 특히, SynerGen-VL은 2.4B 활성화된 파라미터만으로 8B 파라미터를 갖춘 Emu3과 비슷한 이미지 이해 및 생성 성능을 달성하여 차세대 통합 MLLM으로서의 강력한 잠재력을 강조합니다.

### [Apollo: An Exploration of Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2412.10360)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.10360.png)

Vote: 84

Authors: Felix Juefei-Xu, Nikhil Mehta, Xide Xia, Orr Zohar, Philippe Hansen-Estruch, Ning Zhang, Licheng Yu, Tong Xiao, Yann Dubois, Xiaohan Wang, Xiaofang Wang, Serena Yeung-Levy

- ***What's New***: 본 연구는 Apollo라는 새로운 대형 다중모달 모델(APollo; Large Multimodal Models; LMMs)을 소개하여 동영상 이해를 위한 혁신적인 설계와 학습 방법을 제안합니다. 이러한 모델은 다양한 크기에서도 기존 모델보다 우수한 성능을 발휘하며, 특히 LongVideoBench와 MLVU 등의 벤치마크에서 최첨단 성과를 보여줍니다. Apollo 모델은 작은 파라미터 크기로도 큰 모델의 성능을 능가할 수 있음을 입증하며 연구 분야에 빠른 프로토타입 개발을 촉진할 것입니다.
- ***Technical Details***: Apollo는 SigLIP-SO400M 및 InternVideo2 비디오 인코더의 조합을 사용하여 모든 비디오 프레임을 임베딩합니다. 이러한 입력은 Perciever Resampler를 사용하여 LLM의 숨은 차원에 맞게 적절히 조정됩니다. 연구에서는 프레임 당 초당 프레임수(fps) 및 토큰수(tps)를 조정하여 모델의 성능을 극대화했습니다. 또한, 비디오 샘플링에서 정규 샘플링보다 fps 샘플링이 우수함을 입증했습니다.
- ***Performance Highlights***: 실험 결과, Apollo-3B는 대부분의 기존 7B 모델을 능가하며 Video-MME에서는 58.4, MLVU에서는 68.7을 기록했습니다. Apollo-7B는 MLVU에서 70.9의 점수를 기록하며, 30B 이상의 파라미터를 가진 모델을 포함하여 7B 모델 중 최고의 성과를 보여주었습니다. 이러한 성과는 신중한 설계와 학습 전략이 모델의 규모를 늘리지 않고도 성능을 크게 개선할 수 있음을 강조합니다.

### [FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing](https://arxiv.org/abs/2412.07517)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07517.png)

Vote: 7

Authors: Fan Tang, Xiangyu He, Peisong Wang, Changwang Mei, Yingying Deng

- ***What's New***: FireFlow는 ReFlow 기반 모델의 놀라운 생성 능력을 유지하면서 정확한 반전(Inversion)과 편집(Editing)을 가능하게 하는 최초의 zero-shot 접근법입니다. 새로운 수치적 솔버(Solver)를 설계하여 ReFlow의 역변환에 중요한 정확성을 제공하며, 1차 Euler 방법의 실제 효율성을 유지하면서 2차 솔버의 정밀도를 달성했습니다.
- ***Technical Details***: FireFlow는 ReFlow 모델의 ODE를 다루기 위한 새로운 수치적 솔버를 도입하여 반전 및 편집의 어려움을 해결했습니다. 이 방법은 2차 정확도를 달성하면서 1차 솔버와 동일한 계산 비용을 유지하며, 중간 속도 근사치를 활용하여 중복 계산을 줄이고 반전 과정을 안정화합니다. 이렇게 하여 ReFlow 모델의 일정한 속도 특성을 최대한 활용할 수 있습니다.
- ***Performance Highlights***: 제안된 솔버는 최첨단 기술보다 3배 빠른 런타임 속도를 달성했으며, 훈련이 필요 없는 상태에서 뛰어난 재구성 오차 감소와 편집 결과를 제공합니다. 또한 Text-to-Image 생성 테스트에서 기존 방법들보다 향상된 FID와 CLIP 점수를 기록하였습니다.

