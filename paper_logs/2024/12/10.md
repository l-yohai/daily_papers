## Daily Papers (2024-12-10)

### [Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06769.png)

Vote: 23

Authors: Shibo Hao, Jason Weston, Zhiting Hu, Xian Li, DiJia Su, Sainbayar Sukhbaatar, Yuandong Tian

- ***What's New***: 새로운 패러다임인 Coconut(Chain of Continuous Thought)은 대형 언어 모델(LLM)의 연속 잠재 공간(Continuous Latent Space) 내 추론 패러다임을 소개합니다. 이는 CoT(Chain-of-Thought) 방식을 개선하여 언어 공간의 제약 없이 모델이 여러 대안적인 추론 단계를 동시에 인코딩하여 문제 해결을 돕습니다.
- ***Technical Details***: Coconut은 LLM의 마지막 숨겨진 상태를 '연속적 사고(Continuous Thought)'로 활용하며, 이를 다음 입력 임베딩으로 직접 활용합니다. 이 접근방식은 언어 공간 내의 추론 제약에서 벗어나, 완전하게 미분 가능한 연속적 사고를 최적화합니다. 다단계 학습 전략을 통해 언어 기반 추론 사슬을 활용하여 모델을 촉진합니다.
- ***Performance Highlights***: Coconut은 수학 추론(GSM8k) 및 논리 추론(ProntoQA, ProsQA) 과제에서 CoT를 초과하는 성능을 발휘하였습니다. 특히, ProsQA에서 더 적은 추론 토큰을 사용하면서도 뛰어난 계획 능력을 보여주었습니다. 실험 결과는 잠재적 추론 방식의 가능성을 강조하며, 향후 연구 방향에 대한 귀중한 통찰을 제공합니다.

### [MotionShop: Zero-Shot Motion Transfer in Video Diffusion Models with Mixture of Score Guidance](https://arxiv.org/abs/2412.05355)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05355.png)

Vote: 0

Authors: Pinar Yanardag, Connor Dunlop, Hidir Yesiltepe, Tuna Han Salih Meral

- **What's New**: MotionShop은 새로운 접근 방식인 Mixture of Score Guidance(MSG)를 통해 비디오 확산 모델에서 제로샷 모션 전송을 가능케 합니다. 이는 모션 점수와 콘텐츠 점수를 분해하여 높은 정밀도의 모션 전송을 구현하며, 다양한 스토리텔링과 창의적인 장면 변형을 유지합니다. 추가적인 학습 없이 사전 훈련된 모델에서 직접 작동합니다.
- **Technical Details**: MSG는 확산 모델의 점수 공간에서 잠재 에너지를 혼합하여 모션 전송을 공식화합니다. 모션 전송을 통계역학의 잠재 에너지 혼합으로 다루어 보존되는 모션 패턴의 무결성을 통해 창의적인 장면 변형을 지원합니다. 또한, MotionBench라는 포괄적인 모션 전송 벤치마크도 도입되어 200개의 소스 비디오와 1000개의 전송된 모션이 포함되어 다양한 시나리오에서의 체계적인 평가를 가능하게 합니다.
- **Performance Highlights**: MotionShop은 Motion Fidelity(0.913)와 Temporal Consistency(0.928)에서 첨단 성능을 달성하며, 경쟁 방법들과 비교하여 우수한 모션 및 텍스트 정렬을 유지합니다. 특히, 사용자 연구에서 모션 보존, 시간 일관성 및 텍스트 기반 수정에서 타 방법을 능가하는 결과를 보여주었습니다.

### [Global and Dense Embeddings of Earth: Major TOM Floating in the Latent Space](https://arxiv.org/abs/2412.05600)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05600.png)

Vote: 4

Authors: Jędrzej S. Bojanowski, Marcin Kluczek, Mikolaj Czerkawski

- ***What's New***: 이번 연구에서는 'Major TOM'이라는 커뮤니티 프로젝트의 확장을 통해 지구 관측 데이터를 위한 글로벌 및 밀도 높은 임베딩 데이터셋을 제공하고 표준화하는 방안을 제안하였습니다. 이는 세계적인 스케일로 지리 공간적 이미지 임베딩 데이터를 최초로 무료로 공개하는 중요한 결과입니다.
- ***Technical Details***: 이 연구에서는 4개의 다른 사전 훈련된 모델을 통해 얻은 글로벌 임베딩을 분석하였습니다. 데이터셋은 'Major TOM' 핵심 데이터셋을 기반으로 구축되었으며, 그림자 및 반사율 보정을 포함한 이미지 전처리 과정을 거쳐 임베딩 모델에 입력된 뒤 geoparquet 포맷으로 아카이빙됩니다. 중앙 단위의 프레이그먼트 함수가 적용되어 표준화된 Major TOM 그리드 셀을 일관되게 조각내는 과정도 설명되었습니다.
- ***Performance Highlights***: 첫 번째 'Major TOM' 임베딩 릴리스에서는 169억 개 이상의 임베딩이 62TB 이상의 원시 데이터를 처리하여 생성되었으며, 이는 3.5백만 개의 독특한 이미지에서 9.368조 픽셀을 추출한 것입니다. 각 모델에 대해 주성분 분석을 통해 초기 시각화를 수행하여, 모델 간 지역적 및 전역적 특성이 어떻게 차이를 보이는지를 입증하였습니다.

### [If You Can't Use Them, Recycle Them: Optimizing Merging at Scale Mitigates Performance Tradeoffs](https://arxiv.org/abs/2412.04144)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04144.png)

Vote: 1

Authors: Honglak Lee, Tom Hosking, Arash Ahmadian, Tom Sherborne, Matthias Gallé, Ahmet Üstün, Lu Wang, Muhammad Khalifa, Yi-Chern Tan

- ***What's New***: 이 논문은 기존에 버려졌던 '일반적(generalist)' 체크포인트(checkpoint)를 병합하여 성능 최적화를 시도하는 새로운 접근법을 소개합니다. 대규모 모델(예: 100B 이상의 파라미터)을 대상으로 하며, 다양한 작업 간의 성능 저하를 줄이는 방향으로 병합을 최적화합니다.
- ***Technical Details***: 이 연구에서는 진화적 최적화(evolutionary optimization)를 사용하여 여러 체크포인트를 병합함으로써 작업 간 트레이드오프(tradeoff)를 최소화합니다. 16개의 서로 다른 training runs에서 얻어진 체크포인트를 병합 대상으로 하며, 각 체크포인트는 코드 생성, 수학적 추론, 지침 따르기(instruction following)와 같은 서로 다른 모델 능력에서의 트레이드오프를 가지고 있습니다. 특히, 학습 없는 모델 최적화를 통해 최소 트레이드오프를 달성하는 최적 병합을 찾는 데 집중합니다.
- ***Performance Highlights***: 최적화된 병합 모델들은 가장 높은 평균 성능을 가진 단일 모델보다 각 과제에서 더 나은 성능을 보였습니다. 특히, 모델 병합이 두 개 또는 세 개의 과제 세트에 대해 성능 트레이드오프를 줄이는 것을 확인했습니다. 또한, 병합을 통해 성능이 좋지 않은 체크포인트가 전체 성능 향상에 기여할 수 있음을 발견했습니다.

### [RL Zero: Zero-Shot Language to Behaviors without any Supervision](https://arxiv.org/abs/2412.05718)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05718.png)

Vote: 1

Authors: Peter Stone, Amy Zhang, Pranaya Jajoo, Scott Niekum, Caleb Chuck, Samyak Parajuli, Max Rudolph, Harshit Sikchi, Siddhant Agarwal

- ***What's New***: RL Zero는 지도 없이 언어를 행동으로 변환하는 새로운 방법을 제안합니다. 이 방법은 사전교육된 무감독 강화학습(RL) 에이전트를 통해 상상한 관찰을 실제 행동 정책으로 연결하는 것으로, 어떠한 추가적인 데이터 라벨링이나 학습 없이 다양한 작업을 수행할 수 있습니다.
- ***Technical Details***: RL Zero는 'Imagine', 'Project', 'Imitate'라는 세 가지 단계로 구성됩니다. 에이전트는 언어 지시에 기반한 작업을 상상하고, 상상한 시퀀스를 목표 도메인에 투영하여 정책을 학습합니다. 이를 통해 무감독 강화학습 에이전트가 관찰 데이터만을 이용하여 상상한 행동을 흉내 낼 수 있도록 합니다. 함수 기반 모델을 사용하여 상상한 시퀀스와 에이전트 관찰 간의 유사성을 평가하여 실질적인 행동 시퀀스를 생성합니다.
- ***Performance Highlights***: RL Zero는 시뮬레이션된 다양한 도메인에서 최초로 감독 없이 제로샷 언어-행동 변환을 달성했습니다. 특히, 유튜브에서 수집한 영상과 같은 교차 구현 비디오에서도 제로샷으로 정책을 생성할 수 있음을 보여줍니다. 실험 결과, RL Zero는 주어진 언어 지시에 대해 기대한 행동을 생성할 수 있는 가능성을 보여줍니다.

### [MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views](https://arxiv.org/abs/2412.06767)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06767.png)

Vote: 3

Authors: Tomoki Ichikawa, Antoine Guédon, Ko Nishino, Kohei Yamashita

- ***What's New***: 이 논문은 MAtCha Gaussians라는 새로운 표현 방식을 소개합니다. 이는 드물게 제공되는 이미지들만으로도 고품질 3D 형상 메쉬와 사실적인 새로운 보기 렌더링을 동시에 실현할 수 있는 모델입니다. MAtCha Gaussians는 2D 이미지 평면에서 가우시안 서페(lsurfels)로 차트를 아틀라스화하여 장면 형상을 모형화합니다.
- ***Technical Details***: MAtCha는 단일 뎁스 추정 모델(monocular depth estimator)로 차트를 초기화한 후, 가우시안 렌더링으로 차트를 정제합니다. 이 차트들은 가벼운 신경망 차트 변형 모델을 사용하여 2D에서 정제되어, 3D보다 보다 효율적으로 장면의 최적화를 가능하게 합니다. MAtCha는 소수를 차지하는 RGB 이미지에서 스파스(Sparse)-뷰 SfM 모델과 결합되어, 앞경물체와 배경의 선명하고 정확한 표면 메쉬를 회수할 수 있게 합니다.
- ***Performance Highlights***: MAtCha는 적은 수의 이미지로도 정확한 장면 형상을 분 단위 내에 재구성할 수 있으며, 실험 결과 가장 적은 입력 뷰와 계산 시간으로 최첨단 표면 재구성과 사실주의를 달성했습니다. 이는 추후 비전, 그래픽스, 로보틱스 등 다양한 시각 애플리케이션에서 형식적 기하학과 사실주의를 요구하는 경우에도 기초 도구로 활용될 수 있습니다.

### [Gated Delta Networks: Improving Mamba2 with Delta Rule](https://arxiv.org/abs/2412.06464)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06464.png)

Vote: 2

Authors: Songlin Yang, Ali Hatamizadeh, Jan Kautz

- ***What's New***: Gated DeltaNet은 최신 하드웨어에 최적화된 병렬 학습 알고리즘을 개발하여 Adaptive Memory Control을 위한 게이팅(gating)과 정밀한 메모리 수정이 가능한 델타 업데이트 규칙(delta update rule)을 결합한 새로운 딥러닝 아키텍처입니다. 게이티드 델타 규칙(gated delta rule)을 활용하여 메모리 제어의 유연성을 제공하고 다양한 벤치마크에서 기존 Mamba2 및 DeltaNet 모델보다 높은 성능을 보여줍니다.
- ***Technical Details***: Gated DeltaNet 아키텍처는 현대 하드웨어에 맞춰 최적화된 병렬 학습 알고리즘을 사용하여 폭넓은 벤치마크에서 메모리 관리의 유연성을 강조하며, Chunkwise Parallel Form을 통해 효율적인 학습을 사용합니다. 이 아키텍처는 게이티드 갱신 법칙(gated update rule)과 델타 규칙(delta rule)의 장점을 결합하여 보다 효율적인 키-값 연관 학습을 지원합니다.
- ***Performance Highlights***: Gated DeltaNet은 언어 모델링, 상식적 추론, 문맥 내 검색 등 다양한 태스크에서 Mamba2와 DeltaNet을 능가합니다. 또한 혼합 아키텍처를 통해 학습 효율성을 높이고 태스크 수행 능력을 향상시켜, 특히 긴 문맥 이해(long-context understanding)와 같은 복잡한 작업에서도 뛰어난 결과를 보여줍니다.

### [CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction](https://arxiv.org/abs/2412.06782)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06782.png)

Vote: 3

Authors: Shangke Lyu, Zhaoxin Fan, Siteng Huang, Donglin Wang, Zhefei Gong, Wei Zhao, Pengxiang Ding, Mingyang Sun

- ***What's New***: CARP은 로봇 비주모터 정책 학습을 위한 새로운 패러다임으로, Coarse-to-Fine AutoRegressive Prediction을 통해 기존의 자가 회귀 모델(Autoregressive Model; AM)과 확산 모델(Diffusion Model; DM)의 한계를 극복하고자 합니다. CARP는 행동 생성 과정을 다단계로 나누어 글로벌 구조와 시간적 지역성을 포착하며, 의도에서 세부 실행까지의 예측을 정제합니다.
- ***Technical Details***: CARP는 두 가지 주요 부분으로 구성됩니다. 첫째로, 다중 스케일 동작 토큰화를 통해 행동 시퀀스의 글로벌 구조와 시간적 지역성을 포착합니다. 둘째로, Coarse-to-Fine 자가 회귀 예측을 통해 동작 시퀀스를 점진적으로 정제합니다. 이러한 방법은 이산 토큰 맵을 사용하여 전통적인 자가 회귀 정책(Cross-Entropy 손실함수 활용)에서 발생하는 장기 의존성 문제를 완화시킵니다.
- ***Performance Highlights***: 시뮬레이션과 실제 세계 실험에서 CARP는 최고의 확산 모델과 동등하거나 더 나은 성능을 보였으며, 추론 속도는 약 10배 더 빠르고, 필요한 매개변수는 1-5%에 불과합니다. 특히 다중 작업 환경에서도 정제된 예측을 통해 효율적으로 높은 성과를 유지하였습니다.

### [ProcessBench: Identifying Process Errors in Mathematical Reasoning](https://arxiv.org/abs/2412.06559)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06559.png)

Vote: 36

Authors: Zhenru Zhang, Bowen Yu, Keming Lu, Dayiheng Liu, Runji Lin, Chujie Zheng, Jingren Zhou, Junyang Lin, Beichen Zhang

- ***What's New***: PROCESSBENCH는 수학적 추론에서 오류를 식별하는 능력을 측정하기 위해 설계된 새로운 벤치마크입니다. 이 벤치마크는 주로 대회 및 올림피아드 수준의 수학 문제를 다루며, 3,400개의 테스트 사례를 포함합니다. 각 사례는 전문가에 의해 오류 위치가 주석 처리된 단계별 솔루션으로 구성되어 있습니다. 모델은 오류가 발생한 최초의 단계를 식별하거나 모든 단계가 올바른지 판단해야 합니다.
- ***Technical Details***: PROCESSBENCH는 오류 식별 능력을 평가하기 위해 프로세스 보상 모델(Process Reward Models; PRMs)과 비평 모델(Critic Models)을 포함한 다양한 모델 유형을 광범위하게 평가합니다. PRMs는 PRM800K 데이터셋에 대해 간단하게 미세 조정된 모델과 비교되며, 비평 모델은 단계별로 솔루션을 비평하도록 일반 언어 모델을 프롬프트하여 평가됩니다. 데이터셋은 수학 문제와 단계별 솔루션으로 구성되며, 각 솔루션 단계에서 오류를 식별하는 것이 과제입니다.
- ***Performance Highlights***: 개방형 소스 모델 가운데 QwQ-32B-Preview는 평가에서 민감한 오류 식별 능력을 나타내며, 비공식 모델인 GPT-4o와 경쟁하며, 추론 전문화 모델인 o1-mini에 비해 뒤쳐집니다. PRMs는 특히 더 어려운 수학 문제에서 일반 언어 모델 드리븐 비평 모델보다 성능이 낮게 나타났습니다. 이를 통해 현재 PRMs의 일반화 능력과 데이터 합성 방법론의 한계를 강조합니다.

### [You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale](https://arxiv.org/abs/2412.06699)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06699.png)

Vote: 7

Authors: Zhengxiong Luo, Tiejun Huang, Xinlong Wang, Haoge Deng, Lulu Tang, Baorui Ma, Huachen Gao

- ***What's New***: 이 논문에서는 가상 세계 즉 여러 가지 다양한 세계(ad hoc virtual worlds)에서 3D 모델을 생성하기 위해 웹 규모의 동영상(Web-scale Internet videos)을 활용하여 3D 생성 모델을 훈련하는 방법인 See3D를 제안합니다. 이 모델은 3D 기하 정보나 카메라 포즈 주석 없이도 대규모의 인터넷 동영상에서 학습하여 확장 가능한 3D 생성이 가능합니다.
- ***Technical Details***: See3D는 멀티뷰 영상에서 3D 프라이어를 학습하기 위해 2D-inductive visual signal을 도입합니다. 이는 동영상 데이터에 무작위 마스크 및 시간 종속 노이즈를 부여하여 생성된 순수 2D-유도 신호로, 3D 구조가 없는 데이터를 통해 카메라 움직임을 암묵적으로 제어할 수 있게 합니다. 이 방법은 가우시안 스플래팅(Gaussian Splatting) 및 워핑 기반의 3D 생성 프레임워크에도 적용 가능합니다.
- ***Performance Highlights***: See3D는 단일 및 희소 재구성 벤치마크에서 두드러진 성과를 보여줍니다. 제안된 모델은 고비용의 제한된 3D 데이터셋에서 훈련된 모델들을 뛰어넘는 zero-shot 및 개방형 세계 생성 기능을 가지고 있습니다. 또한, 추가적인 파인튜닝 없이도 3D 편집과 같은 이미지-조건부 3D 생성 작업을 자연스럽게 지원합니다.

### [Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation](https://arxiv.org/abs/2412.04432)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04432.png)

Vote: 9

Authors: Yuying Ge, Yizhuo Li, Yixiao Ge, Ying Shan

- ***What's New***: Divot은 디퓨전 프로세스를 활용하여 비디오 표현 학습을 위한 비디오 토크나이저(Video Tokenizer)로서 소개되었습니다. 이 연구에서는 비디오의 공간적 특징과 시간적 동력을 포착하여 대형 언어 모델(LLM; Large Language Model)에서 현실적인 비디오 클립을 생성할 수 있는 표현으로 변환하는 방법을 제시하고 있습니다.
- ***Technical Details***: Divot 토크나이저는 사전 학습된 Vision Transformer (ViT) 인코더, 공간 및 시간 변환기(Spatial-Temporal Transformer), Perceiver Resampler로 구성되어 있으며, 낮은 프레임 속도로 샘플링된 비디오 프레임에서 비디오 표현을 얻습니다. 그런 다음 이 비디오 표현은 사전 학습된 비디오 디퓨전 모델을 조건으로 사용하여 VAE 잠재 변수에 추가된 노이즈를 예측합니다. 분산 혼합 모델(Gaussian Mixture Model; GMM)을 사용하여 비디오 특성의 분포를 모델링함으로써 비디오-텍스트 자동회귀 및 텍스트-비디오 생성이 가능합니다.
- ***Performance Highlights***: Divot-LLM은 다양한 비디오 이해 및 생성 벤치마크에서 경쟁력 있는 성과를 달성하였으며, 5백만 개의 비디오-텍스트 쌍을 학습하여 이미지 캡션 및 비디오 스토리텔링과 같은 작업에서 뛰어난 성능을 보여줍니다. 특히, 비디오 설명 데이터로 미리 훈련되어 인스트럭션 튜닝을 통해 상호 연결된 내러티브와 일관된 비디오를 생성하는 비디오 스토리텔링에서도 강점을 보입니다.

### [Unraveling the Complexity of Memory in RL Agents: an Approach for Classification and Evaluation](https://arxiv.org/abs/2412.06531)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06531.png)

Vote: 53

Authors: Egor Cherepanov, Nikita Kachaev, Artem Zholus, Aleksandr I. Panov, Alexey K. Kovalev

- ***What's New***: 이 논문은 강화 학습(이하 RL) 에이전트의 메모리 복잡성을 평가하고 분류하는 방법을 제안합니다. 이 연구는 메모리를 RL 에이전트의 필수 속성으로 정의하고, 메모리 유형을 장기 메모리(Long-term Memory; LTM)와 단기 메모리(Short-term Memory; STM), 절차적 기억과 선언적 기억으로 구분하여 명확하게 분류합니다.
- ***Technical Details***: 논문은 RL 에이전트의 메모리를 평가하기 위한 실험적 방법론을 제안합니다. 이 방법론은 메모리 집약적 환경(memory-intensive environment)에서 에이전트 메모리의 성능을 평가하며, 에이전트의 메모리 메커니즘(memory mechanisms)을 테스트하고 이를 통해 메모리 DM과 메타 강화 학습(Meta-RL)에서의 메모리 유형을 평가합니다. 또한 강화 학습에서의 메모리 문제를 해결하기 위해 RNN(Recurrent Neural Networks)과 트랜스포머(Transformers)를 사용한 다양한 메모리 메커니즘이 소개됩니다.
- ***Performance Highlights***: 제안된 방법론을 사용하여 메모리 강화 에이전트에 대한 실험을 수행한 결과, 장기 메모리와 단기 메모리의 구별과 평가에 있어 명확한 단계를 제공하며, 실험 설정이 잘못될 경우 에이전트의 메모리 능력에 대해 잘못된 결론을 내릴 수 있음을 보였습니다. 특히, 메모리 메커니즘을 통한 올바른 실험 설계가 에이전트의 장기와 단기 메모리 성능을 명확히 평가하도록 돕는 것으로 나타났습니다.

### [Exploring Multi-Grained Concept Annotations for Multimodal Large Language Models](https://arxiv.org/abs/2412.05939)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05939.png)

Vote: 8

Authors: Xiao Xu, Wanxiang Che, Yuxi Xie, Tianhao Niu, Min-Yen Kan, Libo Qin

- ***What's New***: 이 연구에서는 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)을 위한 새로운 데이터셋인 MMGIC를 소개합니다. 이 데이터셋은 다중 그레이드 개념 주석을 통해 시각과 언어의 정렬을 다층적으로 개선할 수 있습니다. 또한, 이미지-캡션 데이터와의 협력을 통해 성능을 향상시킬 수 있음을 실험적으로 입증합니다.
- ***Technical Details***: MMGIC에는 이미지, 객체 레이블, 객체 영역 등이 포함되어 있습니다. 데이터는 Open Images, Objects365, V3Det, Visual Genome 같은 기존의 대규모 객체 감지 데이터셋에서 수집됩니다. 객체와 관련된 다양한 애노테이션을 통합하여 MLLMs가 개념을 더 잘 이해하고 생성할 수 있도록 돕습니다.
- ***Performance Highlights***: MMGIC를 사용한 MLLM은 POPE 벤치마크에서 3.95% 개선, SEED-Bench에서는 2.34% 개선을 보였습니다. 이미지 캡션 데이터와의 협력을 통해 이미지 캡셔닝과 생성 작업에서 유의미한 성능 향상을 이루었습니다.

### [Turbo3D: Ultra-fast Text-to-3D Generation](https://arxiv.org/abs/2412.04470)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04470.png)

Vote: 1

Authors: Fujun Luan, Hao Tan, Tianwei Yin, Sai Bi, Zexiang Xu, Kai Zhang, Yiwei Hu, Shubham Tulsiani, Hanzhe Hu

- ***What's New***: Turbo3D는 텍스트에서 고품질의 3D 생성물을 1초도 안 되는 시간 내에 생성할 수 있는 초고속 텍스트-3D 전환 시스템입니다. 이 시스템은 혁신적인 Dual-Teacher Distillation 접근 방식을 통해 다수의 뷰에 대해 일관되면서도 사실적인 3D 생성을 가능하게 합니다.
- ***Technical Details***: Turbo3D는 다수 뷰(Multi-View; MV) 생성기와 단일 뷰(Single-View; SV) 생성기를 결합한 Dual-Teacher Distillation 기법을 도입하여 빠른 뷰 일관성을 유지하면서 고휘도 출력을 보장합니다. 생성된 다수 뷰의 잠재 표현에서 직접 3D를 복원하는 Latent GS-LRM을 사용하여 효율성을 더욱 향상시킵니다.
- ***Performance Highlights***: Turbo3D는 CLIP Score 27.61과 VQA Score 0.76을 기록하며, 단 0.35초의 추론 시간으로 다른 최신 방법들보다 빠르게 3D 생성을 수행합니다. 이러한 성과를 통해 Turbo3D는 높은 품질과 효율성을 동시에 달성했습니다.

### [Robust Multi-bit Text Watermark with LLM-based Paraphrasers](https://arxiv.org/abs/2412.03123)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03123.png)

Vote: 5

Authors: Yang Liu, Hang Li, Yuanshun Yao, Xiaojun Xu, Jinghan Jia

- ***What's New***: 이 연구는 LLM(대규모 언어 모델)을 활용하여 텍스트 내용을 부호화하는 다중 비트 텍스트 워터마크 기법을 제안합니다. 기존 방법들과 달리, 두 개의 서로 다른 LLM 기반 패러프레이저를 사용하여 텍스트를 대체 문구로 변형함으로써 워터마크를 삽입하고, 이를 검출할 수 있도록 설계되었습니다.
- ***Technical Details***: 제안된 시스템은 LLM 기반 패러프레이저를 워터마크 인코더로, 텍스트 분류기를 워터마크 디코더로 사용합니다. 인코딩 단계에서, 사용자가 선택한 '키'에 따라 입력 텍스트를 패러프레이징하여 워터마크된 텍스트를 생성합니다. 디코딩 단계에서는, 패러프레이징된 텍스트에서 코드를 추출하여 원래 선택한 키와 비교합니다. 또한, 'Proximal Policy Optimization (PPO)' 기법을 사용하여 패러프레이징을 최적화하고, 디코더와 인코더를 번갈아가며 훈련하는 방법을 채택했습니다.
- ***Performance Highlights***: 본 연구는 기존 방법들과 비교해 높은 비트 정확도와 텍스트 정확도를 보였으며, 특히 비트 정확도 95% 이상, 검출 AUC 0.99 이상의 성능을 달성했습니다. 추가적인 테스트에서는, 5회 반복 실행으로 비트 정확도 0.99 이상, 검출 AUC 0.9999로 성능이 더욱 향상되었습니다. 이러한 결과는 워드 치환 및 문장 패러프레이징 등의 변동에 대한 강한 강건성을 보여줍니다.

### [Around the World in 80 Timesteps: A Generative Approach to Global Visual Geolocation](https://arxiv.org/abs/2412.06781)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06781.png)

Vote: 9

Authors: David Picard, Loic Landrieu, Vicky Kalogeiton, Nicolas Dufour

- ***What's New***: 이 논문은 세계 전역의 시각적 이미지를 통한 위치 추정 작업에 새로운 생성 접근법을 도입했습니다. 특히, 시각적 지리위치 추정을 위해 확산(diffusion) 및 흐름 매칭(flow matching)을 활용하여 이미지의 위치가 불명확할 수 있다는 문제를 해결하였습니다. 이는 확률적 시각적 지리위치 작업을 처음으로 도입한 것으로, 단일 지점이 아닌 모든 가능한 위치들에 대한 확률 분포를 예측합니다.
- ***Technical Details***: 모델은 확산 및 리만 흐름 매칭(Riemannian Flow Matching)의 방법론을 확장하여 지리적 좌표를 직접 복원하는 방식을 사용합니다. 이 방식은 지구의 구형 기하학적 특성을 학습 과정에 반영할 수 있게 하여, 이미지 내용과 위치 간의 관계를 정확하게 모델링합니다. 또한, 위치에 대한 확률 분포를 예측하기 위해 새로운 밀도 추정 방법을 도입하였으며, 특정 이미지를 기반으로 위치의 확률을 계산할 수 있는 방법을 제공합니다.
- ***Performance Highlights***: 제안된 방법은 OpenStreetView-5M, iNat21, YFCC-100M의 세 가지 대규모 데이터세트에서 기존의 최첨단 지리위치 방법보다 높은 정확도를 달성하였습니다. 특히, 리만 흐름 매칭을 활용했을 때 유럽인 평면 공간에서의 모델보다 성능이 개선되었고, 예측된 분포가 테스트 이미지와 일관되게 맞아떨어져 더 나은 성능을 보였습니다. 이는 다수의 위치가 합리적인 추정일 수 있는 애매한 이미지에서 두드러지게 나타났습니다.

