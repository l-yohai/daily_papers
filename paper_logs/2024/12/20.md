## Daily Papers (2024-12-20)

### [Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception](https://arxiv.org/abs/2412.14233)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14233.png)

Vote: 4

Authors: Yuxiang Zhao, Jing Hao, Jiang-Jiang Liu, Yanpeng Sun, Zechao Li, Xiaofan Li, Jingdong Wang, Ke Zhu, Gang Zhang

- ***What's New***: 이 논문에서는 멀티모달 인식을 위한 시각적 전문가들(Visual Specialists)을 활용하여 이미지 캡션을 개선하는 새로운 접근법을 제안합니다. 이 방법은 시각적 인식 및 추론을 위한 LMM(Large Multimodality Models)의 성능 향상을 목표로 합니다.
- ***Technical Details***: DCE(Descriptive Caption Enhancement)는 사전 훈련된 다양한 시각적 전문가들을 활용하여 객체의 저수준 및 세밀한 속성(예: 깊이, 감정, 세밀한 카테고리)과 객체 간의 관계(예: 상대적 위치와 인간-객체 상호작용)를 조사합니다. 이 속성들을 통합하여 보다 풍부한 이미지 캡션을 생성하는데, 이를 위해 Qwen-72B와 같은 대규모 언어 모델들이 사용됩니다.
- ***Performance Highlights***: DCE를 통해 생성된 캡션은 LLaVA-v1.5 및 LLaVA-NeXT 모델의 시각-언어 정렬을 크게 개선하였으며, 14개의 벤치마크 테스트에서 탁월한 성능을 보여주었습니다. DCE를 활용한 데이터셋으로 훈련된 모델들이 모든 VQA 및 대규모 멀티모달 벤치마크에서 최상의 성능을 기록하며, 이는 이미지 캡션의 상세도와 정확도가 모델의 인식 능력 향상에 중요함을 입증합니다.

### [Flowing from Words to Pixels: A Framework for Cross-Modality Evolution](https://arxiv.org/abs/2412.15213)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15213.png)

Vote: 16

Authors: Mannat Singh, Qihao Liu, Xi Yin, Andrew Brown, Alan Yuille

- ***What's New***: CrossFlow는 새로운 크로스 모달 (Cross-Modality) 변형 프레임워크로 첫 번째로 모달리티 간의 직접적인 매핑을 학습할 수 있는 방법을 제안합니다. 이는 노이즈 분포나 조건부 메커니즘의 필요성을 없애고, 간단한 모델 구조로도 최신 기술 수준과 견줄만한 성능을 보입니다.
- ***Technical Details***: CrossFlow는 Variational Encoder를 이용해 입력 데이터를 변환하여 동일한 모양의 타겟 모달리티로 매핑하며, Classifier-free Guidance를 위한 이진 조건 인디케이터를 도입합니다. 텍스트-이미지 변환에서는 크로스 어텐션 없이 vanilla Transformer를 사용하여 이미지 생성을 수행합니다.
- ***Performance Highlights***: CrossFlow는 텍스트-이미지 생성 작업에서 기존의 크로스 어텐션 기반 방법보다 더 나은 성능을 보이며, 학습 데이터와 모델 크기가 같은 상황에서 standard flow matching보다 우수한 확장성과 스케일링 특성을 보여줍니다. 이는 이미지 캡셔닝, 깊이 추정 및 이미지 초해상도와 같은 다양한 작업에서도 높은 성능을 유지합니다.

### [MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval](https://arxiv.org/abs/2412.14475)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14475.png)

Vote: 44

Authors: Ze Liu, Yueze Wang, Bo Zhao, Defu Lian, Yongping Xiong, Junjie Zhou, Chen Jason Zhang, Shitao Xiao, Zheng Liu

- ***What's New***: MegaPairs는 범용 멀티모달 검색(Universal Multimodal Retrieval)을 위한 새로운 데이터 합성 방법을 제안합니다. 이 방법은 비전-언어 모델(Vision-Language Models; VLMs)과 오픈 도메인 이미지를 활용하여 대량의 합성 데이터를 생성합니다. 이 데이터를 통해 멀티모달 검색기의 성능을 크게 향상시킬 수 있으며, 26백만 개 이상의 훈련 예제를 생산하여 여러 크기의 모델을 훈련하였습니다.
- ***Technical Details***: MegaPairs는 이질적 KNN 트리플렛을 구성하여 오픈 도메인 이미지에서 연관된 이미지 쌍을 샘플링합니다. 이를 위해 CLIP 비전 인코더를 사용하여 시각-의미적 상관관계, DINO 비전 인코더를 사용하여 시각-패턴 상관관계, CLIP 텍스트 인코더를 사용하여 캡션 상관관계를 측정합니다. 단일 데이터 항목에는 이미지 쌍과 그 관계를 설명하는 텍스트 지시사항이 포함됩니다.
- ***Performance Highlights***: MMRet 모델들은 4개의 인기 있는 이미지 검색 벤치마크(CIR)와 MMEB에서 제로샷 성능에서 최첨단 결과를 달성했습니다. 특히, 기존 SOTA 모델보다 최대 8.1% 개선된 성능을 보여주며, 로라 기법을 활용한 파인 튜닝을 통해 다운스트림 작업에서도 지속적인 성능 향상을 확인했습니다.

### [Progressive Multimodal Reasoning via Active Retrieval](https://arxiv.org/abs/2412.14835)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14835.png)

Vote: 46

Authors: Chenghao Zhang, Guanting Dong, Zhicheng Dou, Ji-Rong Wen, Mengjie Deng, Yutao Zhu

- ***What's New***: 이 논문은 멀티모달 대형 언어모델(Multimodal Large Language Models; MLLMs)의 복잡한 추론 능력을 향상시키기 위해 AR-MCTS라는 보편적인 프레임워크를 제안합니다. 이는 MCTS와 액티브 검색(Active Retrieval; AR)을 통해 자동으로 단계별 주석을 생성하고, 복잡한 추론 문제 해결을 위한 핵심 인사이트를 귀결시킵니다.
- ***Technical Details***: 모델은 다양한 데이터 소스로부터 핵심적인 검색(information retrieval)을 통해 문제 해결 인사이트를 받아오며, MCTS 알고리즘을 활용해 단계별 추론 공간의 다양성과 신뢰성을 높입니다. 또한, PRM(Process Reward Model)을 설계하여 단계별 선호도를 최적화하고 지도 학습을 통해 멀티모달 추론 검증을 자동화합니다.
- ***Performance Highlights***: AR-MCTS 프레임워크는 세 가지 복잡한 멀티모달 추론 데이터셋에서 다양한 멀티모달 모델 성능을 향상시키는 것으로 확인되었습니다. 이 접근 방식은 견본 다양성과 검증 정확성을 최적화하여 신뢰할 수 있는 멀티모달 추론을 가능하게 합니다.

### [UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency](https://arxiv.org/abs/2412.15216)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15216.png)

Vote: 4

Authors: Alessio Tonioni, Yongqin Xian, Federico Tombari, Thomas Hofmann, Enis Simsar

- ***What's New***: 이 논문에서는 Cycle Edit Consistency(CEC)라는 새로운 무감독 학습 기법을 소개하여, 편집 지침에 기반한 이미지 편집을 구현합니다. 이는 편집된 이미지의 정답 데이터를 필요로 하지 않으며, 다양한 실제 이미지-캡션 쌍에 대해 모델을 학습시킬 수 있습니다.
- ***Technical Details***: UIP2P 모델은 CLIP 임베딩 공간에서 텍스트와 이미지의 정렬을 활용하여, 텍스트 지시에 따라 이미지를 정확히 편집하고 원래 콘텐츠의 일관성을 유지하는데 중점을 둡니다. 주요 구성 요소로는 텍스트 및 이미지 방향 일관성(Text and Image Direction Consistency), 주의 맵 일관성(Attention Map Consistency), 재구성 일관성(Reconstruction Consistency), 그리고 가변 확산 단계를 통한 통합 예측(Unified Prediction with Varying Diffusion Steps)이 있습니다.
- ***Performance Highlights***: UIP2P 모델은 다양한 실제 이미지 데이터 셋에서 고품질의 정교한 편집을 제공합니다. 실험 결과, InstructPix2Pix, MagicBrush, HIVE와 같은 기존의 지도 학습 기반 모델과 비교해도 경쟁력 있는 성능을 보이며, 특히 대규모 주석 데이터셋 없이도 확장성과 다재다능함을 입증했습니다.

### [How to Synthesize Text Data without Model Collapse?](https://arxiv.org/abs/2412.14689)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14689.png)

Vote: 17

Authors: Hengli Li, Bowen Zhou, Daixuan Cheng, Zhouhan Lin, Xuekai Zhu, Ermo Hua, Xingtai Lv, Ning Ding, Kaiyan Zhang, Zilong Zheng

- ***What's New***: 이 논문에서는 모델 붕괴(Model Collapse) 없이 텍스트 데이터를 합성할 수 있는 방법을 제안합니다. 이는 AI 모델이 자체 생성 데이터를 반복적으로 학습함으로써 성능이 저하되는 모델 붕괴를 방지하는 방안을 제시합니다. 특히, 인간이 생성한 데이터에 대한 토큰 수준 편집(Token-Level Editing)을 통해 반합성 데이터(Semi-Synthetic Data)를 만듭니다.
- ***Technical Details***: 연구진은 다양한 비율로 합성 데이터를 혼합하여 언어 모델을 사전 학습한 결과, 합성 데이터의 비율이 높아질수록 모델 성능이 저하되는 것을 발견했습니다. 합성 데이터의 분포가 인간이 생성한 데이터 분포를 충분히 커버하지 못하고 n-gram 특징이 과도하게 집중되는 현상을 통계적으로 분석했습니다. 이러한 문제를 해결하기 위해 제안된 토큰 수준 편집 기법은 모델이 높은 확신을 가진 데이터 포인트를 다시 샘플링하고 대체하여 모델 붕괴를 방지합니다.
- ***Performance Highlights***: GPT-2 Small 모델의 사전 학습 결과, 합성 데이터의 비율에 따라 Perplexity(PPL)가 증가함에 따라 성능이 저하되는 경향을 보였습니다. 예를 들어, 합성 데이터가 75%인 경우 PPL이 크게 증가하여 모델의 일반화 성능을 저하시켰습니다. 그러나 제안된 토큰 편집 기법을 적용한 사례에서는 기존 데이터 대비 성능이 향상되었습니다.

### [Move-in-2D: 2D-Conditioned Human Motion Generation](https://arxiv.org/abs/2412.13185)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13185.png)

Vote: 1

Authors: Difan Liu, Feng Liu, Hsin-Ping Huang, Zhan Xu, Jui-Hsien Wang, Yang Zhou, Ming-Hsuan Yang

- ***What's New***: Move-in-2D는 장면 이미지에 조건을 둔 2D 기반 인간 동작 생성 방식으로, 다양한 장면에 적응할 수 있는 동작 시퀀스를 생성할 수 있는 새로운 접근 방식을 제시합니다. 이 방법은 장면 이미지와 텍스트 프롬프트를 입력으로 받아, 장면에 맞춰진 동작 시퀀스를 생성하는 디퓨전 모델을 활용하며, 기존에 동작 제어 신호로 사용되던 사전 녹화된 동작 시퀀스에 의존하지 않습니다.
- ***Technical Details***: 제안된 Move-in-2D 방법은 대규모 비디오 데이터셋을 기반으로 단일 사람의 활동을 포함하는 비디오를 수집하고, 각 비디오를 해당하는 인간 동작으로 주석하여 모델을 훈련합니다. 이 데이터셋을 이용한 조건부 디퓨전 모델은 줌래규 칸였사 장면 이미지와 텍스트 프롬프트를 기반으로 인간 동작을 생성할 수 있으며, 대규모 언어 모델(LLM)의 'In-context Learning'에서 영감을 받아 장면 및 텍스트 입력을 공유 토큰 공간으로 변환하여 트랜스포머 기반의 디퓨전 모델에 통합합니다.
- ***Performance Highlights***: Move-in-2D는 텍스트와 장면 이미지로부터 사람 움직임을 예측하며, 이는 비디오 합성 작업에서 인간 동작의 품질을 개선하는 데 기여합니다. 이 접근 방식은 다양한 빌라성 있고 프롬프트에 맞춘 동작을 생성하여 비디오 합성 모델들이 복잡한 인간 활동을 생성할 수 있게 합니다.

### [DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation](https://arxiv.org/abs/2412.15200)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15200.png)

Vote: 8

Authors: Wang Zhao, Yuejiang Dong, Ying Shan, Jiale Xu, Yan-Pei Cao

- ***What's New***: DI-PCG는 일반적인 이미지 조건으로부터 효율적인 역절차적 콘텐츠 생성(Inverse Procedural Content Generation; I-PCG)을 위한 혁신적인 확산 모델 기반 방법을 제시합니다. 가벼운 확산 변환기(Diffusion Transformer) 모델을 사용하여 절차적 생성기의 파라미터를 노이즈 제거 대상, 관찰된 이미지를 조건으로 하여 파라미터 생성을 제어합니다.
- ***Technical Details***: DI-PCG는 7.6M의 네트워크 파라미터와 30시간의 GPU 학습을 통해 효율적으로 작동합니다. 생성자의 파라미터를 계속적인 매개변수로 변환하여 학습하며, DINOv2를 통해 이미지 특징을 추출하고, 교차 주의 메커니즘을 적용하여 절차적 생성기의 파라미터 공간을 잘 맞춥니다.
- ***Performance Highlights***: DI-PCG는 수 초 내에 절차적 생성기의 원하는 파라미터를 샘플링할 수 있으며, 인버스 PCG 및 이미지-3D 생성 작업에서 높은 품질의 3D 자산을 생성합니다. 양적 및 질적 실험을 통해 DI-PCG의 효과성을 검증하였으며, 디테일하고 정밀한 3D 메쉬를 제공하여 기존 방법보다 우수한 성능을 보여줍니다.

### [AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling](https://arxiv.org/abs/2412.15084)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15084.png)

Vote: 7

Authors: Wei Ping, Yang Chen, Mohammad Shoeybi, Zihan Liu, Bryan Catanzaro

- ***What's New***: AceMath는 최첨단 수학 문제 해결 능력을 갖춘 모형군으로, 사후 훈련(post-training)과 보상 모델링(reward modeling)을 통해 복잡한 수학 문제를 해결하는 데 특화돼 있습니다. AceMath-72B-Instruct라는 새로운 모델은 Qwen2.5-Math-72B-Instruct, GPT-4o 및 Claude-3.5 Sonnet을 뛰어넘는 성능을 자랑합니다.
- ***Technical Details***: AceMath는 수퍼바이즈드 파인 튜닝(Supervised Fine-Tuning, SFT)을 통해 일반 분야에서 경쟁력 있는 성능을 달성한 후, 엄선된 프롬프트와 인위적으로 생성된 응답를 사용해 수학 분야에 특화된 파인 튜닝을 수행합니다. 또한, AceMath-RewardBench라는 벤치마크를 구축하여 다양한 문제와 난이도에 대한 보상 모델을 평가합니다. AceMath-72B-RM라는 결과 모델은 최첨단 보상 모델을 지속적으로 능가합니다.
- ***Performance Highlights***: AceMath-72B-Instruct 모델은 다양한 수학 추론 벤치마크에서 최고 평균 rm@8 점수를 기록하며, Qwen2.5-Math-72B-Instruct와 같은 기존 모델을 뛰어넘는 성능을 자랑합니다. 특히, 초대형 오픈 소스 모델들과의 비교 실험에서 수학 논리와 해결 능력에서 후한 점수를 받으며 향후 연구에서 중요한 방향성을 제시하고 있습니다.

### [LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks](https://arxiv.org/abs/2412.15204)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15204.png)

Vote: 22

Authors: Jiajie Zhang, Jie Tang, Hao Peng, Yushi Bai, Xin Lv, Juanzi Li, Shulin Cao, Yuxiao Dong, Lei Hou, Jiazheng Xu, Xiaozhi Wang, Shangqing Tu

- ***What's New***: LongBench v2는 LLMs(대규모 언어 모델)의 긴 맥락 처리 능력을 평가하기 위해 설계된 벤치마크입니다. 이 벤치마크는 6개의 주요 작업 범주에서 503개의 도전적인 객관식 질문으로 구성되어 있습니다. 이 데이터셋은 다양한 전문 배경을 가진 100명 가까운 고학력 개인들로부터 수집되었습니다.
- ***Technical Details***: LongBench v2는 단일 문서 질문 응답, 다중 문서 질문 응답, 긴 맥락 학습, 긴 대화 기록 이해, 코드 저장소 이해, 그리고 긴 구조화된 데이터 이해 등 총 6개의 작업 범주와 20개의 세부 작업으로 구성되어 있습니다. 데이터 수집 시에는 자동화 및 수동 검토 프로세스를 결합하여 데이터의 품질과 난이도를 보장합니다. 긴 맥락의 문제를 해결하기 위해 모델의 추론 능력과 테스트 시간 컴퓨팅 확장이 중요함을 강조합니다.
- ***Performance Highlights***: 인간 전문가가 제한 시간 15분 내에 달성한 정확도는 53.7%입니다. 반면 o1-preview 모델은 57.7%의 정확도를 달성하여 인간의 성과를 넘어섰습니다. 이 결과는 장기간 맥락의 도전에 대처하기 위해 향상된 추론 능력과 컴퓨팅 자원의 확장이 필요함을 강조합니다.

### [Predicting the Original Appearance of Damaged Historical Documents](https://arxiv.org/abs/2412.11634)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.11634.png)

Vote: 4

Authors: Lianwen Jin, Zhenhua Yang, Yongxin Shi, Yuyi Zhang, Chongyu Liu, Dezhi Peng

- ***What's New***: 이 논문에서는 손상된 역사적 문서의 원래 모습을 예측하는 새로운 작업인 Historical Document Repair (HDR)을 소개합니다. 이를 위해 대규모 데이터셋인 HDR28K와 확산 기반 네트워크인 DiffHDR을 제안합니다. HDR28K는 28,552개의 손상-수리 이미지 쌍을 포함하며, 다양한 스타일의 열화를 포함하고 있습니다. 이 연구는 문서 처리의 새로운 방향을 개척할 수 있는 잠재력을 가지고 있습니다.
- ***Technical Details***: HDR28K 데이터셋은 다양한 스타일의 열화를 통해 손상 이미지를 생성하여 손상 유형의 다양성을 높였습니다. 또한 DiffHDR은 이미지의 손상된 영역을 연속적인 확산 단계로 변환하여 문맥적 및 시각적 일관성을 높이기 위해 설계된 문자 인지 손실(Character Perceptual Loss)을 포함한 확산 기반 네트워크입니다. 이 모델은 손상 문서의 원래 모습을 픽셀 레벨에서 재구성하며, 문서 편집 및 텍스트 블록 생성으로 확장될 수 있습니다.
- ***Performance Highlights***: DiffHDR은 HDR28K 데이터셋에서 훈련된 결과, 기존 방법을 크게 능가했으며, 실제 손상된 문서 처리에서도 탁월한 성능을 보였습니다. 특히 문자 정확성 지표(Rec-ACC)에서 두 번째로 좋은 성능을 보인 방법보다 11.9635% 더 높은 성능을 기록했습니다. 이 연구는 값진 문화 유산의 보전에 기여할 수 있습니다.

### [LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis](https://arxiv.org/abs/2412.15214)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15214.png)

Vote: 12

Authors: Yujun Shen, Limin Wang, Qifeng Chen, Ka Leong Cheng, Qiuyu Wang, Hao Ouyang, Hanlin Wang, Wen Wang

- ***What's New***: LeviTor은 이미지-비디오 합성에서 3D 궤적 제어(3D Trajectory Control)를 도입하여 물체의 움직임을 보다 정확하게 제어할 수 있는 새로운 방법을 제시합니다. LeviTor는 심도 정보(Depth Information)와 K-평균(K-means) 클러스터링된 객체 마스크를 결합하여 3D 정보를 명시적으로 추정하지 않고도 물체의 궤적을 제어합니다.
- ***Technical Details***: LeviTor는 사전 훈련된 비디오 생성 모델을 미세 조정하여 효율적인 3D 궤적 제어를 통합하는 기술을 채택했습니다. 객체 마스크를 클러스터링된 포인트로 추상화하고, 이러한 정보를 비디오 확산 모델(Video Diffusion Model)에 제어 신호(Control Signal)로 입력합니다. 또한, 심도 추정 네트워크인 DepthAnythingV2를 사용하여 각 프레임의 상대적 심도 맵을 예측합니다. 사용자 친화적인 추론 파이프라인을 설계하여 사용자들이 2D 이미지에 경로를 그리고 심도 값을 조절하여 손쉽게 3D 궤적을 입력할 수 있도록 하였습니다.
- ***Performance Highlights***: LeviTor는 기존의 방법들이 실패한 상황에서도 정확한 3D 궤적 제어를 달성하며, 질적으로 높은 수준의 비디오를 생성할 수 있습니다. 수치적으로도 DAVIS 데이터셋 기준으로 FID와 FVD 측면에서 다른 방법들보다 뛰어난 성능을 나타냅니다. 따라서 LeviTor는 다양한 장면에서 복잡한 물체의 움직임과 상호작용을 잘 캡처합니다.

### [TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation](https://arxiv.org/abs/2412.14642)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14642.png)

Vote: 4

Authors: Dongzhan Zhou, Qing Li, Junxian Li, Yunqing Liu, Jiatong Li

- ***What's New***: TOMG-Bench는 대형 언어 모델(Large Language Models; LLMs)의 텍스트 기반 오픈 도메인 분자 생성 능력을 평가하는 최초의 벤치마크입니다. 이 벤치마크는 분자 편집(Editing), 분자 최적화(Optimization), 맞춤형 분자 생성(Customization) 세 가지 주요 작업으로 구성되어 있습니다.
- ***Technical Details***: TOMG-Bench는 각 주요 작업마다 5,000개의 테스트 샘플이 포함된 세 개의 하위 작업으로 구성된 데이터셋을 포함합니다. 우리는 또한 자동화된 평가 시스템을 개발하여 생성된 분자의 품질과 정확성을 측정하도록 설계했습니다. OpenMolIns라는 특수한 지시 조정(Instruct Tuning) 데이터셋을 통해 Llama-3.1-8B 모델이 모든 오픈 소스 일반 LLM을 뛰어넘을 수 있었습니다.
- ***Performance Highlights***: Claude-3.5 모델이 35.92%의 가중 평균 정확도를 기록하며 가장 높은 성능을 보였습니다. OpenMolIns 데이터셋으로 조정된 Galactica-125M 모델은 Llama-3-70B-Instruct 및 GPT-3.5-turbo 모델을 능가하여 강력한 성능을 발휘했습니다.

### [Qwen2.5 Technical Report](https://arxiv.org/abs/2412.15115)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15115.png)

Vote: 234

Authors: Junyang Lin, Jian Yang, Jianxin Yang, Beichen Zhang, Mingfeng Xue, Xuancheng Ren, Le Yu, Yu Wan, Jianwei Zhang, Keming Lu, Jiaxi Yang, Huan Lin, Yuqiong Liu, Kexin Yang, Binyuan Hui, Keqin Bao, Chengyuan Li, Qin Zhu, Zihan Qiu, Haoran Wei, Rui Men, Yang Fan, Baosong Yang, Tianhao Li, Qwen, Fei Huang, Yang Su, Yichang Zhang, Bo Zheng, Kai Dang, Jianhong Tu, Jingren Zhou, Dayiheng Liu, Runji Lin, Tingyu Xia, Pei Zhang, Bowen Yu, Mei Li, Zhenru Zhang, Xingzhang Ren, An Yang, Zeyu Cui

- ***What's New***: Qwen2.5는 새로운 대형 언어 모델 시리즈로, 사전 훈련과 후속 훈련 단계에서 개선되었습니다. 특히, 사전 훈련 데이터는 7조 토큰에서 18조 토큰으로 확장되었으며, 이는 상식, 전문 지식, 추론 능력을 강화합니다. 후속 훈련에서는 정교한 감독 학습과 오프라인 및 온라인 강화 학습(DPO, GRPO)을 통해 사람의 선호도에 더 잘 맞추고 긴 텍스트 생성과 구조적 데이터 분석, 지시사항 준수를 개선합니다.
- ***Technical Details***: Qwen2.5의 개방 가중치(open-weight) 모델은 0.5B에서 72B 파라미터의 다양한 사이즈를 제공하며, Mixture-of-Experts(MoE) 모델인 Qwen2.5-Turbo, Qwen2.5-Plus도 포함됩니다. 모델은 Transformer 기반의 디코더 아키텍처를 유지하고, Grouped Query Attention(GQA), SwiGLU 활성화 함수, Rotary Positional Embeddings(RoPE)을 통합했습니다. 또한 MoE 모델은 전문가 라우팅 메커니즘을 활용해 토큰을 상위 전문가에게 전달합니다.
- ***Performance Highlights***: Qwen2.5 모델은 다양한 벤치마크에서 우수한 성능을 입증하며, 특히 Qwen2.5-72B-Instruct는 Llama-3-405B-Instruct와 비슷한 성능을 보여줍니다. Qwen2.5-Turbo와 Qwen2.5-Plus는 비용 효율성이 뛰어나며, GPT-4o-mini 및 GPT-4o와 경쟁력 있는 성능을 제공합니다. 또한, 수학 및 코딩을 위한 전문 모델을 학습하는 데 중요한 역할을 합니다.

### [AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation](https://arxiv.org/abs/2412.15191)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.15191.png)

Vote: 1

Authors: Kwot Sin Lee, Moayed Haji-Ali, Sergey Tulyakov, Ivan Skorokhodov, Willi Menapace, Vicente Ordonez, Alper Canberk, Aliaksandr Siarohin

- ***What's New***: AV-Link는 최초로 비디오와 오디오 생성 작업을 통합한 프레임워크로, 기존의 사전학습된 특징 추출기를 사용하지 않고도 두 모달리티 간의 시간 정렬을 맞추는 방법을 제안합니다. 이를 통해 양방향 정보 교환을 가능하게 하며, 몰입형 미디어 생성에 잠재력을 보여줍니다.
- ***Technical Details***: AV-Link는 비디오-오디오 및 오디오-비디오 생성에 있어 고정된 흐름 매칭 모델의 활성화 기능을 활용하여 시간적으로 정렬된 크로스 모달 조건을 제공합니다. Fusion Block을 사용하는데, 이 블록은 시간에 맞춰 정렬된 셀프 어텐션 작용을 통해 두 모달리티 간의 정보를 교환할 수 있게 합니다. 사전학습된 특징 추출기를 사용하지 않고, 미디어 생성의 흐름 모델을 그대로 사용하여 정확한 시간정렬과 세만틱 정렬을 동시에 제공합니다.
- ***Performance Highlights***: AI 비디오 생성의 새로운 지평을 연 AV-Link는 VGGSounds 벤치마크에서 최첨단 성능을 보여주며, Onset ACC에서 최고 기준보다 36.5% 포인트 개선된 결과를 나타냅니다. 사용자 연구에서는 '영화 생성 오디오' 대회에서 56.8%의 선호도를 기록하며, AV-Link가 비디오와 오디오의 생생하고 시각적으로 정렬된 콘텐츠 생성에서 강한 입지를 가지고 있음을 확인했습니다.

### [Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion](https://arxiv.org/abs/2412.14462)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14462.png)

Vote: 13

Authors: Jixuan He, Hanspeter Pfister, Wanhua Li, Ye Liu, Junsik Kim, Donglai Wei

- ***What's New***: 이번 논문에서는 인지 가능성에 기반한 객체 삽입(Affordance-Aware Object Insertion)을 위한 새로운 접근방식인 Mask-Aware Dual Diffusion (MADD) 모델을 제안하였습니다. 이는 기존의 인간 중심의 이미지 합성과는 달리, 일반적인 객체-장면 구성 프레임워크로 확장되어 어떠한 객체도 다양한 위치 프롬프트를 바탕으로 장면에 자연스럽게 삽입할 수 있도록 설계되었습니다.
- ***Technical Details***: 기술적 관점에서, MADD 모델은 듀얼 스트림 아키텍처를 사용하여 RGB 이미지와 삽입 마스크를 동시에 디노이징합니다. 또한 SAM-FB라는 대규모 데이터셋을 구축하여 3백만 개 이상의 예제와 3,000개 이상의 객체 카테고리를 포함하여 이 작업을 지원합니다. SAM-FB 데이터셋은 SA-1B에서 유래된 다양한 객체 카테고리의 포그라운드 이미지를 제공합니다.
- ***Performance Highlights***: 실험 결과, 제안된 MADD 방법은 최첨단 기법들보다 더 나은 성능을 보이며, 'In-the-wild' 이미지에서도 큰 범용성을 보여줍니다. 이는 객체를 세부 위치 정보 없이도 자동으로 삽입할 수 있는 능력을 갖추고 있다는 점에서 두드러집니다. 사용자 연구를 통해 MADD가 다른 방법들보다 더욱 높은 품질의 합성을 제공함을 확인했습니다.

### [PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation](https://arxiv.org/abs/2412.14283)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.14283.png)

Vote: 2

Authors: Negar Hassanpour, Jiao He, Liyao Jiang, Mohammad Salameh, Fengyu Sun, Di Niu, Mohammadreza Samadi

- ***What's New***: PixelMan은 훈련이나 역전환 과정 없이 픽셀 조작 및 생성 기법을 통해 일관된 객체 편집을 실현하는 방법으로, 기존 사전 훈련된 텍스트-이미지 확산 모델을 활용하여 최대 16개의 추론 단계 내에서 최고 성능을 발휘합니다. 이는 기존의 훈련 기반 및 훈련 없이 가능한 방법(대개 50단계 필요)을 능가합니다.
- ***Technical Details***: PixelMan은 픽셀 공간에서 소스 객체를 목표 위치로 복사한 후 일관성이 유지된 이미지를 생성하기 위해 픽셀 조작 이미지를 기준점으로 삼고, 다양한 최적화 기술을 도입하여 기존 위치의 객체를 인페인트합니다. 세 갈래의 역전환 없는 샘플링 접근법을 설계하여 객체와 배경의 높은 이미지 일관성을 유지하면서 누락된 부분만 생성하도록 합니다. 또한, 객체의 일관된 인페인팅을 가능하게 하기 위해 셀프 어텐션(Self-Attention) 누출 방지 기법을 제안했습니다.
- ***Performance Highlights***: PixelMan은 객체, 배경 및 이미지 의미 일관성 측정에서 우수한 성능을 나타내며, 평균 대기 시간을 낮추고 NFE 수를 줄여 현재 인기 있는 방법보다 더 나은 결과를 보여줍니다. COCOEE 데이터셋 및 ReS 데이터셋에서의 정량적 결과와 시각적 비교는 PixelMan이 일관성 및 이미지 품질 평가(IQA) 지표에서 더 높은 성능을 보여주며, 훈련 없이 16개의 추론 단계만으로 목표를 달성했습니다.

### [DateLogicQA: Benchmarking Temporal Biases in Large Language Models](https://arxiv.org/abs/2412.13377)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.13377.png)

Vote: 0

Authors: Madiha Kazi, MingZe Tang, Gagan Bhatia, Cristina Mahanta

- ***What's New***: DateLogicQA는 다양한 날짜 형식과 시간적 맥락, 그리고 추론 유형을 포괄하는 190개의 질문으로 구성된 벤치마크입니다. 특히 대형 언어 모델의 시간적 편향을 평가하기 위해 고안되었으며, Semantic Integrity Metric을 도입하여 토크나이제이션(Tokenization)의 품질을 평가합니다. 주요한 두 가지 편향으로는 표현 수준 편향(Representation-Level Bias)과 논리 수준 편향(Logical-Level Bias)이 있습니다.
- ***Technical Details***: DateLogicQA 데이터셋은 상식적, 사실적, 개념적, 수리적 추론을 포함하는 네 가지 카테고리로 나뉩니다. 이는 모든 질문이 과거, 현재, 미래라는 세 가지 시간적 맥락 내에서 일곱 가지 날짜 형식을 특징으로 하여 구조화되었습니다. 이 벤치마크는 LLMs의 날짜 형식 이해 및 해석 능력을 평가하며, 주요 목표는 토크나이제이션 과정을 통해 발생할 수 있는 해석적 편향을 평가하는 것입니다. 또한 인간 평가를 통해 자동화된 메트릭 이상의 통찰을 제공합니다.
- ***Performance Highlights***: 대형 언어 모델의 시간적 추론에 대한 평가 결과, 모델의 크기 및 아키텍처가 성능에 중요한 영향을 미친다는 것을 입증했습니다. 예를 들어, GPT-4-turbo 모델은 63%의 정확한 응답 비율로 최고 성과를 보였으며, 잘못된 응답 비율이 16%로 가장 낮았습니다. 시간적 맥락에 따라 미래를 다룰 때 모델 성능이 더 높게 나타났으며, 이는 모델의 예측 및 생성 능력과 관련되어 있음을 시사합니다. 또한 날짜 형식에 따라 성능이 상당히 차이가 나며, 표준화된 형식에서 더 나은 성능을 보였습니다.

