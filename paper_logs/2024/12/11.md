## Daily Papers (2024-12-11)

### [HARP: Hesitation-Aware Reframing in Transformer Inference Pass](https://arxiv.org/abs/2412.07282)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07282.png)

Vote: 4

Authors: Seung-won Hwang, Romain Storaï

- ***What's New***: HARP는 Transformer 기반의 인퍼런스 과정에서 주저함(Hesitation)과 리프레이밍(Reframing)을 통해 성능을 개선하는 새로운 방법을 제안합니다. 이는 기존 Transformer의 forward pass를 수정하여, 토큰 생성 시 불확실성을 느낄 때 추가적인 연산을 수행함으로써 성능을 향상시킵니다.
- ***Technical Details***: HARP는 Token-level 불확실성을 기반으로 선택적으로 추가 연산을 적용하여 토큰 생성의 결정을 지원합니다. Shannon 엔트로피를 활용하여 모델의 불확실성을 측정하고, 불확실성이 높을 경우 임베딩에 dropout을 적용하여 입력 시퀀스를 다른 시각에서 재구성합니다.
- ***Performance Highlights***: HARP는 다양한 업무와 모델에서 최대 5.16%까지 성능 향상을 이루며, Beam Search보다 2배 빠른 인퍼런스 속도를 유지합니다. 이 개선은 모델의 성능을 향상시키는 동시에 계산 효율성을 유지하는 데 기여합니다.

### [Frame Representation Hypothesis: Multi-Token LLM Interpretability and Concept-Guided Text Generation](https://arxiv.org/abs/2412.07334)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07334.png)

Vote: 11

Authors: Kazuhiro Fukui, Erica K. Shimomoto, Lincon S. Souza, Pedro H. V. Valois

- ***What's New***: 이 연구에서는 LLM(대형 언어 모델)에 대한 해석 가능성과 제어성을 높이기 위한 Frame Representation Hypothesis를 소개합니다. 이는 기존의 Linear Representation Hypothesis(LRH)를 다중 토큰에 적용하여 텍스트 생성 시 언어적 개념을 더 잘 이해하고 조종할 수 있게 합니다.
- ***Technical Details***: Frame Representation Hypothesis는 다중 토큰 구조를 프레임(Frames)이라는 순서 있는 벡터 열로 간주하여, LLM에서 사용하는 텍스트 데이터를 보다 잘 해석할 수 있도록 합니다. 이 프레임을 활용하여, Top-k Concept-Guided Decoding을 통해 지정된 개념을 사용하여 텍스트 생성을 직관적으로 조종할 수 있습니다. 이를 통해 성 차별 및 언어 편향 등을 드러내고 잠재적으로 개선하는 방법을 제시합니다.
- ***Performance Highlights***: Llama 3.1, Gemma 2, Phi 3 등 여러 LLM 모델 패밀리를 검증하여 성별과 언어적으로 편향된 내용을 드러내고 이를 개선할 가능성을 보여주었습니다. 해석 가능성과 제어성의 측면에서 현 모델들이 가지고 있는 한계를 노출시켰습니다.

### [Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation](https://arxiv.org/abs/2412.07338)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07338.png)

Vote: 2

Authors: Marco Avvenuti, Lorenzo Cima, Alessio Miaschi, Stefano Cresci, Amaury Trujillo, Felice Dell'Orletta

- ***What's New***: 이 논문에서는 대형 언어 모델(Large Language Models; LLMs)을 사용하여 온라인 독성을 해결하기 위해 상황에 맞춘 반대 의견(Contextualized Counterspeech)을 생성하는 방법을 제안합니다. 기존의 반대 의견이 일괄적이라면, 이 연구는 커뮤니티, 대화, 사용자에 맞게 적응하여 더 설득력 있는 대응을 생성합니다.
- ***Technical Details***: 본 연구에서는 LLaMA2-13B 모델을 활용하여 반대 의견을 생성하며, 다양한 상황적 정보와 미세 조정(fine-tuning) 전략을 실험합니다. 커뮤니티 스타일에 맞춘 적응(Adaptation)과 사용자 특성에 대한 개인화(Personalization)를 결합하여 36가지 설정을 평가하였고, 알고리즘 평가와 인간 평가 모두를 통해 각 설정의 효과성을 분석하였습니다.
- ***Performance Highlights***: 인간 평가에서 [Ba Pr Hi] 설정이 상대적으로 높은 적합성(adqeuacy)과 설득력(persuasiveness)을 보여주었으며, 그것은 상황에 맞춘 반대 의견의 잠재적 효과를 입증합니다. 그러나 알고리즘 평가와 인간 평가 간에 상당한 불일치가 나타났으며, 이는 향후 평가 방법론의 정교함을 요구합니다.

### [ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer](https://arxiv.org/abs/2412.07720)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07720.png)

Vote: 1

Authors: Yuxuan Song, Wei-Ying Ma, Yufei Huang, Jinyi Hu, Zhiyuan Liu, Maosong Sun, Mingxuan Wang, Hao Zhou, Shengding Hu

- ***What's New***: ACDiT는 자기회귀 조건부 모델링(Autoregressive Conditional Modeling)과 확산 변환기(Diffusion Transformer)의 보간을 통해 통합 모델을 제안합니다. 이 연구는 두 방법론을 융합하여 시각 정보를 모델링하고, 다양한 모달리티(Multimodality)를 아우르는 통합 모델의 가능성을 탐색합니다.
- ***Technical Details***: ACDiT는 블록 단위의 자기회귀 조건부 확산 변환기(Autoregressive Blockwise Conditional Diffusion Transformer)로 각 블록의 크기가 조정 가능합니다. 학습 중 Skip-Causal Attention Mask(SCAM)을 통한 간단한 구현이 가능하며, 추론 시 확산 정화(Diffusion Denoising)와 자기회귀 디코딩(Autoregressive Decoding)을 반복하여 효율성을 높입니다.
- ***Performance Highlights***: ACDiT는 ImageNet과 UCF-101 영상 생성 작업에서 유사한 모델 스케일의 자기회귀 기반과 비교해 뛰어난 성능을 보였습니다. 또한, 지속적인 시각 생성 작업에 유망한 백본으로 평가받고 있습니다.

### [Mobile Video Diffusion](https://arxiv.org/abs/2412.07583)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07583.png)

Vote: 15

Authors: Amir Ghodrati, Amirhossein Habibian, Ioannis Lelekas, Denis Korzhenkov, Haitam Ben Yahia

- ***What's New***: 이 논문은 고사양 GPU의 필요성을 줄여 모바일 기기에서 동영상 생성이 가능하게 하는 최초의 모바일 최적화 비디오 확산 모델(Mobile Video Diffusion)을 소개합니다. 공간-시간 UNet을 기반으로 메모리 및 계산 비용을 줄였습니다.
- ***Technical Details***: MobileVD는 공간-시간 UNet의 프레임 해상도를 줄이고, 다중 스케일 시간 표현을 통합하며, 채널과 시간 블록의 수를 줄이는 두 가지 새로운 가지치기(shearing) 방법을 도입하여 모바일에 최적화된 기능을 구현했습니다. 또한 적대적 미세 조정을 통해 노이즈 제거를 한 단계로 줄였습니다.
- ***Performance Highlights***: 모델 MobileVD는 523배 더 효율적이며(1817.2 vs 4.34 TFLOPs), 약간의 품질 저하(FVD 149 vs 171)를 보였습니다. Xiaomi 14-Pro에서 1.7초 만에 14 × 512 × 256 px 클립의 잠재 변수를 생성할 수 있습니다.

### [Chimera: Improving Generalist Model with Domain-Specific Experts](https://arxiv.org/abs/2412.05983)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05983.png)

Vote: 5

Authors: Hongbin Zhou, Conghui He, Lei Bai, Renqiu Xia, Song Mao, Tianshuo Peng, Bin Wang, Tao Chen, Mingsheng Li, Botian Shi, Bo Zhang, Xiangyu Yue, Renrui Zhang, Aojun Zhou

- ***What's New***: Chimera는 기존의 대형 멀티모달 모델(Large Multi-modal Models; LMMs)에 전문 분야 전용 전문가(Domain-Specific Experts)를 통합하여 일반주의 모델의 성능을 향상시키는 새로운 멀티모달 파이프라인입니다. 이를 통해 차트, 테이블, 수학 및 문서 도메인에서 최첨단 성능을 달성하며, 멀티모달 추론과 시각적 콘텐츠 추출 작업에서도 뛰어난 성과를 보입니다.
- ***Technical Details***: Chimera는 프로그레시브 트레이닝 전략을 사용하여 전문가 모델의 피처(feature)를 일반주의 LMM의 입력과 통합합니다. 잘 정렬된 일반 비주얼 인코더가 최적화 불균형을 초래하는 문제를 해결하기 위해 Generalist-Specialist Collaboration Masking(GSCM) 메커니즘을 도입했습니다. 여러 전문가 모델의 인코더를 단일 LMM에 통합하여 다양한 전문 지식을 효과적으로 결합합니다.
- ***Performance Highlights***: Chimera는 MathVista와 MathVerse 같은 멀티모달 추론 벤치마크에서 각각 64.9와 32.4의 정확도를 기록하며, 유사한 규모의 LMMs에서 새로운 최첨단(State-Of-The-Art; SOTA) 성과를 달성했습니다. 차트, 테이블 및 문서 도메인에서 시각적 콘텐츠 추출 작업에서도 기존의 전문가 모델을 능가하거나 대등한 성과를 보여줍니다.

### [MoViE: Mobile Diffusion for Video Editing](https://arxiv.org/abs/2412.06578)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06578.png)

Vote: 13

Authors: Amir Ghodrati, Amirhossein Habibian, Fatih Porikli, Ioannis Lelekas, Noor Fathima, Adil Karjauv

- ***What's New***: MoViE는 모바일에서 실행 가능한 최초의 비디오-투-비디오(방식)의 디퓨전 모델입니다. Xiaomi-14 Pro에서 초당 12프레임의 영상 편집이 가능하며, 메모리 제한이 없는 상태에서 긴 영상을 처리할 수 있습니다.
- ***Technical Details***: MoViE는 가벼운 오토인코더를 활용한 아키텍처 최적화를 통해 높은 효율성을 자랑합니다. 특히, 멀티모달 가이던스(Multimodal Guidance) 디스틸레이션을 도입해 한 번의 포워드 패스(Forward Pass)로 텍스트 및 이미지 가이던스를 사용하는 방법을 개발했습니다. 또한, 새로운 적대적 디스틸레이션 스키마를 통해 샘플링 단계를 줄여 모델의 제어가능성을 유지합니다.
- ***Performance Highlights***: MoViE는 기존 경쟁 모델들에 비해 FLOPs를 66% 절약하는 동시에, GPU 지연 시간을 단일 프레임 기준으로 10배 향상시켰습니다. Xiaomi-14 Pro에서의 테스트 결과, 초당 12.5 프레임이라는 인상적인 처리 속도를 보여주었습니다. 이는 최신 모바일 비디오 편집 애플리케이션에서 실시간 텍스트 기반 비디오 편집의 가능성을 실현하는 중요한 발전입니다.

### [LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation](https://arxiv.org/abs/2412.05148)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05148.png)

Vote: 8

Authors: Donald Shenaj, Ondrej Bohdal, Umberto Michieli, Mete Ozay, Pietro Zanuttigh

- ***What's New***: LoRA.rar는 하이퍼네트워크(Hypernetworks)를 통해 콘텐츠 및 스타일에 조건화된 이미지 생성을 위한 로우랭크 적응 파라미터(LoRAs)를 결합하는 새로운 방법입니다. 이 방법은 이미지 품질을 향상시키면서 결합 과정을 4000배 이상 가속화하여 실시간으로 개인화된 이미지를 생성할 수 있도록 합니다.
- ***Technical Details***: LoRA.rar는 소량의 0.5M 파라미터를 가진 하이퍼네트워크를 사전 훈련하여, 콘텐츠와 스타일 LoRAs의 임의의 조합에 대해 결합 계수를 예측합니다. 이는 새로운 콘텐츠-스타일 조합에 대해 테스트 시점에서 미세 조정 없이 일반화할 수 있습니다. 평가를 위해 기존 평가 지표의 한계를 분석하고, 멀티모달 대형 언어 모델(MLLM; Multimodal Large Language Models)을 사용한 새로운 평가 프로토콜을 제안합니다.
- ***Performance Highlights***: LoRA.rar은 MLLM 평가와 인간 평가에서 기존의 방법들보다 일관되게 더 높은 성능을 나타냅니다. 평균적으로 엑시스 테스트 정확도를 뛰어넘었으며, ZipLoRA 대비 더 짧은 시간 안에 더 많은 주제-스타일 조합에서 좋은 품질의 이미지를 생성할 수 있습니다.

### [UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics](https://arxiv.org/abs/2412.07774)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07774.png)

Vote: 16

Authors: Yuqian Zhou, Hui Ding, Nanxuan Zhao, Soo Ye Kim, Zhifei Zhang, Yijun Li, Qing Liu, Jianming Zhang, Xi Chen, Yilin Wang, Hengshuang Zhao, He Zhang, Zhe Lin

- ***What's New***: UniReal는 다양한 이미지 생성 및 편집 작업을 해결하기 위한 통합 프레임워크로, 기존 솔루션들이 작업마다 다른 접근 방식을 취하는 반면, UniReal은 이미지 작업을 비연속적인 비디오 생성으로 간주하여 하나의 일반적 형식으로 통합하고, 대규모 비디오 데이터를 통해 실제 세계의 동적 패턴을 학습하여 그림자, 반사, 포즈 변화 및 객체 상호작용을 고급스럽게 처리할 수 있습니다.
- ***Technical Details***: UniReal은 비디오 생성 모델의 구조를 기반으로 전체 집중(attention)을 사용하여 프레임 간의 관계를 모델링합니다. 각각의 입력 이미지 종류를 세 가지 축으로 구분하여 편집 대상 이미지, 삽입 또는 보존할 요소를 지닌 참조 이미지, 레이아웃 또는 형태를 규제할 조건 맵으로 나누고, 계층적 프롬프트 체계를 통해 여러 작업 간의 통합과 조합을 지원합니다.
- ***Performance Highlights***: UniReal은 다양한 이미지 생성 및 편집 작업에서 기존 작업 전용 모델들과 비교해 우수한 성능을 보여주며, 특히 CLIP(omnidirectional multimodal models)과 DINO(unsupervised vision transformers) 등의 벤치마크에서 뛰어난 결과를 기록합니다. 또한, UniReal은 데이터를 필요로 하지 않는 새로운 응용에서 출현 능력을 보입니다.

### [DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation](https://arxiv.org/abs/2412.07589)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07589.png)

Vote: 26

Authors: Jianzong Wu, Jingbo Wang, Xiangtai Li, Yunhai Tong, Chao Tang, Yanhong Zeng

- ***What's New***: DiffSensei는 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)과 확산 모델(Diffusion Models)을 결합하여 개인 맞춤형 만화 콘텐츠를 생성할 수 있는 새로운 프레임워크입니다. 기존의 한계를 극복하고 캐릭터 표현 및 상호작용을 세밀하게 제어할 수 있는 새로운 방식의 만화 생성 작업을 도입했습니다.
- ***Technical Details***: DiffSensei는 확산 기반 이미지 생성기와 멀티모달 대형 언어 모델을 통합하여 텍스트와 호환 가능한 캐릭터 맞춤 기능 어댑터로 역할을 수행합니다. 캐릭터의 피처를 세밀하게 조정하여 패널별 텍스트 큐에 맞춰 표현, 자세 및 동작을 유연하게 조정할 수 있습니다. MangaZero라는 대규모 데이터셋은 43,264개의 만화 페이지와 427,147개의 어노테이트된 패널을 포함하며, 다양한 캐릭터 상호작용과 움직임을 지원합니다.
- ***Performance Highlights***: DiffSensei는 자동 평가 지표(FID, CLIP, DINO-I, DINO-C)에서 기존 모델을 지속적으로 초과하며, 이야기 시각화 품질 면에서도 뛰어난 성능을 나타냈습니다. 특히, 사람의 선호도 조사에서 텍스트-이미지 정렬, 스타일 일관성, 캐릭터 일관성 및 이미지 품질에서 높은 평가를 받았습니다. 이를 통해 DiffSensei는 기존의 스토리 비주얼라이제이션 모델보다 발전된 기능을 수행하며 강력한 일반화를 입증했습니다.

### [Fully Open Source Moxin-7B Technical Report](https://arxiv.org/abs/2412.06845)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06845.png)

Vote: 7

Authors: Pu Zhao, Enfu Nan, Xuan Shen, Yue Chen, Timothy Rupprecht, Lei Lu, Yu Huang, Wei Wang, Zhenglun Kong, Sung-En Chang, Yixin Shen, Changdi Yang, Yanzhi Wang, Yong He, Yumei He, Xingchen Xu

- ***What's New***: Moxin 7B는 완전한 오픈소스 언어 모델(LLM)로, Model Openness Framework(MOF)에 따라 개발되어 완전한 공개 코드와 데이터셋을 제공합니다. 이는 GPT-4와 같은 독점 모델이 아닌 Mistral과 LLaMA와 같은 오픈소스 LLM이 발전하는 과정에서 투명성과 재현성의 가치를 중시합니다.
- ***Technical Details***: Moxin 7B 모델은 Mistral 아키텍처를 확장하여 36개의 블록과 Grouped-Query Attention(GQA) 및 Sliding Window Attention(SWA)을 포함하며 길이 제한 없이 긴 문맥을 처리합니다. ColossalAI를 통해 고속 훈련이 가능하며, SlimPajama와 같은 엄선된 대규모 텍스트 및 코드 데이터셋을 활용합니다. AdamW 옵티마이저와 코사인 학습률 감쇠를 사용하여 훈련을 최적화합니다.
- ***Performance Highlights***: Moxin-7B는 AI2 Reasoning Challenge, HellaSwag와 같은 다양한 벤치마크에서 LLaMA 2-7B 등 기존 7B 모델을 능가하는 성과를 기록했습니다. 특히, PIQA에서 82.24%의 점수를 기록하여 복잡한 추론 작업에서도 성능이 탁월함을 보여주었습니다. Chat 모델 역시 MTbench에서 Llama 2 13B와 같은 일부 강력한 모델과 비교하여 높은 성능을 보였습니다.

### [OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations](https://arxiv.org/abs/2412.07626)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07626.png)

Vote: 15

Authors: Conghui He, Chao Xu, Jiawei Zhu, Zhiyuan Zhao, Yuan Qu, Minghao Liu, Xiaomeng Zhao, Man Jiang, Zhongying Tu, Pei Chu, Botian Shi, Zhenxiang Li, Qunshu Lin, Hongbin Zhou, Rui Zhang, Linke Ouyang, Fan Wu, Bin Wang, Jin Shi, Bo Zhang

- ***What's New***: OmniDocBench는 PDF 문서 Parsing을 위한 새로운 멀티 소스 벤치마크로, 자동화된 문서 콘텐츠 추출의 발전을 목표로 합니다. 이 벤치마크는 아홉 가지 다양한 문서 유형과 19개의 레이아웃 카테고리 레이블, 14개의 속성 레이블을 포함하여 다양하고 포괄적인 평가를 가능하게 합니다.
- ***Technical Details***: OmniDocBench는 다양한 문서 페이지를 자동 주석, 수작업 검토 및 전문가 리뷰를 통해 고품질의 평가 세트로 구성하며, 레이아웃 감지와 콘텐츠 인식을 위한 세부 주석이 포함돼 있습니다. 인증된 데이터는 9가지 문서 페이지 타입, 여러 레이아웃 카테고리 및 속성(annotation)을 포함합니다.
- ***Performance Highlights***: MinerU와 Mathpix와 같은 파이프라인 도구는 텍스트 인식에서 뛰어난 성능을 보여주었으며, GPT-4o는 다목적 비전-언어 모델 중 최고 성능을 보였습니다. 다양한 문서 유형 및 특정 속성이 있는 페이지에서의 평가 결과는 모델이 상이한 시나리오에서 어떻게 성능을 발휘하는지에 대한 중요한 통찰력을 제공합니다.

### [ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance](https://arxiv.org/abs/2412.06673)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06673.png)

Vote: 8

Authors: Wei Zhang, Lu Hou, Junwei Yang, Chunwei Wang, Guansong Lu, Runhui Huang, Jianhua Han, Hang Xu

- ***What's New***: ILLUME는 하나의 통합된 대형 언어 모델(Multimodal Large Language Model; MLLM)로, 시각적 이해와 생성 능력을 결합시키고, 데이터 효율성을 향상시킨 새로운 비전 토크나이저(vision tokenizer)와 단계별 학습 절차를 통해 데이터셋 크기를 줄이면서도 경쟁력 있는 성능을 달성합니다. 특히, 모델이 스스로 생성한 이미지와 텍스트 설명 간의 일관성을 평가해 시너지를 높이는 자기 향상 멀티모달 정렬(scheme)을 도입하였습니다.
- ***Technical Details***: ILLUME는 시멘틱 정보를 결합한 비전 토크나이저를 사용하여 이미지-텍스트 정렬을 효율적으로 수행하고, 세 가지 단계의 학습 절차를 적용하여 모델을 훈련합니다. 첫 단계에서는 새로운 비전 어댑터를 초기화하며, 두 번째 단계에서는 1500만 개의 데이터로 통합된 이미지-텍스트 정렬을 수행합니다. 마지막 단계에서 모델은 다양한 태스크를 다룰 수 있도록 미세 조정됩니다. 또한, 자기 향상 멀티모달 정렬(scheme)은 모델이 스스로 생성한 내용과 텍스트 설명 간의 불일치를 평가하고 분석하도록 훈련하여, 이미지 해석 능력을 향상시킵니다.
- ***Performance Highlights***: ILLUME는 시각적 이해와 생성 벤치마크에서 최첨단 상태의 모델과 동등하거나 그 이상의 성능을 보입니다. 특히, MJHQ30K에서는 FID 점수 7.76을 기록하며 고품질 이미지 생성을 보여주었고, 다양한 시각 이해 벤치마크에서 높은 순위를 기록하였습니다. 이러한 결과들은 ILLUME의 효과적인 학습 설계와 멀티모달 정렬(scheme)의 잠재력을 나타냅니다.

### [Evaluating and Aligning CodeLLMs on Human Preference](https://arxiv.org/abs/2412.05210)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05210.png)

Vote: 40

Authors: Lei Zhang, Liqun Yang, Zeyu Cui, Yibo Miao, Yichang Zhang, Binyuan Hui, Jiaxi Yang, Junyang Lin, Ke Jin, Jian Yang

- ***What's New***: 이 연구는 코드 큰 언어 모델(Code LLMs)의 인간 선호도와의 정렬을 평가하기 위한 새로운 벤치마크인 CodeArena를 소개합니다. 이는 실제 사용자의 쿼리로부터 선별된 397개의 고품질 샘플을 포함하며, 코드 생성뿐만 아니라 인간 선호도와의 정렬 여부를 평가합니다. 또한 SynCode-Instruct라는 약 200억 토큰 규모의 다양한 합성 지시 코퍼스를 제안하여, 인스트럭션 기반 미세 조정의 효과성을 검증합니다.
- ***Technical Details***: CodeArena 벤치마크는 40개의 범주에 걸친 397개의 샘플로 구성되어 있으며, 사용자 쿼리로부터 비롯된 실제 문제의 복잡성과 다양성을 모사하는 것을 목표로 합니다. SynCode-Instruct는 웹소스를 통해 스케일된 인스트럭션을 기반으로 한 대규모 합성 지시 코퍼스입니다. 이 연구에서는 GPT4o를 활용하여 모델 성능을 평가했으며, 모형적으로 생성된 응답이 인간의 선호도를 얼마나 잘 반영하는지를 평가합니다.
- ***Performance Highlights***: CodeArena 벤치마크에서 오픈 소스 LLMs와 클로즈드 소스 LLMs 간에 상당한 성능 차이가 발견되었습니다. 특히 Qwen-Coder 등 오픈 소스 모델과 o1 및 Claude 시리즈 같은 클로즈드 소스 모델 사이의 명확한 성능 간극이 존재하였으며, 이는 코드 작업에서 AI 모델이 인간 선호도와의 정렬이 얼마나 중요한지를 강조합니다.

### [STIV: Scalable Text and Image Conditioned Video Generation](https://arxiv.org/abs/2412.07730)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07730.png)

Vote: 49

Authors: Cha Chen, Yizhou Sun, Chen Chen, Wenze Hu, Tsu-Jui Fu, Jesse Allardice, Kai-Wei Chang, Liangchen Song, Zhengfeng Lai, Yinfei Yang, Bowen Zhang, Wei Liu, Yifan Jiang, Jiasen Lu, Lezhi Li, Yiran Fei, Zongyu Lin

- ***What's New***: STIV는 텍스트-이미지 조건의 비디오 생성 방식을 제공하는 새로운 체계로, 프레임 교체를 통해 이미지 조건을 Diffusion Transformer(DiT)에 통합하고, 텍스트 조건은 이미지-텍스트 결합 분류기 없는 가이던스를 통해 적용합니다. 이를 통해 STIV는 텍스트-비디오(T2V)와 텍스트-이미지-비디오(TI2V) 작업을 동시에 수행할 수 있습니다.
- ***Technical Details***: STIV 프레임워크는 PixArt-α 기반으로, 변형 인코더(VAE)를 사용하여 입력 프레임을 공간 및 시간 임베딩으로 변환하며, DiT 같은 학습 가능한 블록을 통해 처리합니다. Factorized Spatial-Temporal Attention 기법을 사용해 공간과 시간 차원을 효과적으로 처리하며, Joint Image-Text Classifier-Free Guidance(JIT-CFG)를 통해 이미지와 텍스트 조건을 결합하여 가이던스를 제공합니다.
- ***Performance Highlights***: 8.7B 파라미터 모델은 VBench T2V에서 83.1 점을 기록하며, CogVideoX-5B 등을 능가했습니다. 같은 크기의 모델은 VBench I2V 작업에서 90.1 점을 기록하여 최고 성능을 보여줍니다. 이는 현재 존재하는 오픈 및 클로즈드 소스의 최신 모델보다 우수한 성능을 발휘함을 보여줍니다.

### [Video Motion Transfer with Diffusion Transformers](https://arxiv.org/abs/2412.07776)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07776.png)

Vote: 12

Authors: Alexander Pondaven, Aliaksandr Siarohin, Sergey Tulyakov, Philip Torr, Fabio Pizzati

- ***What's New***: DiTFlow는 동영상 확산 변환기(Diffusion Transformers; DiT)를 위한 최초의 동작 이전 방식으로, 참조 동영상의 이동을 새로 생성된 동영상으로 전송하는 방법입니다. 특히, 최적화 기반의 훈련이 필요 없는 전략을 채택하여 새로운 영상에서 제로-샷(Zero-shot) 방식으로 동작 이전을 가능하게 합니다.
- ***Technical Details***: DiTFlow는 사전 훈련된 DiT를 사용하여 참조 동영상의 주의력 지도를 분석하고 'Attention Motion Flow(AMF)'라는 패치 단위의 이동 신호를 추출합니다. 이 신호를 통해 영상 생성 과정에서 잠재 변수의 최적화를 유도하며, 특히 DiT의 위치 임베딩에 대한 최적화 전략을 통해 제로-샷 동작 이전을 가능하게 합니다.
- ***Performance Highlights***: DiTFlow는 최신 기법들과의 비교에서 뛰어난 성능을 보이며, 특히 Motion Fidelity(MF) 메트릭에서 높은 점수를 기록합니다. 두 가지 DiT 모델(2B와 5B) 모두에서 SMM, MOFT와 같은 기존 방법들보다 보여주며, 사용자 연구에서도 모션 일치도와 프롬프트 일치도에서 높은 평가를 받았습니다.

### [Maya: An Instruction Finetuned Multilingual Multimodal Model](https://arxiv.org/abs/2412.07112)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07112.png)

Vote: 13

Authors: Drishti Sharma, Alham Fikri Aji, Genta Indra Winata, Anthony Susevski, Roshan Santhosh, Ashvanth. S, Nahid Alam, Ryan Sze-Yin Chan, Snegha A, Isha Chaturvedi, Karthik Reddy Kanjula, Shayekh Bin Islam, Abhipsha Das, Surya Guthikonda, Timothy Chung, Chen Liu, Snehanshu Mukherjee, S M Iftekhar Uddin, Bala Krishna S Vegesna

- ***What's New***: Maya는 저자들이 제안한 오픈 소스 다중 언어 다중 모달 모델(Multilingual Multimodal Model)로, 특히 8개의 언어를 지원하는 다국어 이미지-텍스트 사전 학습 데이터셋을 소개합니다. 이 모델은 언어 및 문화적 이해를 향상시켜 현재의 VLM(Vision Language Model)들이 저자원이 언어 및 문화적 맥락에서 겪는 제한점을 해소합니다.
- ***Technical Details***: Maya는 LLaVA라는 기존 프레임워크를 기반으로 구축되었으며, 신규 사전 학습 데이터셋은 이미지-텍스트 조합의 다양성을 촉진하여 문화적으로 민감한 VLM 개발을 지원합니다. 데이터셋은 55만8000장의 이미지로 구성되었으며, 독창적인 유독성(토시시티)이 없는 데이터를 통해 더욱 안전한 훈련 코퍼스를 제공합니다. SigLIP을 비전 인코더로 사용하여 다중 언어와 조화를 이루도록 설계되었습니다.
- ***Performance Highlights***: Maya는 PALO 다국어 평가 세트에서 10개의 주요 언어를 대상으로 경쟁적으로 평가되었고, LLaVA-13B를 능가하며 PALO-13B와 비교해 거의 동일한 성능을 보였습니다. 비록 사전 학습 데이터셋은 8개의 언어로만 제한되었으나, PALO 지침 튜닝 데이터셋을 통해 10개의 언어에 대해 튜닝되었습니다. Maya는 특히 아랍어에서도 우수한 성능을 드러내었습니다.

### [Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment](https://arxiv.org/abs/2412.04835)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04835.png)

Vote: 2

Authors: Ran Tian, Yilin Wu, Jitendra Malik, Masayoshi Tomizuka, Chenfeng Xu, Andrea Bajcsy

- ***What's New***: 이 논문에서는 인간의 피드백 없이도 시각적 로봇 정책(Visuomotor Robot Policy)의 정렬을 효율적으로 학습할 수 있는 새로운 방법인 RAPL(Representation-Aligned Preference-based Learning)을 제안합니다. RAPL은 사전 학습된 비전 인코더(Vision Encoder)를 조정하여 사용자의 시각적 표현에 맞춘 다음, 특징 매칭을 통해 조밀한 시각적 보상을 구성합니다.
- ***Technical Details***: RAPL은 관측만을 통해 시각적 보상을 학습하는 방법으로, 인간의 선호 피드백을 최소화하면서도 사용자의 시각적 표현과 맞추는 방식으로 사전 학습된 비전 인코더를 세밀 조정합니다. 이 과정을 통해 RAPL은 최적의 전송을 기반으로 한 시각적 보상을 사용하여 로봇의 시각적 모터 정책을 정렬합니다. 실험은 X-Magical 벤치마크 및 Franka Panda 로봇 조작 작업을 통해 이루어졌습니다.
- ***Performance Highlights***: 시뮬레이션 및 하드웨어 실험에서 RAPL은 기존의 RLHF(Reinforcement Learning from Human Feedback)보다 훨씬 적은 인간의 데이터를 활용하여 높은 품질의 보상을 학습하였으며, 5배 적은 인간 선호 데이터를 사용하면서도 세 가지 물체 조작 작업에서 성공적으로 사전 학습된 확산 정책(Diffusion Policies)을 정렬할 수 있었습니다.

### [ObjCtrl-2.5D: Training-free Object Control with Camera Poses](https://arxiv.org/abs/2412.07721)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07721.png)

Vote: 7

Authors: Chen Change Loy, Yushi Lan, Zhouxia Wang, Shangchen Zhou

- ***What's New***: ObjCtrl-2.5D는 이미지-비디오 생성(image-to-video generation)에서 객체 제어(object control)의 정밀도와 다양성을 향상시키기 위한 학습이 필요 없는 새로운 접근법입니다. 본 연구에서는 기존의 2D 방식 대신, 깊이 정보가 포함된 3D 궤적(trajectory)을 제어 신호로 사용하며, 이를 통해 객체의 회전을 포함한 복잡한 운동 제어가 가능해졌습니다.
- ***Technical Details***: ObjCtrl-2.5D는 조건부 이미지에서 깊이 정보를 추출하여 2D 궤적을 3D 궤적으로 확장하고, 이를 카메라 포즈(camera poses)로 변환합니다. 이러한 카메라 포즈를 기반으로 기존의 카메라 모션 제어(image-to-video generation model)를 활용하여 추가적인 학습 없이 객체 모션 제어를 수행합니다. 더불어, 객체를 배경과 분리하는 레이어 제어 모듈(Layer Control Module; LCM)과 객체의 영역 내에서 프레임 간 와핑된 저주파수 잠재(latent)를 공유하는 방법을 도입하여 제어 정확도를 높였습니다.
- ***Performance Highlights***: 다양한 실험을 통해 ObjCtrl-2.5D는 기존 학습 기반 및 학습 없이 사용할 수 있는 방법들과 비교해 더 높은 제어 정확도를 보여주었습니다. 특히, 기하학적 투영 알고리즘을 사용하여 3D 궤적으로 변환된 카메라 포즈는 2D 방법보다 궤적 정렬에 있어서 유리한 결과를 냈습니다. 이러한 성능 향상은 사용자 지정 카메라 포즈를 수용함으로써 더욱 복잡한 객체 운동 제어를 가능케 합니다.

### [3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation](https://arxiv.org/abs/2412.07759)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07759.png)

Vote: 16

Authors: Menghan Xia, Xintao Wang, Xiao Fu, Dahua Lin, Xian Liu, Pengfei Wan, Xiaoyu Shi, Ziyang Yuan, Sida Peng, Di Zhang

- ***What's New***: 3DTrajMaster는 영상 생성에서 3D 공간 내 여러 개체의 움직임을 제어할 수 있는 새로운 프레임워크입니다. 이 모델은 사용자가 원하는 6자유도(Degrees of Freedom; 6DoF)의 위치와 회전 시퀀스를 기반으로 다수 개체의 3D 동작을 조작하는 능력을 특징으로 합니다.
- ***Technical Details***: 3DTrajMaster의 핵심은 다수의 3D 경로를 참고하여 개체를 삽입하는 플러그 앤 플레이 방식의 3D-모션 그라운디드 오브젝트 인젝터(3D-motion grounded object injector)입니다. 이 구성 요소는 사용자로부터 입력된 개체 별 3D 경로 시퀀스를 통해 각 개체를 제어합니다. 또한, 비디오 도메인 적응기(domain adaptor)를 사용하여 영상 품질 저하를 방지하고, 학습 데이터 부족 문제를 해결하기 위해 360도 모션 데이터셋을 구축하였습니다.
- ***Performance Highlights***: 다양한 실험 결과, 3DTrajMaster는 멀티-개체 3D 모션 제어에 있어 정확도와 일반화에서 현존하는 최고 수준의 성능을 나타냈습니다. 특히 여러 개체를 포함한 입력에서 우수한 성능을 발휘하며, 3D 차원의 회전과 위치를 정밀하게 조정할 수 있습니다.

### [GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis](https://arxiv.org/abs/2412.06089)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06089.png)

Vote: 2

Authors: Parag Singla, Ashish Goswami, Prathosh A. P, Harman Singh, Santhosh Rishi Deshineni, Satyam Kumar Modi

- ***What's New***: GraPE 프레임워크는 복합적인 텍스트-이미지(T2I) 생성 작업을 세 가지 단계로 분해하는 혁신적인 접근법을 제시합니다. 이는 기존의 Diffusion 모델을 이용한 이미지 생성, 무상 훈련 가능한 Multi-Modal LLM(MLLM)을 통한 오류 식별 및 수정 계획 수립, 텍스트 기반 이미지 편집 모델을 통한 수정 작업 수행의 과정을 포함합니다. 이 접근법은 어느 이미지 생성 및 편집 모델과도 결합할 수 있는 모듈형 특성을 가지고 있습니다.
- ***Technical Details***: GraPE 프레임워크는 초기 이미지 생성을 위해 STOTA(Diffusion) 모델을 사용하며, MLLM을 통해 개별 객체와 특성을 기반으로 오류를 식별하고 수정 계획을 수립합니다. 이어서 각 수정 계획을 순차적으로 수행해 최종 이미지를 생성합니다. 프레임워크는 PixEdit와 AURORA를 포함한 강력한 편집 모델을 활용하여 높은 성능을 제공합니다.
- ***Performance Highlights***: GraPE는 다양한 T2I 모델 성능을 최대 20+ 포인트 향상시키며 특히 구조가 작은 모델에서 더 큰 개선 효과를 보여줍니다. 또한, 복합적 텍스트 프롬프트에서 더욱 효율적으로 작동하며, Flicker-Bench와 같은 일반 T2I 태스크에서도 2%에서 23%의 성능 향상이 관측되었습니다.

### [FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models](https://arxiv.org/abs/2412.07674)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07674.png)

Vote: 17

Authors: Tong Wu, Mengchen Zhang, Ryan Po, Guandao Yang, Dahua Lin, Jiaqi Wang, Yinghao Xu, Ziwei Liu, Gordon Wetzstein

- ***What's New***: FiVA는 텍스트-이미지 변환(diffusion) 모델을 위한 세밀한 시각적 속성 데이터셋입니다. 이 데이터셋은 사용자가 원천 이미지의 다양한 시각적 속성을 조합하여 새로운 이미지 생성에 활용할 수 있도록 지원하며, 시각적 속성에 대한 잘 정리된 분류 체계를 가지고 있으며, 약 100만 개의 고품질 이미지와 시각적 속성 주석을 포함하고 있습니다.
- ***Technical Details***: FiVA 데이터셋은 시각적 속성의 체계적인 분류를 포함하며, 다양한 시각적 속성을 포함하는 텍스트 프롬프트를 생성하여 이미지 데이터 페어를 자동 생성하는 파이프라인을 구축했습니다. 또한, FiVA-Adapter라는 프레임워크를 제안하여 여러 원천 이미지에서 분리된 시각적 속성을 결합하여 보다 사용자 친화적인 맞춤형 이미지 생성이 가능합니다. Q-former라는 멀티모달 인코더를 이미지 피처 인코더에 통합하여 시각적 속성을 추출하고 이를 대상 이미지에 적용할 수 있습니다.
- ***Performance Highlights***: 실험 결과, FiVA-Adapter는 속성 추출에서 높은 정확도를 보였으며 목표 프롬프트에 대한 높은 텍스트 정렬과 유연성을 자랑합니다. 사용자 스터디와 CLIP 점수 측정에서, FiVA-Adapter는 기존 방법들보다 주제 정확도와 속성 결합 조건에서 우수한 성과를 보여 새로운 태스크에 대한 도전적 과제를 해결하는 데 기여했습니다.

### [Hidden in the Noise: Two-Stage Robust Watermarking for Images](https://arxiv.org/abs/2412.04653)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.04653.png)

Vote: 20

Authors: Niv Cohen, Kasra Arabi, Chinmay Hegde, Benjamin Feuer, R. Teal Witter

- ***What's New***: 이번 연구에서는 독특한 소음을 이용한 손실 없는 이미지 워터마킹 방법과 효율성을 높이기 위한 이단계 워터마킹 프레임워크가 제안되었습니다. 이 방법은 이미지 생성 시 초기 소음(initial noise)과 생성된 푸리에 패턴(Fourier patterns)을 사용하여 높은 위조 및 제거 공격에 대한 강인성을 제공합니다.
- ***Technical Details***: 이 연구는 초기 소음을 사용하여 워터마킹을 수행하는 방식을 도입하며, 이를 통해 변형 없는 이미지를 생성할 수 있습니다. 또한, WIND라는 두 단계의 워터마킹 방법을 소개하여 초기 소음을 그룹화하고 효율적으로 탐색 공간을 줄였습니다. 생성 단계에서는 비밀 salt와 인덱스를 사용하여 소음을 생성하고 생성된 소음의 그룹 인덱스를 푸리에 패턴으로 삽입합니다. 이후 검출 단계에서는 재구성한 소음을 이용하여 해당 그룹 인덱스를 찾아내고, 해당 그룹에서 대응되는 초기 소음을 검색합니다.
- ***Performance Highlights***: WIND의 경우 다양한 이미지 변형 공격에서도 최고 수준의 강인한 성능을 보여주었으며, 제거 및 위조 공격에 대한 저항력을 입증했습니다. 특히 JPEG 압축, 회전, 노이즈 추가 등의 변형에서도 높은 워터마크 인식 정확도를 달성했습니다. 나아가, WIND는 기존의 트리-링(Tree-Ring)과 링ID(RingID)를 사용한 워터마킹 방법보다 우수한 검출 정확도를 제공했습니다.

### [Granite Guardian](https://arxiv.org/abs/2412.07724)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07724.png)

Vote: 14

Authors: Manish Nagireddy, Inge Vejsbjerg, Muhammad Zaid Hameed, Giulio Zizzo, Werner Geyer, Kieran Fraser, Inkit Padhi, Erik Miehling, Giandomenico Cornacchia, Tejaswini Pedapati, Kush R. Varshney, Elizabeth M. Daly, Michael Desmond, Pierre Dognin, Ambrish Rawat, Prasanna Sattigeri, Mark Purcell, Martín Santillán Cooper, Qian Pan, Michael Hind, Subhajit Chaudhury, Keerthiram Murugesan

- ***What's New***: 그레나이트 가디언(Granite Guardian) 모델은 대형 언어 모델(LLM)의 입력 및 출력에서 발생할 수 있는 위험을 자동으로 감지하여 LLM의 안전하고 책임 있는 사용을 가능하게 합니다. 이 모델은 사회적 편향, 욕설, 폭력, 성적 내용, 비윤리적 행동, 탈출 위험(jailbreaking), 문맥 관련성, 근거 및 답변 관련성과 같은 다양한 위험 차원을 포괄하며, 특히 문맥 관련성과 RAG(Retrieval-Augmented Generation) 관련 문제를 해결합니다. 또한, 공개적으로 제공되어 사회적 책임 있는 AI 개발을 우선으로 하려는 커뮤니티에 기여합니다.
- ***Technical Details***: 그레나이트 가디언 모델은 그레나이트 3.0 언어 모델을 기반으로 하여 2B 및 8B 버전으로 제공되며, 사람이 주석한 데이터 및 합성 데이터를 조합한 독창적인 데이터세트로 학습됩니다. 모델은 악의적인 공격 및 RAG 관련 위험을 위한 합성 데이터를 생성하며, 이는 실제 사례에 대한 안전 조치를 강화합니다. 이 모델들은 AUC 점수에서 0.871 및 0.854를 기록하여, 보호장치 및 RAG 환각 관련 벤치마크에서 최첨단 위험 감지를 달성합니다.
- ***Performance Highlights***: 그레나이트 가디언 8B 모델은 상위 분류에서 AUC 0.871을 기록하며, 경쟁 모델 대비 성능이 뛰어난 것으로 나타났습니다. 기저 데이터셋의 조화로운 평가에서 우수한 AUPRC 0.846을 달성하며, 특히 악의적인 위험 감지 측면에서 높은 평균 AUC를 확보하여, 강력한 보안 정렬을 달성하였습니다. 플래그십 8B 모델은 RAG 경우에서 지표 대비 우수한 성능을 보였으며, 전체적으로 매우 개선된 안전 배정 성능을 입증하였습니다.

### [EMOv2: Pushing 5M Vision Model Frontier](https://arxiv.org/abs/2412.06674)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06674.png)

Vote: 10

Authors: Haoyang He, Teng Hu, Yabiao Wang, Yong Liu, Xiangtai Li, Chengjie Wang, Zhucun Xue, Jiangning Zhang, Dacheng Tao

- ***What's New***: EMOv2는 경량 모델 설계 관점에서 새로운 인프라를 제시하며, 5M 규모의 경량 시각 모델 성능 한계를 탐색합니다. 이를 위해 IRB를 기반으로 하는 CNN과 Transformer 모델의 장점을 통합하여 하나의 Meta Mobile Block(MMBlock)을 구축하였습니다.
- ***Technical Details***: 본 연구는 MobileNetV2의 Inverted Residual Block(IRB)와 Transformer의 Multi-Head Self-Attention(MHSA) 모듈을 통합하여 Meta Mobile Block(MMBlock)을 추상화하였습니다. 또한, i2RMB라 불리는 개선된 Inverted Residual Mobile Block을 설계하여, SEW-MHSA를 통해 원거리와 이웃 정보를 동시에 모델링할 수 있는 확장 윈도우 주의를 도입하였습니다.
- ***Performance Highlights***: EMOv2는 ImageNet-1K에서 분류 태스크의 경우, 효율적인 다른 방법들에 비해 Top-1 정확도가 79.4로 탁월하며, MobileNet 및 MobileViT 시리즈를 포함한 기존 모델들보다 거의 모든 가운데 뛰어난 성능을 발휘합니다. 또한 MS-COCO 2017 데이터셋에서 물체 탐지 성능은 SSDLite와 RetinaNet을 활용하여 전 세대 모델 및 최신 경량 모델들보다 월등히 높은 정확도를 보였습니다.

### [Perception Tokens Enhance Visual Reasoning in Multimodal Language Models](https://arxiv.org/abs/2412.03548)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.03548.png)

Vote: 10

Authors: Zelun Luo, Cheng-Yu Hsieh, Dongping Chen, Ethan Shen, Mahtab Bigverdi, Linda G. Shapiro, Ranjay Krishna

- ***What's New***: Perception Tokens는 언어가 부족한 상황에서 시각적 추론을 강화하기 위해 도입된 본질적인 이미지 표현 방식입니다. 이는 대형 멀티모달 언어 모델(MLM)에 보조 추론 토큰으로 작용하여, 예를 들어 깊이 관련 작업에서 Perception Tokens를 통해 깊이 맵을 생성함으로써 문제 해결 능력을 향상시킵니다. AU-RORA라는 새로운 훈련 방법이 개발되었으며, 이는 MLM의 시각적 입력에 대한 추론 능력을 강화하기 위해 Perception Tokens를 활용합니다.
- ***Technical Details***: AU-RORA는 MLM을 시각적 인지 토큰을 사용하여 훈련하는 알고리즘입니다. VQVAE를 통해 중간 이미지 표현(깊이 맵 등)을 토큰화하며, 이 토큰화를 다중 작업 훈련 프레임워크에서 사용합니다. 기본 LLaVA 모델을 기반으로 하여, 시각적 추론 작업을 수행하도록 MLM을 보조합니다. 이 과정에서 시각적 인지 토큰을 체인 오브 생각(CoT) 토큰처럼 사용하여 추론상의 장벽을 극복합니다.
- ***Performance Highlights***: AU-RORA는 카운팅 벤치마크에서 BLINK에서 +10.8%, CVBench에서 +11.3%, SEED-Bench에서 +8.3%의 성능 향상을 달성했으며, 데이터셋 전반에 대한 일반화에서 파인 튜닝 접근 방식을 능가했습니다. 또한 상대적 깊이 추정에서 BLINK에서 6% 이상의 성능 향상을 보여, 시각 기반 추론에서의 성능을 크게 개선하였습니다.

### [A New Federated Learning Framework Against Gradient Inversion Attacks](https://arxiv.org/abs/2412.07187)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.07187.png)

Vote: 1

Authors: Xiaodan Zhang, Wenhao Chen, Yuyin Zhou, Liangqiong Qu, Shuang Zeng, Pengxin Guo, Weihong Ren

- ***What's New***: Gradient Inversion Attacks (GIA)를 방어하기 위한 새로운 페더레이티드 러닝(Federated Learning; FL) 프레임워크인 HyperFL가 소개되었습니다. 이 프레임워크는 공유 매개변수와 지역 개인 데이터 간의 직접적인 연결을 끊음으로써 프라이버시 보호를 향상시킵니다.
- ***Technical Details***: HyperFL는 하이퍼네트워크(Hypernetworks)를 사용하여 지역 모델의 매개변수를 생성하고, 하이퍼네트워크 매개변수만 서버에 업로드하여 집계를 진행합니다. 이렇게 함으로써 모델의 분산 학습 과정에서 개인 데이터의 노출을 방지합니다. 각 로컬 모델은 공유된 특징 추출기와 비공개 분류기로 분해되며, 하이퍼네트워크는 특징 추출기 매개변수를 사설 클라이언트 임베딩에서 생성합니다.
- ***Performance Highlights***: 본 연구는 새로운 HyperFL 프레임워크가 GIA에 대한 높은 프라이버시 보호 능력을 유지하면서도 FedAvg와 유사한 성능을 달성했음을 이론적 분석과 광범위한 실험 결과를 통해 입증합니다. HyperFL는 기존의 차분 프라이버시(Differential Privacy; DP) 기반 방법들과 비교해 데이터 유출 방어 능력이 높고 성능 유틸리티 손실이 적습니다.
- ***Conclusion***: HyperFL는 기존 방어 메커니즘에 의존하지 않고도 GIA를 효과적으로 방어함으로써 프라이버시와 유틸리티 간의 균형을 제공합니다. 연구자들이 향후 강화된 프라이버시 보호 FL 프레임워크를 개발하는 데 있어 방향성을 제시할 것으로 기대됩니다.

