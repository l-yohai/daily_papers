## Daily Papers (2024-12-13)

### [Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition](https://arxiv.org/abs/2412.09501)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09501.png)

Vote: 8

Authors: Shu Liu, Longxiang Tang, Tianyuan Qu, Yuechen Zhang, Senqiao Yang, Chengyao Wang, Yanwei Li, Yukang Chen, Jingyao Li, Yuqi Liu, Zhisheng Zhong, Shaozuo Yu, Sitong Wu, Jiaya Jia, Eric Lo

- ***What's New***: Lyra는 다중 모달 대형 언어 모델(Multimodal Large Language Models; MLLM)에서 스피치 중심 역량을 강화한 새로운 프레임워크로, 오디오 및 비디오와 같은 긴 콘텐츠를 처리하는 기능을 추가했습니다. 기존의 모델들이 스피치를 잘 통합하지 못한 부분을 획기적으로 개선하여 다양한 비전-언어, 비전-스피치, 스피치-언어 벤치마크에서 최첨단 성능을 달성했습니다.
- ***Technical Details***: Lyra는 다중 모달 라이즈드 적응(Multi-Modality LoRA) 모듈과 잠재 다중 모달 정규화기(Latent Cross-Modality Regularizer), 잠재 다중 모달 추출기(Latent Multi-Modality Extractor)를 사용하여 스피치와 다른 모달리티 간의 관계를 강화했습니다. 고품질의 광범위한 데이터셋에는 약 150만 개의 다중 모달 데이터 샘플과 12K 긴 스피치 샘플이 포함되어 있어 복잡한 긴 스피치 입력을 처리할 수 있습니다.
- ***Performance Highlights***: Lyra는 다양한 벤치마크에서 뛰어난 성능을 입증했습니다. 특히, 이미지-언어 및 이미지-스피치, 스피치-언어 작업에서 기존 모델들보다 높은 결과를 보이며, 더 적은 데이터와 연산 자원으로도 우수한 성능을 발휘합니다. 긴 문맥 콘텐츠 처리 성능 또한 기존 모델 대비 크게 향상된 점이 주요 성과로 평가됩니다.

### [Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages](https://arxiv.org/abs/2412.09025)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09025.png)

Vote: 2

Authors: Advait Joglekar, Srinivasan Umesh

- ***What's New***: Shiksha는 8개의 인도 언어에 대해 영어-인도 및 인도-인도 번역 쌍을 포함하는 280만 개 이상의 고품질 번역 데이터를 생성하였고, 이를 통해 과학적, 기술적, 교육적 도메인의 번역 태스크에서 성능을 향상시켰습니다. 또한, 이 데이터셋을 활용한 NMT(Natural Machine Translation) 모델도 개발되었습니다.
- ***Technical Details***: 이 연구는 NPTEL(National Programme on Technology Enhanced Learning)의 인간 번역 비디오 강의 자막을 비텍스트 마이닝(bitext mining)하여 행한 것입니다. 데이터는 특수한 알고리즘을 사용하여 정렬되었으며, 36개의 언어 쌍을 처리할 수 있는 대량의 병렬 코퍼스를 형성하였습니다. 모델은 LoRA (Low-Rank Adaptation)를 사용하여 매개변수 효율적 미세 조정을 통해 트레이닝되었습니다.
- ***Performance Highlights***: Shiksha 모델은 자체 테스트 세트에서 기존 NLLB 및 IndicTrans2 모델을 능가하는 성능을 보였으며, Flores+ 벤치마크에서도 일반 번역 태스크에 대해 평균적으로 모수보다 2 BLEU 이상 향상된 결과를 보여주었습니다. 이는 다양한 언어 쌍에서 우수한 일관성을 나타냅니다.

### [Word Sense Linking: Disambiguating Outside the Sandbox](https://arxiv.org/abs/2412.09370)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09370.png)

Vote: 4

Authors: Roberto Navigli, Luigi Procopio, Edoardo Barba, Andrei Stefan Bejgu, Alberte Fernández-Castro

- ***What's New***: 이 논문에서는 Word Sense Linking (WSL)이라는 새로운 태스크를 소개합니다. 이 태스크는 주어진 텍스트와 참조 의미 인벤토리(reference sense inventory)를 기반으로 명확하게 구별되지 않은 범위(spans)를 식별하고 가장 적합한 의미로 연결하도록 요구합니다. 이는 기존 Word Sense Disambiguation (WSD)의 제약을 완화하여 실제 응용 프로그램에 더 가깝게 다가가도록 설계되었습니다.
- ***Technical Details***: WSL은 Concept Detection (CD), Candidate Generation (CG), Word Sense Disambiguation (WSD)의 세 가지 서브태스크로 나누어 수행됩니다. 모델은 처음에 전체 텍스트에 대해 CG를 수행하여 가능한 후보 의미 목록을 생성하고, CD를 통해 텍스트에서 명확히 구분되지 않은 범위를 식별합니다. 그런 다음, 각각의 범위를 가장 적절한 의미와 연결합니다. 제안된 구조는 retriever-reader 패러다임에 기반하며, 모든 범위와 후보를 동시에 처리할 수 있습니다. 이는 평가 데이터셋과 함께 다양한 설정 하에 그 성능이 평가됩니다.
- ***Performance Highlights***: WSL에서 제안된 모델은 기존 WSD 시스템의 확장과 비교하여 상당한 성능 향상을 보여줍니다. 모든 세팅에서 높은 점수를 기록하며, 특히 실제 응용에서 사용되기에 적합한 속도와 정확성을 갖춥니다. 비교된 모델들과 달리 문맥화된 후보 생성을 통해 더 강력한 표현을 돕습니다.

### [Phi-4 Technical Report](https://arxiv.org/abs/2412.08905)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08905.png)

Vote: 40

Authors: Anh Nguyen, Caio C. T. Mendes, Yue Wu, Russell J. Hewett, Piero Kauffmann, Marah Abdin, Dingli Yu, Adil Salim, Shital Shah, Ronen Eldan, Mojan Javaheripi, James R. Lee, Rachel Ward, Harkirat Behl, Xin Wang, Cyril Zhang, Gustavo de Rosa, Sébastien Bubeck, Suriya Gunasekar, Jyoti Aneja, Eric Price, Yi Zhang, Yin Tat Lee, Yuanzhi Li, Michael Harrison, Olli Saarikivi, Weishung Liu

- ***What's New***: Phi-4는 데이터 품질에 중점을 두고 개발된 140억 매개변수 언어 모델로, 합성 데이터(synthetic data)를 전략적으로 통합하여 STEM 중심 QA 능력에서 기존의 교사 모델(GPT-4)을 능가합니다.
- ***Technical Details***: Phi-4는 합성 데이터에 중점을 두어 훈련되어, 문제 해결 능력을 강화합니다. 훈련 과정에는 다중 에이전트 프롬프트(multi-agent prompting), 자기 수정(self-revision), 지시 반전(instruction reversal) 기술 등이 활용되었습니다. 세 가지 핵심 기둥으로 훈련: 합성과 유기적(organic) 데이터의 선별 필터링을 통한 훈련, 새로운 사후 훈련(post-training) 기법 도입입니다.
- ***Performance Highlights***: Phi-4는 여러 표준 벤치마크에서 비슷한 크기의 언어 모델들보다 뛰어난 성능을 보였으며, 특히 GPQA와 MATH 같은 논리력 중심의 벤치마크에서 우수한 성과를 나타냈습니다. 또한, Phi-4는 2024년 AMC 테스트에서 크기 대비 높은 성능을 보여 과적합 없이 경쟁 모델들을 능가했습니다.

### [Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion](https://arxiv.org/abs/2412.09593)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09593.png)

Vote: 13

Authors: Xingang Pan, Xin Huang, Ziwei Liu, Zexin He, Tengfei Wang

- ***What's New***: Neural LightRig는 단일 이미지에서 객체의 표면 노멀과 PBR 소재를 추정하기 위해 multi-light diffusion 모델을 활용하는 혁신적인 프레임워크를 소개합니다. 이 접근법은 다중 조명 조건을 시뮬레이션하여 복잡한 역반사 문제를 해결합니다.
- ***Technical Details***: Neural LightRig는 학습된 대규모 diffusion models의 조명 프라이어를 활용하여 synthetic relighting dataset에서 다중 조명 diffusion 모델을 구축합니다. 이 모델은 여러 방향으로 포인트 조명된 일관된 이미지를 생성하며, 특정 U-Net backbone을 갖춘 large G-buffer 모델을 사용하여 표면 노멀과 소재를 예측합니다. 또한, 학습 및 예측 사이의 도메인 차이를 줄이기 위해 다양한 데이터 증가 전략을 적용합니다.
- ***Performance Highlights***: 정량적 평가 결과, Neural LightRig는 기존 최신 방법론을 크게 능가하며, 특히 표면 노멀 및 PBR 소재 추정과 단일 이미지 relighting에서 우수한 성능을 보입니다. 추가적인 실험 결과는 프로젝트 페이지에서 제공됩니다.

### [SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training](https://arxiv.org/abs/2412.09619)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09619.png)

Vote: 14

Authors: Sergey Tulyakov, Yerlan Idelbayev, Huseyin Coskun, Anujraaj Goyal, Xijie Huang, Yanyu Li, Dongting Hu, Yanwu Xu, Junli Cao, Rajesh Singh, Anil Kag, Aarush Gupta, Arpit Sahni, Dishani Lahiri, Jierun Chen, Kwang-Ting Cheng, Mingming Gong, Jian Ren, S. -H. Gary Chan

- **What's New**: SnapGen은 모바일 디바이스에서 고해상도 텍스트-이미지 생성 모델을 효율적인 구조 및 학습 기법을 통해 구현한 혁신적인 연구입니다. 초소형 및 고속 텍스트-이미지(T2I) 모델이 모바일 플랫폼에서 고화질 이미지를 생성할 수 있도록 설계되었습니다.
- **Technical Details**: SnapGen은 네트워크 아키텍처 선택을 체계적으로 검토하여 모델 매개변수와 지연 시간을 줄이는 동시에 고품질 생성을 보장합니다. 크로스 아키텍처(Knowledge Distillation from a large model)를 활용하여 멀티레벨 방법으로 학습을 가이드합니다. Adversarial Guidance와 Knowledge Distillation을 결합하여 빠른 생성을 가능하게 합니다.
- **Performance Highlights**: SnapGen은 모바일 디바이스에서 1.4초 만에 1024² px 이미지 생성을 처음으로 구현했으며, 372M 매개변수로 ImageNet-1K에서 2.06의 FID를 기록했습니다. GenEval 및 DPG-Bench와 같은 텍스트-이미지 벤치마크에서 수십억 매개변수의 대형 모델을 능가합니다.

### [RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios](https://arxiv.org/abs/2412.08972)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08972.png)

Vote: 7

Authors: Sitao Cheng, Liangming Pan, Ruiwen Zhou, William Yang Wang, En Yu, Wenyue Hua, Xiaobao Wu

- ***What's New***: RULEARENA는 복잡한 현실 세계의 규칙을 따르는 대형 언어 모델(LLMs)의 추론 능력을 평가하기 위한 새로운 벤치마크입니다. 항공사 수하물 요금, NBA 거래, 세금 규정을 포함한 세 가지 실제 도메인을 다루며, LLMs가 긴 맥락 이해, 논리적 추론, 정확한 수학적 계산을 수행할 수 있는 능력을 평가합니다.
- ***Technical Details***: RULEARENA는 세 가지 대표적인 현실 세계 시나리오에서 쌍으로 제공되는 정답을 갖춘 816개의 테스트 문제로 구성되어 있습니다. LLMs는 도메인별 작업 지침과 참조 규칙이 제공된 상태에서 문제를 해결해야 하며, 모든 문제는 최신 LLMs의 규칙 기반 추론 능력을 평가하도록 설계되어 있습니다.
- ***Performance Highlights***: 현재의 최첨단 LLMs, 예를 들어 GPT-4o나 Claude-3.5 Sonnet과 같은 모델들은 복잡한 규칙이 필요한 추론 작업에서 실패율이 높으며, 여러 규칙이나 사실을 통합하는데 어려움을 겪고 관련 없는 정보에 쉽게 혼란을 느낍니다. 일반적으로 문제별 정답 정확도(Acc(t))는 만족스럽지 못한 상태이며, 해결이 가장 간단한 테스트에서도 낮은 성공률을 보입니다.

### [JuStRank: Benchmarking LLM Judges for System Ranking](https://arxiv.org/abs/2412.09569)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09569.png)

Vote: 9

Authors: Ariel Gera, Odellia Boni, Roy Bar-Haim, Lilach Eden, Asaf Yehudai, Yotam Perlitz

- ***What's New***: JuStRank는 새로운 대규모 벤치마크(JuStRank)로 LLM 기반 판사들이 시스템을 순위 매기는 능력을 평가합니다. JuStRank는 최첨단 판사(Judges)를 비교하여 모델을 정확하게 순위 매기는 능력을 평가하고 인간 기반의 순위와의 비교를 통해 품질을 확인합니다.
- ***Technical Details***: 이 연구는 여러 시스템의 출력을 합산하여 판사의 품질을 평가하고 인간 기반의 순위와 비교하여 시스템의 전체 품질을 평가하는 시스템 기반 평가 방식(system-level evaluation)을 도입합니다. JuStRank는 48개의 최신 판사들을 포함하며, 일반적인 LLMs와 보상 모델이 포함되어 있습니다.
- ***Performance Highlights***: 최고 성능을 보인 판사들은 인간 순위와의 Kendall의 순위 상관계수로 측정했을 때 높은 일치를 보였으며, 몇몇 8B 파라미터 보상 모델들은 훨씬 큰 LLM 모델들과 동등한 순위 능력을 보였습니다. 수집된 데이터를 통한 세부적인 분석은 판사의 결정력과 편향성을 평가합니다. 일부 판사는 특정 시스템에 대해 긍정적 혹은 부정적 편향을 보이는 경향이 있었습니다.

### [Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders](https://arxiv.org/abs/2412.09586)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09586.png)

Vote: 5

Authors: Daniel Bolya, James M. Rehg, Judy Hoffman, Sangmin Lee, Fiona Ryan, Ajay Bati

- ***What's New***: Gaze-LLE는 대규모로 학습된 인코더를 통해 시선 목표를 추정하는 새로운 방법론을 제안합니다. 기존의 멀티 브랜치 방식을 간소화하여 하나의 DINOv2 이미지 인코더로 통합된 피처 표현을 사용하고, 블록의 위치에 따라 시선 목표를 디코딩하는 접근 방식을 도입하였습니다.
- ***Technical Details***: Gaze-LLE는 DINOv2 인코더로부터 포착된 장면의 피처 표현을 사용하여 연산량이 적은 모듈로 시선을 디코딩합니다. 사람의 머리 위치를 조건화하는 방법으로 '위치 인코딩(Head Positioning)'을 사용하며, 피처 표현을 업데이트하고 시선 해트맵을 예측하는 경량 트랜스포머 모듈을 사용합니다.
- ***Performance Highlights***: Gaze-LLE는 주요 시선 추정 벤치마크에서 최첨단 성능을 달성하여, 이전의 방법들보다 학습 가능한 파라미터 수를 1~2차례 줄이면서도 효율성을 개선하였습니다. 모델은 GazeFollow 데이터셋에서 에러율이 높을 경우에도 경쟁력 있는 성능을 보여주며, 1.5시간 미만의 GPU 시간으로 최적의 성능을 달성했습니다.

### [OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation](https://arxiv.org/abs/2412.09585)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09585.png)

Vote: 6

Authors: Humphrey Shi, Jianwei Yang, Zhengyuan Yang, Jitesh Jain, Jianfeng Gao

- ***What's New***: 이 연구는 OLA-VLM을 제안하며, 기존의 멀티모달 대형 언어 모델(Multimodal Large Language Models; MLLMs)에 비해 개선된 시각적 표현을 얻기 위해 중간에서 시각 정보를 증류해 넣는 접근법을 최초로 제시하였습니다. 이는 모델이 보다 효율적인 시각적 인식을 할 수 있도록 돕습니다.
- ***Technical Details***: OLA-VLM 모델은 예측 임베딩 최적화를 사용해 LLM의 중간 표현에 시각적 정보를 증류하는 방식을 채택합니다. 모든 실험은 CLIP-ViT-L과 Llama3-8b를 기본 비전 인코더와 언어 모델로 하는 조건에서 수행되었으며, 인지 예측(embedding prediction) 손실을 최적화해 중간 표현을 개선하도록 설계되었습니다.
- ***Performance Highlights***: CV-Bench 벤치마크에서 OLA-VLM은 기존 LLaVA-1.5 모델에 비해 시각 정보의 품질을 개선하였으며, 심층 및 거리 인식 작업에서 최대 8.7% 성능이 향상되었습니다. 또한 다양한 벤치마크에서 평균적으로 최대 2.5%까지 성능을 향상시키는 것으로 나타났습니다.

### [SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts](https://arxiv.org/abs/2412.05552)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05552.png)

Vote: 2

Authors: Zun Wang, Yicong Hong, Mohit Bansal, Chongyang Zhao, Qi Wu, Gengze Zhou

- **What's New**: 이번 연구에서는 다양한 네비게이션 작업을 통합하여 일반적인 언어 안내 시각 네비게이션(language-guided visual navigation)의 프레임워크를 제안합니다. 특히, 상태 적응적 전문가 혼합(State-Adaptive Mixture of Experts; SAME) 모델을 통해 언어의 다양한 세밀도와 동적 관찰에 기반하여 결정을 할 수 있는 에이전트를 소개합니다. SAME은 서로 다른 언어와 시각적 관찰을 처리하여 포괄적인 언어 지침을 효과적으로 해석하고 실행할 수 있는 다목적 시스템을 제공합니다.
- **Technical Details**: SAME 모델은 Mixture of Experts(MoE) 접근법에서 영감을 받아 개발되었습니다. 이는 단계마다 에이전트 상태에 따라 전문가 네트워크를 선택하도록 하는 라우팅 메커니즘에 의해 활성화되는 것이 특징입니다. 이러한 방법은 탐색과 명령 추종 등 다양한 네비게이션 스킬을 효과적으로 학습하도록 하며, 많은 에이전트가 다중 시각적 관찰에 의존해야 하는 점을 고려해 시각적 질의에 MoE를 적용했습니다.
- **Performance Highlights**: SAME 모델은 7개의 주요 언어 안내 네비게이션 작업을 동시에 처리하며, 기존의 일부 작업 전용 모델과 비교해 동등거나 더 우수한 성능을 보였습니다. R2R, RxR-EN, REVERIE 등에서 평균 성공률이 3% 증가하였으며, OBJECTNAV-MP3D에서도 42.7% SPL을 기록하여 향상된 성능을 보입니다.

### [LoRACLR: Contrastive Adaptation for Customization of Diffusion Models](https://arxiv.org/abs/2412.09622)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09622.png)

Vote: 4

Authors: Thomas Hofmann, Enis Simsar, Federico Tombari, Pinar Yanardag

- ***What's New***: LoRACLR은 멀티 LoRA(low-rank adaptation) 모델들을 결합하여 단일 모델로 다양한 개념을 동시에 생성할 수 있는 새로운 방법을 제시합니다. 이 기법은 각 모델의 가중치 공간을 정렬하고 병합하는 대조적 목표를 사용하여 개념 간의 상호 간섭을 최소화하며, 기존의 LoRA 모델을 재학습 없이 사용할 수 있도록 합니다.
- ***Technical Details***: LoRACLR은 다중 LoRA 모델들을 하나의 통합 모델로 병합하기 위해 대조적 손실 목적을 도입합니다. 이 방법은 개별 LoRA 모델에서 생성된 입력 출력 쌍(Xi, Yi)을 사용하여 동일한 개념에서는 쌍을 끌어당기고, 다른 개념 간에는 쌍을 밀어내어 간섭을 방지합니다. 대조적 손실 목적은 개념의 고유성을 보존하면서 모델의 통합된 합성을 가능하게 합니다.
- ***Performance Highlights***: LoRACLR은 이미지 정렬 및 정체성 보존에서 다른 기법들에 비해 우수한 성능을 보였으며, 특히 복잡한 멀티 개념 합성에서도 각 개념의 정체성과 고충실도를 일관되게 유지할 수 있음을 입증했습니다. 또한, 12개의 개념 결합이 5분 이내에 이루어져 실용적이며 실효성이 높습니다.

### [Video Creation by Demonstration](https://arxiv.org/abs/2412.09551)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09551.png)

Vote: 5

Authors: Long Zhao, Hartwig Adam, Xuhui Jia, Jennifer J. Sun, Yihong Sun, Bharath Hariharan, Ting Liu, Hao Zhou, Yandong Li, Liangzhe Yuan

- ***What's New***: Video Creation by Demonstration는 주어진 다른 장면의 컨텍스트 이미지와 데모 비디오를 바탕으로, 물리적으로 타당한 비디오를 생성하는 새로운 비디오 생성 경험을 제공합니다. 이를 위해 𝛿-Diffusion이라는 새로운 자기지도 학습 방법을 제시하였으며, 이는 라벨이 없는 비디오에서 조건부 미래 프레임 예측을 통해 학습됩니다.
- ***Technical Details***: 이 연구에서는 명시적인 신호 없이 비디오 생성을 조절하는 데 필요한 최대의 유연성과 표현력을 위한 암시적 잠재 제어(Implicit Latent Control)를 사용합니다. 𝛿-Diffusion은 비디오 기초 모델(Video Foundation Model)의 외관 병목 설계를 활용하여 데모 비디오로부터 최소화된 외관 누출을 통해 행동 잠재 변수(Action Latents)를 추출합니다. 또한, 두 단계로 구성된 훈련 접근 방식을 통해, 행동 개념을 기반으로 향후 프레임을 예측하는 확산 모델(Diffusion Model)을 학습합니다.
- ***Performance Highlights***: 𝛿-Diffusion은 인간 선호도와 대규모 기계 평가 모두에서 관련 기준보다 우수한 성능을 나타내며, 상호작용 세계 시뮬레이션에 대한 가능성을 보여줍니다. 특히, 𝛿-Diffusion은 일상적인 활동, 에고-중심 시각에서 복잡한 로봇 동작에 이르기까지 범위가 넓은 행동 개념을 포함하는 높은 정밀도의 비디오를 생성하는 데 능력을 보입니다.

### [Normalizing Flows are Capable Generative Models](https://arxiv.org/abs/2412.06329)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06329.png)

Vote: 3

Authors: David Berthelot, Navdeep Jaitly, Josh Susskind, Tianrong Chen, Preetum Nakkiran, Huangjie Zheng, Miguel Angel Bautista, Shuangfei Zhai, Jiatao Gu, Ruixiang Zhang

- ***What's New***: TARFLOW는 Transformer 기반 Masked Autoregressive Flows(MAFs)를 확장한 새로운 아키텍처로, 이미지의 픽셀을 직접 모델링하고 생성할 수 있는 강력한 Normalizing Flow(NF) 모델을 제공합니다. TARFLOW는 Gaussian 노이즈 증강, 사전 학습 후 디노이징 절차 및 class-conditional, unconditional 설정을 위한 효과적인 가이던스 방식을 도입하여 샘플 품질을 향상시킵니다.
- ***Technical Details***: TARFLOW는 이미지 패치 상의 Autoregressive Transformer 블록의 스택으로 구성되어 있으며, 층별로 Autoregression 방향이 교차하도록 설계되어 있습니다. 모든 Transformation은 시퀀스의 이미지를 허용하는 Causal Vision Transformer를 사용하여 구현되며, 이는 비선형 변환을 가능하게 합니다. Gaussian 노이즈를 사용한 증강은 모델이 더 일반화하여 더 넓은 입력 분포를 처리할 수 있게 도와줍니다.
- ***Performance Highlights***: TARFLOW는 이미지 가능도 추정에서 Sub-3 BPD 성능을 달성하며 이전 최상의 방법을 크게 뛰어넘는 결과를 기록했습니다. 또한, 생성된 샘플의 품질과 다양성 면에서 diffusion models와의 경쟁력을 처음으로 Normalizing Flow 모델 단독으로 보여주었으며, 최고 수준의 FID 점수를 기록하여 샘플 품질을 입증했습니다.

### [InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions](https://arxiv.org/abs/2412.09596)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09596.png)

Vote: 59

Authors: Conghui He, Shuangrui Ding, Qipeng Guo, Han Lv, Kai Chen, Yu Qiao, Junbo Niu, Bin Wang, Wenwei Zhang, Yifei Li, Wei Li, Xiaoyi Dong, Yuhang Zang, Dahua Lin, Yuhang Cao, Lin Chen, Jiaqi Wang, Haodong Duan, Xinyue Zhang, Zheng Nie, Min Zhang, Xilin Wei, Xin Chen, Zhongying Tu, Pan Zhang, Jiaye Ge, Jingwen Li, Xingcheng Zhang, Rui Qian

- ***What's New***: InternLM-XComposer2.5-OmniLive (IXC2.5-OL)는 스트리밍 비디오와 오디오 입력을 실시간으로 상호작용할 수 있는 종합적인 멀티모달 시스템입니다. IXC2.5-OL은 스트리밍 인식(Streaming Perception), 멀티모달 장기 기억(Memory), 추론(Reasoning) 모듈로 구성되어 있으며, 인간과 유사한 인지 기능을 AI에 부여하려는 목표를 갖고 있습니다.
- ***Technical Details***: IXC2.5-OL은 세 가지 주요 모듈로 구성되어 있습니다. (1) 스트리밍 인식 모듈은 멀티모달 정보를 실시간으로 처리하여 중요한 정보를 기억(Memory)에 저장하고 사용자 질의가 발생할 경우 추론(Reasoning)을 유도합니다. (2) 멀티모달 장기 기억 모듈은 단기 기억을 통합하여 이를 장기 기억으로 압축하여 효율적인 검색과 정확도를 향상시킵니다. (3) 추론 모듈은 인식 모듈에 의해 활성화되어 질의에 응답하고 추론 작업을 수행합니다.
- ***Performance Highlights***: IXC2.5-OL은 오디오 및 비디오 벤치마크에서 뛰어난 성능을 보여줍니다. 오디오 인식(Auto Speech Recognition; ASR) 벤치마크에서는 WenetSpeech와 LibriSpeech에서 경쟁력 있는 결과를 기록하였고, MLVU, Video-MME, MMBench-Video, MVBench 같은 비디오 이해 벤치마크에서도 동급 모델들과 비교해 SOTA(최고의 성능)를 달성했습니다. 특히, StreamingBench에서는 73.79%의 성과로 실시간 비디오 상호작용에서 그 역량을 입증했습니다.

### [VisionArena: 230K Real World User-VLM Conversations with Preference Labels](https://arxiv.org/abs/2412.08687)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08687.png)

Vote: 2

Authors: Krishna Mandal, Christopher Chou, Joseph E. Gonzalez, Trevor Darrell, Ion Stoica, Koki Mashita, Wei-Lin Chiang, Lisa Dunlap

- ***What's New***: VisionArena는 VLM(Visual-Language Models)와 사용자 간의 실제 대화를 통해 이들의 상호작용을 분석하기 위한 데이터셋으로, 230K 개의 대화를 포함하고 있습니다. 기존에 존재하던 벤치마크들과는 달리 여러 터번 대화나 다양한 맥락을 포함하여 VLM의 역동적인 사용자 상호작용을 보다 잘 반영합니다.
- ***Technical Details***: VisionArena는 기본적으로 세 가지 하위 데이터셋으로 구성되어 있습니다. 첫째, VisionArena-Chat은 20만 건의 사용자와 VLM 간의 단일 및 다중 회전 대화기록으로 이루어져 있습니다. 둘째, VisionArena-Battle은 3만 건의 대화로 이루어져 있으며, 사용자 선호도를 기반으로 무명 VLM을 비교합니다. 마지막으로 VisionArena-Bench는 500개의 다양한 사용자 프롬프트로 이루어져 있어 자동 벤치마크와 모델 순위를 효율적으로 대체할 수 있습니다.
- ***Performance Highlights***: VisionArena 데이터셋을 활용하여 finetuning한 VLM은 LLaVA-Instruct-158K와 비교하여 MMMU에서 17점, WildVision 벤치마크에서 46점의 성능 개선을 보였습니다. 이 데이터셋은 특히 VLM의 공간 추론 및 계획 영역에서의 어려움을 강조하며, 이를 개선하기 위한 방향성을 제시합니다.

### [Learned Compression for Compressed Learning](https://arxiv.org/abs/2412.09405)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09405.png)

Vote: 7

Authors: Dan Jacobellis, Neeraja J. Yadwadkar

- ***What's New***: WaLLoC(Wavelet Learned Lossy Compression)는 모드에 구애받지 않는 손실 압축 프레임워크로 제안되어 고효율 인코딩과 높은 압축 비율, 균형 잡힌 차원 축소를 동시에 제공합니다. 이 프레임워크는 기존 압축 방법의 주요 제한점을 해결하며, 다양한 머신 러닝 모델의 압축 도메인 운영을 가속화하는 데 도움을 줍니다.
- ***Technical Details***: WaLLoC 아키텍처는 잉여 신호를 노출하기 위한 역변환이 가능한 웨이브렛 패킷 변환(Wavelet Packet Transform)과 비대칭 오토인코더, 엔트로피 보틀넥을 조합하여 인코딩 비용을 최소화하며 엔트로피 코딩으로 높은 압축 비율을 달성합니다. 많은 자연 신호의 서브 대역 웨이브렛 계수가 일반화된 가우시안 분포(GGD)를 따르는 점을 이용하여, 훈련 중 추가적 잡음을 더하여 양자화-내성과 효율성을 동시에 성취합니다.
- ***Performance Highlights***: WaLLoC는 RGB 이미지에 대해 기존 VAE를 사용한 Stable Diffusion 3보다 약 12배 높은 압축 비율과 유사한 품질을 제공합니다. 스테레오 오디오의 경우에도 동일한 품질의 향상을 제공하면서 300배 이상의 인코딩 처리량을 제공합니다. 또한, 이미지 분류 및 문서 이해 작업에서 높은 정확도를 유지하면서 압축을 통한 인퍼런스 속도를 대폭 향상시킵니다.

### [DisPose: Disentangling Pose Guidance for Controllable Human Image Animation](https://arxiv.org/abs/2412.09349)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09349.png)

Vote: 5

Authors: Long Chen, Zhihong Zhu, Yaowei Li, Junjie Cao, Xuxin Cheng, Yuhang Yang, Hongxiang Li

- ***What's New***: 이 연구에서는 DisPose라는 새로운 모듈을 제안하여, 사람이 영상 애니메이션을 컨트롤할 수 있도록 돕습니다. 이 모듈은 밀도 높은 입력 없이도 스켈레톤 포즈와 참고 이미지로부터 효율적인 제어 신호를 추출합니다. 이는 기존 모델에 통합 가능하며, 애니메이션의 품질과 일관성을 개선합니다.
- ***Technical Details***: DisPose는 스켈레톤 포즈를 움직임 필드 가이드와 키포인트 대응으로 분리하여 제어 신호를 추출합니다. 이는 기존 비디오 생성 방법에 플러그-앤-플레이 형식으로 통합되며, 밀도 높은 입력을 필요로 하지 않습니다. 모션 필드는 스켈레톤 포즈를 사용하여 추정되고, 참고 이미지를 기반으로 조건부 모션 확산을 통해 얻어진 희소 및 밀도 높은 모션 필드로 구성됩니다. 하이브리드 ControlNet 구조를 사용하여, 추출된 가이던스 피쳐를 유연하게 통합하여 비디오 생성의 품질을 향상시킵니다.
- ***Performance Highlights***: TikTok 데이터셋에 대한 평가에서, 제안한 방법은 VBench 점수가 더 높은 것으로 나타나며, FID-FVD 및 FVD 점수도 줄어드는 것으로 나타났습니다. 이는 제안한 방법이 인간 인식과 일치하는 고품질의 영상을 생성함을 의미합니다. 다양한 스타일의 참조 이미지를 사용하여 전례 없는 데이터셋에서도 강력한 일반성을 입증하였습니다.

### [EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM](https://arxiv.org/abs/2412.09618)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09618.png)

Vote: 17

Authors: Dazhong Shen, Hongsheng Li, Zhuofan Zong, Dongzhi Jiang, Hao Shao, Bingqi Ma, Yu Liu, Guanglu Song

- ***What's New***: EasyRef는 다중 참조 이미지를 학습에 포함할 수 있는 새로운 플러그앤플레이(plug-and-play) 적응 기법입니다. 일반적으로 사용되는 평균 이미지 임베딩과 달리, EasyRef는 멀티모달 대형 언어 모델(multimodal large language model; MLLM)을 통해 일관된 시각적 요소를 효율적으로 캡처하여 텍스트 프롬프트와의 조합을 가능하게 했습니다.
- ***Technical Details***: EasyRef는 미리 학습된 확산 모델(diffusion model)과 MLLM을 활용하여 다중 참조 이미지와 텍스트 프롬프트를 인코딩합니다. 이러한 참조 표현들은 조건 프로젝터를 통해 확산 모델의 잠재 공간에 맵핑됩니다. 참조 이미지들에 대한 효율적인 표현 결합을 위해 학습 가능한 토큰을 사용하며, 이는 MLLM의 마지막 레이어에 통합되어 효율성을 높였습니다. 또한, 점진적인 학습 전략을 도입함으로써 MLLM의 세부 감각 능력을 향상시켰습니다.
- ***Performance Highlights***: 새로운 MRBench 벤치마크 실험에서 EasyRef는 튜닝 없는 방법(IP-Adapter)과 튜닝 기반 방법(LoRA)보다 다양한 도메인에서 높은 미학적 품질과 강력한 제로샷 일반화 성능을 보여주었습니다. IP-Adapter-SDXL과 비교할 때, DINO-I에서 0.223점 더 높으며, 이는 레퍼런스 이미지와 텍스트 프롬프트의 높아진 일치를 의미합니다.

### [The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective](https://arxiv.org/abs/2412.09460)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09460.png)

Vote: 3

Authors: Svein Arne Brygfjeld, Magnus Breder Birkenes, Peng Liu, Petter Mæhlum, Liljia Øvrelid, Vladislav Mikhailov, David Samuel, Freddy Wetjen, Rolv-Arild Braaten, Wilfred Østgulen, Javier de la Rosa, Jon Atle Gulla, Stephan Oepen, Aslak Sira Myhre, Lemei Zhang, Erik Velldal, Andrey Kutuzov, Tita Enstad

- ***What's New***: 노르웨이 저작권 자료가 대형 언어 모델(Large Language Models; LLMs)의 성능에 미치는 영향을 평가한 최초의 연구입니다. 이 연구는 노르웨이언 언어 처리에서 저작권 자료의 긍정적 결과를 구체적으로 제시하며, 저작권자에 대한 보상 체계 설정에 기여할 수 있는 실질적인 데이터를 제공합니다.
- ***Technical Details***: 다양한 구성의 LLM을 학습시키기 위해 주로 노르웨이언 데이터로 구성된 코퍼스를 구축했습니다. 이 데이터는 저작권 자료와 비저작권 자료로 구분되며, 모델들은 텍스트 생성, 번역, 요약, 질문답변, 감정 분석 등의 NLP 태스크에서 성능을 비교합니다. 모델 학습에는 Mistral 아키텍처 기반의 모델들이 사용되었으며, AMD MI250X 및 NVIDIA H100과 같은 다양한 가속기로 총 27만 GPU 시간을 사용했습니다. 또한, 베이스, 원본, 그리고 번역 도서, 신문 등의 서브셋으로 데이터를 세분화해 다양한 연습 효과를 비교했습니다.
- ***Performance Highlights***: 저작권 자료를 포함한 확장 모델은 기본 모델 대비 평균적으로 6.73%의 성능 향상을 보여주었으며, 비소설 도서와 신문을 추가한 경우에도 비슷한 6.52%의 향상을 보였습니다. 하지만, 픽션 도서를 추가한 경우에는 오히려 성능이 약간 하락하는 결과가 나타났습니다. 이는 픽션이 모델 생성 텍스트의 다양성을 높일 수 있지만, 일반적인 성능 향상에는 기여하지 못함을 시사합니다.

### [FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction](https://arxiv.org/abs/2412.09573)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09573.png)

Vote: 3

Authors: Shenghua Gao, Jiale Xu, Ying Shan

- ***What's New***: FreeSplatter는 정확한 카메라 포즈 없이도 다양한 시점에서 3D 가우시안(3D Gaussians)을 고품질로 재구성하는 획기적인 프레임워크로, 카메라 파라미터를 몇 초 만에 복구할 수 있습니다. 이 기술은 대형 변환 모델(transformer architecture)의 셀프 어텐션 블록(self-attention blocks)을 이용하여 이미지 토큰을 3D 가우시안 프리미티브로 변환합니다.
- ***Technical Details***: FreeSplatter는 칼리브레이션되지 않은 희소 시점 이미지에서 픽셀별로 3D 가우시안을 예측하며, 입력 카메라 포즈가 필요 없는 단일 스트림(transformer architecture)으로 구현되었습니다. 두 가지 모델 변형, 객체 중심 FreeSplatter-O와 장면 중심 FreeSplatter-S를 각기 다른 데이터셋에서 훈련하여 높은 재구성 품질과 포즈 추정 정확도를 달성합니다. 훈련 중 가우시안의 위치는 픽셀 정렬 손실(pixel-alignment loss)을 통해 카메라 광선에 맞춰지도록 하였습니다.
- ***Performance Highlights***: FreeSplatter-O는 이전의 포즈 의존 대형 재구성 모델보다 많은 면에서 성능이 우수하며, FreeSplatter-S는 MASt3R와 견줄 만한 포즈 추정 정확도를 ScanNet++ 및 CO3D 벤치마크에서 달성하였습니다. 실험은 FreeSplatter이 시각적 진실성을 유지하면서도 세부 사항을 충실히 재현할 수 있음을 시사하며, 특히 가우시안의 예측 위치 조정 및 속성 최적화가 동시다발적으로 이루어지는 과정에서 유리한 성능을 나타내었습니다.

### [Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions](https://arxiv.org/abs/2412.08737)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08737.png)

Vote: 28

Authors: Willie Neiswanger, Jiarui Zhang, Tianyu Yu, Jinyi Hu, Ollie Liu

- ***What's New***: Euclid는 최신 MLLM (Multimodal LLM)의 낮은 수준의 시각적 지각(LLVP)을 향상시키기 위해 고성능의 합성 데이터(Synthetic Data)를 이용한 새로운 접근 방식을 소개합니다. 특히, Geoperception 벤치마크를 통해 이 모델의 기하학적 인식 능력을 평가하며, 최고 성능을 자랑하는 Gemini 모델을 뛰어넘는 성과를 보이고 있습니다.
- ***Technical Details***: Euclid 모델은 합성 멀티모달 데이터만을 이용하여 훈련되었으며, 다단계 학습(Training with a Data Curriculum)을 통해 고난도의 기하학적 이해 과제를 학습합니다. 특히, Geoperception 벤치마크는 MLLM의 2D 기하학 정보 전사 능력을 평가하기 위해 설계된 데이터셋입니다. Euclid의 발전은 기하학적 요소의 고충실도 시각적 설명을 생성할 수 있는 합성 데이터 엔진과 커리큘럼 기반의 복잡성을 점진적으로 증가시키는 학습법에서 기인합니다.
- ***Performance Highlights***: Euclid 모델은 Geoperception 벤치마크에서 Gemini-1.5-Pro 모델을 특정 기하 벤치마크 작업에서는 최대 58.56%, 전반적으로는 평균 10.65% 성능 향상을 기록했습니다. 특히, Euclid는 PointLiesOnLine 작업에서 기존 모델의 3배 이상의 정확도인 82.98%를 기록하며 눈에 띄게 뛰어난 결과를 보였습니다.

### [Multimodal Latent Language Modeling with Next-Token Diffusion](https://arxiv.org/abs/2412.08635)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.08635.png)

Vote: 21

Authors: Wenhui Wang, Furu Wei, Yutao Sun, Hangbo Bao, Jianyong Wang, Shaohan Huang, Zhiliang Peng, Li Dong

- ***What's New***: 본 연구는 다중 모드 생성 모델(multimodal generative models)을 위한 통합 접근법으로, 연속 데이터(이미지, 오디오, 비디오 등)와 이산 데이터(텍스트, 코드 등)를 함께 처리할 수 있는 잠재 언어 모델링(LatentLM)을 제안합니다. LatentLM은 변분 오토인코더(VAE)와 인과적 트랜스포머(causal Transformers)를 결합하여 연속 데이터를 잠재 벡터 형태로 표현하며, 이러한 벡터의 자귀적 생성을 위한 '다음 토큰 확산(next-token diffusion)' 기법을 도입합니다.
- ***Technical Details***: 잠재 언어 모델링(LatentLM)은 연속 데이터를 VAE로 잠재 벡터로 변환한 뒤, 인과적 트랜스포머를 사용한 자귀적 생성 방식을 통해 처리합니다. 추가적으로, 다음 토큰 확산(next-token diffusion)을 활용해 연속 벡터를 예측 및 생성하며, 이 과정에서 N개의 트랜스포머 레이어와 RMSNorm 및 SwiGLU 등의 개선점을 도입했습니다. σ-VAE를 사용하여 잠재 공간의 분산을 유지하고자 하는데, 이는 자귀적 모델링의 도전 과제인 분산 붕괴를 막기 위한 것입니다.
- ***Performance Highlights***: 이미지 생성 실험에서, LatentLM은 ResNet 기반의 Diffusion Transformer보다 성능과 확장성이 우수합니다. 또한, 다중모드 대형 언어 모델과 결합 시, 텍스트-이미지, 이미지-텍스트, 음성 합성(텍스트-음성) 등 다양한 모드에서 Transfusion이나 벡터 양자화 모델과 비교하여 뛰어난 성능을 보였습니다. 특히, 텍스트-음성 합성에서는 연산에 필요한 디코딩 단계를 10배 줄이면서도 VALL-E 2 모델을 능가하는 스피커 유사도 및 견고성을 지녔습니다.

### [Arbitrary-steps Image Super-resolution via Diffusion Inversion](https://arxiv.org/abs/2412.09013)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09013.png)

Vote: 3

Authors: Chen Change Loy, Zongsheng Yue, Kang Liao

- ***What's New***: Diffusion Inversion을 활용한 새로운 이미지 초해상도(SR) 기술이 소개되었습니다. 이 기술은 대규모 사전 학습된 확산 모델의 풍부한 이미지 사전(prior)을 활용하여 SR 성능을 향상시키는 것을 목표로 합니다. 이는 사용자가 강등(degradation) 유형이나 특정 요구 사항에 따라 임의의 샘플링 단계 수를 조정할 수 있는 효율적이고 유연한 샘플링 메커니즘을 제공합니다.
- ***Technical Details***: 본 기술의 핵심은 부분 잡음 예측(Partial noise Prediction; PnP)을 설계하여 확산 모델의 중간 상태를 구축하는 것입니다. 이는 잡음 예측 네트워크(noise predictor)를 사용하여 LR 이미지에서 최적의 잡음 맵을 추정함으로써 달성됩니다. 잡음 예측기는 주어진 LR 이미지 기반으로 중간 상태를 통해 샘플링을 초기화하며, 다양한 시작 단계를 선택할 수 있어 샘플링 과정을 유연하게 제어할 수 있습니다.
- ***Performance Highlights***: InvSR은 최신 상태의 기술과 비교하여 단일 샘플링 단계에서도 우수한 성능을 보였으며, 다단계 샘플링에서도 견줄 수 있는 품질을 유지하면서도 효율성을 제공합니다. 특히 실세계 데이터셋에서 비참조(non-reference) 메트릭에서 대부분의 경쟁 방법을 능가하는 성능을 기록하였습니다.

### [PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh Representations](https://arxiv.org/abs/2412.05994)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.05994.png)

Vote: 10

Authors: Youngjoon Hong, Namgyu Kang, Jaemin Oh, Eunbyung Park

- ***What's New***: PIG(Physics-Informed Gaussians)는 기존의 PINNs(Physics-Informed Neural Networks)의 단점을 극복하는 새로운 방식으로, 가우시안 함수와 경량화된 신경망을 결합하여 복잡한 편미분방정식(PDE)을 더욱 효율적으로 근사합니다. 특히 가우시안의 평균과 분산을 학습 가능하도록 하여, 훈련 과정에서 위치와 형태를 동적으로 변경할 수 있습니다.
- ***Technical Details***: 이 방법은 가우시안의 학습 가능한 위치와 형상(paralleled Gaussians)을 통해 입력 좌표의 특징 벡터(feature vector)를 가중 합으로 추출합니다. FEϕ(x)의 출력은 각 가우시안으로부터 유도되고, 가우시안과 이들의 파라미터가 훈련 과정에서 최적화되어 각 PDE의 해를 정밀하게 예측합니다.
- ***Performance Highlights***: PIG는 Allen-Cahn 방정식을 비롯한 다양한 PDE에서 경쟁력 있는 정확도를 입증했으며, L2 오차 측면에서 기존의 MLP 기반 방법보다 빠른 수렴 속도를 보였습니다. 특히, PIG는 Klein-Gordon 방정식에서 다른 최신 방법보다 한 차원 높은 정확도를 달성했습니다. 이와 같이 가우시안 기반의 접근 방식이 추가적인 효율성을 제공함을 확인했습니다.

### [AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials](https://arxiv.org/abs/2412.09605)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.09605.png)

Vote: 15

Authors: Dunjie Lu, Caiming Xiong, Yuchen Mao, Zekun Wang, Yiheng Xu, Zhennan Shen, Tao Yu, Junli Wang

- ***What's New***: AgentTrek는 웹 튜토리얼을 활용하여 대규모 웹 에이전트 경로 데이터를 합성해내는 새로운 파이프라인을 도입했습니다. 이는 LLM의 기능과 GUI 에이전트를 위한 다단계 맥락-풍부한 훈련 데이터의 필요성을 효과적으로 연결해 줍니다.
- ***Technical Details***: AgentTrek 파이프라인은 세 가지 주요 단계로 구성됩니다. 첫째, 웹에서 GUI 튜토리얼을 자동으로 수집합니다. 둘째, 수집된 튜토리얼을 가이드로 사용하여 실제 웹 환경에서 에이전트가 작업을 수행하게 하고, 이를 통해 경로 데이터를 수집합니다. 마지막으로, 이 데이터를 활용하여 GUI 에이전트를 훈련 및 미세조정합니다. 이러한 과정에서 시각-언어 모델(VLM)이 튜토리얼을 기반으로 에이전트의 작업을 안내하며, 생성된 경로 데이터의 정확성을 평가합니다.
- ***Performance Highlights***: AgentTrek로 훈련된 에이전트는 기존 데이터로 훈련된 에이전트보다 탁월한 기초 다짐 및 계획 능력을 발휘했습니다. 이로써 AgentTrek의 데이터가 웹 기반 GUI 작업 향상에 미치는 강력한 영향을 보여주었습니다. 또한, AgentTrek는 웹 기반 환경에서 대규모 GUI 에이전트를 훈련하는 데 있어 비용 효율성을 유지하면서도 인간 주석 데이터를 활용한 기존 방법보다 더 높은 성과를 달성했습니다.

### [ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities](https://arxiv.org/abs/2412.06745)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2412.06745.png)

Vote: 2

Authors: Adhiraj Ghosh, Vishaal Udandarao, Matthias Bethge, Ameya Prabhu, Sebastian Dziadzio, Samuel Albanie

- ***What's New***: ONEBench는 기존의 고정된 테스트 데이터셋의 한계를 극복하기 위해 개발된 새로운 벤치마크입니다. 이는 다양한 평가 데이터셋을 하나의 통합된 샘플 풀(pool)로 결합, 사용자가 관심 있는 특정 기능에 대한 맞춤형 평가 벤치마크를 생성할 수 있도록 합니다. 이를 통해 데이터 편향을 줄이며 모델 평가를 집단적인 샘플 단위 테스트의 선택 및 집계 과정으로 프레임하게 됩니다.
- ***Technical Details***: ONEBench는 이질성(heterogeneity) 및 불완전성(incompleteness)이라는 두 가지 주요 과제를 해결하기 위한 이론과 실용 알고리즘을 개발합니다. 이질성은 이진, 수치 및 순서 데이터 등 다양한 지표의 집계를 포함하며, 불완전성은 서로 다른 데이터 하위 집합을 평가하는 모델을 비교합니다. 이를 해결하기 위해 Plackett-Luce 모델을 사용하여 모델의 성능을 안정적으로 집계하고 신속한 수렴을 가능하게 합니다. ONEBench는 언어모델(LLM)과 비전-언어 모델(LMM)용으로도 각각의 평가 방법을 제공합니다.
- ***Performance Highlights***: 실험 결과, Plackett-Luce 기반 순위 집계 방법은 기존의 Elo 또는 LMArena와 같은 다른 방법들보다 향상된 성능을 보여줍니다. 예를 들어, Up to 95%의 측정값이 없어도 견고하게 유지되며, 평가 비용을 최대 20배까지 절감할 수 있는 가능성을 보여줍니다.

