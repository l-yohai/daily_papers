## Daily Papers (2024-03-07)

### [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/tymX0hLyiKdpwCt7sNZ2h.png)

Vote: 79

Authors: Zhangyang Wang, Anima Anandkumar, Yuandong Tian, Jiawei Zhao, Beidi Chen, Zhenyu Zhang

- 대형 언어 모델(LLMs)의 학습은 가중치와 최적화 상태의 증가로 인해 상당한 메모리 문제를 제기합니다.
- 일반적인 메모리 절감 방법 중 하나인 저랭크 적용(LoRA)은 각 층에 고정된 사전 훈련된 가중치에 학습 가능한 저랭크 행렬을 추가하여 학습 가능한 매개변수와 최적화 상태를 줄입니다.
- 하지만 이러한 방식은 저랭크 부공간으로 매개변수 검색을 제한하고 훈련 동역학을 변경하여 종종 전체 랭크 가중치 사용 시보다 성능이 떨어질 수 있으며, 경우에 따라 전체 랭크 워밍업이 필요할 수 있습니다.
- 본 연구에서는 Gradient Low-Rank Projection (GaLore)이라는 훈련 전략을 제안하여, LoRA와 같은 일반적인 저랭크 적용 방법보다 메모리 효율성을 높이면서 전체 매개변수 학습이 가능하도록 합니다.
- GaLore 접근 방식은 최대 65.5%까지 최적화 메모리 사용량을 줄이면서 LLaMA 1B 및 7B 아키텍처의 C4 데이터셋을 사용한 사전 학습 및 GLUE 태스크에서 RoBERTa를 미세 조정할 때 효율성과 성능을 모두 유지합니다.
- 8비트 GaLore는 BF16 기준 대비 최대 82.5%까지 최적화 메모리를 줄이고 전체 훈련 메모리는 63.3%까지 줄일 수 있습니다.
- 특히, 모델 병렬 처리나 체크포인팅, 오프로딩 전략 없이도 24GB 메모리(NVIDIA RTX 4090 등)를 가진 소비자 그래픽 카드에서 7B 모델을 사전 학습할 수 있는 가능성을 처음으로 입증했습니다.

### [ShortGPT: Layers in Large Language Models are More Redundant Than You Expect](https://arxiv.org/abs/2403.03853)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8My5G3ybxfrV5_fP1CE47.png)

Vote: 28

Authors: Qingyu Zhang, Xianpei Han, Xin Men, Bingning Wang, Weipeng Chen, Mingyu Xu, Hongyu Lin, Yaojie Lu

- 대규모 언어 모델(LLM)의 성능이 향상됨에 따라 모델의 크기가 수십억 또는 수조 개의 매개변수를 포함할 정도로 급격히 증가하고 있습니다.
- 이 연구에서 LLM의 많은 레이어가 서로 매우 유사하며 일부 레이어는 네트워크 기능에 거의 영향을 미치지 않는 것으로 밝혀졌습니다.
- 이러한 관찰을 바탕으로, 각 LLM 레이어의 중요도를 측정하기 위해 '블록 영향도(Block Influence, BI)'라는 메트릭을 정의하였습니다.
- BI 점수에 근거하여 중복 레이어를 직접 삭제하는 간단한 가지치기 방법을 제안하며, 이 방법을 ShortGPT라고 합니다.
- 실험 결과, ShortGPT는 기존의 최첨단 모델 가지치기 방법들보다 우수한 성능을 보였습니다.
- 또한, ShortGPT는 양자화와 같은 방법과 정교하게 결합해 매개변수와 연산량을 더욱 줄일 수 있는 가능성을 보여줍니다.
- 복잡한 가지치기 기술 대신 단순한 레이어 제거를 통해 더 나은 결과를 달성할 수 있다는 것은 모델 구조에 상당한 중복성이 있다는 것을 시사합니다.

### [SaulLM-7B: A pioneering Large Language Model for Law](https://arxiv.org/abs/2403.03883)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/omI2rPBGGEFR_UdqJdu7-.png)

Vote: 23

Authors: Vera Lúcia Raposo, Sofia Morgado, Andre F. T. Martins, Telmo Pessoa Pires, Pierre Colombo, Dominic Culver, Rui Melo, Malik Boudiaf, Fabrizio Esposito, Caio Corro, Michael Desa

- 본 논문에서는 법률 분야에 특화된 대규모 언어 모델(Large Language Model, LLM)인 SaulLM-7B를 소개합니다.
- SaulLM-7B는 법률 텍스트 이해와 생성을 위해 명시적으로 설계된 최초의 LLM으로, 70억 개의 파라미터를 가지고 있습니다.
- Mistral 7B 아키텍처를 기반으로, SaulLM-7B는 300억 개 이상의 토큰을 포함하는 영어 법률 코퍼스로 훈련되었습니다.
- 이 모델은 법률 문서를 이해하고 처리하는 데 있어 최첨단의 능력을 나타냅니다.
- 추가적으로, 법률 데이터셋을 활용한 새로운 지시적 미세조정 방법을 통해 SaulLM-7B의 법률 업무 수행 능력을 더욱 향상시킬 수 있음을 제시합니다.
- SaulLM-7B는 CC-BY-SA-4.0 라이센스 하에 공개되었습니다.

### [Learning to Decode Collaboratively with Multiple Language Models](https://arxiv.org/abs/2403.03870)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/f6IFPPjX3joBiG06IVhEe.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/f6IFPPjX3joBiG06IVhEe.mp4" muted="false"></video></div>

Vote: 10

Authors: Bailin Wang, Hunter Lang, Shannon Zejiang Shen, Yoon Kim, David Sontag

- 본 논문에서는 서로 다른 큰 언어 모델들(LLM)이 토큰 수준에서 생성물을 교차하며 협력하도록 가르치는 방법을 제안한다.
- 다음 토큰을 어느 LLM이 생성할지를 숨겨진 변수로 모델링하며, 이 변수를 최적화하여 LLM이 자동적으로 생성 시점을 배우고 "보조" 언어 모델 중 하나에 생성을 요청한다.
- 직접적인 감독 없이도 본래 LLM이 본연의 토큰 생성과 보조 모델 호출을 결정하며, 이는 특정 작업에 맞추어 각 모델의 전문성을 융합시킨다.
- 일반적인 베이스 LLM이 도메인 전문 모델을 호출하여 활용하는 크로스 도메인 상황에서 협력적 디코딩이 특히 유용하다.
- 지시사항 이행, 도메인별 질문 응답(QA), 그리고 추론 작업에서, 공동 시스템의 성능이 개별 모델들의 성능을 초과하는 것을 보여준다.
- 학습된 숨겨진 결정의 질적 분석을 통해, 우리의 방법으로 훈련된 모델들은 템플릿 채우기 같은 여러 흥미로운 협력 패턴을 보인다는 것을 보여준다.
- 해당 연구의 코드는 https://github.com/clinicalml/co-llm 에서 확인할 수 있다.

### [Enhancing Vision-Language Pre-training with Rich Supervisions](https://arxiv.org/abs/2403.03346)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/eHfQKPXI4-QiXCAFWrgUO.png)

Vote: 10

Authors: Kunyu Shi, Vijay Mahadevan, Pengkai Zhu, Zhuowen Tu, Shabnam Ghadar, Srikar Appalaraju, Stefano Soatto, Edouard Belval, Oren Nuriel, Yuan Gao

- 본 논문에서는 대규모 웹 스크린샷 렌더링에서 추출한 데이터를 활용하여 시각-언어 모델을 위한 새로운 사전 교육 패러다임인 'Strongly Supervised pre-training with ScreenShots(S4)'를 제안합니다.
- S4는 HTML 요소의 트리 구조 및 공간적 위치 정보를 활용하여 대규모 주석 데이터와 함께 10가지 사전 교육 과제를 신중하게 설계합니다. 
- 이러한 과제들은 다양한 도메인에 걸친 다운스트림 작업과 유사하며, 주석을 얻는 것은 비용이 적게 듭니다.
- 실험 결과, S4를 통한 사전 교육 방법이 현재 스크린샷 기반 사전 교육 목표에 비해 이미지-텍스트 모델의 성능을 크게 향상시킴을 확인했으며, 테이블 감지에서 최대 76.1% 개선을 실현하고, 위젯 캡셔닝에서 적어도 1% 이상의 성능 향상을 보였습니다.

### [3D Diffusion Policy](https://arxiv.org/abs/2403.03954)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/OQ36C04VDmEsRwJBDuESl.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/OQ36C04VDmEsRwJBDuESl.mp4" muted="false"></video></div>

Vote: 9

Authors: Huazhe Xu, Kangning Zhang, Gu Zhang, Muhan Wang, Chenyuan Hu, Yanjie Ze

- 로봇에게 숙련된 기술을 효율적으로 가르치는 방법으로 모방 학습을 제공하지만, 복잡한 기술을 견고하고 일반적으로 학습하는 것은 대량의 인간 데모가 필요합니다.
- 이러한 문제를 해결하기 위해, 저희는 3D 시각적 표현을 활용하여 조건부 행동 생성 모델인 확산 정책에 힘을 더한 새로운 시각 모방 학습 접근법인 3D 확산 정책(DP3)을 제시합니다.
- DP3의 핵심 설계는 효율적인 포인트 인코더로부터 추출된 간결한 3D 시각적 표현을 사용하는 것입니다.
- 시뮬레이션 작업 72개를 포함한 실험에서, DP3는 10개의 데모만으로 대부분의 작업을 성공적으로 처리하고, 기준 모델들 대비 55.3% 상대적인 개선을 보입니다.
- 실제 로봇 작업 4개에서, 각 작업마다 40개의 데모만을 가지고 85%의 높은 성공률로 정밀한 제어를 보여주며, 공간, 시점, 외관, 인스턴스 등 다양한 면에서 뛰어난 일반화 능력을 시현합니다.
- 실제 로봇 실험에서, DP3는 안전 요건을 거의 위반하지 않는 반면, 기준 방법들은 종종 위반하여 인간의 개입이 필요합니다.
- 저희의 광범위한 평가는 실제 로봇 학습에서 3D 표현의 중요성을 강조합니다.
- 관련 비디오, 코드, 및 데이터는 https://3d-diffusion-policy.github.io 에서 확인할 수 있습니다.

### [Backtracing: Retrieving the Cause of the Query](https://arxiv.org/abs/2403.03956)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/FWwSrWQdr1tvUCJzsA6Ni.png)

Vote: 6

Authors: Omar Khattab, Rose E. Wang, Pawan Wirawarn, Noah Goodman, Dorottya Demszky

- 온라인 콘텐츠 포털에서 사용자가 이해를 돕기 위해 질문하는 경우, 정보 검색 시스템은 답변을 제공하지만 콘텐츠 제작자가 사용자의 질문 원인을 파악하는 데 직접적인 도움을 주지는 않습니다.
- 본 논문에서는 사용자 쿼리의 원인이 된 텍스트 부분을 검색하는 backtracing이라는 작업을 소개하고 있습니다.
- 강의, 뉴스 기사, 대화 등 세 가지 실제 상황에서 backtracing이 콘텐츠 전달과 커뮤니케이션 개선에 중요함을 정식화합니다.
- 이 연구는 bi-encoder, 재순위 부여 방법, 가능성 기반 방법, 그리고 ChatGPT를 포함한 대중적인 정보 검색 방법과 언어 모델링 방법의 제로샷 성능을 평가하고 있습니다.
- 전통적인 정보 검색 시스템은 의미상 관련성 있는 정보를 검색하지만, 종종 사용자의 질문을 유발한 실제 인과적 맥락을 놓칩니다.
- 결과는 backtracing에 대한 개선의 여지가 있으며 새로운 검색 접근법이 필요함을 보여줍니다.
- 연구자들은 backtracing에 대한 향후 검색 시스템을 개선하고 사용자 질문에 영향을 주는 언어적 트리거를 식별하는 시스템을 시도하도록 벤치마크를 제공하기를 희망합니다.
- 연구에 사용된 코드와 데이터는 https://github.com/rosewang2008/backtracing에서 오픈 소스로 제공되고 있습니다.

### [Stop Regressing: Training Value Functions via Classification for Scalable Deep RL](https://arxiv.org/abs/2403.03950)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/dlMyIDdgoqn_vI4Oaduqr.png)

Vote: 6

Authors: Yevgen Chebotar, Aviral Kumar, Adrien Ali Taïga, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Quan Vuong, Jordi Orbay, Ted Xiao, Aleksandra Faust, Jesse Farebrother, Rishabh Agarwal

- 깊은 강화 학습(RL)에서 가치 함수는 중요한 구성 요소로, 이를 훈련하기 위해 평균 제곱 오차 회귀 목표를 사용하나, 대규모 네트워크에서의 확장이 도전적이었습니다.
- 이 연구에서는, 분류를 사용한 간단한 방식이 강화 학습의 확장성을 향상시킬 수 있는지 조사하였습니다.
- 가치 함수를 범주형 크로스 엔트로피로 훈련하는 것이 아타리 2600 게임, 로봇 손 조작, 체스, 워들과 같은 다양한 영역에서 성능과 확장성을 크게 향상시킨다는 것을 보여줍니다.
- 체계적인 분석을 통해 범주형 크로스 엔트로피의 주요 이점이 가치 기반 RL의 내재적 문제점들, 예를 들어 노이즈가 많은 목표와 비정상성을 완화하는 데에서 비롯됨을 보여줍니다.
- 본 논문은 가치 함수를 범주형 크로스 엔트로피로 훈련시키는 간단한 전환만으로도 깊은 RL의 확장성을 크게 개선할 수 있음을 주장하며, 이로써 해당 분야에서 최신의 결과를 달성할 수 있음을 보여줍니다.

### [Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling](https://arxiv.org/abs/2403.03234)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/hMgnjRd5lnfIAc2wGxXm3.png)

Vote: 3

Authors: Volodymyr Kuleshov, Aaron Gokaslan, Tri Dao, Albert Gu, Yair Schiff, Chia-Hsiang Kao

- 장거리 토큰 상호작용, 유전체 업스트림 및 다운스트림 영역의 영향 그리고 DNA의 역상보성(RC)을 모델링하는 것은 생물학과 유전체학에 도전 과제를 제시한다.
- 장거리 Mamba 블록에서 발전된 이중방향 기능을 지원하는 BiMamba 구성요소와 RC 등변성을 추가로 지원하는 MambaDNA 블록을 포함한 새로운 아키텍처를 제안한다.
- MambaDNA는 다양한 사전 학습 및 미세 조정 전략과 결합되어 RC 등변성을 가진 첫 번째 이중방향 장거리 DNA 언어 모델 가족인 Caduceus를 기반으로 한다.
- Caduceus는 다운스트림 벤치마크에서 이전의 장거리 모델들을 뛰어넘는 성능을 보이며, 특히 도전적인 장거리 변이 효과 예측 작업에서 이중방향성이나 등변성을 활용하지 않는 모델들보다 10배 큰 모델의 성능을 상회한다.

