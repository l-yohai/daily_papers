## Daily Papers (2024-03-22)

### [MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?](https://arxiv.org/abs/2403.14624)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14624.png)

Vote: 25

Authors: Renrui Zhang, Hongsheng Li, Yichi Zhang, Pan Lu, Haokun Lin, Aojun Zhou, Ziyu Guo, Pengshuo Qiu, Kai-Wei Chang, Peng Gao, Dongzhi Jiang

- 멀티모달 큰 언어 모델(MLLM)의 진보가 시각적 맥락에서 상당한 성능을 보여주었지만, 시각적 수학 문제를 해결하는 능력은 충분히 평가되거나 이해되지 않았습니다.
- 현존하는 벤치마크가 텍스트 질문 내에 과도한 시각적 내용을 포함하여 MLLM이 다이어그램을 진정으로 해석하지 않고도 답을 추론할 수 있도록 도와줄 가능성이 있습니다.
- 이를 극복하기 위해, MathVerse라는 새로운 시각적 수학 벤치마크를 소개하여 MLLM의 능력을 공정하고 깊이 있게 평가합니다.
- 고품질의 2,612개의 다양한 과목의 수학 문제를 공개적으로 이용 가능한 자료에서 선별하고, 인간 주석자들이 멀티모달 정보 내용의 차이를 가지는 여섯 가지 버전으로 변환하여 총 15K개의 테스트 샘플을 제공합니다.
- 이로써 MathVerse는 MLLM이 수학적 추론을 위한 시각적 다이어그램을 실제로 얼마나 이해할 수 있는지 평가할 수 있습니다.
- 추가적으로, 출력 답변의 세부적인 평가를 위해 ‘사고의 고리(Chain-of-Thought-CoT)’ 평가 전략을 제안합니다.
- 단순한 참/거짓 판단 대신, GPT-4(V)를 사용하여 중요한 추론 단계를 적응적으로 추출하고, 이를 상세하게 오류 분석하여 MLLM의 중간 CoT 추론 품질을 드러냅니다.
- 우리는 MathVerse 벤치마크가 MLLM의 미래 개발을 안내하는 독특한 통찰을 제공할 수 있기를 바랍니다.
- 프로젝트 페이지: https://mathverse-cuhk.github.io

### [DreamReward: Text-to-3D Generation with Human Preference](https://arxiv.org/abs/2403.14613)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14613.png)

Vote: 22

Authors: Xinzhou Wang, Yueqi Duan, Fangfu Liu, Jun Zhu, Zhengyi Wang, Qixiu Li, Junliang Ye, Yikai Wang

- 텍스트 프롬프트로부터 3D 콘텐츠 생성이 최근 놀라운 성과를 보였으나, 현재의 텍스트-투-3D 기법들은 종종 인간의 선호도와 일치하지 않는 결과를 생성합니다.
- 본 논문에서는 인간의 선호 반응 피드백에서 배우고 텍스트-투-3D 모델을 개선하는 종합적인 방법론인 'DreamReward'를 제시합니다.
- 우선, 체계적인 주석 파이프라인을 포함한 25k 전문가 비교를 수집하여, 인간의 선호도를 효과적으로 인코딩하는 첫 번째 범용 텍스트-투-3D 인간 선호 보상 모델인 'Reward3D'를 구축합니다.
- 3D 보상 모델을 기반으로, 저자들은 이론적 분석을 수행하고, 재정의된 평가자와 함께 멀티뷰 확산 모델을 최적화하기 위한 직접 조정 알고리즘인 'Reward3D Feedback Learning (DreamFL)'를 제시합니다.
- 이론적 증명과 광범위한 실험 비교를 바탕으로, DreamReward는 인간의 의도와의 프롬프트 일치성에서 현저한 향상과 함께 고해상도 및 3D 일관성 결과를 성공적으로 생성합니다.
- 인간의 피드백으로부터 배워 텍스트-투-3D 모델을 개선하는 큰 잠재력이 우리의 결과로 입증됩니다.

### [Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference](https://arxiv.org/abs/2403.14520)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14520.png)

Vote: 13

Authors: Wei Zhao, Donglin Wang, Han Zhao, Siteng Huang, Pengxiang Ding, Min Zhang

- 최근 다양한 분야에서 멀티모달 대규모 언어 모델(MLLM)의 적용이 두드러진 성공을 거두고 있지만, 기존 MLLM은 계산 효율이 낮은 쿼드라틱 복잡도를 가진 트랜스포머 네트워크로 이루어져 있다.
- 이러한 기초 모델의 효율성을 개선하기 위해, 연구팀은 선형 계산 복잡성을 가진 MLLM인 Cobra를 제안하며, 이는 효율적인 맘바 언어 모델을 시각적 모달리티와 통합시켰다.
- 다양한 모달 융합 방식을 탐구하고 연구하여 효과적인 멀티모달 맘바를 만드는 것을 목표로 한다.
- 광범위한 실험을 통해 Cobra가 현재의 계산 효율적인 최신 방법들 예를 들어 LLaVA-Phi, TinyLLaVA, MobileVLM v2와 매우 경쟁적인 성능을 내면서도 Cobra의 선형 순차 모델링으로 인해 더 빠른 속도를 달성한다는 것을 보여준다.
- 놀랍게도 Cobra는 매개변수 수가 약 43% 정도인데도 LLaVA와 비슷한 성능을 달성한다.
- 또한, Cobra는 시각적 착시와 공간 관계 판단에서 잘 수행함으로써 고난도의 예측 벤치마크에서 뛰어난 결과를 보여준다.
- 연구팀은 Cobra의 모든 코드를 오픈소스로 공개할 예정이며 제안된 방법이 향후 MLLM의 복잡성 문제에 대한 연구를 촉진할 수 있기를 희망한다.
- 프로젝트 페이지는 다음 주소에서 확인할 수 있다: https://sites.google.com/view/cobravlm.

### [AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks](https://arxiv.org/abs/2403.14468)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14468.png)

Vote: 11

Authors: Max Ku, Weiming Ren, Huan Yang, Cong Wei, Wenhu Chen

- 기존의 비디오-투-비디오 편집 방식의 한계를 극복하기 위해 본 논문에서는 새로운 훈련이 필요 없는 프레임워크인 AnyV2V를 소개합니다.
- AnyV2V는 두 단계로 비디오 편집을 단순화합니다: 첫 번째 프레임을 수정하기 위해 이미지 편집 모델을 사용하고, 이미지-투-비디오 생성 모델을 통해 DDIM 반전과 특징 주입을 사용합니다.
- 이 프레임워크는 다양한 이미지 편집 도구를 통합하여 비디오 편집 작업의 폭을 넓힐 수 있으며, 기존 방식에서 달성할 수 없었던 참조 기반 스타일 전달, 주제 중심 편집, 신원 조작 등의 새로운 편집 작업도 지원할 수 있습니다.
- AnyV2V는 이미지-투-비디오 모델을 통합하여 원본 비디오와의 외형 및 동작 일관성을 유지하며, 사용자의 다양한 요구를 충족시킬 수 있습니다.
- 프롬프트 기반 편집에서 AnyV2V는 기존 방법보다 프롬프트 일치도에서 35%, 사람의 선호도에서 25% 향상된 성능을 보여줍니다.
- 논문은 AnyV2V가 이미지 편집 방법이 급속하게 발전함에 따라, 그 호환성을 통해 다양한 사용자 요구에 대응할 수 있는 가능성을 가지고 있음을 제안합니다.

### [Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition](https://arxiv.org/abs/2403.14148)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14148.png)

Vote: 10

Authors: Anima Anandkumar, Sihyun Yu, Boyi Li, Weili Nie, De-An Huang, Jinwoo Shin

- 비디오 확산 모델은 최근 생성 품질에서 큰 진전을 이루었지만 여전히 높은 메모리와 계산 요구사항에 제한을 받고 있습니다.
- 이를 해결하기 위해, 우리는 사전 훈련된 이미지 확산 모델을 확장한 새로운 효율적인 비디오 생성을 위한 '콘텐츠-모션 잠재 확산 모델(CMD)'을 제안합니다.
- 본 연구는 비디오를 공통 콘텐츠를 표현하는 콘텐츠 프레임과, 비디오 내 움직임을 나타내는 저차원 모션 잠재 표현으로 응축시키는 오토인코더를 제안합니다.
- 사전 훈련된 이미지 확산 모델을 미세 조정함으로써 콘텐츠 프레임을 생성하고, 새롭고 가벼운 확산 모델을 통해 모션 잠재 표현을 생성합니다.
- 이 논문의 주요 혁신은 사전 훈련된 이미지 확산 모델을 직접 활용할 수 있는 컴팩트한 잠재 공간 설계에 있어, 이는 이전의 잠재 비디오 확산 모델에서는 이루어지지 않았습니다.
- 이에 따라 CMD는 두드러지게 향상된 생성 품질과 감소된 계산 비용을 실현합니다.
- 예를 들어, CMD는 512x1024 해상도와 길이 16인 비디오를 3.1초 내에 7.7배 빠르게 샘플링할 수 있습니다.
- 또한, CMD는 WebVid-10M 데이터셋에서 FVD 점수가 212.7로, 기존 최고 기록인 292.4보다 27.3% 더 나은 성능을 보여줍니다.

### [GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation](https://arxiv.org/abs/2403.14621)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14621.png)

Vote: 9

Authors: Zifan Shi, Wang Yifan, Gordon Wetzstein, Ceyuan Yang, Sida Peng, Yinghao Xu, Hansheng Chen, Yujun Shen

- 본 논문에서는 희소한 시점 이미지로부터 3D 자산을 약 0.1초 내에 복원할 수 있는 대규모 복원기인 GRM을 소개합니다.
- GRM은 피드포워드 트랜스포머 기반 모델로, 멀티뷰 정보를 효율적으로 통합하여 입력 픽셀을 픽셀 정렬 가우시안으로 변환하고, 장면을 나타내는 밀집 분포 3D 가우시안 세트를 생성하기 위해 이를 투영 해제합니다.
- 트랜스포머 아키텍처와 3D 가우시안의 사용은 확장 가능하고 효율적인 복원 프레임워크를 가능하게 합니다.
- 광범위한 실험 결과는 본 방법의 재구성 품질과 효율성 면에서 대안들에 비해 우수함을 입증합니다.
- GRM은 기존의 멀티뷰 확산 모델과 통합하여 텍스트-투-3D 및 이미지-투-3D와 같은 생성 작업에서의 잠재력을 보여줍니다.
- 프로젝트 웹사이트 주소는 https://justimyhxu.github.io/projects/grm/ 입니다.

### [ReNoise: Real Image Inversion Through Iterative Noising](https://arxiv.org/abs/2403.14602)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14602.png)

Vote: 8

Authors: Daniel Garibi, Or Patashnik, Andrey Voynov, Daniel Cohen-Or, Hadar Averbuch-Elor

- 본 연구에서는 사전 훈련된 디퓨전 모델의 도메인으로 실제 이미지를 역변환하는 과정에서 높은 복원 정확도를 달성하는 새로운 인버전 방법을 소개한다.
- 이를 위해 연구팀은 기존 디퓨전 샘플링 과정을 거꾸로 진행하며 효율적인 작동 대비 고품질 비율을 갖는 반복적인 재노이징 기법을 적용한다.
- 해당 기법은 예측된 포인트를 더 정확하게 근사하기 위해 사전 훈련된 디퓨전 모델을 반복적으로 적용하고 이러한 예측들을 평균화한다.
- 다양한 샘플링 알고리즘 및 최신 가속 디퓨전 모델을 사용하여 ReNoise 기술의 성능을 평가하였고, 정확성과 속도 측면에서 그 효과를 입증하였다.
- 또한, 연구진은 텍스트 기반 이미지 편집을 통해 실제 이미지에 대한 방법의 편집 가능성을 유지함을 확인하였다.

### [MyVLM: Personalizing VLMs for User-Specific Queries](https://arxiv.org/abs/2403.14599)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14599.png)

Vote: 8

Authors: Sergey Tulyakov, Daniel Cohen-Or, Yuval Alaluf, Elad Richardson, Kfir Aberman

- 최근의 대규모 시각-언어 모델(VLM)은 시각적 콘텐츠에 대한 텍스트 설명을 이해하고 생성하는 놀라운 능력을 보여주었지만, 사용자 특정 개념에 대한 이해는 부족합니다.
- 이 연구에서는 VLM의 개인화를 향한 첫 걸음으로, 사용자가 제공한 개념들을 학습하고 추론할 수 있도록 모델을 활성화하는 방법을 탐구합니다.
- 예를 들어, 모델이 이미지 속 당신을 인식하고 당신이 무엇을 하고 있는지 소통할 수 있는지, 개인적인 경험과 관계를 반영할 수 있는지 등을 탐색합니다.
- 다양한 사용자 특정 개념을 효과적으로 인식하기 위하여, 우리는 VLM에 외부 개념 헤드를 추가하여 특정 타겟 개념이 주어진 이미지 안에 존재하는지 식별하는 기능을 할 수 있게 만들었습니다.
- 개념을 식별한 뒤, VLM의 중간 특징 공간에서 새로운 개념 임베딩을 학습하여, 언어 모델이 생성된 반응에 타겟 개념을 자연스럽게 통합하도록 안내합니다.
- 우리는 BLIP-2와 LLaVA에 대한 개인화된 이미지 캡셔닝을 위해 이 기술을 적용하고, 개인화된 시각적 질문-응답에 대한 적용 가능성도 보여줍니다.
- 우리의 실험은 학습된 개념에 대한 보이지 않는 이미지로의 일반화 능력을 입증하고 동시에 관련 없는 입력에 대한 모델 행동을 유지하고 있음을 보여줍니다.

### [Explorative Inbetweening of Time and Space](https://arxiv.org/abs/2403.14611)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14611.png)

Vote: 7

Authors: Xuaner Zhang, Zheng Ding, Michael J. Black, Haiwen Feng, Zhihao Xia, Simon Niklaus, Victoria Abrevaya

- 시작 프레임과 끝 프레임만 주어진 상태에서 임의의 카메라와 주제의 움직임을 합성하는 비디오 생성을 제어하는 일반화된 작업인 'bounded generation'을 소개합니다.
- 추가 훈련이나 원 모델의 미세 조정 없이 이미지-비디오 모델의 내재된 일반화 능력을 최대로 활용하는 것을 목표로 합니다.
- 제안된 새로운 샘플링 전략인 'Time Reversal Fusion'은 시작 프레임과 끝 프레임에 각각 조건을 부여한 시간적으로 전방 및 후방 탈락 경로를 융합합니다.
- 융합된 경로는 두 프레임을 부드럽게 연결하는 비디오를 생성하여 신뢰할 수 있는 주제의 움직임, 정적 장면의 새로운 관점 및 두 경계 프레임이 동일할 때 이음새 없는 비디오 루핑을 생성합니다.
- 우리는 이미지 쌍의 다양한 평가 데이터 세트를 큐레이션하고 가장 가까운 기존 방법들과 비교합니다.
- Time Reversal Fusion은 모든 하위 작업에서 관련 작업보다 우수한 성능을 보여 주며, 경계 프레임에 의해 유도되는 복잡한 움직임과 3D 일관된 관점을 생성할 수 있는 능력을 보여줍니다.
- 프로젝트 페이지는 https://time-reversal.github.io에서 확인할 수 있습니다.

### [StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN](https://arxiv.org/abs/2403.14186)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14186.png)

Vote: 6

Authors: Kwanggyoon Seo, Junyong Noh, Jongwoo Choi, Amirsaman Ashtari

- 우리는 사전에 훈련된 StyleGAN을 사용하여 정지된 풍경 이미지로부터 자동으로 시네마그래프를 생성할 수 있는 방법을 제안한다.
- 최근 무조건적인 비디오 생성에서의 성공에 영감을 받아, 고품질 시네마그래프를 합성하기 위한 강력한 사전 훈련 이미지 생성기를 활용한다.
- 기존 방식과 달리, 우리의 접근 방식은 사전 훈련된 StyleGAN의 잠재 공간뿐만 아니라 GAN 인버전과 시네마그래프 생성을 위해 깊은 특징 공간을 활용한다.
- 다양한 해상도에서 사전 훈련된 StyleGAN의 중간 특징을 왜곡하는 멀티스케일 깊은 특징 왜곡(Multi-Scale Deep Feature Warping, MSDFW)을 제안한다.
- MSDFW을 사용함으로써 생성된 시네마그래프는 고해상도를 유지하고, 설득력 있는 루핑 애니메이션을 보여준다.
- 우리는 사용자 연구 및 최신 시네마그래프 생성 방법과 사전 훈련된 StyleGAN을 사용하는 비디오 생성 방법과의 정량적 비교를 통해 우리 방법의 우수성을 입증한다.

### [Recourse for reclamation: Chatting with generative language models](https://arxiv.org/abs/2403.14467)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14467.png)

Vote: 4

Authors: William Isaac, Jackie Kay, Kevin R. McKee, Jennifer Chien

- 연구자들과 개발자들은 고객 서비스, 정보 검색, 콘텐츠 생성 등의 설정에서 생성 언어 모델 출력을 조절하기 위해 점점 더 독성 점수를 의존하고 있습니다.
- 그러나 독성 점수는 관련 정보에 대한 접근을 차단하고, 문화적 규범을 고착화하며, 특히 소외계층의 언어 회수 과정을 방해할 수 있습니다.
- 이 연구에서는 생성 언어 모델에 대한 알고리즘 구제의 개념을 확장하여, 사용자가 독성 필터링의 임계값을 동적으로 설정함으로써 원하는 예측을 달성할 수 있는 새로운 메커니즘을 제공합니다.
- 이를 통해 사용자는 기본 시스템과의 상호 작용에 비해 더 큰 자율성을 행사할 수 있습니다.
- 30명의 참가자를 대상으로 한 초기 연구는 제안된 구제 메커니즘이 모델 출력의 고정 임계값 독성 필터링보다 사용성을 향상시킨다는 가능성을 뒷받침합니다.
- 향후 연구는 독성 점수 산정, 모델의 조작 가능성, 사용자의 자율성, 그리고 특히 생성 언어 모델과 상호작용 시 많은 커뮤니티가 겪는 편견과의 교차점을 탐구해야 합니다.

### [Gaussian Frosting: Editable Complex Radiance Fields with Real-Time Rendering](https://arxiv.org/abs/2403.14554)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14554.png)

Vote: 4

Authors: Antoine Guédon, Vincent Lepetit

- 본 논문은 고품질의 3D 효과를 실시간으로 렌더링하고 편집할 수 있는 새로운 메쉬 기반 표현 방식인 Gaussian Frosting을 제안한다.
- 이 방법은 이미지에서 방사선 필드를 근사하는 3D 가우스 분포를 최적화하는 최근의 3D Gaussian Splatting 프레임워크를 기반으로 한다.
- 최적화 중에 가우스 분포에서 기본 메쉬를 추출한 다음, 메쉬 주변에 가변 두께의 적응형 가우스 층을 구축하여 표면 근처의 미세한 디테일과 부피 효과(예: 머리카락이나 잔디)를 보다 잘 포착한다.
- 이 층을 Gaussian Frosting이라고 부르며, 이는 케이크에 코팅된 프로스팅 층을 닮았으며, 재질이 더 퍼져있을수록 프로스팅 층은 더 두꺼워진다.
- Mesh를 변형, 재조정, 편집 또는 애니메이션할 때 가우스체가 프로스팅 층 내부에 머물면서 자동으로 매개변수를 조정하도록 가우스체의 매개변수화를 도입한다.
- 제안된 표현은 Gaussian splatting을 이용한 효율적인 렌더링뿐만 아니라 기본 메쉬를 수정함으로써 편집 및 애니메이션이 가능하다.
- 저자들은 다양한 합성 및 실제 장면에서 해당 방법의 효과를 보여주고 기존의 표면 기반 접근법을 능가함을 증명한다.
- 본 프로젝트의 코드와 웹 기반 뷰어는 추가적인 기여로 제공될 예정이며, 프로젝트 페이지 주소도 공개한다.

