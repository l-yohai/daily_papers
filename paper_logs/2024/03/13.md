## Daily Papers (2024-03-13)

### [MoAI: Mixture of All Intelligence for Large Language and Vision Models](https://arxiv.org/abs/2403.07508)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.07508.png)

Vote: 34

Authors: Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yong Man Ro

- 영어 제목의 "Mixture of All Intelligence for Large Language and Vision Models"를 한국어로 만들면 "대규모 언어 및 비전 모델을 위한 모든 지능의 혼합(MoAI)"가 될 수 있다.
- 최근의 추세에 따르면, 대규모 언어 모델(LLMs)과 지시 학습(instruction tuning)이 주목받고 있으며, 이는 대규모 언어-비전 모델(LLVM)의 등장으로 이어졌다.
- 현 LLVM들은 세분화된 컴퓨터 비전(CV) 모델을 통해 얻을 수 있는 자세한 실세계 장면 이해를 활용하는 데 미흡하다.
- 본 논문에서는 외부 세분화, 탐지, 장면 그래프 생성(SGG), 광학 문자 인식(OCR) 모델의 출력을 활용하는 새로운 LLVM인 MoAI를 제시한다.
- MoAI는 두 가지 새로운 모듈인 MoAI-압축기(MoAI-Compressor)와 MoAI-믹서(MoAI-Mixer)를 통해 작동한다.
- MoAI-압축기는 외부 CV 모델의 출력을 언어화하고 이를 효율적으로 정렬 및 압축하여 VL 과제에 필요한 보조 비주얼 정보를 활용한다.
- MoAI-믹서는 시각적 특징, 외부 CV 모델로부터의 보조 특징, 그리고 언어 특징을 전문가 혼합 개념을 사용하여 융합한다.
- 이러한 통합을 통해 MoAI는 객체 존재, 위치, 관계, OCR과 같이 실세계 장면 이해와 관련된 다양한 제로샷 VL 과제에서 기존의 오픈소스 및 클로즈소스 LLVM을 크게 능가한다.
- 이 모델은 추가적인 시각 지시 학습 데이터셋을 만들거나 모델 크기를 확대하지 않고도 성능을 향상시키는 결과를 보여준다.

### [Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.07815.png)

Vote: 22

Authors: Michael Bohlke-Schneider, Andrew Gordon Wilson, Danielle C. Maddix, Lorenzo Stella, Shubham Kapoor, Sebastian Pineda Arango, Huibin Shen, Michael W. Mahoney, Xiyuan Zhang, Oleksandr Shchur, Kari Torkkola, Pedro Mercado, Syama Sundar Rangapuram, Jasper Zschiegner, Yuyang Wang, Caner Turkmen, Abdul Fatir Ansari

- 이 연구에서는 사전 훈련된 확률 시계열 모델을 위한 단순하지만 효과적인 프레임워크인 Chronos를 소개합니다.
- Chronos는 시계열 값을 고정된 어휘로 스케일링하고 양자화하여 토큰화하고, 이 토큰화된 시계열을 교차 엔트로피 손실을 통해 기존의 트랜스포머 기반 언어 모델 아키텍처에서 훈련합니다.
- 20M부터 710M까지의 매개변수를 갖는 T5 패밀리에 기반한 Chronos 모델이 공개 데이터셋 대규모 컬렉션과 일반화를 향상시키기 위해 가우시안 프로세스로 생성된 합성 데이터셋에서 사전 훈련되었습니다.
- 42개의 데이터셋으로 구성된 포괄적인 벤치마크에서 Chronos 모델은 훈련 데이터 셋을 포함한 데이터셋에서 다른 방법보다 상당히 뛰어난 성능을 보였습니다.
- 또한, Chronos는 훈련된 데이터셋에서와 비교하여 예측 작업에서 훈련되지 않은 새로운 데이터셋에 대해 동등하거나 때때로 더 우수한 제로샷 성능을 보입니다.
- 연구 결과는 Chronos 모델이 다양한 도메인의 시계열 데이터를 활용하여 본 적 없는 예측 작업에서의 제로샷 정확도를 향상시킬 수 있음을 보여주며, 사전 훈련된 모델을 예측 파이프라인을 크게 단순화하는 유용한 도구로 위치시킵니다.

### [Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM](https://arxiv.org/abs/2403.07816)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.07816.png)

Vote: 19

Authors: Xi Victoria Lin, Baptiste Rozière, Wen-tau Yih, Daniel Li, Jacob Kahn, Vasu Sharma, Hu Xu, Jason Weston, Sainbayar Sukhbaatar, Xian Li, Olga Golovneva

- 이 연구는 코딩, 수학적 추론, 세계 지식 등 여러 전문 분야에서 기능할 수 있는 대규모 언어 모델(Large Language Models, LLMs)을 효율적으로 훈련하는 방법을 조사합니다.
- 연구진이 제안하는 Branch-Train-MiX(BTX) 방법론은 기본 모델에서 시작하여 전문가 모델들을 통신 비용을 줄이고 고속처리로 병렬적으로 분기시켜 훈련시킵니다.
- 개별적으로 비동기적으로 훈련된 전문가들의 feedforward 매개변수들을 모아 Mixture-of-Expert(MoE) 층의 전문가로 통합하고, 나머지 매개변수들은 평균내며 MoE-튜닝 단계에서 토큰 수준 라우팅을 학습합니다.
- BTX는 라우팅 학습 없이 전문가들을 통합하는 Branch-Train-Merge 방법론과 비동기 전문가 훈련 단계없이 희소 업사이클링하는 방법론을 일반화한 것입니다.
- 비교 실험을 통해 BTX가 정확성과 효율성의 균형에서 대안적 접근 방법들과 비교하여 가장 우수한 성과를 낸다는 것을 보여줍니다.

### [Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings](https://arxiv.org/abs/2403.07750)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.07750.png)

Vote: 16

Authors: Sahand Sharifzadeh, Shreya Pathak, Anastasija Ilic, Dharshan Kumaran, Charles Blundell, Jovana Mitrovic, Christos Kaplanis, Andrea Banino

- 고품질의 수작업으로 레이블링된 이미지-캡션 데이터셋 생성의 병목 현상을 해소하기 위해 대규모 언어 모델(Large Language Models, LLMs)과 이미지 생성 모델을 활용하여 합성 이미지-텍스트 쌍을 이용한 비주얼-언어 모델(Visual-Language Models, VLMs) 훈련 방법을 제안한다.
- LLM에서 생성한 캡션으로부터 이미지 임베딩을 합성하기 위해 텍스트-이미지 모델을 사전 훈련하고, 이 합성된 쌍을 VLM 훈련에 사용한다.
- 실제 데이터의 일부만을 사용하더라도, 합성 데이터로 훈련한 VLM이 이미지 캡셔닝에서 인간이 만든 데이터로만 훈련한 모델과 비슷한 성능을 보이는 것을 광범위한 실험을 통해 입증한다.
- 특히, 합성 데이터셋을 추가로 활용함으로써 기준 모델을 17% 향상시킨다.
- 또한, 이미지 임베딩 공간에서 합성하는 것이 픽셀 공간에서 합성하는 것보다 25% 빠름을 보여준다.
- 이 연구는 대규모 맞춤형 이미지 데이터셋을 생성하고, VLM의 성능을 향상시키며, 데이터 효율성과 자원 활용을 개선하여 다양한 분야에 걸쳐 널리 적용될 수 있는 유망한 기술을 소개한다.

### [Motion Mamba: Efficient and Long Sequence Motion Generation with Hierarchical and Bidirectional Selective SSM](https://arxiv.org/abs/2403.07487)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.07487.png)

Vote: 8

Authors: Hao Tang, Akide Liu, Zeyu Zhang, Richard Hartley, Bohan Zhuang, Ian Reid

- 인간의 동작 생성을 위한 Motion Mamba 모델은 효율적이며 장기간의 동작 시퀀스를 생성하는 데 중점을 두고 있다.
- 최신 SSM(State Space Models) 중 하나인 Mamba는 하드웨어 친화적 설계로 장기 시퀀스 모델링에서 우수한 성능을 보이고 있으며, 이를 모션 생성에 적용하는 방향이 유망하다.
- 동작 시퀀스를 모델링하도록 특수하게 설계된 아키텍처의 결여로 인해 SSM을 모션 생성에 적용하는 것은 어려움이 있다.
- 이러한 도전에 대응하기 위하여, 저자들은 SSM을 활용한 최초의 모션 생성 모델인 Motion Mamba를 제안한다.
- 저자들은 HTM(Hierarchical Temporal Mamba) 블록을 설계하여 대칭적인 U-Net 구조를 통해 다양한 독립적인 SSM 모듈의 앙상블을 활용하여 시간적 데이터를 처리하고 프레임 간의 동작 일관성을 유지한다.
- 또한, BSM(Bidirectional Spatial Mamba) 블록을 설계하여 시간 프레임 내에서 정확한 동작 생성을 향상시키기 위해 양방향으로 잠재적인 포즈를 처리한다.
- 제안된 방법은 HumanML3D 및 KIT-ML 데이터셋에서 이전의 최고의 확산 기반 방법에 비해 최대 50%의 FID 개선과 최대 4배 빠른 속도를 달성했다.
- 이는 고품질 장기간 동작 모델링 및 실시간 인간 동작 생성의 강력한 능력을 나타낸다.
- 프로젝트의 자세한 내용과 데이터는 https://steve-zeyu-zhang.github.io/MotionMamba/ 웹사이트에서 확인할 수 있다.

### [FAX: Scalable and Differentiable Federated Primitives in JAX](https://arxiv.org/abs/2403.07128)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.07128.png)

Vote: 8

Authors: Keith Rush, Zachary Garrett, Zachary Charles

- 본 논문에서는 데이터 센터 및 크로스-디바이스 애플리케이션에서 대규모 분산 및 페더레이티드(연합) 계산을 지원하기 위해 설계된 JAX 기반 라이브러리인 FAX를 제시합니다.
- FAX는 JAX의 샤딩 메커니즘을 활용하여 TPU 타게팅과 Pathways를 포함한 최첨단 JAX 런타임을 기본적으로 사용할 수 있게 합니다.
- 연합 계산을 위한 기본 요소들을 JAX의 원시 연산으로 통합함으로써, FAX는 세 가지 주요 이점을 제공합니다.
- 첫째, FAX 계산은 XLA HLO로 변환될 수 있습니다.
- 둘째, FAX는 연합 자동 미분(Federated Automatic Differentiation)의 전체 구현을 제공하여, 연합 계산의 표현을 크게 단순화합니다.
- 셋째, FAX 계산은 기존의 생산적인 크로스-디바이스 연합 컴퓨팅 시스템으로 해석될 수 있습니다.
- FAX가 데이터 센터에서 프로그래밍하기 쉽고, 성능이 우수하며, 연합 계산을 위한 확장 가능한 프레임워크를 제공함을 보여줍니다.
- FAX는 https://github.com/google-research/google-research/tree/master/fax 에서 사용할 수 있습니다.

### [DragAnything: Motion Control for Anything using Entity Representation](https://arxiv.org/abs/2403.07420)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.07420.png)

Vote: 8

Authors: Rui Zhao, Di Zhang, Wejia Wu, Yan Li, Tingting Gao, Zhuang Li, David Junhao Zhang, Yuchao Gu, Yefei He, Mike Zheng Shou

- 'DragAnything'는 제어 가능한 비디오 생성에서 어떠한 객체의 움직임도 제어할 수 있는 개체 표현(entity representation)을 사용하는 새로운 방법을 소개합니다.
- 마스크나 깊이 지도 등 다른 안내 신호를 얻는 것이 많은 노력을 요구할 때, 사용자가 상호작용하는 동안 경로(trajectory)를 그리기만 하면 되므로, 경로 기반 접근법은 더 사용자 친화적입니다.
- 개체 표현은 개방형 도메인 임베딩으로서, 배경을 포함하여 다양한 엔티티의 움직임을 제어할 수 있는 능력을 부여합니다.
- 또한, 본 연구에서 제시한 개체 표현은 여러 개체의 동시이면서 구별되는 움직임 제어를 가능하게 합니다.
- 광범위한 실험을 통해 'DragAnything'가 FVD, FID 및 사용자 연구에서 상태-최신 기술보다 뛰어난 성능을 달성함을 입증하였으며, 특히 객체 움직임 제어 측면에서 기존 방법들(예: DragNUWA)을 휴먼 보팅으로 26% 향상시켰다는 결과를 보여줍니다.

### [Learning Generalizable Feature Fields for Mobile Manipulation](https://arxiv.org/abs/2403.07563)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.07563.png)

Vote: 6

Authors: Yafei Hu, Xiaolong Wang, Yuchen Song, Ri-Zhao Qiu, Nikolay Atanasov, Sebastian Scherer, Jiteng Mu, Ruihan Yang, Jianglong Ye, Yang Fu, Ge Yang

- 모바일 조작을 위한 일반화 가능한 특징 필드를 공부하는 이 논문에서는 로봇이 환경 내에서 탐색하고 물체를 조작하는 데 사용할 수 있는 통합된 방식으로 객체와 장면을 표현하는 문제를 다룬다.
- 이 작업은 GeFF(Generalizable Feature Fields)라고 하는 신경 특징 필드를 제안하여, 복잡한 기하학적 구조를 포착하고 세밀한 의미론적 이해를 돕는 것과 아울러 넓은 물리적 규모의 복잡성을 포착하는 일반화 가능한 장면 레벨 표현으로써 기능한다.
- GeFF는 생성적인 신규 시점 합성을 사전 학습 과제로 삼고, 결과로 얻어진 풍부한 장면 사전 지식을 CLIP 특징 증류를 통해 자연 언어와 맞춰 나간다.
- 조작 장치가 장착된 네 발 로봇에 GeFF를 배치하여, 동적 장면에서의 개방형 어휘 모바일 조작 수행 시 GeFF의 일반화 능력과 러닝 시간을 평가한다.

