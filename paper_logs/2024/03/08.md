## Daily Papers (2024-03-08)

### [Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference](https://arxiv.org/abs/2403.04132)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/g-NioYzA0w1stNisWTTJm.png)

Vote: 22

Authors: Ion Stoica, Joseph E. Gonzalez, Banghua Zhu, Wei-Lin Chiang, Michael Jordan, Hao Zhang, Lianmin Zheng, Dacheng Li, Anastasios Nikolas Angelopoulos, Tianle Li, Ying Sheng

- 대형 언어 모델(LLMs)이 새로운 기능과 응용 프로그램을 가능하게 했지만 인간의 선호도와의 일치를 평가하는 것은 여전히 중요한 도전이 되고 있다.
- 이 문제를 해결하기 위해, 'Chatbot Arena'라는 인간 선호도를 기반으로 한 LLMs 평가를 위한 오픈 플랫폼이 소개되었다.
- 연구 팀은 페어와이즈 비교 방식을 사용하고, 다양한 사용자 기반을 통한 크라우드소싱을 활용하여 메소드를 개발했다.
- 이 플랫폼은 몇 개월 동안 운영되어 24만 건 이상의 투표를 모았으며, 이 논문에서는 플랫폼을 설명하고 지금까지 수집한 데이터를 분석한다.
- 연구진은 효율적이고 정확한 평가 및 모델 순위를 매기기 위해 입증된 통계적 방법을 사용하고 있다고 설명한다.
- 크라우드소싱된 질문이 다양하고 차별화되어 있으며, 크라우드소싱된 인간의 표가 전문가 평가자들의 것과 잘 일치함을 확인했다.
- 이러한 분석은 Chatbot Arena의 신뢰성을 강력하게 뒷받침한다.
- 그 고유한 가치와 개방성으로 인해, Chatbot Arena는 주요 LLM 개발자들과 회사들에 의해 널리 인용되는 가장 참조된 LLM 리더보드 중 하나로 부상했다.
- 이 데모는 https://chat.lmsys.org 에서 공개적으로 이용 가능하다.

### [PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation](https://arxiv.org/abs/2403.04692)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ziRGvTG3Wa3-IxiEklvnw.png)

Vote: 22

Authors: Xiaozhe Ren, Huchuan Lu, Lewei Yao, Ping Luo, Enze Xie, Yue Wu, Zhongdao Wang, Chongjian Ge, Zhenguo Li, Junsong Chen

- 본 논문에서는 4K 해상도에서 직접 이미지를 생성할 수 있는 확산 변환 모델(Diffusion Transformer, DiT)인 PixArt-Σ를 소개합니다.
- PixArt-Σ는 이전 모델인 PixArt-α보다 눈에 띄게 더 높은 품질과 텍스트 프롬프트와의 더 나은 일치도를 제공하는 주목할 만한 발전을 보여줍니다.
- '약-강 훈련(weak-to-strong training)'이라는 프로세스를 통해, PixArt-Σ는 기본적인 PixArt-α의 사전 훈련을 활용하여, 더 고품질의 데이터를 포함시키며 '약한' 기반에서 '강한' 모델로 진화합니다.
- PixArt-Σ는 고품질의 훈련 데이터와 더욱 정확하고 상세한 이미지 캡션을 결합했다는 점이 특징입니다.
- 또한, DiT 프레임워크 내에서 새로운 주의 모듈을 제안하여 키(key)와 값(value)을 압축함으로써 효율성을 크게 향상시키고 초고해상도 이미지 생성을 용이하게 합니다.
- 이러한 개선으로 인해 PixArt-Σ는 기존의 텍스트-투-이미지 확산 모델들보다 현저히 작은 모델 크기(0.6B 파라미터)를 가지면서도 우수한 이미지 품질과 사용자 프롬프트 준수 능력을 달성합니다.
- PixArt-Σ는 4K 이미지를 생성할 수 있는 능력을 갖추고 있어, 영화 및 게임 산업과 같은 분야에서 고품질 시각 콘텐츠의 효율적인 생산을 강화할 수 있습니다.

### [Teaching Large Language Models to Reason with Reinforcement Learning](https://arxiv.org/abs/2403.04642)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wFKm6Ml2h73abCp4v5hsz.png)

Vote: 20

Authors: Eric Hambro, Roberta Raileanu, Yuqing Du, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Sainbayar Sukhbaatar, Sharath Chandra Raparthy, Christoforos Nalmpantis, Alex Havrilla

- 인간 피드백으로부터 강화학습(RLHF)은 언어 모델 출력을 인간의 선호도에 맞추기 위한 강력한 방법으로 부상하였습니다.
- 이 연구는 여러 강화학습 알고리즘들(전문가 반복, 근접 정책 최적화(PPO), 반환 조건부 RL)이 대규모 언어 모델(LLM)의 추론 능력 향상에 미치는 성능을 조사하였습니다.
- 희귀 보상과 밀집 보상이 모델에게 제공되며, 이는 휴리스틱 방법 또는 학습된 보상 모델을 통해 이루어집니다.
- 연구는 다양한 모델 크기 및 초기화와 더불어 감독되는 미세 조정(SFT) 데이터의 유무를 포함하여 시작하였습니다.
- 전반적으로 모든 알고리즘이 비슷한 성능을 보였으며, 대부분의 경우에서는 전문가 반복이 가장 좋은 결과를 보였습니다.
- 놀랍게도, 전문가 반복의 샘플 복잡성은 PPO와 유사하며, 사전 훈련된 체크포인트에서 수렴하는 데 최대 10^6개의 샘플 정도가 필요했습니다.
- 연구는 강화학습 동안 모델이 SFT 모델에 의해 생성된 해결책을 넘어서는 탐색에 실패하는 이유를 조사하였습니다.
- SFT 훈련 동안 maj@1과 pass@96 메트릭 성능 사이의 트레이드오프가 있었지만, 반대로 강화학습은 동시에 두 성능을 향상시킨다는 것을 논의합니다.
- 마지막으로 연구는 RLHF와 LLM 미세 조정에서 RL의 미래 역할에 대한 연구 결과의 의미를 논의합니다.

### [Yi: Open Foundation Models by 01.AI](https://arxiv.org/abs/2403.04652)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/yjdlaxT0u_n0CKGk7fIZ-.png)

Vote: 19

Authors: Qiang Liu, Alex Young, Chao Li, Heng Li, Xiaoyi Ren, Senbin Yang, Chengen Huang, Xiaohui Hu, Tao Yu, Wenhao Huang, Jing Chang, Shawn Yue, Guanwei Zhang, Bei Chen, Jiangcheng Zhu, Peng Liu, +, 01. AI, Kaidong Yu, Ge Zhang, Jianqun Chen, Wen Xie, Shiming Yang

- 01.AI 팀이 Yi 모델 패밀리를 소개했으며, 여기에는 강력한 다차원 능력을 보여주는 언어 및 다중 모달 모델 시리즈가 포함되어 있습니다.
- Yi 모델 패밀리는 6B 및 34B 사전 훈련된 언어 모델을 기반으로 하며, 채팅 모델, 200K 긴 문맥 모델, 깊이 업스케일된 모델, 그리고 시각-언어 모델로 확장되었습니다.
- 기본 모델은 MMLU와 같은 다양한 벤치마크에서 강력한 성능을 달성했으며, 파인튜닝된 채팅 모델은 AlpacaEval과 Chatbot Arena와 같은 주요 평가 플랫폼에서 높은 인간 선호도를 기록했습니다.
- 스케일러블한 슈퍼 컴퓨팅 인프라와 클래식 트랜스포머 아키텍처를 기반으로, Yi 모델의 성능은 데이터 엔지니어링 노력으로 인한 데이터 품질에 주로 기인하는 것으로 분석됩니다.
- 사전 훈련을 위해, 연속된 데이터 중복 제거 및 품질 필터링 파이프라인을 사용하여 3.1조 토큰의 영어 및 중국어 코퍼스를 구성했습니다.
- 파인튜닝 단계에서는 10K 미만의 소규모 지시 데이터셋을 여러 번 반복하여 정제하며, 모든 인스턴스가 머신 러닝 엔지니어에 의해 직접 검증되었습니다.
- 시각-언어 분야에서는 채팅 언어 모델과 비전 트랜스포머 인코더를 결합하고 모델을 훈련하여 시각적 표현을 언어 모델의 의미 공간에 맞추도록 합니다.
- 경량의 지속적인 사전 훈련을 통해 문맥 길이를 200K까지 확장하고, 이를 통해 강력한 '바늘 찾기' 검색 성능을 보여주었습니다.
- 사전 훈련된 체크포인트의 깊이를 확장함으로써 성능이 더 향상된다는 사실을 보여주었습니다.
- 최적화된 데이터를 사용하여 모델 매개변수를 계속 확장함으로써 보다 강력한 선도적 모델을 이끌어낼 수 있을 것으로 기대한다고 밝혔습니다.

### [LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error](https://arxiv.org/abs/2403.04746)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/EtQghz4H4sfuzrxYPinsL.png)

Vote: 14

Authors: Yu Su, Boshi Wang, Hao Fang, Benjamin Van Durme, Jason Eisner

- 대규모 언어 모델(LLMs)이 최신 정보를 획득하고 외부 환경에서 중대한 행동을 취하기 위해 도구 사용은 필수적이지만 기존에는 도구 사용에 대한 정확성에 관한 연구가 부족했다.
- GPT-4를 포함한 기존 LLMs가 도구 사용 시 약 30%에서 60%의 정확도에 그치며, 실제 신뢰할 수 있는 수준까지는 이르지 못했다.
- 이 연구에서는 생물학적으로 영감을 받은 '시뮬레이션된 시행착오(STE)' 방법을 제안하여, 시행착오, 상상력, 그리고 기억이라는 세 가지 중요 메커니즘을 통해 성공적인 도구 사용 행위를 조율한다.
- STE는 LLM의 '상상력'을 이용하여 도구 사용을 위한 가능한 시나리오를 시뮬레이션 한 다음, 실행 피드백으로부터 학습하기 위해 도구와 상호 작용한다.
- 단기 및 장기 기억은 각각 탐색의 깊이와 폭을 향상시키는 데 사용된다.
- ToolBench에 대한 광범위한 실험을 통해 STE가 in-context 학습 및 미세 조정 설정에서 LLM의 도구 학습을 크게 개선하여 Mistral-Instruct-7B의 성능을 46.7% 향상시키고 GPT-4 보다 더 나은 성능을 보여준다는 것을 입증했다.
- 또한, 간단한 경험 재생 전략을 통해 도구에 대한 효과적인 연속 학습 가능성도 보여준다.

### [How Far Are We from Intelligent Visual Deductive Reasoning?](https://arxiv.org/abs/2403.04732)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/BXVTNtc2CFDFuU_wvJChb.png)

Vote: 14

Authors: Jiatao Gu, Shuangfei Zhai, Navdeep Jaitly, Yizhe Zhang, Ruixiang Zhang, He Bai, Josh Susskind

- 최근 GPT-4V와 같은 시각-언어 모델(VLMs)이 다양한 시각 언어 작업에서 놀라운 발전을 보였습니다.
- 본 연구는 시각 기반 추론이라는 더 복잡하고 덜 탐구된 영역에서 현재 최신 VLMs의 실제 넘어야 할 취약점을 발견하였습니다.
- 시각 단서에만 의존한 다단계 관계 및 추론 능력을 평가하기 위해 레이븐의 순차 매트릭스(RPM)을 활용했습니다.
- Mensa IQ 테스트, IntelligenceTest 및 RAVEN을 포함한 세 가지 다양한 데이터 세트에서 인텍스트 학습, 자기 일관성, 사고의 연쇄(Chain-of-Thought)와 같은 표준 전략을 사용하여 여러 인기 있는 VLMs에 대한 포괄적인 평가를 수행했습니다.
- VLMs가 복잡한 RPM 예시에서 여러 가지 혼동 추상 패턴을 인식하고 이해하지 못하는 것이 주된 문제인 것으로 분석되었음을 발견하였습니다.
- 이 연구는 텍스트 기반 추론에서의 여러 VLMs의 인상적인 능력에도 불구하고, 우리가 시각적 추론 능력에 있어서는 이에 상응하는 전문성을 달성하기까지 여전히 멀다는 것을 밝혀냈습니다.
- VLMs의 도전 과제에 적용될 때 VLMs에 있어서 효과적인 것으로 보이던 표준 전략들이 반드시 시각적 추론 작업의 문제점을 해결하지는 못한다는 것이 확인되었습니다.

### [StableDrag: Stable Dragging for Point-based Image Editing](https://arxiv.org/abs/2403.04437)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/fvJMbDlBIGJycmrI42VOw.png)

Vote: 12

Authors: Xiaotong Zhao, Guozhen Zhang, Shengming Cao, Kai Ma, Yutao Cui, Limin Wang

- 점 기반 이미지 편집에 대한 관심이 DragGAN의 등장 이후 주목할 만하게 증가하였습니다.
- 최근에 DragDiffusion은 확산 모델에 드래깅 기술을 적용하여 생성 품질을 더욱 향상시켰습니다.
- 그러나 기존 드래깅 스킴에는 부정확한 점 추적과 불완전한 움직임 감독이라는 두 가지 주요한 단점이 있어 만족스럽지 못한 결과를 초래할 수 있습니다.
- 이러한 문제를 해결하기 위해, StableDrag라 명명된 안정적이고 정확한 드래그 기반 편집 프레임워크를 구축했습니다.
- 이 프레임워크는 차별화된 점 추적 방법과 움직임 감독을 위한 신뢰도 기반의 잠재적인 강화 전략을 설계했습니다.
- 첫 번째 방법은 업데이트된 핸들 점들을 정확하게 위치시키는 데 도움을 주어 장거리 조작의 안정성을 높입니다.
- 두 번째 방법은 모든 조작 단계에서 최적화된 잠재 계층이 가능한 한 고품질을 유지하도록 보장합니다.
- 이러한 독특한 설계 덕분에, StableDrag-GAN과 StableDrag-Diff를 포함하는 두 가지 유형의 이미지 편집 모델을 구현하여 보다 안정적인 드래깅 성능을 달성했습니다.
- 광범위한 질적 실험과 DragBench에 대한 정량적 평가를 통해 이 모델들은 더 안정적인 드래깅 성능을 보여줍니다.

### [Pix2Gif: Motion-Guided Diffusion for GIF Generation](https://arxiv.org/abs/2403.04634)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/g3OCXU3aP1Up4xYD-VhQS.png)

Vote: 10

Authors: Jianwei Yang, Jianfeng Gao, Hitesh Kandala

- Pix2Gif는 텍스트와 동작 크기 프롬프트에 의해 제어되는 이미지 변환 문제로 GIF(비디오) 생성을 위한 모션 가이드 확산 모델을 제시합니다.
- 모델이 동작 가이드를 따르도록 보장하기 위해, 두 종류의 프롬프트에 조건을 걸고 원본 이미지의 특징을 공간적으로 변형시키는 새로운 모션 가이드 와핑(warping) 모듈을 제안합니다.
- 변형된 특징 맵이 대상 이미지와 동일한 공간 내에 있도록 하여 내용의 일관성과 연관성을 확보하기 위해 지각적 손실(perceptual loss)을 도입했습니다.
- 모델 훈련을 준비하기 위해, 주체의 시간적 변화에 대한 풍부한 정보를 제공하는 TGIF 비디오-캡션 데이터셋에서 일관된 이미지 프레임을 추출하여 데이터를 세심하게 수집했습니다.
- 사전 훈련 후, 다양한 비디오 데이터셋에 대해 제로샷 방식으로 모델을 적용합니다.
- 광범위한 질적 및 양적 실험을 통해 모델의 효과를 입증하였으며, 텍스트에서 의미 있는 프롬프트뿐만 아니라 모션 가이드에서 공간적 프롬프트도 포착합니다.
- 모든 모델은 16xV100 GPU를 하나의 노드에서 트레이닝하였고, 코드, 데이터셋, 모델은 https://hiteshk03.github.io/Pix2Gif/ 에서 공개되었습니다.

### [Common 7B Language Models Already Possess Strong Math Capabilities](https://arxiv.org/abs/2403.04706)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/eSnCEMNFcNuj00GmG8yur.png)

Vote: 10

Authors: Han Hu, Yixuan Wei, Nanning Zheng, Zheng Zhang, Chen Li, Houwen Peng, Weiqi Wang, Jingcheng Hu

- 일반적인 언어 모델에서 수학적 능력이 대규모 스케일이나 수학 관련 사전 훈련을 필요로 한다고 알려져 있었으나, LLaMA-2 7B 모델은 흔한 사전 훈련만으로도 강력한 수학 능력을 보유하고 있음을 증명한다.
- LLaMA-2 7B 모델은 256회 무작위 생성을 통해 최적의 응답을 선택하였을 때 GSM8K 벤치마크에서 97.7%, MATH 벤치마크에서 72.0%의 인상적인 정확도를 달성했다.
- 기본 모델이 처한 주요 문제는 일관성 있게 모델이 가진 수학적 능력을 유도하는 것이 어렵다는 것으로, 첫 번째 답변의 정확도는 GSM8K에서 49.5%, MATH에서 7.9%로 급격히 하락한다.
- 단순히 SFT(Structured Fine-Tuning) 데이터의 규모를 확대함으로써 정답 생성의 신뢰성을 크게 향상시킬 수 있다는 점을 발견했다.
- 실제 수학 문제의 부족으로 확장 가능성에 한계가 있음에도, 대략 백만 개의 샘플까지 확장했을 때 뚜렷한 포화 상태를 보이지 않는 실제 데이터와 거의 같은 효과를 가진 합성 데이터를 사용하였다.
- 이 간단한 방법은 LLaMA-2 7B 모델을 사용하여 GSM8K에서 82.6%, MATH에서 40.6%의 정확도를 달성함으로써 이전 모델들보다 각각 14.2%, 20.8% 향상시켰다.
- 다양한 추론 복잡성과 오류 유형에 걸쳐 스케일링 행동에 대한 통찰력도 제공한다.

### [Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis](https://arxiv.org/abs/2403.04116)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/eEKROluYolawjgMtgxIYr.png)

Vote: 2

Authors: Yuanhao Cai, Xiaokang Yang, Jiahao Wang, Alan Yuille, Zongwei Zhou, Yixun Liang, Yulun Zhang, Angtian Wang

- 엑스레이 신규 시점 프로젝션 렌더링을 위해, 기존 NeRF 기반 방법에 대한 단점인 긴 훈련 시간과 느린 추론 속도를 극복하고자 본 논문에서는 3D 가우시안 스플래팅 기반 프레임워크인 X-Gaussian을 제안합니다.
- 엑스레이 이미징의 등방성 특징에서 영감을 얻어, 시점 방향의 영향을 배제하면서 3D 점의 방사강도를 예측하도록 라디에이티브 가우시안 포인트 클라우드 모델을 재설계하였습니다.
- CUDA 구현을 포함하는 Differentiable Radiative Rasterization (DRR)을 개발하여, 모델 기반으로 이뤄진 능률적인 연산을 가능하게 합니다.
- 또한, Angle-pose Cuboid Uniform Initialization (ACUI) 전략을 맞춤 설정하여 엑스레이 스캐너의 파라미터를 직접적으로 사용하고, 스캔된 객체를 둘러싼 큐브모양 내에서 포인트 위치를 균일하게 샘플링합니다.
- 실험 결과에 따르면, 본 논문에서 제안하는 X-Gaussian은 최신의 방법들보다 6.5 dB 개선된 성능을 보이며, 훈련 시간은 15% 이하로 줄이고, 추론 속도는 73배 이상 향상시킨 것으로 나타났습니다.
- 희소 뷰 CT 복원에 적용한 결과, 본 방법의 실용성을 드러내며, 코드와 모델은 https://github.com/caiyuanhao1998/X-Gaussian 에서 공개될 예정이다.
- 훈련 과정의 시각화 데모 비디오도 https://www.youtube.com/watch?v=gDVf_Ngeghg 에서 볼 수 있습니다.

