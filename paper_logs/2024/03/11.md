## Daily Papers (2024-03-11)

### [ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment](https://arxiv.org/abs/2403.05135)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.05135.png)

Vote: 31

Authors: Yixiao Fang, Xiwei Hu, Pei Cheng, Rui Wang, Bin Fu, Gang Yu

- ELAA는 저자들이 소개하는 효율적인 대형 언어 모델 어댑터로, 텍스트-이미지 디퓨전 모델들에게 강력한 대형 언어 모델(Large Language Models, LLM)을 장착하여 U-Net이나 LLM의 재학습 없이도 텍스트 정렬 능력을 향상시킨다.
- 기존 텍스트-이미지 생성 분야에서 뛰어난 성능을 보여준 디퓨전 모델은 대부분 CLIP을 텍스트 인코더로 사용하는데, 이는 다수의 객체, 상세한 속성, 복잡한 관계, 장문의 정렬 등을 이해하는 데에 한계가 있다.
- ELLA는 두 사전 훈련된 모델 간의 결합을 원활하게 하기 위하여 다양한 의미 정렬 커넥터 설계를 탐구하고, 새로운 모듈인 타임스텝 인식 의미 커넥터(Timestep-Aware Semantic Connector, TSC)를 제안하여 LLM에서 타임스텝에 따라 변화하는 조건을 추출한다.
- 이 접근방식은 디노이징 과정의 다양한 단계에서 의미적 특징을 적응시켜 여러 단계에 걸쳐 긴급하고 복잡한 프롬프트를 해석하는 데에 도움을 준다.
- ELLA는 커뮤니티 모델과 도구에 쉽게 통합되어 프롬프트를 따르는 능력을 개선할 수 있다.
- 저자들은 1,000개의 복잡한 프롬프트로 이루어진 도전적인 벤치마크인 Dense Prompt Graph Benchmark (DPG-Bench)를 소개하여 텍스트-이미지 모델이 복잡한 프롬프트를 따르는 능력을 평가한다.
- 광범위한 실험을 통해 ELLA가 다양한 속성과 관계를 포함한 다중 객체 구성에서 특히 최신 방법론들과 비교하여 복잡한 프롬프트를 따르는 데 있어 우수함을 입증하였다.

### [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/abs/2403.05530)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.05530.png)

Vote: 24

Authors: Mia Glaese, Orhan Firat, Yuanzhong Xu, Andrew Dai, Radu Soricut, Julian Schrittwieser, Angeliki Lazaridou, Dmitry Lepikhin, Ioannis Antonoglou, Nikolay Savinov, Jean-baptiste Alayrac, Thibault Sottiaux, Rohan Anil, +, Benjamin Lee, Katie Millican, Fabio Viola, Machel Reid, Sebastian Borgeaud, Denis Teplyashin, Timothy Lillicrap, Malcolm Reynolds, Ethan Dyer

- 본 보고서에서는 여러 장의 문서와 수시간의 비디오 및 오디오를 포함한 수백만 토큰의 세부 정보를 회상하고 추론할 수 있는 계산 효율성이 뛰어난 멀티모달 전문가 모델인 Gemini 1.5 Pro의 최신 모델을 소개합니다.
- Gemini 1.5 Pro는 다양한 모달리티에 걸쳐 장문 컨텍스트 검색 작업에서 거의 완벽한 회상 능력을 달성하고, 장문 문서 QA, 장문 비디오 QA 및 장문 컨텍스트 ASR에서 최고 수준의 성능을 향상시켰습니다.
- Gemini 1.0 Ultra의 최고 수준 성능과 동등하거나 이를 초과하는 성능을 다양한 벤치마크에서 달성하였습니다.
- Gemini 1.5 Pro의 장문 컨텍스트 능력의 한계를 연구한 결과, 최소 1000만 토큰 까지 계속된 개선 및 거의 완벽한 검색 (>99%) 성과를 보임으로서, 기존 모델인 Claude 2.1(20만 토큰)과 GPT-4 Turbo(12만8천 토큰)를 크게 뛰어넘는 성과를 보여줍니다.
- 대규모 언어 모델의 새로운 능력에 대해 놀라운 예를 들어보자면, 전 세계에 200명 미만의 사용자가 있는 Kalamang 언어의 문법 매뉴얼을 제공받았을 때, 모델은 동일한 내용으로 학습한 사람과 유사한 수준으로 영어에서 Kalamang으로 번역을 학습할 수 있었습니다.

### [DeepSeek-VL: Towards Real-World Vision-Language Understanding](https://arxiv.org/abs/2403.05525)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.05525.png)

Vote: 20

Authors: Wen Liu, Chong Ruan, Haoyu Lu, Kai Dong, Yaofeng Sun, Tongzheng Ren, Bo Zhang, Hanwei Xu, Bo Liu, Bingxuan Wang, Zhuoshu Li, Zhenda Xie, Chengqi Deng, Jingxiang Sun

- 'DeepSeek-VL'은 웹 스크린샷, PDF, OCR, 차트, 지식 기반 콘텐츠 등 실제 시나리오를 광범위하게 다루는 다양하고 확장 가능한 데이터를 활용하여 실제 비전과 언어 이해 응용 프로그램용으로 설계된 오픈 소스 시각-언어(VL) 모델을 소개합니다.
- 이 접근법은 실제 사용자 시나리오에서 추출한 사용 사례 분류학을 만들고, 그에 따른 지시 튜닝 데이터셋을 구축하여 실용적인 응용 프로그램에서 모델의 사용자 경험을 대폭 향상시킵니다.
- DeepSeek-VL은 고효율의 하이브리드 비전 인코더를 도입해 1024 x 1024의 고해상도 이미지를 효율적으로 처리하면서도 다양한 시각적 작업에서 중요한 의미론적 및 상세 정보를 포착하는 데 필요한 비교적 적은 계산 오버헤드를 유지합니다.
- 우수한 VL 모델이 우선적으로 강력한 언어 능력을 가져야 한다는 전제 하에, 사전 훈련 동안 LLM 기능을 유지하기 위해 처음부터 LLM 훈련을 통합하고 시각과 언어 모달리티 간의 경쟁 역학을 세밀하게 관리하는 효과적인 VL 사전 훈련 전략을 조사합니다.
- 1.3B 및 7B 모델을 포함한 DeepSeek-VL 가족은 실제 응용 프로그램에서의 시각-언어 챗봇으로서 뛰어난 사용자 경험을 보여주며, 동일한 모델 크기에서 다양한 시각-언어 벤치마크에서 최첨단 혹은 경쟁력 있는 성능을 달성하는 동시에 언어 중심 벤치마크에서의 견고한 성능을 유지합니다.
- 이 기반 모델을 기반으로 한 혁신을 촉진하기 위해 1.3B 및 7B 모델을 모두 공개적으로 접근 가능하게 만들었습니다.

### [Personalized Audiobook Recommendations at Spotify Through Graph Neural Networks](https://arxiv.org/abs/2403.05185)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.05185.png)

Vote: 13

Authors: Shawn Lin, Alice Wang, Hugues Bouchard, Andreas Damianou, Francesco Fabbri, Sandeep Ghael, Marco De Nadai, David Nyhan, Fabrizio Silvestri, Vladan Radosavljevic, Mounia Lalmas-Roelleke, Ang Li, Laura Kim, Paul Gigioli

- 스포티파이는 음악과 팟캐스트 콘텐츠에 이어 최근 오디오북을 대규모 사용자 기반에 소개했으나 개인화된 추천에 있어 중대한 도전을 직면했습니다.
- 오디오북은 구매 전 쉽게 미리 들어볼 수 없기 때문에 추천의 관련성이 더욱 중요하며, 새로운 콘텐츠 유형 도입은 극단적인 데이터 희소성 문제를 야기합니다.
- 이러한 문제에 대처하기 위해, 스포티파이는 사용자의 팟캐스트 및 음악 선호도를 활용하고 2T-HGNN, 즉 이종 그래프 신경망(HGNNs)과 투 타워(2T) 모델을 포함하는 확장성 있는 추천 시스템을 도입했습니다.
- 이 새로운 접근법은 세밀한 아이템 간 관계를 밝힐 수 있으면서도 낮은 대기 시간과 복잡성을 보장합니다.
- 사용자를 HGNN 그래프에서 분리하고 혁신적인 멀티-링크 이웃 샘플러를 제안함으로써 HGNN 모델의 복잡성을 상당히 줄였습니다.
- 수백만 명의 사용자를 대상으로 한 실증 평가에서는 개인화된 추천의 품질이 크게 향상되었으며, 새로운 오디오북 시작률이 46% 증가하고 스트리밍 비율이 23% 향상되었습니다.
- 놀랍게도 이 모델의 영향은 오디오북을 넘어 이미 확립된 콘텐츠인 팟캐스트에도 긍정적인 영향을 미쳤습니다.

### [VideoElevator: Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models](https://arxiv.org/abs/2403.05438)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.05438.png)

Vote: 12

Authors: Zheng Hui, Yabo Zhang, Peiran Ren, Wangmeng Zuo, Xiangyang Ji, Xuansong Xie, Yuxiang Wei, Xianhui Lin

- 텍스트-이미지 확산 모델(T2I)이 현실감 있고 미적인 이미지를 생성하는 데 있어 놀라운 능력을 보여준 반면, 텍스트-비디오 확산 모델(T2V)은 훈련 비디오의 질과 양이 부족하여 프레임 품질과 텍스트 정렬이 뒤처져 있다.
- 본 논문에서는, 텍스트-이미지 확산 모델의 우수한 성능을 사용하여 T2V의 성능을 향상시키는 훈련이 필요 없는 플러그 앤 플레이 방법인 VideoElevator를 소개한다.
- VideoElevator는 전통적인 T2V 샘플링과 다르게, 각 샘플링 단계를 시간적 움직임 개선과 공간적 품질 향상으로 명확히 분해한다.
- 시간적 모션 정제는 T2V를 활용하여 시간적 일관성을 강화하고, T2I에 필요한 노이즈 분포로 역변환하는 것을 포함한다.
- 공간적 품질 향상은 팽창된 T2I를 사용하여 덜 노이즈가 있는 잠재 변수를 예측하고, 더 사실적인 세부 사항을 추가한다.
- 다양한 T2V와 T2I 조합 아래에서 광범위한 프롬프트에 대한 실험을 실시했으며, VideoElevator가 기존 T2I를 활용하여 T2V 기반 성능을 향상시킬 뿐만 아니라 개인화된 T2I와 함께 스타일리시한 비디오 합성을 촉진한다는 결과를 얻었다.
- 해당 연구의 코드는 https://github.com/YBYBZhang/VideoElevator 에서 확인할 수 있다.

### [CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion](https://arxiv.org/abs/2403.05121)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.05121.png)

Vote: 11

Authors: Yuxiao Dong, Ming Ding, Zhuoyi Yang, Wendi Zheng, Weihan Wang, Xiaotao Gu, Jiayan Teng, Jie Tang, Jidong Chen

- 최신 텍스트-이미지 생성 시스템은 확산 모델에 의해 주로 이끌려 왔으나, 한 단계로 이루어지는 텍스트-이미지 확산 모델은 계산 효율성과 이미지 세부 사항의 개선에 있어 어려움이 있습니다.
- 이를 해결하기 위해, CogView3는 릴레이 확산을 텍스트-이미지 생성 영역에 처음으로 도입하는 새로운 계단식 프레임워크로서 낮은 해상도의 이미지를 먼저 생성한 후 릴레이 기반의 고화질화를 적용합니다.
- 이 방법론은 텍스트-이미지 출력물이 경쟁력을 갖추게 할 뿐만 아니라 훈련 및 추론 비용을 크게 줄여줍니다.
- 실험 결과에 따르면, CogView3는 현재 최고 수준의 오픈 소스 텍스트-이미지 확산 모델인 SDXL에 비해 인간 평가에서 77.0% 우수한 성능을 보이며 추론 시간은 절반에 불과합니다.
- 또한 CogView3의 보다 간소화된 변형은 SDXL의 추론 시간의 1/10만을 사용하면서도 비슷한 성능을 달성합니다.

### [CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model](https://arxiv.org/abs/2403.05034)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.05034.png)

Vote: 9

Authors: Hang Su, Yikai Wang, Chendong Xiang, Dajiang Yu, Shuo Chen, Jun Zhu, Yifei Chen, Zhengyi Wang, Chongxuan Li

- 기존의 피드포워드 3D 생성 모델인 Large Reconstruction Model (LRM)은 생성 속도가 뛰어나지만, 3D 데이터의 한정된 크기와 느린 학습 때문에 아키텍처의 기하학적 사전 지식을 활용하지 않아 종종 최적화되지 못한 품질을 낳습니다.
- 본 논문에서는 고해상도의 한 장의 이미지로부터 3D 텍스처 메쉬를 생성하는 Convolutional Reconstruction Model (CRM)을 제안하여 이러한 제한을 극복하고자 합니다.
- CRM은 희소한 3D 데이터를 고려하여 네트워크 설계에 기하학적 사전 지식을 통합하는 것의 필요성을 강조합니다.
- 이 모델은 트라이플레인 시각화가 여섯 개의 정사영 이미지의 공간적 대응 관계를 보인다는 핵심 관찰 결과를 기반으로 합니다.
- CRM은 단일 입력 이미지로부터 여섯 개의 정사영 이미지를 생성한 후, 이들을 컨볼루셔널 U-Net에 입력하여 픽셀 수준의 강력한 정렬 능력을 활용하고 큰 대역폭을 통해 고해상도 트라이플레인을 생성합니다.
- 또한, CRM은 Flexicubes라는 기하학적 표현을 사용하여 텍스처가 입혀진 메쉬에 대한 직접적인 엔드-투-엔드 최적화를 용이하게 합니다.
- 결론적으로, 우리의 모델은 어떠한 테스트 시간 최적화 없이도 단 10초 만에 이미지에서 고도의 텍스처 메쉬를 전달합니다.

