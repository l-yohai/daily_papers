## Daily Papers (2024-03-25)

### [Can large language models explore in-context?](https://arxiv.org/abs/2403.15371)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15371.png)

Vote: 16

Authors: Dylan J. Foster, Keegan Harris, Aleksandrs Slivkins, Akshay Krishnamurthy, Cyril Zhang

- 본 연구는 최신 대형 언어 모델(LLMs)이 강화학습 및 의사결정의 핵심 능력인 탐색을 수행할 수 있는지 여부를 조사합니다.
- 기존 LLMs의 내재된 성능에 초점을 맞추고, 훈련 조정 없이 단순 다중 팔 슬롯머신 환경에서 LLMs를 에이전트로 배치합니다.
- 환경 설명과 상호작용 기록을 LLM 프롬프트 내에, 즉 문맥 내에 완전히 포함시켜 실험을 진행합니다.
- GPT-3.5, GPT-4, 및 Llama2 모델을 활용해 다양한 프롬프트 디자인으로 실험한 결과, 모델들은 별도의 상당한 개입 없이는 탐색을 견고하게 수행하지 못했습니다.
- 유일하게 만족스런 탐색 행동을 보인 경우는 GPT-4가 사고의 연쇄(chain-of-thought) 추론을 사용하고, 외부에서 요약된 상호작용 기록을 충분한 통계로 제시했을 때입니다.
- 추론 과정을 포함하는 경우를 제외하고 요약되지 않은 기록으로는 견고한 탐색 행동이 나타나지 않았습니다.
- 이러한 결과는 복잡한 설정에서 가능하지 않을 수 있는 외부 요약이 LLM 기반 의사 결정 에이전트로부터 바람직한 행동을 얻기 위해 중요하다는 것을 시사합니다.
- 복잡한 환경에서 LLM 기반 의사결정 에이전트를 강화하기 위해 세밀한 조정이나 데이터셋 큐레이션 등의 비차별적 알고리즘 개입이 필요할 수 있다는 결론을 내립니다.

### [LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement](https://arxiv.org/abs/2403.15042)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15042.png)

Vote: 14

Authors: Kurt Keutzer, Nicholas Lee, Thanakul Wattanawong, Sehoon Kim, Gopala Anumanchipali, Karttikeya Mangalam, Michael W. Mahoney, Amir Gholami, Sheng Shen

- 기존의 사전 훈련된 대규모 언어 모델(LLMs)은 자연어 처리 작업에 있어 현 시점의 최고 성능을 자랑하지만, 실제 세계의 응용 프로그램에서는 만족스러운 성능에 도달하기 위해 여전히 미세 조정(fine-tuning)이 필요합니다.
- 많은 응용 프로그램이 저데이터(low-data) 환경에 있어 미세 조정이 어려운데, 이를 해결하기 위해 저자들은 LLM2LLM이라는 반복적인 데이터 증강 전략을 제안합니다.
- LLM2LLM은 초기 데이터 세트를 사용하여 학생(student) LLM을 미세 조정한 후, 잘못된 데이터 포인트들을 평가하고 추출하여, 이를 바탕으로 교사(teacher) LLM이 생성한 합성 데이터를 다시 훈련 데이터에 추가합니다.
- 이 방법론은 학습하는 동안 LLM에 의해 잘못 예측된 데이터 포인트에서 나오는 신호를 강화하고, 이러한 데이터 포인트를 데이터셋에 재통합하여 LLM에게 더 도전적인 예제에 집중하도록 만듭니다.
- LLM2LLM은 저데이터 환경에서 LLM의 성능을 크게 향상시키며, 전통적인 미세 조정 및 기타 데이터 증강 기준보다 우수한 성능을 보여줍니다.
- LLM2LLM은 수고로운 데이터 큐레이션에 대한 의존도를 낮추고, 데이터 제약이 있는 도메인과 작업을 다루기 위한 확장 가능하며 성능이 뛰어난 LLM 솔루션으로 가는 길을 열어줍니다.
- GSM8K, CaseHOLD, SNIPS, TREC, SST-2 데이터셋에서 미세 조정 대비 최대 24.2%, 32.6%, 32.0%, 52.6%, 39.8%의 성능 향상을 LLaMA2-7B 학생 모델을 사용하여 저데이터 환경에서 달성했습니다.

### [Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance](https://arxiv.org/abs/2403.14781)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14781.png)

Vote: 12

Authors: Junming Leo Chen, Xun Cao, Shenhao Zhu, Yinghui Xu, Siyu Zhu, Zuozhuo Dai, Hao Zhu, Yao Yao

- 본 연구에서는 3D 인간 매개변수 모델인 SMPL(Skinned Multi-Person Linear)을 이용해 형태 정렬 및 움직임 유도를 강화하고 인간 이미지 애니메이션을 생성하는 새로운 방법을 소개합니다.
- 이 방법은 복잡한 인간의 기하학적 형태와 움직임을 정확하게 포착하기 위해 몸체 형태와 포즈의 통합적 표현을 확립하고 있습니다.
- SMPL 시퀀스에서 추출한 렌더링된 깊이 이미지, 노멀 맵(normal maps), 의미 지도(semantic maps) 등과 함께, 뼈대 기반의 움직임 지도를 사용하여 잠재 디퓨전 모델에 포괄적인 3D 형태와 상세한 포즈 속성을 풍부하게 합니다.
- 다층 모션 퓨전 모듈을 활용하여 공간 영역에서 형태와 움직임 잠재 표현을 융합하며, 이는 자기 주의 메커니즘(self-attention mechanisms)을 결합해 사용됩니다.
- 3D 인간 매개변수 모델을 움직임 유도로 표현함으로써 참조 이미지와 출처 동영상 움직임 간의 인체 형태의 매개변수 정렬을 수행할 수 있습니다.
- 벤치마크 데이터셋에서 실시한 실험 평가는 이 방법론이 포즈와 형태 변화를 정확히 포착하는 고품질의 인간 애니메이션을 생성하는데 우수한 능력을 가짐을 입증합니다.
- 제안된 야생 데이터셋에서 우수한 일반화 능력 또한 보여줍니다. 프로젝트 페이지는 다음과 같습니다: https://fudan-generative-vision.github.io/champ/.

### [ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars](https://arxiv.org/abs/2403.15383)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15383.png)

Vote: 9

Authors: Rynson W. H. Lau, Tengfei Wang, Ziwei Liu, Gerhard Hancke, Zhenwei Wang

- 실제 3D 애플리케이션이 종종 일관된 주제를 공유하는 대규모 3D 자산 갤러리를 필요로 하지만, 기존 3D 자산을 테마에 맞춰 사용자화하여 생성하는 것은 여전히 열려있는 도전적인 문제입니다.
- 이 연구에서 제시된 ThemeStation은 몇 가지 예시들을 바탕으로 주제를 인식하는 3D 자산을 생성하는 새로운 방법을 제안합니다.
- ThemeStation은 주어진 예시들과 테마적으로 일치하면서도 다양한 변형을 가진 3D 자산을 생성하는 두 가지 목표, 즉 통일성(unity)과 다양성(diversity)을 추구합니다.
- 두 단계 프레임워크를 설계하여 첫째로 개념 이미지를 그린 다음, 참조 정보를 사용한 3D 모델링 단계를 진행합니다.
- 입력 예시물과 합성된 개념 이미지 모두의 사전 정보를 동시에 활용하는 새로운 이중 점수 증류(DSD, dual score distillation) 손실을 제안합니다.
- 광범위한 실험과 사용자 연구를 통해 ThemeStation이 다양하고 주제를 고려한 고품질의 3D 모델을 생성하는 데 있어 이전 연구들을 능가함을 확인했습니다.
- 또한, ThemeStation은 제어 가능한 3D 대 3D 생성 같은 여러 애플리케이션을 가능하게 합니다.

### [InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding](https://arxiv.org/abs/2403.15377)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15377.png)

Vote: 9

Authors: Yifei Huang, Rongkun Zheng, Guo Chen, Kunchang Li, Yansong Shi, Yinan He, Xinhao Li, Baoqi Pei, Yu Qiao, Hongjie Zhang, Limin Wang, Yali Wang, Songze Li, Zun Wang, Tianxiang Jiang, Jilan Xu, Jiashuo Yu, Yi Wang

- InternVideo2는 행동 인식, 비디오-텍스트 작업, 비디오 중심 대화에서 최신 성능을 달성하는 새로운 비디오 기반 모델(ViFM)을 소개합니다.
- 점진적 훈련 패러다임을 채택하여 마스킹된 비디오 토큰 재구성, 교차 모달 대조 학습, 다음 토큰 예측 등의 다양한 자가 또는 약한 지도 학습 프레임워크를 통합합니다.
- 다양한 전처리 과제를 통해 모델이 다른 수준의 구조와 의미 정보를 포착하도록 훈련 단계를 안내합니다.
- 데이터 수준에서는 의미적으로 비디오를 분할하고 비디오-오디오-음성 캡션을 생성하여 비디오와 텍스트 간의 정렬을 개선합니다.
- InternVideo2는 데이터와 모델 크기를 모두 확장하여 다양한 비디오 및 오디오 작업에서 좋은 성능을 입증합니다.
- 모델은 캡셔닝, 대화 및 긴 비디오 이해 벤치마크와 같은 다양한 비디오 관련 작업에서 다른 모델을 능가하며, 긴 시간적 맥락을 이해하고 추론할 수 있는 능력을 강조합니다.
- 코드와 모델은 https://github.com/OpenGVLab/InternVideo2/ 에서 사용할 수 있습니다.

### [DragAPart: Learning a Part-Level Motion Prior for Articulated Objects](https://arxiv.org/abs/2403.15382)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15382.png)

Vote: 7

Authors: Chuanxia Zheng, Ruining Li, Andrea Vedaldi, Christian Rupprecht

- DragAPart라는 방법을 소개하며, 이 방법은 이미지와 드래그 세트를 입력으로 사용하여 동일한 객체의 새로운 상태 이미지를 드래그 동작과 호환되도록 생성할 수 있습니다.
- 기존의 객체 재배치에 초점을 맞춘 연구와 달리, DragAPart는 서랍 여닫기와 같은 부품 수준의 상호작용을 예측합니다.
- 특정 운동 구조나 객체 범주에 제한되지 않는 일반적인 모션 모델을 학습하는 대리 문제로 이 문제를 연구합니다.
- 이미지 생성기를 사전 훈련시키고 드래그-어-무브(Drag-a-Move)라는 새로운 합성 데이터셋에서 미세 조정하는 것으로 시작합니다.
- 새로운 드래그 인코딩과 데이터셋 무작위화와 결합하여, 새로운 모델은 실제 이미지와 다양한 범주에 잘 일반화됩니다.
- 이전의 모션 제어 생성기들과 비교하여, 우리는 부품 수준에서 훨씬 더 나은 모션 이해를 시연합니다.

### [StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text](https://arxiv.org/abs/2403.14773)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14773.png)

Vote: 7

Authors: Levon Khachatryan, Humphrey Shi, Vahram Tadevosyan, Roberto Henschel, Hayk Poghosyan, Shant Navasardyan, Zhangyang Wang, Daniil Hayrapetyan

- StreamingT2V는 텍스트 지시에 따라 고품질의 동영상을 생성하는 텍스트-비디오 확산 모델을 사용하여 다양하고 개성 있는 콘텐츠 생성을 용이하게 합니다.
- 기존 방법은 주로 단기 비디오 생성에 초점을 맞추어 긴 비디오 합성으로 무작정 확장할 경우 아직는 명확한 절단이 발생합니다.
- StreamingT2V는 이러한 한계를 극복하기 위해 부드러운 전환을 가진 80, 240, 600, 1200 프레임 이상의 장기 비디오 생성을 위한 자기회귀적 접근방식을 소개합니다.
- 핵심 구성요소로는 (i) 주의 기반 메커니즘을 통해 이전 청크의 특징을 현재 생성에 조건을 부여하는 단기 메모리 블럭인 조건부 주의 모듈(CAM), (ii) 초기 장면을 잊지 않기 위해 첫 비디오 청크로부터 상위 장면 및 객체 특징을 추출하는 장기 메모리 블럭인 외모 보존 모듈, 그리고 (iii) 청크 사이의 일관성을 해치지 않으면서 무한히 긴 비디오에 대해 자기회귀적으로 비디오 향상기를 적용할 수 있는 무작위 혼합 접근법이 포함됩니다.
- 실험 결과는 StreamingT2V가 높은 운동량을 생성하는 반면, 경쟁하는 이미지-비디오 방법들은 자기회귀적 방식으로 순진하게 적용될 때 비디오 정체에 취약하다는 것을 보여줍니다.
- 따라서, 우리는 일관성과 운동 모두에서 경쟁자를 능가하는 고품질의 매끄러운 텍스트-긴 비디오 생성기인 StreamingT2V를 제안합니다.
- 해당 연구의 코드는 https://github.com/Picsart-AI-Research/StreamingT2V 에서 이용 가능합니다.

### [VidLA: Video-Language Alignment at Scale](https://arxiv.org/abs/2403.14870)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14870.png)

Vote: 6

Authors: Mubarak Shah, Fan Fei, Benjamin Z. Yao, Son Tran, Jayakrishnan Unnikrishnan, Mamshad Nayeem Rizve, Trishul Chilimbi, Belinda Zeng

- 본 논문에서는 대규모로 비디오-언어 정렬을 수행하기 위해 VidLA라는 접근법을 제안합니다.
- 기존 접근법은 짧은 범위와 긴 범위의 시간적 의존성을 모두 포착하지 못하며, 이미지-텍스트 기본 모델과 결합하기 어려운 복잡한 계층적 심층 네트워크 구조를 사용합니다.
- VidLA는 이러한 제한을 해결하기 위해 네트워크 구조를 단순화하고, 비디오의 시간적 계층성을 고려하여 다양한 시간 해상도에서 작동하는 데이터 토큰 세트를 사용합니다.
- 간단한 두 타워 구조를 사용하여 이미 훈련된 이미지-텍스트 기본 모델로부터 비디오-언어 모델을 초기화함으로써 최종 성능을 향상시킵니다.
- 기존의 비디오-언어 정렬 작업은 대규모의 의미적으로 정렬된 학습 데이터 부족으로 어려움을 겪었으나, 최근의 LLM을 활용해 지금까지 가장 큰 비디오-언어 데이터셋을 큐레이션하여 보다 나은 시각적 구심을 제공합니다.
- 기존의 비디오-텍스트 데이터셋이 짧은 클립만을 포함하는 반면, 우리의 데이터셋은 다양한 길이의 비디오 클립으로 구성되어 시간적으로 계층화된 데이터 토큰이 다양한 시간 규모에서 더 나은 표현을 추출하는 데 도움이 됩니다.
- 실험 결과, VidLA는 여러 검색 벤치마크에서 최신 기법을 능가하며, 특히 긴 비디오에서 뛰어난 성능을 보이고, 분류 벤치마크에서도 경쟁력 있는 성능을 보임을 확인하였습니다.

### [SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series](https://arxiv.org/abs/2403.15360)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15360.png)

Vote: 6

Authors: Vijay S. Agneeswaran, Badri N. Patro

- 트랜스포머는 시퀀스 믹싱을 위한 주목 네트워크(attention networks)와 채널 믹싱을 위한 MLPs를 널리 채택하며, 다양한 분야에서 혁신적인 성과를 달성하는 데 중추적인 역할을 하고 있습니다.
- 최근 연구에서는 주목 네트워크의 낮은 유도 편향(inductive bias)과 입력 시퀀스 길이에 대한 제곱 복잡도(quadratic complexity)와 같은 이슈가 지적되었습니다.
- 긴 시퀀스 길이를 처리하기 위해 S4와 같은 상태 공간 모델(SSMs)과 Hippo, Global Convolutions, liquid S4, LRU, Mega, Mamba 등이 등장하였습니다.
- Mamba는 현존하는 SSM 중 최첨단 기술임에도 불구하고, 컴퓨터 비전 데이터셋을 위한 큰 네트워크로 확장 시 안정성 문제가 있습니다.
- SiMBA라는 새로운 아키텍처를 제안하는데, 이는 채널 모델링을 위해 Einstein FFT (EinFFT)를 도입하고 시퀀스 모델링을 위해 Mamba 블록을 사용합니다.
- SiMBA는 이미지와 시계열 벤치마크 데이터셋에서 광범위한 성능 연구를 통해 기존 SSM들을 능가하는 성능을 보여주며, 최첨단 트랜스포머와의 성능 격차를 해소했습니다.
- 눈에 띄게 SiMBA는 ImageNet과 Stanford Car, Flower와 같은 전송 학습 벤치마크들은 물론 7개의 시계열 벤치마크 데이터셋에서 새로운 최첨단 SSM으로 자리매김했습니다.
- 프로젝트 페이지는 이 웹 사이트에서 확인할 수 있습니다: ~https://github.com/badripatro/Simba.

### [LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis](https://arxiv.org/abs/2403.15385)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15385.png)

Vote: 5

Authors: James Lucas, Sanja Fidler, Xiaohui Zeng, Kevin Xie, Jun Gao, Jonathan Lorraine, Antonio Torralba, Tianshi Cao

- 최근의 텍스트-투-3D 생성 접근법은 인상적인 3차원 결과를 생산하나, 각 프롬프트마다 최대 한 시간까지 소요되는 시간 소모적인 최적화가 필요합니다.
- 할당 기반 방법론인 ATT3D는 여러 프롬프트를 동시에 최적화하여 텍스트-투-3D 합성의 효율성을 향상시키나, 고주파의 기하학 및 텍스처 세부 사항을 포착하지 못하고, 큰 프롬프트 세트로 확장하는 데 어려움을 겪어 일반화 성능이 떨어집니다.
- LATTE3D는 이러한 제한을 해결하여 더 큰 프롬프트 세트에서 빠르고 고품질의 생성을 달성하기 위해 고안되었습니다.
- 우리의 방법의 핵심은 1) 확장 가능한 아키텍처 구축과 2) 다양하고 복잡한 훈련 프롬프트에 대해 견고함을 달성하기 위해 3차원 데이터를 최적화 과정에서 활용하는 것입니다. 여기에는 3D 인식 확산 사전, 모양 정규화, 모델 초기화가 포함됩니다.
- LATTE3D는 단일 순방향 패스에서 매우 상세한 텍스처 메쉬를 생성하기 위해 신경 필드와 텍스처 표면 생성을 모두 할당합니다.
- LATTE3D는 400밀리초 내에 3D 객체를 생성할 수 있으며, 빠른 테스트시간 최적화를 통해 더욱 향상될 수 있습니다.

### [FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions](https://arxiv.org/abs/2403.15246)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15246.png)

Vote: 4

Authors: Dawn Lawrie, Kyle Lo, Benjamin Chang, Benjamin Van Durme, Orion Weller, Sean MacAvaney, Luca Soldaini, Arman Cohan

- 현대의 큰 언어 모델들은 사용자의 다양한 작업을 가능하게 하는 복잡하고 긴 지시를 따를 수 있는 능력이 있습니다.
- 그러나 정보 검색(IR) 모델은 거의 대부분이 지시 없이 쿼리만 입력으로 사용하고, 지시를 사용하는 소수의 최근 모델들도 그 사용 방법이 불분명합니다.
- 우리는 실제 세계 지시를 더 잘 따르도록 IR 모델을 돕기 위한 훈련 세트와 함께, 엄격한 지시 평가 벤치마크를 포함한 FollowIR 데이터셋을 소개합니다.
- TREC 회의의 오랜 역사를 기반으로, TREC가 문서의 관련성을 결정하기 위해 인간 주석자에게 지시(내레이티브)를 제공하는 것처럼, IR 모델도 이러한 상세한 지시를 이해하고 관련성을 결정할 수 있어야 합니다.
- 우리는 주석자의 지시를 변경하고 관련 문서를 재주석함으로써 IR 모델이 지시를 얼마나 잘 따르는지 측정할 수 있는 새로운 짝 평가 프레임워크를 통해 평가 벤치마크를 시작합니다.
- 우리의 결과는 기존 검색 모델이 지시를 올바르게 사용하지 못하며, 기본 키워드에 사용하고 장문 정보 이해에 어려움을 겪고 있음을 나타냅니다.
- 하지만, 복잡한 지시를 따를 수 있는 IR 모델을 학습시킬 수 있다는 것을 보여줍니다: FollowIR-7B라는 새로운 모델은 우리의 훈련 세트에 튜닝 후 중요한 개선을 보여주었습니다(13% 이상).

### [AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models](https://arxiv.org/abs/2403.15157)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15157.png)

Vote: 3

Authors: Yajie Xue, Yuhao Wu, Qingwei Lin, Qi Zhang, Xiaoting Qin, Si Qin, Yuyi Liang, Xiaoyu Gou, Minghua Ma, Shilin He, Yu Kang, Dongmei Zhang, Saravan Rajmohan, Chaoyun Zhang, Zicheng Ma

- 소프트웨어 개발에서 사용자의 경험, 의견 및 요구를 담고 있는 매우 중요한 자료로서 사용자의 직접적인 피드백을 효과적으로 분석하는 것은 어려운 작업입니다.
- 이 논문은 대규모 피드백 분석을 위해 자연어 인터페이스를 사용하여 대규모 언어 모델(LLMs)을 활용하는 혁신적인 분석 프레임워크인 Allhands를 소개합니다.
- Allhands는 기존 피드백 분석 작업 흐름을 따라 초기에 피드백 분류 및 주제 모델링을 수행하여 구조적으로 향상된 형식으로 변환하고, 정확성과 강건성, 일반화 및 사용자 친화성을 높이기 위해 LLMs를 통합합니다.
- 나아가, Allhands는 자연어로 사용자의 다양한 질문을 해석하고, 그것들을 실행 가능한 파이썬 코드로 변환하여 텍스트, 코드, 표, 이미지를 포함한 포괄적인 다중 모달 응답을 제공하는 LLM 에이전트를 사용합니다.
- 세 가지 다양한 피드백 데이터셋을 통해 Allhands를 평가하는 실험은 분류와 주제 모델링을 포함한 분석의 모든 단계에서 뛰어난 효과를 달성하며, 포괄적이고 정확하며 읽기 쉬운 응답으로 사용자에게 "무엇이든 물어보세요" 경험을 제공한다는 것을 보여줍니다.
- 최종적으로, Allhands는 이 분야에서 첫 번째로 자연어 인터페이스를 통해 다양하고 맞춤화된 인사이트 추출을 지원하는 포괄적인 피드백 분석 프레임워크로 자리 잡고 있습니다.

### [Compiler generated feedback for Large Language Models](https://arxiv.org/abs/2403.14714)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14714.png)

Vote: 3

Authors: Dejan Grubisic, Hugh Leather, Chris Cummins, Volker Seeker

- 이 논문은 대규모 언어 모델을 활용한 컴파일러 최적화의 새로운 패러다임을 소개하며, LLVM 어셈블리 코드 크기 최적화를 목표로 한다.
- 모델은 최적화되지 않은 LLVM IR을 입력으로 받아 최적화된 IR, 최적의 최적화 패스, 그리고 최적화 전후의 명령어 수를 생성한다.
- 생성된 최적화 패스로 입력을 컴파일하고 예측된 명령어 수가 정확한지, 생성된 IR이 컴파일 가능한지, 컴파일된 코드와 일치하는지 평가한다.
- 이러한 피드백을 대규모 언어 모델에 다시 제공하여 코드 최적화를 다시 시도하게 한다.
- 제안된 접근 방식은 기존 모델에 비해 -Oz 옵션보다 0.53%의 추가 개선을 이끌어낸다.
- 피드백에 더 많은 정보를 추가하는 것이 직관적으로 보이지만, 단순한 샘플링 기법이 10개 이상의 샘플을 주어졌을 때 훨씬 더 높은 성능을 달성한다.

