## Daily Papers (2024-03-06)

### [Design2Code: How Far Are We From Automating Front-End Engineering?](https://arxiv.org/abs/2403.03163)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/fCIN3z4hez-rSH7zh6IbP.png)

Vote: 48

Authors: Zhengyuan Yang, Ruibo Liu, Diyi Yang, Chenglei Si, Yanzhe Zhang

- 최근 생성 AI는 멀티모달 이해와 코드 생성에서 놀라운 성과를 이루었으며, 이는 시각적 디자인을 코드 구현으로 직접 변환할 수 있는 새로운 프론트엔드 개발 패러다임을 가능하게 할 수 있습니다.
- 본 연구에서는 Design2Code 작업을 정식화하고, 484개의 다양한 실제 웹 페이지들을 대상으로 종합적인 벤치마킹을 실시했습니다.
- 스크린샷을 입력으로 받아 주어진 참조 웹페이지들을 직접 렌더링 할 수 있는 코드 구현을 현재의 멀티모달 LLM(Large Language Models)이 얼마나 잘 생성하는지 평가하기 위한 자동 평가 척도를 개발했습니다.
- 자동 척도에 더해, 종합적인 인간 평가를 보완했으며, GPT-4V와 Gemini Pro Vision에서 효과가 있음을 보여주는 멀티모달 프롬프팅 방법도 개발했습니다.
- 오픈소스 Design2Code-18B 모델을 미세 조정하여 Gemini Pro Vision의 성능에 맞출 수 있었습니다.
- 인간 평가와 자동 메트릭 모두 GPT-4V가 이 작업에서 다른 모델보다 가장 뛰어난 성능을 보였으며, 평가자들은 GPT-4V가 생성한 웹 페이지가 49%의 경우에서 시각적 외관과 콘텐츠 면에서 원본 참조 웹 페이지를 대체할 수 있다고 생각했습니다.
- 놀랍게도 GPT-4V가 생성한 웹 페이지의 64%는 원본 참조 웹 페이지보다 더 낫다고 평가되었습니다.
- 자세한 메트릭 분석에 따르면, 오픈소스 모델들은 주로 입력 웹페이지에서 시각적 요소들을 불러오는 것과 올바른 레이아웃 디자인을 생성하는 데에 있어서 뒤떨어지는 반면, 텍스트 내용과 색채 등은 적절한 미세 조정을 통해 크게 향상될 수 있는 것으로 나타났습니다.

### [Scaling Rectified Flow Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2403.03206)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SQl8gNof4uWZEFxtztnk3.png)

Vote: 22

Authors: Tim Dockhorn, Robin Rombach, Patrick Esser, Andreas Blattmann, Axel Sauer, Harry Saini, Yam Levi, Frederic Boesel, Yannik Marek, Dominik Lorenz, Alex Goodwin, Zion English, Jonas Müller, Dustin Podell, Rahim Entezari, Sumith Kulal, Kyle Lacey

- 직선을 따라 데이터와 노이즈를 연결하는 새로운 생성 모델인 Rectified Flow를 통해 고해상도 이미지 합성을 위한 연구가 진행되었습니다.
- 기존의 노이즈 추출 기술을 개선하고, 인지적으로 중요한 스케일에 편향을 주어 Rectified Flow 모델 훈련을 향상시켰습니다.
- 대규모 연구를 통해, 이 접근법이 고해상도 텍스트-이미지 합성에서 기존의 확립된 확산 모델보다 우수한 성능을 보임을 입증했습니다.
- 또한, 별도의 가중치를 사용하는 트랜스포머 기반의 새로운 아키텍처를 소개하여 텍스트와 이미지 토큰 간의 양방향 정보 흐름이 가능하게 하고, 텍스트 이해, 타이포그래피 및 사람의 평가 지표까지 개선했습니다.
- 이 아키텍처는 예측 가능한 스케일링 추세를 따르고, 다양한 지표 및 인간 평가를 통해 검증된 텍스트-이미지 합성능력의 개선과 낮은 검증 손실 사이의 상관관계를 보여줍니다.
- 최대 모델은 최신 모델을 능가하는 성과를 내며, 연구 결과와 코드, 모델 가중치를 공개할 예정입니다.

### [NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](https://arxiv.org/abs/2403.03100)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ie9-sneEMA254_UASOJZh.png)

Vote: 14

Authors: Jiang Bian, Zhizheng Wu, Yanqing Liu, Siliang Tang, Lei He, Sheng Zhao, Dongchao Yang, Tao Qin, Detai Xin, Wei Ye, Kaitao Song, Zeqian Ju, Yichong Leng, Kai Shen, Yuancheng Wang, Xiang-Yang Li, Jinyu Li, Xu Tan, Shikun Zhang

- 최근 대규모 텍스트-음성 변환(TTS) 모델은 큰 진전을 이루었지만, 여전히 음질, 유사성 및 억양에서 한계가 있다.
- 이러한 문제를 해결하고자, NaturalSpeech 3는 다양한 속성(콘텐츠, 억양, 음색, 음향 디테일)을 개별 하위공간으로 분해하고 각각을 개별적으로 생성하는 새로운 TTS 시스템을 제안한다.
- 이 시스템은 콘텐츠, 억양, 음색, 음향 디테일 각각의 하위공간을 분리하는 팩터화 벡터 양자화(FVQ)를 도입한 신경 코덱을 설계한다.
- 또한, 해당 프롬프트에 따라 각 하위공간 내 속성을 생성하는 팩터화된 확산 모델을 제안한다.
- 이러한 분할 정복 방식의 팩터화 설계를 통해 NaturalSpeech 3는 복잡한 음성을 더 효과적이고 효율적으로 모델링한다.
- 실험 결과, NaturalSpeech 3는 품질, 유사성, 억양, 이해도 면에서 최신 TTS 시스템을 뛰어넘는 성능을 보여준다.
- 1B 매개변수와 20만 시간의 훈련 데이터로 확장하여 성능을 더욱 향상시키는 것에 성공하였다.

### [Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters](https://arxiv.org/abs/2403.02677)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/TYHO4NNx1DP1ekGrpXrWJ.png)

Vote: 11

Authors: Weizhi Wang, Xifeng Yan, Heng Wang, Linjie Yang, Khalil Mrini, Yu Tian, Sateesh Kumar

- 본 연구에서는 보다 진보된 다중모달 언어 모델(MLMs)을 활용하여 이미지-텍스트 데이터를 필터링하는 새로운 프레임워크를 제안하였습니다.
- 이 접근 방법은 최근의 MLMs의 발전을 통합하며, 기존 필터링 방법들(예: CLIPScore)보다 더 뛰어난 성능을 보입니다.
- 이미지-텍스트 데이터의 품질을 종합적으로 측정하기 위해 네 가지 독특하면서도 상호 보완적인 지표를 설계하였습니다.
- 높은 품질의 지시 데이터를 구축하고 MLMs를 데이터 필터로 세밀하게 조정하기 위한 새로운 파이프라인이 마련되었습니다.
- CLIPScore와 비교할 때, 우리의 MLM 필터는 더 정확하고 포괄적인 점수를 제공하여 필터링된 데이터의 질을 직접적으로 향상시키고 사전 훈련된 모델의 성능을 높였습니다.
- 인기 있는 기초 모델들(예: CLIP 및 BLIP2)과 다양한 하류 작업에 대해 CLIPScore를 상당히 개선하는 결과를 달성했습니다.
- 우리의 MLM 필터는 다양한 모델과 작업에 일반화할 수 있으며, CLIPScore에 대한 대체물로 사용될 수 있습니다.
- 추가적인 탈락 연구(ablation study)는 우리의 MLM 필터 디자인 선택을 검증하기 위해 제공되었습니다.

### [Wukong: Towards a Scaling Law for Large-Scale Recommendation](https://arxiv.org/abs/2403.02545)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Fe1oBxmvxcCMUGw0nK38_.png)

Vote: 11

Authors: Liang Luo, Xi Liu, Shen Li, Daifeng Guo, Jade Nie, Yantao Yao, Maxim Naumov, Jongsoo Park, Yuxin Chen, Guna Lakshminarayanan, Yuchen Hao, Buyun Zhang, Ellie Dingqiao Wen, Yanli Zhao, Wenlin Chen

- 현재의 추천 모델들은 큰 언어 모델과 같은 스케일링 법칙을 보이지 않는다는 문제점을 가지고 있으며, 이는 복잡해지는 실세계 데이터셋에 모델을 적응시키는데 있어서 상당한 도전 과제를 제시합니다.
- 본 논문에서는 계층화된 팩터화 머신(factorization machines)만을 기반으로 한 효과적인 네트워크 구조와 그에 상응하는 스케일링 전략을 제안하며, 'Wukong'이라 명명합니다.
- Wukong의 독특한 설계는 단순히 더 높고 넓은 층을 통해 다양하고 모든 순서의 상호작용을 포착할 수 있게 만듭니다.
- 여섯 개의 공개 데이터셋에서 광범위한 평가를 수행했으며, Wukong이 상태-아트(state-of-the-art) 모델들을 품질면에서 일관되게 능가함을 입증합니다.
- 내부의 대규모 데이터셋에서 Wukong의 스케일링 능력을 평가한 결과, Wukong은 상태-아트 모델들에 비해 품질 면에서 우월함을 유지하면서, 100 Gflop 또는 GPT-3/LLaMa-2급 전체 트레이닝 컴퓨트 규모를 넘어서는 모델 복잡성의 두 개의 크기 순서에 걸쳐 스케일링 법칙을 유지하는 것을 보여줍니다.
- 이전의 연구들이 성과를 보이지 못한 영역에서 Wukong은 여전히 모델 품질 면에서 뛰어나며 스케일링 법칙을 유지함을 확인할 수 있습니다.

### [Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use](https://arxiv.org/abs/2403.02626)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/bIgB3HiDXrYC-YPXylneZ.png)

Vote: 8

Authors: Neil Gordon Alldrin, Ranjay Krishna, Wenlei Zhou, Ariel Fuxman, Jan Dlabal, Imad Eddine Toubal, Chun-Ta Lu, Howard Zhou, Otilia Stretcu, Hao Xiong, Tom Duerig, Enming Luo, Aditya Avinash

- 본 논문에서는 콘텐츠 검열이나 야생동물 보전과 같은 구체적 혹은 주관적 시각 개념을 인식하는 모델이 필요한 애플리케이션들을 위하여 많은 수작업을 필요로 하는 전통적 분류기 개발 방법에서 벗어나, 최소한의 인간 노력으로 주관적 시각 분류를 가능하게 하는 새로운 프레임워크를 제안한다.
- 이 프레임워크는 자연어 상호작용을 통해 2,000장의 이미지 라벨링에서 단 100장의 라벨링과 몇 가지 대화로 노력을 대폭 줄임으로써 인간의 귀찮은 반복 작업을 줄인다.
- 최근의 기초 모델, 즉 대규모 언어 모델과 시각-언어 모델의 진보를 활용하여 대화를 통해 개념 공간을 구성하고 훈련 데이터 포인트를 자동으로 라벨링한다.
- 특히 모델은 군중 소싱된 주석을 필요로 하지 않으며, 이는 구현 비용이 민감한 시나리오에서도 배포가 가능한 경량화된 분류 모델을 생성한다.
- 15개의 주관적 개념과 2개의 공개 이미지 분류 데이터셋을 통해, 훈련된 모델은 전통적인 Agile 모델링 방법뿐만 아니라 ALIGN, CLIP, CuPL 같은 최신 제로샷 분류 모델과 PaLI-X와 같은 대규모 시각 질문 응답 모델보다 우수한 성능을 보여준다.

### [MathScale: Scaling Instruction Tuning for Mathematical Reasoning](https://arxiv.org/abs/2403.02884)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/rWX2wWPNAWQxdN9XBcG_D.png)

Vote: 8

Authors: Benyou Wan, Xingxing Zhang, Zhengyang Tang, Furu Wei

- 대규모 언어 모델(Large Language Models, LLMs)은 문제 해결 능력이 우수하지만 수학 문제 해결 능력은 아직 미흡하다.
- MathScale은 GPT-3.5와 같은 최신 LLMs를 활용하여 고품질의 수학적 추론 데이터를 생성하는 간단하고 확장 가능한 방법을 제안한다.
- 인간의 수학 학습 과정에서 영감을 받아 처음으로 기초 수학 질문에서 주제와 지식 포인트를 추출한 후 개념 그래프를 구축하여 새로운 수학 질문을 생성한다.
- 생성된 MathScaleQA는 200만 개의 수학 질문과 답변 쌍을 포함하는 수학적 추론 데이터셋을 만들어 내며, 데이터셋의 크기 축을 따라 효과적인 확장성을 보인다.
- MathScaleQA는 K-12 교육, 대학 수준, 그리고 경쟁 수준의 수학 문제를 다루는 10개의 데이터셋(예: GSM8K와 MATH)이 포함된 수리 문제 벤치마크({\sc MwpBench})를 구축하여 LLMs의 수학적 추론 능력을 종합적으로 평가한다.
- 오픈 소스 LLMs(예: LLaMA-2와 Mistral)에 MathScaleQA를 적용하여 미세 조정함으로써, 그들의 수학적 추론 능력이 크게 향상되었다.
- MathScale-7B는 {\sc MwpBench}에서 평가될 때 모든 데이터셋에서 동등한 크기의 최고 경쟁 모델들을 42.9%의 마이크로 평균 정확도 및 43.7%의 매크로 평균 정확도로 앞질러 최첨단 성능을 달성했다.

### [MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets](https://arxiv.org/abs/2403.03194)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/hO2GIUYJWIK3firlRKfiA.png)

Vote: 7

Authors: Nikolaos Pappas, Saab Mansour, Yusheng Xie, Arshit Gupta, Hang Su, Hwanjun Song, Justin Sun, Siffi Singh, Igor Shalyminov, Hossein Aboutalebi

- 다양한 대화 데이터가 부족한 문제를 해결하기 위해, 본 논문은 텍스트만 있는 대화 데이터에 주제에 맞는 고품질의 다양한 이미지를 추가할 수 있는 '멀티모달 확장 생성 이미지 대화(MAGID)' 프레임워크를 제안한다.
- 기존 접근 방식과 달리, MAGID는 텍스트-이미지 정렬을 보장하면서 조화로운 이미지를 생성하기 위해 확산 모델을 적용한다.
- MAGID는 이미지 설명 생성 모듈(텍스트 LLM)과 이미지 품질 모듈(미학, 텍스트-이미지 일치, 안전성을 처리) 사이의 혁신적인 피드백 루프를 통해 고품질 멀티모달 대화를 생성한다.
- 세 개의 대화 데이터셋을 사용하여 MAGID의 성능을 기존 최신 모델들과 비교했으며, 자동화된 평가와 인간 평가 모두에서 같거나 더 나은 결과를 보이며, 특히 이미지 데이터베이스가 작을 때 검색 기반 기법 대비 인간 평가에서 큰 개선을 나타냈다.

### [EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs](https://arxiv.org/abs/2403.02775)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0cPQza5cyaAiPiZUlMktL.png)

Vote: 6

Authors: Jianchen Zhu, Kai Liu, Hanlin Tang, Decheng Wu, Yifu Sun, Zhanhui Kang

- 대규모 언어 모델(Large Language Models, LLMs)은 다양한 작업에서 기존 방법보다 탁월한 성능을 보이지만, 고비용의 계산과 높은 메모리 요구 사항으로 인해 실제로 적용하는 데 어려움이 있습니다.
- 모델 양자화는 이러한 부담을 줄이는 효과적인 방법이지만, 기존 작업 대부분은 훈련 데이터의 일부 샘플을 사용하여 양자화된 모델을 보정했으며, 이것은 알려지지 않은 사례와 작업에 대한 양자화된 LLM의 일반화 능력에 영향을 줄 수 있습니다.
- 본 연구에서는 데이터에 의존하지 않는 양자화 방법을 통해 LLM의 일반화 성능을 보장할 수 있는지에 대한 중요한 질문을 탐구합니다.
- EasyQuant라는 훈련 없이 데이터에 독립적인 weight-only 양자화 알고리즘을 제안하며, 이는 무게 분포의 이상치와 양자화 범위 두 가지 요소가 양자화 오류를 줄이는 데 중요하다는 관찰을 기반으로 합니다.
- 이상치(1% 미만)를 그대로 두고 양자화 범위를 최적화함으로써, EasyQuant는 원래 모델에 버금가는 성능을 달성하는 데 놀랍게도 성공합니다.
- EasyQuant는 어떠한 훈련 데이터에도 의존하지 않기 때문에, 양자화된 LLM의 일반화 성능이 안전하게 보장됩니다.
- 또한, EasyQuant는 병렬로 구현될 수 있어서, 100B가 넘는 LLM에 대해서도 몇 분 안에 양자화된 모델을 얻을 수 있습니다.
- 데이터에 의존하지 않는 설정에서 거의 손실 없는 양자화 성능을 LLM에 대해 달성한 것은 이 연구가 처음이며, 우리의 알고리즘은 데이터에 의존하는 방법보다 10배 이상 빠릅니다.

### [MagicClay: Sculpting Meshes With Generative Neural Fields](https://arxiv.org/abs/2403.02460)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/hcPoUoQvvcQy2be9cKa2J.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/hcPoUoQvvcQy2be9cKa2J.mp4" muted="false"></video></div>

Vote: 5

Authors: Vladimir G. Kim, Amit H. Bermano, Noam Aigerman, Amir Barda, Thibault Groueix

- 신경 분야의 최근 발전은 형태 생성에 획기적인 능력을 가져왔지만, 예술적 작업에 필수적인 점진적 제어와 같은 중요한 특성이 부족합니다.
- 반면에 삼각형 메시는 효율성과 직관적인 제어를 제공하는 기하 관련 작업을 위한 선택의 표현방식이지만, 신경 최적화에 적합하지 않습니다.
- 이 논문에서는 메시와 부호 있는 거리 필드(Signed Distance Field, SDF) 표현을 일관되게 유지하는 하이브리드 접근 방식을 소개합니다.
- 'MagicClay'라는 새로운 도구를 통해 사용자는 텍스트 프롬프트에 따라 메시의 특정 영역을 조각할 수 있으며, 동시에 다른 영역은 그대로 유지됩니다.
- 우리의 프레임워크는 모양 최적화의 모든 단계에서 표현 간의 일관성과 정규화를 신중하고 효율적으로 유지합니다.
- 메시 표현을 이용하여 SDF를 더 높은 해상도와 더 빠르게 렌더링하는 방법을 보여줍니다.
- 최근의 차별화 가능한 메시 재구성 작업을 활용하여 SDF에 의해 지시된 대로 필요한 곳에 적응적으로 삼각형을 할당합니다.
- 구현된 프로토타입을 사용하여, 최고의 현대 기술보다 우수한 생성된 기하학을 시연하고, 연속적인 프롬프트 기반 편집을 동일한 메쉬에서 처음으로 허용하는 새로운 일관된 제어를 보여줍니다.

### [Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models](https://arxiv.org/abs/2403.03003)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GBOvqUR4jyJrk7MYI549a.png)

Vote: 5

Authors: Xiaoshuai Sun, Gen Luo, Rongrong Ji, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng

- 현존하는 다중모드 대형 언어 모델(MLLMs)이 세밀한 시각 인식에 있어 아직 한계가 있음에도 불구하고, 본 논문은 이미지 해상도의 관점에서 이 문제를 연구하며, 저해상도와 고해상도 시각 특징의 조합이 이러한 단점을 효과적으로 완화할 수 있음을 밝혀냈습니다.
- 새롭고 효율적인 MLLMs 기법으로 'Mixture-of-Resolution Adaptation' (MRA)을 제안하며, 이는 다른 해상도의 이미지를 위한 두 가지 시각 경로를 채택하고, 새로운 해상도 혼합 어댑터(MR-Adapters)를 통해 고해상도 시각 정보를 저해상도 경로에 통합합니다.
- 이 설계는 MLLM의 입력 시퀀스 길이를 상당히 줄여줍니다.
- MRA를 최근의 MLLM인 LLaVA에 적용하여 새 모델 LLaVA-HR을 제시하였으며, 11개의 시각-언어(VL) 과제에서 광범위한 실험을 통해 LLaVA-HR이 기존 MLLM보다 뛰어난 성능을 보이는 것을 확인했습니다. 예를 들어, TextVQA에서 +9.4%의 향상을 보였습니다.
- 더 중요한 것은 MRA를 적용한 LLaVA-HR의 훈련과 추론이 여전히 효율적이라는 점인데, 예를 들어 LLaVA-1.5 대비 20시간의 훈련과 3배 빠른 추론 속도를 달성했습니다.
- 소스 코드는 https://github.com/luogen1996/LLaVA-HR 에서 공개되었습니다.

### [RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches](https://arxiv.org/abs/2403.02709)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/k604bpCNsruHng3nw1_L-.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/k604bpCNsruHng3nw1_L-.mp4" muted="false"></video></div>

Vote: 4

Authors: Sean Kirmani, Ted Xiao, Ajinkya Jain, Quan Vuong, Stefan Schaal, Jiayuan Gu, Tianhe Yu, Michael Stark, Karol Hausman, Priya Sundaresan, Dorsa Sadigh, Jeannette Bohg, Peng Xu

- 본 연구에서는 시각적 모방 학습(visual imitation learning)에서 손으로 그린 스케치를 목표 사양의 한 방법으로 제안하고 있으며, 이는 자연 언어가 모호할 수 있고 이미지는 과도하게 특정될 수 있는 문제를 해결할 수 있다.
- 사용자가 언어처럼 즉흥적으로 제공하기 쉬우면서도 이미지처럼 공간적 인식을 돕고, 이미지로 할 수 있는 것을 넘어 작업과 관련 없는 객체들을 구별해 주는 데에도 도움을 줄 수 있다.
- RT-Sketch는 원하는 장면의 손그림 스케치를 입력으로 받아 동작을 출력하는 목표 조건부 정책(policy)을 다루며, 이는 합성으로 생성된 목표 스케치와 연관된 트랙터리 데이터셋으로 훈련된다.
- 테이블 위에서의 객체 재배치를 포함하는 여섯 가지 조작 기술에 대해 RT-Sketch를 평가했으며, 실험에서는 이 방법이 간단한 설정에서는 이미지 또는 언어 조건부 에이전트와 비슷한 수준의 성능을 보이면서, 언어 목표가 모호하거나 시각적 방해 요소가 있는 경우 더욱 강건한 성능을 나타내는 것으로 확인되었다.
- 추가적으로, RT-Sketch가 간단한 선 그림에서부터 자세하고 색칠이 된 그림에 이르기까지 다양한 수준의 특정성을 가진 스케치를 해석하고 이에 따라 행동하도록 내재화 할 수 있는 능력을 갖추고 있음을 보여주었다.
- 보충 자료 및 영상에 대해서는 RT-Sketch 공식 웹사이트(http://rt-sketch.github.io) 참조.

### [Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation](https://arxiv.org/abs/2403.02827)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/_D-SOw3dIFkj_wG5hzikg.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/_D-SOw3dIFkj_wG5hzikg.mp4" muted="false"></video></div>

Vote: 3

Authors: Biao Wang, Weijie Li, Bo Zheng, Fanda Fan, Tiezheng Ge, Yiran Zhu, Litong Gong

- 이미지를 비디오로 변환하는 작업(I2V)은 고품질의 세부 사항을 유지하는데 어려움이 있으며, 기존의 이미지 애니메이션 기술은 얼굴이나 인체 자세와 같은 특정한 도메인에 초점을 맞추고 있어 오픈 도메인으로 일반화하기 어렵습니다.
- 최근에 제안된 I2V 프레임워크들은 확산(diffusion) 모델을 기반으로 하여 오픈 도메인 이미지에 대해 동적인 내용을 생성할 수 있으나, 고정밀도를 유지하는 데 실패합니다.
- 저희는 낮은 고정밀도의 주요 원인 두 가지가 이미지 세부 사항의 손실과 화이트 노이즈 제거 과정에서의 예측 편향이라는 것을 발견하였습니다.
- 이를 해결하기 위해, 저희는 주변 이미지 정보를 보충하고 노이즈 교정을 통해 고정밀도를 달성하는 효과적인 방법을 제안합니다.
- 구체적으로, 지정된 이미지를 전제로, 저희 방법은 세부 사항을 보존하기 위해 입력 이미지 레이턴트에 노이즈를 추가한 다음, 노이즈 예측 편향을 해소하기 위해 적절한 교정을 포함하여 노이지 레이턴트를 제거합니다.
- 저희 방법은 튜닝이 필요 없고 플러그 앤 플레이 가능하며, 실험 결과는 생성된 비디오의 고정밀도 개선에 대한 저희 접근 방식의 효과를 입증합니다.
- 더 많은 이미지에서 비디오로의 생성 결과는 프로젝트 웹사이트 https://noise-rectification.github.io에서 확인할 수 있습니다.

