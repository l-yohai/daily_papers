## Daily Papers (2024-03-19)

### [Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation](https://arxiv.org/abs/2403.12015)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12015.png)

Vote: 38

Authors: Tim Dockhorn, Robin Rombach, Frederic Boesel, Patrick Esser, Axel Sauer, Andreas Blattmann

- 확산 모델은 이미지 및 비디오 합성의 주요 진전을 이끌고 있으나, 추론 속도가 느린 문제가 있습니다.
- 'Latent Adversarial Diffusion Distillation (LADD)'라는 새로운 증류 방법을 소개하여 기존 'adversarial diffusion distillation (ADD)'의 한계를 극복했습니다.
- LADD는 고정된 DINOv2 판별기에 의존하는 ADD와 달리 미리 학습된 잠재 확산 모델로부터 생성 특징을 이용합니다.
- 이 방법은 훈련을 단순화시키고 성능을 향상시켜, 고해상도 다양한 화면비의 이미지 합성을 가능하게 합니다.
- LADD를 Stable Diffusion 3 (8B)에 적용하여, 단 4단계의 비가이드 샘플링만으로 최첨단 텍스트-이미지 생성모델과 동등한 성능을 내는 'SD3-Turbo' 모델을 얻었습니다.
- 또한, 시스템적인 규모 확장 행동을 조사하고 이미지 편집 및 인페인팅과 같은 다양한 응용 분야에서 LADD의 효과를 입증했습니다.

### [PERL: Parameter Efficient Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2403.10704)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.10704.png)

Vote: 23

Authors: Hassan Mansoor, Zac Yu, Samrat Phatale, Christiane Ahlheim, Bill Byrne, Simral Chaudhary, Wei Li, Bowen Li, Lucas Dixon, Roman Komarytsia, Abhinav Rastogi, Alex Hutcheson, Zhang Chen, Yonghao Zhu, Jarvis Jin, Hakim Sidahmed, Zhuonan Lin, Saravanan Ganesh, Jessica Hoffmann

- 인간 피드백에서의 강화 학습(RLHF)이 인간의 선호도와 잘 맞는 알고리즘으로 입증되었지만, 학습 과정이 계산 비용이 많이 들고 복잡합니다.
- 본 연구에서는 저자들이 낮은 랭크의 적응(Low-Rank Adaptation, LoRA)이라는 매개변수 효율적 메소드를 사용하여 모델을 훈련시킨 RLHF를 탐구합니다.
- 연구자들은 보상 모델 훈련과 강화 학습을 LoRA를 사용하여 수행하는 "Parameter Efficient Reinforcement Learning" (PERL)을 설정합니다.
- 7개 벤치마크, 그리고 "Taskmaster Coffee"와 "Taskmaster Ticketing"을 포함하는 2가지 새로운 데이터셋에서 전통적인 미세조정과 PERL을 비교합니다.
- PERL은 전통적인 RLHF 설정과 동등한 성능을 보이며, 더 빠른 훈련과 적은 메모리 사용의 장점을 가집니다.
- 이러한 결과는 RLHF의 훈련 성능을 높이면서 대규모 언어 모델의 조정 기술로서의 채택을 제한하는 계산 부담을 줄일 수 있음을 보여줍니다.
- 또한, RLHF 연구를 촉진하기 위해서 "Taskmaster Coffee"와 "Taskmaster Ticketing"라는 2가지 새로운 선호도 데이터셋을 출시했습니다.

### [Larimar: Large Language Models with Episodic Memory Control](https://arxiv.org/abs/2403.11901)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.11901.png)

Vote: 15

Authors: Navrátil, Subhajit Chaudhury, Jiří, Soham Dan, Payel Das, Georgios Kollias, Igor Melnyk, Aurélie Lozano, Pin-Yu Chen, Sarath Swaminathan, Vijil Chenthamarakshan, Sihui Dai, Elliot Nelson

- 본 논문에서는 대규모 언어 모델(Large Language Models, LLMs)에 저장된 지식의 효율적이고 정확한 업데이트를 위해 Larimar이라는 새로운, 뇌에서 영감을 받은 분산 에피소딕 메모리 구조를 제안합니다.
- Larimar의 메모리는 계산 비용이 많이 드는 재학습이나 미세 조정 없이도 지식을 동적으로 즉각 업데이트할 수 있는 기능을 허용합니다.
- 다양한 사실 편집 벤치마크에서의 실험 결과, Larimar는 순차적 편집 설정이 어려운 상황에서도 가장 경쟁력 있는 기준 모델들과 비교해 준수한 정확도를 달성하였습니다.
- 특히 기본 LLM에 따라 4-10배 가속화된 속도와 함께, 제안된 구조가 단순하고 LLM에 구애받지 않으며 일반적이기 때문에 유연성 면에서도 뛰어납니다.
- 또한, Larimar를 이용하여 선택적 사실 잊기와 입력 컨텍스트 길이 일반화 메커니즘을 제공하며, 그 효율성을 입증합니다.
- 이 접근 방식은 대규모 언어 모델을 보다 적응적이고 유연하게 만들어, 지속적으로 변화하는 정보에 적응할 수 있는 기술의 발전을 암시합니다.

### [LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images](https://arxiv.org/abs/2403.11703)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.11703.png)

Vote: 12

Authors: Zanlin Ni, Junbo Cui, Zonghao Guo, Chunjiang Ge, Maosong Sun, Tat-Seng Chua, Ruyi Xu, Yuan Yao, Gao Huang, Zhiyuan Liu

- 이 연구에서는 고정된 크기 및 제한된 해상도로 이미지를 처리하는 기존 LMM의 제한점을 분석하고, GPT-4V와 LLaVA-1.5 를 대표적 예시로 들어 이들의 시각 인코딩 전략에서 발생하는 근본적인 결함을 지적합니다.
- 새로 제시된 LLaVA-UHD 모델은 이미지를 임의의 종횡비 및 고해상도로 효율적으로 인식할 수 있는 대규모 다모달 모델입니다.
- LLaVA-UHD는 다음의 세 가지 핵심 구성 요소를 포함합니다: (1) 고해상도 이미지를 효율적이고 확장 가능한 인코딩을 위해 작고 가변적인 크기의 조각으로 나누는 이미지 모듈화 전략, (2) 시각 인코더에서 나온 이미지 토큰을 더욱 압축하는 압축 모듈, (3) LLM을 위한 조각 토큰을 조직하는 공간 체계.
- LLaVA-UHD는 9가지 벤치마크에서 2-3차수 더 많은 데이터로 훈련된 기존의 정립된 LMM을 능가하는 성능을 보여줍니다.
- 특히, LLaVA-UHD는 LLaVA-1.5 336x336을 기반으로 하여 6배 더 큰 해상도 이미지(즉, 672x1088)를 단 94%의 추론 계산으로 지원하며, TextVQA에서 6.4점의 정확도 향상을 달성합니다.
- 모델은 학술적 환경에서도 효율적으로 훈련될 수 있으며, 8개의 A100 GPU를 사용하여 23시간 만에 (LLaVA-1.5는 26시간) 훈련을 완료할 수 있습니다.
- 연구진은 데이터와 코드를 https://github.com/thunlp/LLaVA-UHD 에서 공개적으로 제공합니다.

### [Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm](https://arxiv.org/abs/2403.11781)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.11781.png)

Vote: 12

Authors: Heliang Zheng, Bin Li, Yi Wu, Chaoyue Wang, Ziqiang Li

- 텍스트-이미지 변환 모델의 최신 발전을 활용한 Infinite-ID은 단일 참조 이미지를 사용하여 특정 정체성을 정확히 포착하는 데 중요한 진전을 이루었습니다.
- 기존 방법들은 주로 참조 이미지를 텍스트 임베딩 공간에 통합하지만, 이로 인해 이미지와 텍스트 정보 간 복잡한 결합이 발생하여 정체성의 충실도와 의미론적 일관성 유지에 어려움이 있었습니다.
- 이러한 문제를 해결하기 위해, Infinite-ID는 정체성 보존 개인화를 위한 ID-의미 분리 패러다임을 제안합니다.
- 우리는 정체성 강화 훈련을 도입하여 추가적인 이미지 교차 주의 모듈을 사용함으로써 충분한 ID 정보를 포착하고 확산 모델의 원래 텍스트 교차 주의 모듈을 비활성화하여 텍스트 입력으로부터의 간섭을 완화합니다.
- 또한, 혼합 주의 모듈과 AdaIN-평균 연산을 결합한 기능 상호 작용 메커니즘을 도입하여 두 스트림을 매끄럽게 통합합니다.
- 이 메커니즘은 정체성의 충실도와 의미론적 일관성을 향상시킬 뿐만 아니라 생성된 이미지의 스타일을 쉽게 조절할 수 있는 기능도 제공합니다.
- 원본 사진 생성 및 스타일 이미지 생성에 대한 광범위한 실험 결과는 우리가 제안한 방법의 우수한 성능을 입증합니다.

### [Generic 3D Diffusion Adapter Using Controlled Multi-View Editing](https://arxiv.org/abs/2403.12032)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12032.png)

Vote: 11

Authors: Leonidas Guibas, Hansheng Chen, Bokui Shen, Gordon Wetzstein, Ruoxi Shi, Yulin Liu, Jiayuan Gu, Hao Su

- 오픈 도메인 3D 객체 합성은 제한된 데이터와 더 높은 계산 복잡성 때문에 이미지 합성에 비해 뒤처져왔습니다.
- 최근 연구에서는 다앙한 뷰의 분산을 조사했지만, 3D 일관성, 시각적 품질, 효율성 측면에서 미흡한 점이 있었습니다.
- 이 논문에서 제안하는 MVEdit는 조상 샘플링을 사용하여 다중 뷰 이미지를 동시에 제거하고 고품질의 텍스처 메쉬를 출력하는 SDEdit의 3D 대응물로 기능합니다.
- MVEdit는 기존의 2D 확산 모델을 기반으로 하여, 마지막 타임스텝의 2D 뷰를 일관된 3D 표현으로 띄우고 다음 타임스텝의 2D 뷰를 렌더링 된 뷰로 조건을 달아 3D 일관성을 달성합니다.
- 이 프레임워크는 2-5분의 추론 시간만에 품질과 속도 사이에서 스코어 증류(score distillation)보다 나은 절충을 성취합니다.
- MVEdit은 텍스트/이미지-부터-3D 생성, 3D-부터-3D 편집, 고품질 텍스처 합성 등 광범위한 응용 분야에서 매우 다재다능하고 확장 가능합니다.
- 평가에 따르면, MVEdit은 이미지-부터-3D 및 텍스트 가이드된 텍스처 생성 작업에서 최신(state-of-the-art) 성능을 보여줍니다.
- 또한, 작은 3D 데이터셋에서 제한된 리소스로 2D 잠재 확산 모델을 미세 조정하는 방법을 도입하여, 빠른 저해상도 텍스트-부터-3D 초기화를 가능하게 합니다.

### [SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion](https://arxiv.org/abs/2403.12008)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12008.png)

Vote: 11

Authors: Christian Laforte, Chun-Han Yao, Robin Rombach, Vikram Voleti, David Pankratz, Adam Letts, Dmitry Tochilkin, Mark Boss, Varun Jampani

- 본 논문에서는 3D 객체의 궤도 비디오를 멀티뷰로 고해상도 생성할 수 있는 새로운 잠재 비디오 확산 모델인 Stable Video 3D (SV3D)를 제안합니다.
- 기존의 3D 생성 작업은 2D 생성 모델을 적용하여 새로운 시점 합성(NVS)과 3D 최적화 기술을 제안하지만, 이러한 방법들은 제한된 시점이나 일관성 없는 NVS로 인해 여러 단점이 있습니다.
- 이 연구는 이미지에서 비디오로의 확산 모델을 새로운 멀티뷰 합성 및 3D 생성에 적합하게 활용함으로써, 비디오 모델의 일반화 및 멀티뷰 일관성을 활용하고, NVS를 위한 명시적인 카메라 제어를 추가합니다.
- 또한 SV3D와 그 NVS 결과물을 이미지에서 3D 생성에 이용하기 위한 개선된 3D 최적화 기법을 제안합니다.
- 다양한 데이터셋에서 2D 및 3D 메트릭과 사용자 연구를 통해 획득한 실험 결과들은 SV3D가 NVS 및 3D 복원 분야에서 기존 작업들과 비교하여 최신의 성능을 달성하였음을 보여줍니다.

### [LightIt: Illumination Modeling and Control for Diffusion Models](https://arxiv.org/abs/2403.10615)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.10615.png)

Vote: 9

Authors: Julien Philip, Kalyan Sunkavalli, Yannick Hold-Geoffroy, Matthias Nießner, Peter Kocsis

- LightIt은 이미지 생성을 위한 명시적인 조명 제어 방법을 도입합니다.
- 최근의 생성 모델들이 조명 제어 기능을 결여하고 있어, 이미지 생성 시 전반적인 분위기나 영화적 연출 등 예술적 요소에 필수적인데, 이를 해결하기 위해 제안된 방식입니다.
- 생성을 셰이딩(음영) 및 노멀 맵과 연동하여 조건을 부여하는 방식으로 조명을 모델링합니다.
- 싱글 바운스 셰이딩을 이용한 조명 모델링은 캐스트 그림자를 포함합니다.
- 실제 이미지와 셰이딩 쌍의 데이터셋을 생성하기 위해 셰이딩 추정 모듈을 먼저 훈련시킵니다.
- 이후 추정된 셰이딩과 노멀을 입력으로 사용하는 제어 네트워크를 훈련합니다.
- 다양한 장면에서 이미지 생성과 조명 제어의 고품질을 입증하는 방법을 제시합니다.
- 생성된 데이터셋을 활용하여, 이미지와 목표 셰이딩에 조건을 부여하여 신원을 보존하는 리라이팅 모델을 훈련합니다.
- LightIt은 조명이 제어되고 일관성 있는 이미지 생성이 가능하며, 특화된 리라이팅 최첨단 방법들과 동등한 수준으로 수행될 수 있는 첫 번째 방법입니다.

### [VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding](https://arxiv.org/abs/2403.11481)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.11481.png)

Vote: 7

Authors: Rujie Wu, Zhi Gao, Qing Li, Yue Fan, Jiaqi Li, Xiaojian Ma, Yuntao Du

- 대규모 언어 모델과 시각-언어 모델을 포함한 여러 기초 모델들을 새로운 통합 메모리 메커니즘과 결합하여 동영상 이해 문제, 특히 긴 비디오에서의 장기간 시간 관계를 포착하는 것에 대한 연구를 진행했다.
- 이를 위해 제안된 다중 모달 에이전트인 VideoAgent는 1) 비디오의 일반적인 시간 이벤트 설명과 객체 중심 추적 상태를 저장하는 구조화된 메모리를 구축하고, 2) 입력된 작업 쿼리에 따라 비디오 구간 지역화 및 객체 메모리 쿼리와 같은 도구를 사용하여 LLM의 제로샷 도구 사용 능력을 활용하여 상호 작용적으로 작업을 해결한다.
- VideoAgent는 여러 장기 비디오 이해 벤치마크에서 인상적인 성능을 보여주었으며, NExT-QA에서 평균 6.6%, EgoSchema에서 26.0%의 성능 향상을 달성했다.
- 이 성과는 오픈소스 모델과 Gemini 1.5 Pro를 포함한 사설 모델 간의 격차를 좁히는 데 기여했다.

### [LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation](https://arxiv.org/abs/2403.12019)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12019.png)

Vote: 7

Authors: Shangchen Zhou, Fangzhou Hong, Bo Dai, Chen Change Loy, Yushi Lan, Shuai Yang, Xuyi Meng, Xingang Pan

- 본 논문은 네트워크 렌더링 분야에서 진보를 이루어온 가운데, 통합된 3D 확산 파이프라인이 아직 결정되지 않은 문제를 해결하기 위한 새로운 프레임워크인 LN3Diff를 소개합니다.
- LN3Diff는 입력 이미지를 구조화되고, 압축된, 3D 잠재 공간으로 인코딩하는 3D 인식 아키텍처와 변분 오토인코더(VAE)를 활용합니다.
- 이 잠재 공간은 고용량 3D 뉴럴 필드로 디코딩되는데, 이때 변압기 기반 디코더를 사용합니다.
- 3D 인식 잠재 공간에서 확산 모델을 훈련함으로써, 우리의 방법은 ShapeNet에서 3D 생성을 위한 최신 성능을 달성하고, 다양한 데이터셋의 단안 3D 재구축 및 조건부 3D 생성에서 우수한 성능을 보여줍니다.
- 또한, LN3Diff는 인스턴스별 최적화가 필요 없는 추론 속도 측면에서 기존의 3D 확산 방법을 능가합니다.
- 제안된 LN3Diff는 3D 생성 모델링에서 중요한 발전을 나타내며, 3D 비전과 그래픽 작업에서의 다양한 응용 가능성을 약속합니다.

### [MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data](https://arxiv.org/abs/2403.11207)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.11207.png)

Vote: 6

Authors: Tong Chen, Cesar Kadir Torrico Villanueva, Paul S. Scotti, Tanishq Mathew Abraham, Reese Kneeland, Kenneth A. Norman, Mihir Tripathy, Charan Santhirasegaran, Ashutosh Narang, Jonathan Xu, Thomas Naselaris

- 뇌 활동으로부터 시각 인식을 재구성하는 방법이 크게 향상되었음에도 불구하고, 실질적인 활용성은 각 대상자마다 수십 시간의 비용이 많이 드는 fMRI 훈련 데이터가 필요하기 때문에 제한되었다.
- 이 연구는 단 1시간의 fMRI 훈련 데이터를 사용하여 고품질 재구성을 보여준다.
- 모델은 7명의 대상자를 거쳐 사전 훈련되며, 새로운 대상자의 최소 데이터로 미세 조정된다.
- 새로운 기능 정렬 절차는 모든 뇌 데이터를 공유 대상자 잠재 공간에 선형적으로 매핑하고, 이어서 CLIP 이미지 공간으로의 공유 비선형 매핑을 수행한다.
- CLIP 공간에서 픽셀 공간으로 매핑하기 위해 Stable Diffusion XL을 CLIP 잠재 변수를 입력으로 받도록 미세 조정한다.
- 이 접근법은 제한된 훈련 데이터로 주제 밖 일반화를 개선하고, 단일 대상자 접근법과 비교할 때 최첨단 이미지 검색 및 재구성 지표를 달성한다.
- MindEye2는 단 한 번의 MRI 방문으로 인식의 정확한 재구성이 가능함을 시연한다.
- 모든 코드는 GitHub에서 사용할 수 있다.

### [DiPaCo: Distributed Path Composition](https://arxiv.org/abs/2403.10616)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.10616.png)

Vote: 4

Authors: Ionel Gog, Rachita Chhaparia, Jiajun Shen, Qixuan Feng, Marc'Aurelio Ranzato, Arthur Douillard, Yani Donchev, Arthur Szlam, Andrei A. Rusu, Adhiguna Kuncoro

- 기계 학습 (ML) 모델의 규모 확장을 가능하게 해주는 신경망 모델의 발전은 지속적인 공학적 도전을 요구하며, 이는 병렬로 작동하는 디바이스 간에 고대역폭 통신이 필수적인 ML 접근 방식에 필요합니다.
- 본 연구에서는 모듈형 아키텍처와 ML 모델 트레이닝 접근법을 공동으로 설계한 새로운 구조인 '분산 경로 조합(DIstributed PAth COmposition, DiPaCo)'을 제안합니다.
- DiPaCo는 훈련 중에 세트에 공유된 모듈을 통해 경로별로 계산을 분배합니다.
- 로컬 SGD(Local-SGD) 영감을 받은 최적화(DiLoCo)를 통해 모듈들을 통신량을 크게 줄여 동기화시키며, 이는 연결이 불량하고 이질적인 워커(작업자) 간의 훈련을 용이하게 하고, 워커의 실패와 강제 종료에도 견딜 수 있는 설계를 보장합니다.
- 추론 시간에는 입력마다 실행해야 하는 단일 경로만 있으며, 모델 압축이 필요하지 않습니다.
- 본 논문의 접근 방식은 덜 동기화되고 더 모듈화된 대규모 학습의 새로운 패러다임을 향한 첫 번째 프로토타입으로 간주됩니다.
- C4 벤치마크를 사용한 실험에서 DiPaCo는 같은 양의 트레이닝 스텝을 사용하면서도 실제 소요 시간은 더 적게하여, 150백만 개의 매개변수를 갖는 256가지의 가능한 경로 중 하나를 선택함으로써, 10억 개의 매개변수를 갖는 밀집된 트랜스포머 언어 모델의 성능을 뛰어넘습니다.

### [VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models](https://arxiv.org/abs/2403.12034)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12034.png)

Vote: 4

Authors: Philip Torr, Filippos Kokkinos, Junlin Han

- 이 논문은 사전 훈련된 비디오 확산 모델을 활용하여 확장 가능한 3D 생성 모델을 구축하는 새로운 패러다임을 제시합니다.
- 3D 데이터의 제한된 가용성으로 인해 고품질의 3D 생성 모델 개발이 어렵습니다; 이미지, 텍스트 또는 비디오와 달리 3D 데이터는 쉽게 접근할 수 없고 획득하기 어렵습니다.
- 이 문제를 해결하기 위해, 대규모의 텍스트, 이미지, 그리고 비디오로 학습된 비디오 확산 모델을 3D 데이터에 대한 지식 원천으로 사용합니다.
- 비디오 확산 모델을 미세 조정하여 다중 시점 생성 능력을 활성화시켜, 피드포워드 3D 생성 모델을 훈련시키기 위한 대규모 합성 다중 시점 데이터셋을 생성합니다.
- 거의 300만 개의 합성 다중 시점 데이터로 훈련된 제안하는 VFusion3D 모델은 단일 이미지에서 수 초 내에 3D 자산을 생성할 수 있으며, 기존 최고의 피드포워드 3D 생성 모델과 비교할 때 우수한 성능을 보입니다.
- 사용자 평가에서 70% 이상의 시간 동안 제안 모델의 결과를 선호함으로써 VFusion3D 모델의 우수성을 확인시켜 줍니다.

