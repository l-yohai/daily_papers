## Daily Papers (2024-03-05)

### [MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies](https://arxiv.org/abs/2403.01422)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/DlcPynp9hELsiG7d35Itl.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/DlcPynp9hELsiG7d35Itl.mp4" muted="false"></video></div>

Vote: 12

Authors: Zhende Song, Chi Zhang, Tao Chen, Chenchen Wang, Jiayuan Fan, Gang Yu, Jiamu Sheng

- 기계가 동영상을 이해하는 방식에 중대한 발전을 보인 다기능 모델에도 불구하고, 영화와 같은 긴 형식의 비디오에서는 그 효과가 감소합니다.
- 이러한 문제를 해결하기 위해, 우리는 긴 비디오에 대한 고품질의 합성 데이터를 생성하기 위해 설계된 새로운 프레임워크인 MovieLLM을 제안합니다.
- MovieLLM은 GPT-4와 텍스트-이미지 변환 모델을 활용하여 자세한 스크립트와 해당 시각적 컨텐츠를 생성하는 능력을 갖추고 있습니다.
- 전통적인 데이터 수집 방법에 비해 MovieLLM의 유연성 및 확장성이 눈에 띄며, 이를 통한 새로운 방법론이 제시됩니다.
- MovieLLM에 의해 생산된 데이터는 복잡한 비디오 내러티브 이해에 있어서 다기능 모델의 성능을 유의미하게 향상시키며, 기존 데이터셋의 희소성 및 편향 문제를 극복합니다.

### [ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models](https://arxiv.org/abs/2403.02084)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/o7F_nwZ73f7iIUc2vqvee.png)

Vote: 7

Authors: Xin Xia, Jiaxiang Cheng, Yuxi Ren, Pan Xie, Min Zheng, Jiashi Li, Lean Fu, Xuefeng Xiao, Huixia Li, Jie Wu

- 텍스트-이미지 모델(예: Stable Diffusion) 및 관련 개인화 기술(예: DreamBooth와 LoRA)의 최근 진보로 고화질 및 창의적 이미지 생성이 가능해졌으나, 훈련된 도메인 외의 해상도로 이미지를 생성할 때 한계가 있습니다.
- 이러한 한계를 극복하기 위해, ResAdapter라는 도메인 일관성 어댑터를 제시하여, 제한 없는 해상도와 종횡비로 이미지를 생성할 수 있는 확산 모델을 소개합니다.
- ResAdapter는 정적 해상도의 이미지를 복잡한 후처리 작업으로 처리하는 기존 다중 해상도 생성 방법과 달리, 동적 해상도로 직접 이미지를 생성합니다.
- 일반 데이터셋에서 학습한 뒤, ResAdapter는 오리지널 스타일 도메인을 유지하면서 개인화된 확산 모델을 사용하여 해상도에 구애받지 않는 이미지를 생성합니다.
- 종합적인 실험을 통해, 단 0.5M의 ResAdapter가 임의의 확산 모델을 사용하여 유연한 해상도의 이미지를 처리할 수 있음을 보여줍니다.
- 추가 실험을 통해, ResAdapter는 이미지 생성을 위한 다른 모듈(예: ControlNet, IP-Adapter, LCM-LoRA 등)과 호환되며, 높은 해상도의 이미지를 효율적으로 생성하기 위해 다른 다중 해상도 모델(예: ElasticDiffusion)에 통합될 수 있음을 증명합니다.
- 프로젝트 링크는 https://res-adapter.github.io 입니다.

### [AtomoVideo: High Fidelity Image-to-Video Generation](https://arxiv.org/abs/2403.01800)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/IvOP7b35PJG4HcRQIEw1j.png)

Vote: 7

Authors: Biao Wang, Yiran Zhu, Tiezheng Ge, Weijie Li, Bo Zheng, Litong Gong, Xiaoyang Kang

- 본 연구에서는 다중 입자상 이미지 주입을 기반으로 주어진 이미지에 대한 더 높은 충실도를 갖는 비디오를 생성하는 새로운 고품질 프레임워크인 AtomoVideo를 제안합니다.
- 고품질 데이터셋과 훈련 전략을 통해, 우수한 시간적 일관성과 안정성을 유지하면서도 보다 큰 움직임의 강도를 달성했습니다.
- AtomoVideo의 구조는 비디오 프레임 예측 작업으로도 유연하게 확장되어 반복적 생성을 통한 긴 시퀀스 예측을 가능하게 합니다.
- 어댑터 훈련의 설계 덕분에, 저희 접근법은 기존의 개인화된 모델과 제어 가능한 모듈과 잘 결합될 수 있습니다.
- 양적 및 질적 평가를 통해, AtomoVideo는 인기 있는 방법들과 비교했을 때 우수한 결과를 달성함을 입증했으며, 더 많은 예시는 프로젝트 웹사이트 https://atomo-video.github.io/ 에서 확인할 수 있습니다.

### [OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on](https://arxiv.org/abs/2403.01779)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Y6sN73WVIXyIE_U2ndYtP.png)

Vote: 5

Authors: Chengcai Chen, Tao Gu, Weifeng Chen, Yuhao Xu

- 이미지 기반 가상 시도(VTON)는 고품질의 인체 착용 이미지 생성과 의류 디테일의 완전한 보존을 필요로 하며, 이 문제를 해결하기 위해 사전 훈련된 잠재 확산 모델을 활용한 'Outfitting over Try-on Diffusion (OOTDiffusion)'을 제안합니다.
- 명시적인 워핑(warping) 과정 없이, OOTDiffusion은 의류의 디테일 특징을 학습하는 의상 UNet을 제안하고, 이를 확산 모델의 잡음 제거 과정에서 대상 인체와 합치기 위한 새로운 아웃피팅 융합 방식을 개발했습니다.
- 아웃핏팅 UNet의 조절 가능성을 강화하기 위해, 분류기 없는 가이드를 통해 의류 특징의 강도를 조절할 수 있는 아웃핏팅 드롭아웃을 도입했습니다.
- VITON-HD 및 Dress Code 데이터셋에서의 광범위한 실험을 통해, OOTDiffusion은 임의의 인간과 의류 이미지에 대해 고품질의 착용 이미지를 효율적으로 생성할 수 있음을 입증하며, 신뢰도와 조절 가능성 측면에서 기존 VTON 방법들을 능가하는 인상적인 성과를 보여줍니다.
- 해당 연구의 소스 코드는 온라인상에서 https://github.com/levihsu/OOTDiffusion에서 확인할 수 있습니다.

### [InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding](https://arxiv.org/abs/2403.01487)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/IbQHHv18BzQqFBTp3km8M.png)

Vote: 5

Authors: Yongfei Liu, Ran He, Quanzeng You, Hongxia Yang, Bohan Zhai, Yiqi Wang, Yunzhe Tao, Haogeng Liu, Huaibo Huang, Xiaotian Han

- 최근 다중모달 대형 언어 모델(MLLMs)의 상당한 진보에도 불구하고, 고해상도 이미지 내 복잡한 세부 사항의 정확한 인식 및 이해에 대한 도전은 여전히 남아 있습니다.
- 이 분야의 중요성에도 불구하고 아직 충분히 조사되지 않았고 이러한 도전을 해결하기 위해, 저희 연구팀은 다양한 해상도의 이미지를 처리하기 위해 특별히 설계된 새로운 구조인 InfiMM-HD를 제안합니다.
- InfiMM-HD는 낮은 계산 비용으로 다양한 해상도로 MLLMs를 확장할 수 있는 혁신을 가능하게 합니다.
- 이 모델은 교차주의 모듈(cross-attention module)과 시각적 창(visual windows)을 포함하여 계산 비용을 줄입니다.
- 네 단계의 훈련 파이프라인과 이러한 건축 설계를 통합함으로써, 우리의 모델은 효율적이고 비용 효과적으로 개선된 시각적 인식을 달성합니다.
- 실증적 연구는 InfiMM-HD의 강건함과 효과를 강조하며, 관련 분야에서의 새로운 탐색을 위한 가능성을 열어줍니다.
- 코드와 모델은 https://huggingface.co/Infi-MM/infimm-hd 에서 확인할 수 있습니다.

### [DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models](https://arxiv.org/abs/2403.00818)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/jP6NnlF0jE2EIspXI5IKG.png)

Vote: 4

Authors: Yehui Tang, Yujie Yang, Wei He, Kai Han, Tianyu Guo, Yunhe Wang, Chengcheng Wang

- 이 논문은 대규모 언어 모델(Large language models)이 Transformer 구조로 인해 겪는 높은 계산 및 메모리 요구 문제에 대응하기 위해 새로운 DenseSSM 방식을 제안합니다.
- DenseSSM은 상태 공간 모델(State Space Model, SSM)의 층 간 은닉 정보 흐름을 강화해서 Transformer에 필적하는 성능을 낼 수 있게 합니다.
- 이 방법은 초기 층의 은닉 상태를 선택적으로 깊은 층에 통합함으로써, 최종 출력에 중요한 세밀한 정보를 보존합니다.
- DenseSSM은 훈련의 병렬성과 추론 효율성을 유지하면서도, RetNet 및 Mamba와 같은 다양한 SSM 형태에 널리 적용할 수 있습니다.
- DenseSSM을 적용한 DenseRetNet은 기존 RetNet보다 최대 5% 향상된 정확도로 공개 벤치마크에서 성과를 보여줍니다.

### [ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models](https://arxiv.org/abs/2403.01807)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ZfA2mttFsD1F8OPmq8jMy.png)

Vote: 4

Authors: Norman Müller, Christian Richardt, Aljaž Božič, Lukas Höllein, Hung-Yu Tseng, Michael Zollhöfer, Matthias Nießner, David Novotny

- 이 논문은 이미 학습된 텍스트-이미지 모델을 기반으로 하여 실제 데이터로부터 단일 노이즈 감소 과정에서 다중 시점 이미지를 생성하는 새로운 방법을 제시합니다.
- 기존 U-Net 네트워크의 각 블록에 3D 볼륨 렌더링과 크로스-프레임-어텐션 레이어를 통합하여, 어떤 시점에서도 더욱 3D 일관성 있는 이미지를 생성할 수 있는 자기회귀 생성 방법을 설계하였습니다.
- 모델은 실제 세계의 객체 데이터셋에서 학습되며, 다양한 고품질 형상과 질감을 가진 인스턴스들을 실제 환경에서 생성할 수 있는 능력을 보여줍니다.
- 기존 방법들과 비교했을 때, 본 방법으로 생성된 결과물은 일관성이 있으며, 시각적 품질 면에서 우수함을 띠고 (-30% FID, -37% KID) 증명됩니다.

### [TripoSR: Fast 3D Object Reconstruction from a Single Image](https://arxiv.org/abs/2403.02151)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/jT5RuWZ4puMLsmKuJOZ3v.png)

Vote: 3

Authors: David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Christian Laforte, Varun Jampani, Ding Liang, Yan-Pei Cao, Dmitry Tochilkin

- 이 기술 보고서에서는 단일 이미지로부터 0.5초 미만의 빠른 속도로 3D 메쉬를 생성할 수 있는 변형 기반 아키텍처를 활용하는 3D 재구성 모델 TripoSR을 소개합니다.
- TripoSR은 데이터 처리, 모델 설계, 훈련 기법에서 중요한 개선을 통해 LRM 네트워크 아키텍처를 발전시켰습니다.
- 공개 데이터셋에서의 평가 결과, TripoSR은 양적 및 질적으로 다른 오픈소스 대안들보다 우수한 성능을 나타냅니다.
- MIT 라이선스 하에 출시된 TripoSR은 연구자, 개발자, 창작자들이 3D 생성 AI 최신 기술을 이용할 수 있도록 지원합니다.

### [3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos](https://arxiv.org/abs/2403.01444)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/qCpOLy41oswsKC2bR2-w9.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/qCpOLy41oswsKC2bR2-w9.mp4" muted="false"></video></div>

Vote: 2

Authors: Lei Zhao, Wei Xing, Han Jiao, Guangyuan Li, Zhanjie Zhang, Jiakai Sun

- 다양한 각도에서 촬영된 동영상으로부터 사실적인 무료 관점 비디오(Free-Viewpoint Videos, FVVs)를 구축하는 것은 어려운 과제로 남아 있지만, 3DGStream이라는 새로운 방법을 통해 실세계 동적 장면의 효율적인 FVV 스트리밍을 위한 해결책을 제시한다.
- 본 방법은 오프라인 트레이닝 없이 각 프레임을 실시간으로 12초 이내에 재구성하고, 초당 200프레임으로 실시간 렌더링하는 기능을 구현한다.
- 3D 가우시안(3DGs)을 이용해 장면을 표현하며, 각 프레임마다 직접 최적화하는 대신, Neural Transformation Cache(NTC)를 활용하여 3DGs의 변환과 회전을 모델링함으로써 FVV 프레임당 훈련 시간과 저장 공간을 현저하게 줄인다.
- 또한, 동적 장면에서 새롭게 등장하는 객체들을 처리하기 위한 적응형 3DG 추가 전략을 제안한다.
- 실험 결과에서 3DGStream은 렌더링 속도, 이미지 품질, 훈련 시간, 모델 저장 공간과 같은 면에서 주요 기존 방법들과 비교하여 경쟁력 있는 성능을 보여준다.

### [Twisting Lids Off with Two Hands](https://arxiv.org/abs/2403.02338)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/iaSj-IHSEorSMS4Uo10TC.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/iaSj-IHSEorSMS4Uo10TC.mp4" muted="false"></video></div>

Vote: 1

Authors: Toru Lin, Jitendra Malik, Haozhi Qi, Pieter Abbeel, Zhao-Heng Yin

- 다중 손가락을 이용한 두 손 조작은 많은 접촉이 요구되는 작업과 복잡한 양손 조정 때문에 로봇공학에서 오랫동안 도전적인 과제로 남아 있었습니다.
- 본 연구에서는 다양한 병과 같은 물체의 뚜껑을 두 손으로 비틀는 문제를 다루며, 시뮬레이션에서 깊은 강화 학습을 이용해 훈련된 정책이 실제 세계로 효율적으로 전환될 수 있음을 보여줍니다.
- 실시간 인식, 보상 설계 및 물리 모델링에 대한 새로운 공학적 통찰을 바탕으로, 해당 정책은 보지 못한 다양한 객체에 걸쳐 일반화 능력을 보여 주며, 동적이고 손재주가 필요한 행동을 선보입니다.
- 이러한 발견은 강화 학습과 시뮬레이션에서 현실로의 전환 (sim-to-real)을 결합한 학습 방법이 전례 없는 복잡성을 가진 조작 문제를 해결하는데 유망한 접근법임을 입증합니다.

### [RT-H: Action Hierarchies Using Language](https://arxiv.org/abs/2403.01823)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kfeU3aAyYylgCLQwkt7zS.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kfeU3aAyYylgCLQwkt7zS.mp4" muted="false"></video></div>

Vote: 1

Authors: Quon Vuong, Suneel Belkhale, Ted Xiao, Jonathan Tompson, Dorsa Sadigh, Pierre Sermanet, Debidatta Dwibedi, Yevgen Chebotar, Tianli Ding

- 언어는 복잡한 개념을 소화하기 쉬운 조각으로 나누는 방법을 제공합니다. 
- 최근의 로봇 모방 학습 연구들은 언어 조건을 달고 시각적 관찰과 고차원의 과제를 기반으로 행동을 예측하는 정책을 사용하고 있습니다.
- 이러한 방식은 자연 언어의 구조를 활용하여 의미적으로 유사한 과제들 사이에서 데이터를 공유할 수 있게 하지만, 과제들이 의미적으로 다양해짐에 따라 데이터 공유가 어려워지고 더 많은 시연 데이터가 필요하게 됩니다.
- 과제와 행동을 연결하는 귀착점으로, 연구팀은 로봇에게 "팔을 앞으로 움직여라"와 같은 더 세밀한 문구로 저수준의 동작을 기술하는 행동의 언어를 가르칩니다.
- 이 중간 단계의 언어 동작을 예측하는 것은 정책이 외관상 다르게 보이는 과제들에 걸쳐 저수준의 동작의 공유된 구조를 학습하도록 강제합니다.
- 언어 동작을 조건으로 한 정책은 실행 중에 인간이 지정한 언어 동작을 통해 쉽게 교정될 수 있습니다.
- 이는 언어로 인한 인간의 개입에서 학습할 수 있는 유연한 정책을 위한 새로운 패러다임을 가능하게 합니다.
- RT-H 방법론은 언어 동작을 사용하여 행동 계층을 구축하며, 먼저 언어 동작을 예측하는 방법을 배우고 이를 바탕으로 고차원의 과제와 연결하여 행동을 예측하며, 모든 단계에서 시각적 맥락을 사용합니다.
- RT-H는 이 언어-행동 계층을 활용하여 다중 과제 데이터셋을 효과적으로 활용하며, 더욱 견고하고 유연한 정책을 학습할 수 있음을 보여줍니다.
- 이러한 정책은 언어 개입에 반응할 뿐만 아니라, 이러한 개입에서 학습하고 원격 조정 개입에서 학습하는 방법을 뛰어넘는 성과를 달성할 수 있습니다.
- 연구의 웹사이트와 동영상은 https://rt-hierarchy.github.io 에서 확인할 수 있습니다.

