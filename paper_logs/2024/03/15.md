## Daily Papers (2024-03-15)

### [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09611.png)

Vote: 45

Authors: Doug Kang, Futang Peng, Chong Wang, +, Xiang Kong, Karanjeet Singh, Zhe Gan, Bowen Zhang, Philipp Dufter, Nan Du, Floris Weers, Haotian Zhang, Hongyu Hè, Tom Gunter, Anton Belyi, Dhruti Shah, Jianyu Wang, Sam Dodge, Aonan Zhang, Jean-Philippe Fauconnier, Xianzhi Du, Max Schwarzer, Brandon McKinzie

- 이 연구에서는 성능이 좋은 복합모달 대규모 언어 모델(MLLMs)을 구축하는 방법에 대해 논의하고, 다양한 아키텍처 구성 요소와 데이터 선택의 중요성을 검토합니다.
- 이미지 인코더, 비전-언어 연결기, 그리고 다양한 사전 훈련 데이터 선택에 대한 광범위한 상세 검증을 통해 몇 가지 중요한 설계 교훈을 도출했습니다.
- 대규모 복합모달 사전 훈련을 위해 이미지-캡션 데이터, 중간에 삽입되는 이미지-텍스트 데이터, 텍스트 전용 데이터의 신중한 혼합이 여러 벤치마크에 걸쳐 최첨단의 소수샷(few-shot) 결과를 달성하는 데 중요하다는 것을 입증했습니다.
- 이미지 인코더와 함께 이미지 해상도 및 이미지 토큰 수는 상당한 영향을 미치지만, 비전-언어 연결기 설계는 비교적 중요도가 낮다는 것을 발견했습니다.
- 제시된 방법을 확대하여 최대 30B 파라미터를 가진 복합모달 모델인 MM1을 구축하였으며, 이는 밀집 모델(dense models)과 전문가 혼합(Mixture of Experts, MoE) 변형 모두를 포함하며 사전 훈련 지표에서 최첨단이고 다양한 확인된 복합모달 벤치마크에서 경쟁력 있는 성능을 달성합니다.
- 대규모 사전 훈련 덕분에 MM1은 맥락 내 학습(in-context learning) 향상 및 다중 이미지 추론과 같은 매력적인 특성을 가지며, 이를 통해 소수샷 사슬 추론(chain-of-thought prompting)이 가능하다는 장점을 갖습니다.

### [Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset](https://arxiv.org/abs/2403.09029)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09029.png)

Vote: 28

Authors: Hugo Laurençon, Léo Tronchon, Victor Sanh

- 웹 개발에서 비전-언어 모델(VLM) 사용은 효율성을 높이고 노코드(No-code) 솔루션을 가능하게 하는 유망한 전략을 제시한다.
- 사용자 인터페이스(UI)의 스크린샷이나 스케치를 제공함으로써, VLM이 해당하는 HTML 코드를 생성할 수 있다.
- VLM의 발전에도 불구하고, 스크린샷을 해당하는 HTML로 전환하는 구체적인 도전은 거의 탐구되지 않았다.
- 이러한 상황은 적합하고 고품질의 데이터셋의 부재 때문인 것으로 추정된다.
- 본 연구는 HTML 코드와 그에 상응하는 스크린샷 2백만 쌍으로 구성된 합성 데이터셋인 WebSight를 소개한다.
- 저자들은 기초 VLM을 WebSight 데이터셋으로 파인튜닝하여 웹페이지 스크린샷을 기능적인 HTML 코드로 변환하는 능력을 보여준다.
- 이 분야의 연구를 촉진하기 위해, WebSight 데이터셋은 오픈소스로 공개되었다.

### [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09629.png)

Vote: 17

Authors: Yijia Shao, Eric Zelikman, Noah D. Goodman, Varuna Jayasiri, Georges Harik, Nick Haber

- 사람들은 글을 쓰거나 말할 때 생각하는 시간을 가질 때가 있는데, 기계학습 모델도 이러한 단계적 추론을 내재화할 수 있다는 것을 연구에서 설명하고 있다.
- Self-Taught Reasoner(STaR, Zelikman et al. 2022)는 질문-응답 및 작업 수행에서 올바른 답을 낳는 추론을 few-shot 예시를 통해 추론함으로써 학습한다.
- 본 논문은 STaR을 일반화한 Quiet-STaR을 소개하며, 여기서 언어 모델은 미래의 텍스트를 설명하는 추론을 각 토큰에서 생성하여 예측 능력을 향상시킨다.
- 연구팀은 반복적인 텍스트 생성의 계산 비용, 내부적인 사고를 생성하거나 사용하는 방법을 모델이 처음에 알지 못하는 문제, 개별 다음 토큰을 예측하는 것 이상의 필요성 등의 도전 과제들을 해결한다.
- 이를 위해 토큰별 병렬 샘플링 알고리즘을 제안하고, 사고의 시작과 끝을 나타내는 학습 가능한 토큰을 사용하며, 확장된 teacher-forcing 기술을 적용한다.
- 생성된 추론은 특히 예측하기 어려운 토큰에 도움을 주며, 언어 모델이 어려운 질문에 직접 답하는 능력을 향상시킨다는 것이 연구를 통해 확인되었다.
- 인터넷 텍스트 코퍼스에서 Quiet-STaR을 사용하여 계속된 사전 훈련을 거친 언어 모델은 GSM8K(5.9%에서 10.9%로)와 CommonsenseQA(36.3%에서 47.2%로)에서 zero-shot 성능 향상을 보였고, 자연 텍스트 내에서 어려운 토큰의 perplexity도 향상되었다.
- 활성화된 결과들은 이러한 테스크들에 대한 미세 조정(fine-tuning) 없이도 이루어진 것으로, 더 일반적이고 확장 가능한 방식으로 추론을 학습할 수 있는 언어 모델로의 발전을 나타낸다.

### [GiT: Towards Generalist Vision Transformer through Universal Language Interface](https://arxiv.org/abs/2403.09394)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09394.png)

Vote: 16

Authors: Liwei Wang, Haiyang Wang, Shaoshuai Shi, Hongsheng Li, Hao Tang, Muhammad Ferjad Naeem, Li Jiang, Bernt Schiele

- 이 논문에서는 단일 ViT(Vision Transformer)만을 사용하여 다양한 시각 분야의 작업을 동시에 적용할 수 있는 단순하지만 효과적인 프레임워크인 GiT(Generalist Vision Transformer)를 제안합니다.
- 대규모 언어 모델에서 널리 사용되는 멀티 레이어 트랜스포머 구조의 범용성에 착안하여 시각 기반 모델(Vision Foundation Model, VFM)로서의 잠재력을 확장하고자 합니다.
- 시각 작업은 일반적으로 검출을 위한 경계 상자 헤드나 세분화를 위한 픽셀 디코더 등과 같은 특정 모듈을 요구하는데, GiT는 범용 언어 인터페이스를 통해 자동 회귀 디코딩 기능을 성공적으로 활용하여 이미지 이해, 희소 인식, 밀집 예측 등과 같은 다양한 시각 작업을 통합합니다.
- GiT 모델은 어떠한 특정 추가 없이 오로지 ViT로만 이루어져 건축학적 단순화를 실현합니다.
- 다양한 대표 벤치마크를 통해 합동 훈련을 받은 GiT는 다중 작업 비주얼 모델로, 과제별 맞춤형 튜닝 없이 새로운 범용 성능 벤치마크를 설정합니다.
- 상호 작업 간의 개선을 촉진하는 GiT는 단독 훈련에 비해 눈에 띄는 성능 향상을 나타냄으로써, 대규모 언어 모델(LLMs)에서 관찰된 유사한 영향력을 반영합니다.
- 27개의 데이터셋으로 훈련을 더 풍부하게 한 GiT는 다양한 작업에서 강력한 제로샷 결과를 달성합니다.
- 단순한 설계로 인해, 이 패러다임은 시각과 언어 간의 건축적 격차를 좁힐 잠재력을 지니고 있습니다.
- 코드와 모델은 https://github.com/Haiyang-W/GiT 에서 공개될 예정입니다.

### [Video Editing via Factorized Diffusion Distillation](https://arxiv.org/abs/2403.09334)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09334.png)

Vote: 16

Authors: Uriel Singer, Adam Polyak, Yaniv Taigman, Devi Parikh, Yuval Kirstain, Shelly Sheynin, Amit Zohar

- "Emu Video Edit (EVE)" 모델을 소개하며, 이는 지도 학습이 없는 비디오 편집 데이터에 의존하지 않고 비디오 편집의 새로운 기준을 제시합니다.
- EVE는 이미지 편집 어댑터와 비디오 생성 어댑터를 별도로 훈련시켜 같은 텍스트-이미지 모델에 연결합니다.
- 비디오 편집을 향해 어댑터들을 조정하기 위해 감독 없는 증류 절차인 "Factorized Diffusion Distillation"을 새롭게 도입합니다.
- 이 절차는 지도 데이터 없이 하나 이상의 교사 모델로부터 동시에 지식을 증류하여 EVE에게 비디오 편집을 가르칩니다.
- EVE는 (i) 이미지 편집 어댑터를 통해 각각의 프레임을 정확하게 편집하고, (ii) 비디오 생성 어댑터를 사용하여 편집된 프레임들 간의 시간적 일관성을 보장하는 지식을 함께 증류합니다.
- 마지막으로, 우리의 접근법이 어댑터들의 추가적인 조합을 조정함으로써 다른 능력을 발휘할 가능성을 보여주기 위해 실험을 진행합니다.

### [StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control](https://arxiv.org/abs/2403.09055)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09055.png)

Vote: 9

Authors: Kyoung Mu Lee, Daniel Sungho Jung, Kanggeon Lee, Jaerin Lee

- 본 논문에서는 텍스트-이미지 합성에 있어 확산 모델의 성공에 힘입어 이미지 생성 및 편집을 위한 차세대 사용자 응용 프로그램의 유망한 후보로 여겨지고 있음을 밝혔습니다.
- 기존 연구는 추론 시간을 줄이거나 지역 기반 텍스트 프롬프트와 같은 미세한 제어를 사용자가 할 수 있도록 하여 확산 모델의 사용성을 향상시키는 데 초점을 맞추었습니다.
- 하지만 저자들은 이러한 두 가지 접근 방식을 통합하는 것이 단순하지 않다는 것을 경험적으로 발견하였고 이는 확산 모델의 잠재력을 제한한다고 지적합니다.
- 이러한 호환성 문제를 해결하기 위해 저자들은 StreamMultiDiffusion이라는 첫 실시간 지역 기반 텍스트-이미지 생성 프레임워크를 제시했습니다.
- 빠른 추론 기술을 안정화시키고 모델을 새로 제안된 멀티-프롬프트 스트림 배치 아키텍처로 재구성하여 기존 솔루션보다 10배 더 빠른 파노라마 생성을 달성하였습니다.
- 또한 단일 RTX 2080 Ti GPU에서 지역 기반 텍스트-이미지 합성을 초당 1.57 프레임의 속도로 생성할 수 있습니다.
- 이 솔루션은 '의미 팔레트(Semantic Palette)'라는 새로운 상호작용 이미지 생성 패러다임을 열었는데, 이는 주어진 다중 손으로 그린 지역에서 높은 품질의 이미지를 실시간으로 생성하며, 각 지역은 지정된 의미(예: 독수리, 소녀)를 인코딩합니다.
- 저자들의 코드와 데모 애플리케이션은 https://github.com/ironjr/StreamMultiDiffusion에서 확인할 수 있습니다.

### [Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding](https://arxiv.org/abs/2403.09626)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09626.png)

Vote: 8

Authors: Yifei Huang, Tong Lu, Guo Chen, Kunchang Li, Zhe Chen, Jiahao Wang, Limin Wang, Baoqi Pei, Jilan Xu, Zhiqi Li

- 컴퓨터 비전 연구에서 동영상 이해는 기본적인 방향 중 하나로, RNN, 3D CNN, 트랜스포머 등 다양한 아키텍처 탐색에 많은 노력이 기울여졌습니다.
- 새로 제안된 상태 공간 모델 아키텍처인 Mamba는 긴 시퀀스 모델링에서의 성공을 동영상 모델링으로 확장할 가능성을 보여줍니다.
- 이 연구에서는 동영상 이해 영역에서 Mamba가 트랜스포머에 대한 실질적인 대안이 될 수 있는지 평가하기 위해 Mamba를 사용하여 동영상을 모델링하는 다양한 역할을 탐구하고 여러 과제에서 Mamba의 우수성을 조사합니다.
- 동영상 모델링을 위한 Mamba의 네 가지 역할을 분류하여, 14개의 모델/모듈로 구성된 비디오 맘바 스위트를 도출하고, 12개의 동영상 이해 작업에 대해 평가합니다.
- 폭넓은 실험을 통해 Mamba가 비디오 전용 및 비디오-언어 과제에서 강력한 가능성을 가지며 효율성-성능 트레이드오프에서 가망성을 보임을 확인했습니다.
- 본 연구가 향후 동영상 이해 연구에 유용한 데이터 포인트와 통찰력을 제공할 수 있기를 기대합니다.
- 연구 관련 코드는 https://github.com/OpenGVLab/video-mamba-suite 에서 공개되어 있습니다.

### [Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring](https://arxiv.org/abs/2403.09333)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09333.png)

Vote: 7

Authors: Yufei Zhan, Ming Tang, Fan Yang, Jinqiao Wang, Hongyin Zhao, Yousong Zhu

- 대용량 시각 언어 모델은 세밀한 객체 인식을 달성했으나, 이미지 해상도의 제한이 복잡하고 조밀한 시나리오에서 특정 전문가의 성능을 뛰어넘는 것에 상당한 장애물이 되고 있습니다.
- 이러한 제한은 GUI 에이전트, 카운팅 등 분야에서의 섬세한 시각 및 언어 참조를 달성하는 모델의 잠재력을 더욱 제한합니다.
- 이 문제를 해결하기 위해, 우리는 시각 및 텍스트 프롬프트를 활용한 유연한 객체 참조를 가능하게 하는 통합 고해상도 범용 모델인 Griffon v2를 소개합니다.
- 높은 이미지 해상도로 효율적으로 스케일링하기 위해, 대규모 언어 모델의 입력 토큰 제약을 극복하는 단순하고 가벼운 다운샘플링 프로젝터를 설계했습니다.
- 이 디자인은 완전한 맥락과 세세한 디테일을 보존하면서, 특히 작은 객체에 대한 다중 모드 인식 능력을 크게 향상시킵니다.
- 이를 기반으로, 사용자 친화적인 상호작용을 가능하게 하는 플러그 앤 플레이 시각 토크나이저를 통해 모델에 시각-언어 공동 참조 기능을 추가 장착했습니다.
- 실험 결과, Griffon v2는 시각 및 텍스트 참조를 통해 관심 있는 모든 객체를 정확하게 찾아내고, REC, 문구 그라운딩, REG 과제에서 최고의 성능을 달성하는 것으로 나타났으며, 객체 탐지 및 객체 계수에서 전문 모델을 능가합니다.
- 데이터, 코드, 그리고 모델은 https://github.com/jefferyZhan/Griffon 에 공개될 예정입니다.

### [LocalMamba: Visual State Space Model with Windowed Selective Scan](https://arxiv.org/abs/2403.09338)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09338.png)

Vote: 7

Authors: Fei Wang, Chang Xu, Shan You, Xiaohuan Pei, Tao Huang, Chen Qian

- 최근 상태 공간 모델의 진보가 언어 이해와 같은 장기 시퀀스 모델링에서 두각을 나타내고 있지만, 비전 작업에서의 CNNs(합성곱 신경망) 및 ViTs(비전 트랜스포머)의 성능을 획기적으로 넘어서지는 못하고 있다.
- 본 논문은 시퀀스 모델링을 위한 스캔 방향 최적화가 비전 맘바(Vision Mamba, ViM)의 성능 향상에 있어 중요한 열쇠임을 주장하며, 기존 ViM 방법론이 공간적 토큰을 평면화하는 과정에서 로컬 2D 의존성을 놓치고 있다고 지적한다.
- 저자들은 이미지를 구별된 윈도우로 나누는 새로운 로컬 스캔 전략을 도입해 로컬 의존성을 효과적으로 포착하고 전체적 관점을 유지할 수 있도록 한다.
- 또한 네트워크 각 계층에서 서로 다른 스캔 패턴을 선호하는 점을 인지하여, 각 계층마다 최적의 스캔 선택을 독립적으로 검색하는 동적 방법을 제안하여 성능을 대폭 향상시킨다.
- 평면 및 계층적 모델에 대한 광범위한 실험을 통해, 본 접근 방식이 이미지 표현을 효과적으로 포착하는데 있어 우수함을 강조한다; 예를 들어, 동일한 1.5G FLOPs를 사용하면서도 ImageNet에서 ViM-Ti를 3.1% 앞서는 성능을 보여준다.
- 논문에서 소개하는 코드는 https://github.com/hunto/LocalMamba에서 확인할 수 있다.

### [BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences](https://arxiv.org/abs/2403.09347)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09347.png)

Vote: 6

Authors: Weilin Zhao, Xu Han, Teng Su, Chuan Shi, Shengnan Wang, Sun Ao, Maosong Sun, Zhiyuan Liu, Cheng Yang

- 변환기 기반 대규모 언어 모델의 성공에 중요한 역할을 하는 효과적인 주의 집중 모듈은 긴 시퀀스를 처리할 때 시간과 메모리의 이차 복잡도 문제에 직면한다.
- 긴 시퀀스 문제의 한 가지 해결책은 분산 클러스터를 활용해 여러 장치(GPUs 등)에 걸쳐 주의 집중 모듈의 계산을 병렬화하는 것이다.
- 그러나 분산 접근법을 채택함으로써 개별 주의 집중 결과를 저장하기 위한 추가 메모리 오버헤드가 생기고 전역 결과로의 개별 결과 집계를 위해 추가 통신 비용이 발생한다.
- 본 논문에서는 클러스터 전역 및 로컬 디바이스 수준에서 메모리 접근 및 통신 작업을 최적화하는 분산 주의 집중 프레임워크인 'BurstAttention'을 제안한다.
- 우리의 실험에서는 긴 시퀀스 처리를 위해 다른 경쟁 분산 주의 집중 솔루션과 BurstAttention을 비교한다. 
- 다양한 길이 설정에 따른 실험 결과, BurstAttention은 경쟁 베이스라인과 비교할 때 긴 시퀀스 처리에 상당한 이점을 제공하며, 통신 오버헤드를 40% 줄이고 8개의 A100에서 32K 시퀀스 길이 훈련 시 2배의 속도 향상을 달성한다.

### [Veagle: Advancements in Multimodal Representation Learning](https://arxiv.org/abs/2403.08773)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.08773.png)

Vote: 6

Authors: Mukunda NS, Rajat Chawla, Tushar Verma, Sukrit Chaterjee, Anmol Gautam, Adarsh Jha, Ishaan Bhola, Arkajit Datta, Ayush Vatsal

- 최근 인공지능 연구자들은 언어와 비전이 결합된 분야에 주목하며 텍스트와 시각 정보를 원활하게 통합하는 다중 모달 모델을 개발하는 데 관심을 보였습니다.
- 이러한 다중 모달 모델은 시각 기반 대화응답, 이미지 캡셔닝 등 다양한 작업들을 해결하는 데 있어 뛰어난 능력을 보이고 있지만, 실제 환경에서의 이미지 해석과 질문 응답에는 여전히 어려움이 있습니다.
- 이 논문에서는 기존 모델의 다중 모달 능력을 향상시키는 새로운 접근방식인 Veagle을 소개합니다.
- Veagle은 이전 연구들의 성공과 통찰력에서 영감을 받아 시각 정보를 언어 모델에 직접 투사하는 독특한 동적 메커니즘을 활용합니다.
- 이 동적 접근 방식은 시각적 맥락에 존재하는 복잡한 세부 사항을 보다 미묘하게 이해할 수 있게 해 줍니다.
- 벤치마크 데이터셋에서의 실험을 통해 Veagle이 시각적 질문 응답과 이미지 이해와 같은 작업에서 기존 모델들 보다 5-6% 개선된 성능을 보여줌으로써 효과를 입증했습니다.
- 이 결과는 Veagle 모델의 다재다능함과 기존 벤치마크를 넘어선 적용 가능성을 강조합니다.

### [VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding](https://arxiv.org/abs/2403.09530)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09530.png)

Vote: 6

Authors: Cindy Yang, Yuexian Zou, Yu Tian, Deshun Yang, Luhui Hu, Chris Kelly, Zihao Li, Jiayin Hu, Zaoshan Huang, Bang Yang

- 텍스트로부터 시각적 요소로의 전환은 이미지 및 비디오 생성, 이미지 내 원하는 요소 식별과 같이 사람들의 일상생활에 도움을 줍니다.
- 이전의 여러 모델들은 이미지 검출, 분류와 같은 명확하게 정의된 객체에 집중했지만, 대규모 언어 모델(Large Language Models, LLMs)은 자연어로부터 시각적 객체로의 변환을 소개하여 텍스트 컨텍스트에 대한 시각적 레이아웃을 제시합니다.
- 대규모 언어 모델들 중 OpenAI의 GPT-4가 최고점에 이르렀으며, 컴퓨터 비전(CV) 분야는 2D 이미지를 3D 표현으로 변환하는 다양한 최신(State-of-the-Art, SOTA) 모델과 알고리즘을 자랑합니다.
- 그러나 알고리즘과 문제 간의 불일치는 바람직하지 않은 결과를 초래할 수 있습니다.
- 이러한 도전에 대응하기 위해, 저희는 최신 비전 모델들을 통합하여 비전 지향 AI 발전을 촉진하는 일원화된 VisionGPT-3D 프레임워크를 제안합니다.
- VisionGPT-3D는 멀티모달 기본 모델의 강점을 기반으로 한 다목적 멀티모달 프레임워크를 제공하며, 다양한 SOTA 비전 모델을 원활하게 통합합니다.
- 이 프레임워크는 2D 깊이 지도 분석에 해당하는 적합한 3D 메시 생성 알고리즘을 식별하고, 텍스트 프롬프트와 같은 다양한 멀티모달 입력에 기반하여 최적의 결과를 생성하는 자동화를 가져옵니다.

### [3D-VLA: A 3D Vision-Language-Action Generative World Model](https://arxiv.org/abs/2403.09631)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09631.png)

Vote: 5

Authors: Chuang Gan, Haoyu Zhen, Xin Yan, Yining Hong, Peihao Chen, Yilun Du, Xiaowen Qiu, Jincheng Yang

- 기존의 시각-언어-행동(VLA) 모델들이 2D 입력에 의존하며, 3D 물리 세계와의 통합이 부족한 문제점을 지적함.
- 인간이 미래 시나리오에 대한 상상을 통해 행동을 계획하는 세계 모델을 갖고 있는 것에 착안하여, 3D-VLA는 3D 인식, 추론, 행동을 유기적으로 연결하는 새로운 유형의 실체 기반 기초 모델을 제안함.
- 3D 기반 대규모 언어 모델(LLM) 위에 구축되고, 상호작용 토큰을 도입하여 실재하는 환경에서의 참여를 가능하게 함.
- 모델에 생성 능력을 주입시키기 위해 일련의 실체 확산(diffusion) 모델들을 훈련하고, 이를 LLM과 정렬하여 목표 이미지와 포인트 클라우드를 예측함.
- 3D-VLA를 훈련시키기 위해 기존 로보틱스 데이터셋에서 방대한 3D 관련 정보를 추출하여 대규모 3D 실체 지시 데이터셋을 큐레이션함.
- 수행한 실험에서, 3D-VLA는 실체 환경에서의 추론, 다중 모달 생성, 계획 능력을 크게 향상시키며, 실제 세계 응용 분야에서의 잠재력을 보여줌.

### [Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering](https://arxiv.org/abs/2403.09622)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09622.png)

Vote: 5

Authors: Zhanhao Liang, Weicong Liang, Yuhui Yuan, Zeyu Liu, Gao Huang, Chong Luo, Ji Li

- 시각적 텍스트 렌더링은 현대의 텍스트-이미지 생성 모델에 있어 근본적인 도전 과제이며, 주요 문제는 텍스트 인코더의 미흡함에 있습니다.
- 정확한 텍스트 렌더링을 달성하기 위해, 문자 인식(character awareness)과 글리프(glyphs)와의 정렬이 텍스트 인코더에 필수적인 요소로 확인되었습니다.
- 글리프-ByT5(Glyph-ByT5)라는 맞춤형 텍스트 인코더를 개발하기 위해, 문자 인식이 가능한 ByT5 인코더를 세심하게 큐레이션된 글리프-텍스트 짝 데이터셋을 사용하여 미세 조정하였습니다.
- Glyph-ByT5를 SDXL과 통합하는 효과적인 방법을 제시하였고, 이를 통해 디자인 이미지 생성을 위한 Glyph-SDXL 모델을 생성하였습니다.
- 이 모델은 텍스트 렌더링의 정확도를 기존 20% 미만에서 거의 90%까지 크게 향상시켰으며, 디자인 이미지 벤치마크에서 주목할 만한 결과를 보였습니다.
- Glyph-SDXL은 이제 수십에서 수백 개의 문자로 된 텍스트 문단을 자동으로 여러 줄로 배치하며, 높은 철자 정확도로 렌더링할 수 있는 능력도 갖추게 되었습니다.
- 마지막으로, 시각적 텍스트가 포함된 고품질의 사실적 이미지 작은 세트와 Glyph-SDXL을 미세 조정함으로써, 실제 이미지에서의 장면 텍스트 렌더링 능력이 크게 향상되었음을 시연하였습니다.
- 이러한 결론들은 다양하고 도전적인 작업을 위해 맞춤형 텍스트 인코더를 디자인하는 데 있어서 추가적인 탐색을 장려할 것으로 기대됩니다.

