## Daily Papers (2024-03-21)

### [Mora: Enabling Generalist Video Generation via A Multi-Agent Framework](https://arxiv.org/abs/2403.13248)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13248.png)

Vote: 38

Authors: Lichao Sun, Ruoxi Chen, Chi Wang, Zhaoxu Li, Zhengqing Yuan, Lifang He, Haolong Jia

- Sora는 2024년 2월 OpenAI에 의해 출시된 대규모 일반 비디오 생성 모델로서 사회 전반에 걸쳐 주목을 받았으며, 그 이후로 Sora의 성능이나 다양한 비디오 생성 작업 지원 능력을 일치시키는 모델은 없었습니다.
- 현재 공개된 비디오 생성 모델은 소수에 불과하며, 대부분이 폐쇄 소스입니다.
- 이러한 격차를 메우기 위해 이 논문은 Sora가 시연한 일반 비디오 생성을 복제하기 위해 여러 첨단 시각 AI 에이전트를 통합한 새로운 다중 에이전트 프레임워크 Mora를 제안합니다.
- Mora는 여러 시각 에이전트를 사용하여 텍스트에서 비디오 생성, 텍스트 조건부 이미지에서 비디오 생성, 생성된 비디오 확장, 비디오에서 비디오 편집, 비디오 연결 및 디지털 세계 시뮬레이션 등 다양한 작업에 대한 Sora의 비디오 생성 기능을 성공적으로 모방할 수 있습니다.
- 광범위한 실험 결과에 따르면, Mora는 다양한 작업에서 Sora의 성능에 근접한 결과를 달성하였지만, 전체적으로 평가할 때 Sora와의 성능 격차는 여전히 명백합니다.
- 결론적으로, 이 프로젝트는 협력적 AI 에이전트를 통한 비디오 생성의 미래 방향을 안내하는 데 도움이 되기를 기대합니다.

### [LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13372.png)

Vote: 29

Authors: Junhao Zhang, Zheyan Luo, Yanhan Ye, Yaowei Zheng, Richong Zhang

- 대규모 언어 모델(LLMs)를 다운스트림 작업에 적응시키기 위한 효율적인 파인 튜닝은 매우 중요하며, LlamaFactory는 다양한 모델에 대한 이러한 방법들을 구현하는 데 필요한 복잡한 노력을 줄이는 통합 프레임워크입니다.
- 이 프레임워크는 100개 이상의 언어 모델에 대한 파인 튜닝을 코딩 없이도 사용자가 유연하게 맞춤 설정할 수 있게 해주며, 내장된 웹 UI 'LlamaBoard'를 통해 접근이 가능합니다.
- 언어 모델링과 텍스트 생성 작업에 대한 효율성과 효과성을 경험적으로 검증하였고, 프레임워크는 GitHub에서 https://github.com/hiyouga/LLaMA-Factory 주소로 공개되어 이미 13,000개 이상의 별표와 1,600개의 포크를 받았습니다.

### [Evolutionary Optimization of Model Merging Recipes](https://arxiv.org/abs/2403.13187)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13187.png)

Vote: 22

Authors: Qi Sun, Makoto Shing, Takuya Akiba, Yujin Tang, David Ha

- 이 논문은 진화 알고리즘을 사용하여 강력한 기초 모델을 자동으로 생성하는 새로운 방법을 제시합니다.
- 모델 병합은 비용 효율성으로 인해 대형 언어 모델(LLM) 개발에 유망한 접근 방식으로 떠올랐지만 현재는 인간의 직관과 도메인 지식에 의존하여 잠재력이 제한됩니다.
- 제안된 진화적 접근 방식은 다양한 오픈 소스 모델들의 효과적인 조합을 자동으로 발견하여, 추가적인 교육 데이터나 계산 없이 그들의 집단 지성을 활용합니다.
- 이 접근법은 개별 모델의 가중치뿐만 아니라 매개변수 공간과 데이터 흐름 공간에서도 작동하여 최적화 범위를 확장합니다.
- 특히, 이 방식을 통해 일본어 LLM에 수학 추론 기능이 있는 등의 다양한 분야를 아우르는 모델 병합이 가능해졌으며, 이러한 일본어 수학 LLM은 전문적으로 훈련되지 않았음에도 불구하고 다양한 일본어 LLM 벤치마크에서 최고 성능을 달성했습니다.
- 또한, 우리의 접근 방식을 통해 생성된 문화적으로 인식된 일본어 VLM(시각 언어 모델)은 일본 문화 특유의 내용을 설명함에 있어 이전의 일본어 VLM들을 능가하는 성능을 보여주었습니다.
- 이 연구는 오픈 소스 커뮤니티에 새로운 최첨단 모델을 기여하는 것뿐만 아니라 모델 구성 자동화를 위한 새로운 패러다임을 소개하며, 기초 모델 개발을 위한 대안적이고 효율적인 접근 방법을 탐색하는 길을 열어줍니다.

### [SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model](https://arxiv.org/abs/2403.13064)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13064.png)

Vote: 21

Authors: Christopher Xie, Suvam Patra, Vasileios Balntas, Edward Miller, Duncan Frost, Jakob Engel, Campbell Orme, Armen Avetisyan, Luke Holland, Samir Aroudj, Fuyang Zhang, Henry Howard-Jenkins, Tsun-Yi Yang, Richard Newcombe

- SceneScript는 기존 메쉬, 복셀 그리드, 포인트 클라우드 또는 방사성 필드로 묘사하는 방법과는 다르게 구조화된 언어 명령어의 시퀀스로 전체 장면 모델을 직접 생성하는 방법을 제안합니다.
- 이 방법은 트랜스포머와 대규모 언어 모델(LLMs)의 최근 성공에 영감을 받아 자동회귀적인 토큰 기반 접근법을 사용합니다.
- SceneScript는 장면 언어 인코더-디코더 구조를 활용하여 인코딩된 시각 데이터로부터 구조화된 언어 명령어의 집합을 직접 추론합니다.
- Aria Synthetic Environments라는 100k의 고품질 인도어 장면으로 구성된 대규모 합성 데이터셋을 생성 및 배포하여 SceneScript를 훈련합니다.
- 이 방법은 건축 레이아웃 추정에서 최고 수준의 결과를 제공하며, 3D 객체 탐지에서도 경쟁력 있는 결과를 보여줍니다.
- SceneScript는 구조화된 언어에 간단한 추가를 통해 새로운 명령어로 쉽게 적응할 수 있는 장점이 있으며, 이는 예를 들어, 대략적인 3D 객체 부분 복원과 같은 작업에 대해 설명합니다.

### [RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS](https://arxiv.org/abs/2403.13806)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13806.png)

Vote: 15

Authors: Michael Oechsle, Fabian Manhardt, Dominik Kaeser, Keisuke Tateno, John Bates, Michael Niemeyer, Rama Gosula, Daniel Duckworth, Marie-Julie Rakotosaona, Federico Tombari

- 본 연구에서는 방사성 필드를 이용하여 점 기반 장면 표현을 최적화하는 새로운 경량화 방법인 RadSplat을 제안하여 복잡한 장면의 강인한 실시간 렌더링을 실현합니다.
- 방사성 필드를 사전 지식과 최적화 신호로 활용함으로써 품질이 향상되고 최적화의 강건성이 증가합니다.
- 새로운 가지치기 기술을 개발하여 전체 점 수를 줄이고 높은 품질을 유지하며, 이로 인해 장면 표현을 더 작고 컴팩트하게 만들어 추론 속도가 빨라집니다.
- 테스트 시간 필터링 방법을 제안하여 렌더링 속도를 더욱 가속화하고 더 큰 주택 크기의 장면으로 확장할 수 있게 합니다.
- 이 방법을 통해 복잡한 캡처의 최첨단 합성을 초당 900프레임이 넘는 속도로 가능하게 합니다.

### [When Do We Not Need Larger Vision Models?](https://arxiv.org/abs/2403.13043)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13043.png)

Vote: 13

Authors: Ziyang Wu, Xin Wang, Baifeng Shi, Trevor Darrell, Maolin Mao

- 비전 모델의 크기를 확장하는 것이 효과적인 시각적 표현을 얻는 일반적인 방법이지만, 더 큰 모델이 필요하지 않는 지점에 대해 논의합니다.
- 이미 훈련된 작은 비전 모델(ViT-B 또는 ViT-L 등)을 다양한 이미지 스케일에서 동결하여 사용하면, 더 큰 모델(ViT-H 또는 ViT-G 등)을 능가할 수 있음을 보여줍니다.
- 해당 방식인 Scaling on Scales (S^2)는 분류, 분할, 깊이 추정, 다중모달 LLM(Multimodal LLM, MLLM) 벤치마크 및 로봇 조작에서 더 큰 모델의 성능을 뛰어넘는 결과를 달성했습니다.
- 특히, S^2는 GPT-4V 같은 모델을 능가하며 V* 벤치마크에서 MLLM의 상세 이해에 있어 최신 성능을 달성합니다.
- S^2가 모델 크기 확장에 비해 바람직한 스케일링 접근법인 조건을 검토합니다.
- 더 큰 모델은 더 어려운 예시에 대한 일반화 능력이 뛰어날 수 있지만, 다중 스케일의 작은 모델이 큰 비전 모델의 특징을 잘 근사할 수 있음을 보여줍니다.
- 이는 현재의 대규모 사전 훈련된 모델들이 학습하는 표현의 대부분이 다중 스케일의 작은 모델들로도 얻을 수 있음을 시사합니다.
- 다중 스케일 작은 모델의 학습 능력이 더 큰 모델과 비교하여 경쟁력이 있으며, S^2와 함께 훈련된 작은 모델은 더 큰 모델의 이점을 맞먹거나 뛰어넘을 수 있음을 보여줍니다.
- 비전 모델에 S^2를 적용할 수 있는 파이썬 패키지를 다음의 링크에서 제공합니다: https://github.com/bfshi/scaling_on_scales

### [IDAdapter: Learning Mixed Features for Tuning-Free Personalization of Text-to-Image Models](https://arxiv.org/abs/2403.13535)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13535.png)

Vote: 13

Authors: Jia Guo, Yongle Zhao, Siying Cui, Xiang An, Xinyu Wei, Ziyong Feng, Jiankang Deng

- 안정된 디퓨전을 이용하여 사용자의 구체적인 프롬프트에 기반한 고화질과 맞춤형 캐릭터 아바타 생성이 가능한 강력한 툴로 자리잡고 있습니다.
- 기존의 맞춤화 방법에는 테스트 타임의 파인 튜닝, 다중 입력 이미지의 요구, 정체성 유지의 낮은 정도, 생성된 결과의 다양성 한계 등의 문제점들이 존재합니다.
- 이러한 문제점들을 해결하기 위해, 단일 얼굴 이미지로부터 개인화된 이미지 생성 시 다양성과 정체성 유지를 향상시키는 'IDAdapter'라는 튜닝 없는 접근 방법을 소개합니다.
- IDAdapter는 텍스트와 시각적 주입을 결합하고 얼굴 정체성 손실을 사용하여 개인화된 개념을 생성 과정에 통합합니다.
- 훈련 단계에서는 특정 정체성의 여러 참조 이미지로부터의 혼합된 특징들을 통합하여 정체성과 관련된 컨텐츠 세부사항을 풍부하게 하여 모델이 이전 작업들보다 더 다양한 스타일, 표정, 각도의 이미지를 생성하도록 유도합니다.
- 광범위한 평가를 통해 우리의 방법이 생성된 이미지의 다양성과 정체성 충실성 모두에서 효과적임을 입증하였습니다.

### [HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models](https://arxiv.org/abs/2403.13447)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13447.png)

Vote: 11

Authors: Fangxun Shu, Lei Zhang, Jiang Liu, Zheqi Lv, Tianwei Lin, Hao Zhou, Hao Jiang, Siliang Tang, Juncheng Li, Wenqiao Zhang, Haoyuan Li, He Wanggui, Yueting Zhuang

- 최근 다중 모드 대규모 언어 모델(MLLMs)의 확대가 다양한 다중 모드 작업에서의 성능 개선을 보여줍니다.
- 기존의 방식인 LLaVA는 시각 기능을 텍스트와 유사한 토큰으로 변환하는 정적 시각-언어 매퍼를 사용하여 언어 모델이 시각 정보를 이해하도록 합니다.
- 하지만, 정적인 튜닝 전략은 같은 파라미터를 공유함으로써 다양한 다중 모드 작업에서 성능을 제한할 수 있습니다.
- 이에 대한 대안으로, 저희는 HyperLLaVA를 도입하는데, 이는 HyperNetworks에서 파생된 동적 시각과 언어 전문가를 통합한 프로젝터와 LLM 파라미터의 적응적 튜닝을 포함합니다.
- 이 전문가들은 시각과 언어 지침을 통해 적응적 파라미터 변동을 생성하며, 이를 통해 두 단계의 훈련에서 동적 프로젝터와 LLM 모델링이 가능합니다.
- 실험 결과, HyperLLaVA는 기존 MLLM 벤치마크인 MME, MMBench, SEED-Bench, LLaVA-Bench에서 LLaVA를 크게 앞섭니다.
- 이 프로젝트는 https://github.com/DCDmllm/HyperLLaVA 링크에서 확인할 수 있습니다.

### [DepthFM: Fast Monocular Depth Estimation with Flow Matching](https://arxiv.org/abs/2403.13788)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13788.png)

Vote: 8

Authors: Pingchuan Ma, Olga Grebenkova, Johannes S. Fischer, Dmytro Kotovenko, Björn Ommer, Ulrich Prestel, Ming Gui, Stefan Andreas Baumann, Vincent Tao Hu

- 단안 깊이 추정은 많은 비전 작업 및 응용 프로그램에 필수적이나, 현재의 판별적 접근 방식은 흐릿한 아티팩트에 제한되고, 최신 생성적 방법은 SDE(Sscious Differential Equation)의 본성으로 인해 샘플링이 느려진다는 문제가 있습니다.
- 우리는 잡음으로부터 시작하는 대신, 입력 이미지에서 깊이 맵으로의 직접적인 매핑을 찾고자 하였으며, 솔루션 공간을 통한 직선적 궤적을 가진 플로우 매칭을 사용하여 이를 효과적으로 프레임할 수 있다는 것을 관찰하였습니다.
- 예비 학습된 이미지 확산 모델이 플로우 매칭 깊이 모델을 위한 충분한 사전 지식으로서 작용할 수 있으며, 이를 통해 합성 데이터만을 사용한 효율적인 훈련으로 실제 이미지에 일반화될 수 있음을 보여주고 있습니다.
- 보조적인 표면 노멀 손실을 활용하면 깊이 추정이 더욱 개선됨을 발견하였습니다.
- 우리의 생성적 접근 방식 덕분에, 모델은 깊이 추정의 신뢰도를 믿을 수 있게 예측할 수 있습니다.
- 복잡한 자연 장면의 표준 벤치마크에서, 합성 데이터만을 사용한 훈련에도 불구하고, 우리의 경량화된 접근 방식은 낮은 계산 비용으로 최첨단의 성능을 보여줍니다.

### [RewardBench: Evaluating Reward Models for Language Modeling](https://arxiv.org/abs/2403.13787)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13787.png)

Vote: 8

Authors: LJ Miranda, Khyathi Chandu, Yejin Choi, Valentina Pyatkin, Jacob Morrison, Nathan Lambert, Noah A. Smith, Hannaneh Hajishirzi, Bill Yuchen Lin, Nouha Dziri, Sachin Kumar, Tom Zick

- 이 논문은 학습된 모델을 인간의 선호도에 맞춰 조정하는 RLHF(reward learning from human feedback)의 핵심인 보상 모델(RMs)을 평가하는 연구 부족 문제에 초점을 맞추고 있다.
- 보상 모델을 평가하는 것은 언어 모델을 조정하는 데 사용되는 불투명한 기술을 이해하고 통합된 가치를 파악할 기회를 제공한다.
- 현재까지 보상 모델의 능력, 훈련 방법, 또는 오픈 소스 모델에 대해 설명하는 자료가 거의 없다.
- 저자들은 보상 모델의 과학적 이해를 향상시키기 위해 평가를 위한 벤치마크 데이터셋과 코드 베이스인 RewardBench를 제시한다.
- RewardBench 데이터셋은 채팅, 추론, 안전에 걸친 만족하는 요건을 갖춘 프롬프트-이긴 경우-진 경우(trios) 컬렉션으로 구성되어 있으며, 이는 보상 모델이 어려운 구조화된 및 분포 외 질의에 대해 어떻게 수행하는지 벤치마킹한다.
- 보상 모델 비교를 위한 특정 데이터셋을 생성하여 어떤 답변이 다른 답변보다 선호되어야 할 미묘하고, 검증 가능한 이유(예: 버그, 부정확한 사실 등)를 포함하고 있다.
- RewardBench 리더보드에서 저자들은 분류기의 직접 MLE 훈련과 Direct Preference Optimization (DPO)과 같은 간접적 보상 모델링을 포함한 다양한 방법으로 훈련된 보상 모델을 평가한다.
- 저자들은 거부 경향, 추론 한계, 명령 따르기의 단점 등 다양한 보상 모델의 RLHF 프로세스에 대한 더 나은 이해를 위한 많은 발견을 소개한다.

### [Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos](https://arxiv.org/abs/2403.13044)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13044.png)

Vote: 7

Authors: Eli Shechtman, Hadi Alzayer, Xuaner Zhang, Michael Gharbi, Zhihao Xia, Jia-Bin Huang

- 본 논문에서는 대략적으로 편집된 이미지를 입력받아 지정된 레이아웃을 따르는 사실적인 결과물을 생성하는 생성 모델을 제안합니다.
- 제안된 방법은 원본 이미지로부터 미세한 디테일을 전달하면서, 각 부분의 정체성을 유지합니다.
- 동시에 새 레이아웃에 정의된 조명과 맥락에 맞게 이미지를 조정합니다.
- 비디오는 객체와 카메라 움직임을 통해 시점, 조명, 물리적 상호작용의 변화에 대한 다양한 관찰 데이터를 제공함으로써, 이 작업을 위한 강력한 지도 자료가 됨을 강조합니다.
- 동일한 비디오에서 임의의 시간 간격으로 추출된 소스와 타겟 프레임의 쌍으로 구성된 이미지 데이터셋을 구축합니다.
- 사용자 편집을 모방하는 두 가지 모션 모델을 사용하여 소스 프레임을 타겟으로 변형시킵니다.
- 사전 훈련된 확산 모델로부터 시작하여 변형된 이미지를 실제 이미지로 변환하도록 모델을 감독합니다.
- 모델 디자인은 소스 프레임으로부터의 세밀한 디테일 전달을 명시적으로 가능하게 하면서, 사용자가 지정한 레이아웃을 밀접하게 따릅니다.
- 간단한 분할과 대략적인 2D 조작을 사용하여 사용자의 입력에 충실한 사실적 편집을 합성하며, 조명의 조화와 편집된 객체 간 물리적 상호작용과 같은 2차 효과들을 해결할 수 있음을 보여줍니다.

### [Reverse Training to Nurse the Reversal Curse](https://arxiv.org/abs/2403.13799)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13799.png)

Vote: 7

Authors: Jason Weston, Olga Golovneva, Zeyuan Allen-Zhu, Sainbayar Sukhbaatar

- 대규모 언어 모델은 "A는 B라는 특징을 가지고 있다"를 학습하더라도 "B는 A의 특징이다"로 일반화하지 못하는 문제인 '반전 저주(Reversal Curse)'를 겪고 있다.
- 이 문제는 지프의 법칙(Zipf's law) 때문에 트릴리언 단위의 토큰으로 학습을 하거나 전체 인터넷 데이터를 사용해도 여전히 발생한다.
- 본 연구는 모든 단어를 두 번 사용하고 사용 가능한 토큰의 양을 두 배로 늘리는 새로운 학습 방식인 '역방향 학습(reverse training)'을 제안한다.
- 역방행 학습은 훈련 문자열을 뒤집어서 학습하지만, 엔티티와 같이 선택된 부문자열은 유지(즉, 뒤집지 않음)한다.
- 데이터와 매치된 역방향 학습 모델은 표준 모델에 비해 표준 작업에서 뛰어난 성능을 보였고, 계산에 매치된 역방향 학습 모델은 반전 작업에서 훨씬 우수한 성능을 제공하여 반전 저주 문제를 해결하는 데 도움을 준다.

### [Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation](https://arxiv.org/abs/2403.13745)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13745.png)

Vote: 6

Authors: Xiaoyu Shi, Dazhong Shen, Fu-Yun Wang, Yu Liu, Xiaoshi Wu, Guanglu Song, Zhaoyang Huang, Hongsheng Li

- 동영상 외부 그리기는 입력 동영상의 화면 밖의 내용을 생성하는 동시에 내부 및 외부 프레임의 일관성을 유지하는 복잡한 작업입니다.
- 기존 방법은 생성 품질이나 유연성 면에서 부족함을 드러내고 있습니다. 
- 본 연구에서는 원본 비디오의 데이터별 특수 패턴과 이미지/비디오 생성 모델의 선험 지식을 활용하는 확산 기반 파이프라인 MOTIA를 소개합니다.
- MOTIA는 두 가지 주요 단계로 구성됩니다: 입력 특화 어댑테이션과 패턴 인식 외부 그리기.
- 입력 특화 어댑테이션 단계에서는 단일 소스 비디오에서 효율적이고 효과적인 유사 외부 그리기 학습을 수행하여 모델이 소스 비디오 내 패턴을 식별하고 배우게 합니다.
- 이 과정은 표준 생성 프로세스와 외부 그리기 간의 격차를 좁히는 데 도움이 됩니다.
- 이어지는 패턴 인식 외부 그리기는 학습된 패턴을 일반화하여 외부 그리기 결과를 생성하기 위한 것입니다.
- 확산 모델의 생성 능력과 소스 비디오에서 얻은 비디오 패턴을 더 잘 활용하기 위한 공간 인식 삽입 및 노이즈 이동과 같은 추가 전략이 제안됩니다.
- 광범위한 평가를 통해 MOTIA의 우수성이 입증되었으며, 널리 인정받는 벤치마크에서 기존 최첨단 방법들을 능가하는 결과를 보여주고 있습니다.
- 특히, 이러한 성과는 과도한 작업 특화 튜닝을 필요로 하지 않으면서 이루어졌습니다.

### [ZigMa: Zigzag Mamba Diffusion Model](https://arxiv.org/abs/2403.13802)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13802.png)

Vote: 6

Authors: Olga Grebenkova, Pingchuan Ma, Bjorn Ommer, Johannes Fischer, Ming Gui, Stefan Andreas Baumann, Vincent Tao Hu

- 확산 모델은 특히 변환기 기반 구조 내에서 규모의 확장성과 2차 복잡성 문제에 오랫동안 시달려 왔다.
- 본 연구는 스테이트-스페이스 모델인 Mamba의 긴 시퀀스 모델링 능력을 활용하여 시각 데이터 생성에 적용할 수 있는 가능성을 확장하는 것을 목표로 한다.
- 첫째, Mamba 기반 시각 방법에서 공간 연속성에 대한 고려 부족이라는 중요한 문제점을 발견했다.
- 둘째, 이 통찰을 바탕으로 Zigzag Mamba라는 간단하고 적용이 가능한, 파라미터가 필요 없는 방법을 도입하여 Mamba 기반 베이스라인을 능가하고, 변환기 기반 베이스라인의 속도와 메모리 사용량을 개선하였다.
- 마지막으로, Zigzag Mamba를 Stochastic Interpolant 프레임워크와 통합하여 FacesHQ 1024x1024, UCF101, MultiModal-CelebA-HQ, MS COCO 256x256과 같은 고해상도 시각 데이터셋의 모델 확장성을 탐구했다.
- 해당 연구의 코드는 https://taohu.me/zigma/ 에서 공개될 예정이다.

### [VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis](https://arxiv.org/abs/2403.13501)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13501.png)

Vote: 6

Authors: Dan Zhang, Anna Khoreva, William Beluch, Margret Keuper, Yumeng Li

- 텍스트를 비디오로 변환하는 기존의 오픈소스 확산 모델들은 길고 동적으로 변화하는 콘텐츠를 생성하는 데 어려움을 겪으며, 텍스트 프롬프트에서 암시한 시각적 변화를 무시하는 경향이 있습니다.
- 본 논문에서는 비디오 생성 과정 중 실시간으로 생성 과정을 변화시켜 시간적 역학을 향상시키고 긴 비디오 생성을 가능하게 하는 Generative Temporal Nursing (GTN)이라는 개념을 소개합니다.
- 우리는 GTN을 위한 방법론인 VSTAR를 제안하며, 이는 Video Synopsis Prompting (VSP)과 Temporal Attention Regularization (TAR) 두 가지 주요 요소로 구성됩니다.
- VSP는 Large Language Models(LMM)를 활용하여 원래 프롬프트를 기반으로 비디오 개요를 자동 생성함으로써 긴 비디오의 다양한 시각적 상태에 대한 정확한 텍스트 안내를 제공합니다.
- TAR는 사전 훈련된 T2V 확산 모델들의 시간적 주의 단위를 정제하는 정규화 기법으로, 비디오 역학을 제어할 수 있게 해줍니다.
- 제안된 접근 방식이 기존 오픈소스 T2V 모델들보다 시각적으로 매력적인 긴 비디오를 생성하는데 있어 우수함을 실험을 통해 입증했습니다.
- VSTAR를 사용함으로써 원하는 시각적 변화를 시간에 맞추어 간과하지 않고 더 잘 반영할 수 있음을 시간적 주의 맵 분석을 통해 보여줍니다.

### [Compress3D: a Compressed Latent Space for 3D Generation from a Single Image](https://arxiv.org/abs/2403.13524)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13524.png)

Vote: 5

Authors: Lei Zhang, Bowen Zhang, Xi Zhao, Yu Li, Tianyu Yang

- 단일 이미지에서 고품질의 3D 자산을 효율적으로 생산하는 것은 도전적인 일임에도 불구하고, 본 논문에서는 3D 모델을 컴팩트한 삼면 잠재 공간(triplane latent space)으로 인코딩하여 3D 기하학 및 텍스처 정보를 효과적으로 압축하는 삼면 자동인코더(triplane autoencoder)를 제안한다.
- 자동인코더 프레임워크 내에서, 저해상도 잠재 표현을 사용하여 고해상도 3D 특징 볼륨에서 특징을 가져오는 3D 인식 교차 주의 메커니즘을 도입함으로써 잠재 공간의 표현 능력을 향상시킨다.
- 연구팀은 이 고도화된 잠재 공간 위에 확산 모델을 훈련시킨다.
- 3D 생성을 위해 이미지 임베딩에만 의존하는 것이 아니라, 이미지 임베딩과 모양 임베딩을 동시에 활용하는 것을 주장한다.
- 구체적으로, 모양 임베딩은 이미지 임베딩에 조건을 부과한 확산 우선 모델을 통해 추정된다.
- 광범위한 실험을 통해, 본 연구가 상태-아트 알고리즘보다 우수한 성능을 달성하는 동시에 더 적은 훈련 데이터와 시간을 요구한다는 것을 입증한다.
- 우리의 접근법은 단일 A100 GPU에서 단 7초만에 고품질의 3D 자산 생성을 가능하게 한다.

### [Towards 3D Molecule-Text Interpretation in Language Models](https://arxiv.org/abs/2401.13923)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2401.13923.png)

Vote: 5

Authors: Yanchen Luo, Kenji Kawaguchi, Qi Tian, Sihang Li, Xiangnan He, Tat-Seng Chua, Zhiyuan Liu, Xiang Wang

- 언어 모델(LMs)은 다양한 분야에 영향을 미쳤지만 3D 분자 구조 이해에 대한 한계로 바이오분자 분야에서의 잠재력이 제한되어왔습니다.
- 이러한 격차를 해소하기 위해, 우리는 3D 분자-텍스트 해석에 집중하고 3D-MoLM(3D-Molecular Language Modeling)를 제안했습니다.
- 3D-MoLM은 언어 모델에 3D 분자 인코더를 장착하여 LM이 3D 분자를 해석하고 분석할 수 있게 합니다.
- 3D 분자-텍스트 프로젝터를 통해 3D 분자 인코더의 표현 공간과 LM의 입력 공간을 연결하는 통합이 이루어집니다.
- 또한, 3D 분자 중심의 지시 튜닝 데이터셋인 3D-MoIT를 세심하게 준비하여 3D-MoLM의 교차 모달 분자 이해 및 지시 수행 능력을 향상시켰습니다.
- 3D 분자-텍스트 정렬 및 3D 분자 중심 지시 튜닝을 통해 3D 분자 인코더와 LM의 통합을 확립하여, 특히 3D 의존적 특성에 초점을 맞춘 분자-텍스트 검색, 분자 캡션 생성, 개방형 텍스트 분자 QA 작업 등의 다운스트림 작업에서 기존 기준을 크게 뛰어넘었습니다.

### [Evaluating Frontier Models for Dangerous Capabilities](https://arxiv.org/abs/2403.13793)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13793.png)

Vote: 5

Authors: Alexandre Kaskasoli, Matthew Rahtz, Sarah Cogan, Seliem El-Sayed, David Lindner, +, Maria Abi Raad, Matthew Aitchison, Lewis Ho, Albert Webson, Victoria Krakovna, Elliot Catt, Sebastian Farquhar, Marcus Hutter, Gregoire Deletang, Anian Ruoss, Ramana Kumar, Sharon Lin, Yannis Assael, Tom Lieberum, Sarah Hodkinson, Heidi Howard, Mary Phuong

- 새로운 인공지능(AI) 시스템의 위험을 이해하기 위해선 그것이 할 수 있고 할 수 없는 것을 알아야 합니다.
- 이전 연구를 기반으로, 위험한 능력 평가를 위한 새로운 프로그램을 소개하고 Gemini 1.0 모델에 대한 시험 평가를 실시했습니다.
- 평가는 설득 및 기만, 사이버 보안, 자기 증식, 자기 추론 등 네 가지 분야를 다룹니다.
- 평가된 모델들에서 강력한 위험한 능력의 증거는 발견되지 않았지만, 초기 경고 신호를 발견했습니다.
- 우리의 목표는 미래 모델을 대비하여 위험한 능력 평가의 엄격한 과학을 발전시키는 데 도움을 주기 위함입니다.

