## Daily Papers (2024-03-29)

### [sDPO: Don't Use Your Data All at Once](https://arxiv.org/abs/2403.19270)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19270.png)

Vote: 12

Authors: Dahyun Kim, Sanghoon Kim, Hyeonwoo Kim, Chanjun Park, Wonho Song, Yungi Kim, Yunsu Kim

- 큰 언어 모델(LLM)의 개발이 진행됨에 따라 인간의 선호도와 그것을 일치시키는 것이 점점 중요해지고 있습니다.
- 우리는 직접적인 선호 최적화(DPO)로 최근 인기를 끄는 것을 확장한 단계적 DPO(sDPO)를 제안합니다.
- 이 접근법은 이용 가능한 선호 데이터셋을 나누어 단계적으로 사용함으로써, 한 번에 모든 데이터를 사용하는 것보다 향상된 방법입니다.
- sDPO는 DPO 훈련 프레임워크 내에서 보다 정밀하게 정렬된 참조 모델 사용을 용이하게 합니다.
- 또한, sDPO는 최종 모델을 더욱 높은 성능으로 훈련시켜, 더 많은 파라미터를 가진 다른 인기 있는 LLM들도 능가하도록 합니다.

### [LITA: Language Instructed Temporal-Localization Assistant](https://arxiv.org/abs/2403.19046)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19046.png)

Vote: 9

Authors: Shijia Liao, Hongxu Yin, Pavlo Molchanov, De-An Huang, Jan Kautz, Zhiding Yu, Subhashree Radhakrishnan

- 최근 다양한 모달리티를 이해할 수 있는 대규모 언어 모델(LLMs)이 비디오 입력에 확장되어 지시사항을 따르는 능력이 향상되었지만, 정확한 시간적 위치를 파악하는 능력에는 뒤처져 있습니다.
- 본 논문에서는 시간 표현(time representation), 아키텍처(architecture), 데이터(data)의 세 가지 중요한 측면에 초점을 맞추어 이러한 한계를 극복하고자 하였습니다.
- 'Language Instructed Temporal-Localization Assistant' (LITA)를 제안하여 이 문제를 해결하였는데, 이는 비디오 길이에 상대적인 타임스탬프를 인코딩하는 시간 토큰(time tokens)을 도입하여 비디오에서 시간을 보다 잘 나타내고자 하였습니다.
- 아키텍처 내에는 세밀한 시간 분해능을 갖는 'SlowFast' 토큰을 도입하여 시간 정보를 포착합니다.
- LITA는 기존에 시간적 데이터를 가진 비디오 데이터셋을 활용하고, 추론 및 시간적 위치를 파악하는 새로운 과제인 'Reasoning Temporal Localization' (RTL)과 이를 위한 'ActivityNet-RTL' 데이터셋을 제안하여 강조합니다.
- 이 새로운 모델은 RTL 과제에서 유의미한 성과를 달성하였으며, 기준 모델 대비 시간적 평균 교차-유니온(mIoU)을 거의 두 배 향상시켰습니다.
- 또한, LITA는 시간적 위치 파악에 대한 강조 덕분에, 기존의 비디오 기반 LLMs보다 비디오 기반 텍스트 생성에서 상당한 개선을 보였으며, 시간 이해(Temporal Understanding)에서 36%의 상대적 개선을 보였습니다.
- 관련 코드는 'https://github.com/NVlabs/LITA'에서 제공됩니다.

### [GaussianCube: Structuring Gaussian Splatting using Optimal Transport for 3D Generative Modeling](https://arxiv.org/abs/2403.19655)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19655.png)

Vote: 9

Authors: Feng Zhao, Baining Guo, Yansong Tang, Yiji Cheng, Bowen Zhang, Chunyu Wang, Jiaolong Yang, Dong Chen

- 3D 가우시안 스플래팅(GS)은 3D 맞춤 충실도와 렌더링 속도 측면에서 뉴럴 레디언스 필드를 크게 능가하는 성과를 달성하였으나, 흩어진 가우시안들로 이루어진 비구조적 표현은 생성 모델링에 상당한 도전을 제기한다.
- 문제를 해결하기 위해, 본 연구는 강력하고 효율적인 생성 모델링을 위한 구조화된 GS 표현인 GaussianCube를 소개한다.
- 고정된 수의 자유롭게 이동 가능한 가우시안을 사용하여 고품질의 맞춤 결과를 제공하는 수정된 밀도 제약 GS 하기 알고리즘을 제안한다.
- 이후 최적 전송(Optimal Transport)을 활용하여 사전에 정의된 복셀 그리드로 가우시안을 재배열한다.
- 구조화된 그리드 표현은 복잡한 설계 없이 표준 3D U-Net을 확산 생성 모델링의 백본으로 사용할 수 있게 한다.
- ShapeNet과 OmniObject3D에서 실시한 광범위한 실험은 우리 모델이 질적으로나 양적으로 최신 성과를 달성하였음을 보여주며, GaussianCube가 강력하고 다양하게 쓰일 수 있는 3D 표현으로써의 잠재력을 강조한다.

### [TextCraftor: Your Text Encoder Can be Image Quality Controller](https://arxiv.org/abs/2403.18978)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.18978.png)

Vote: 6

Authors: Ju Hu, Dhritiman Sagar, Jian Ren, Anil Kag, Yerlan Idelbayev, Yanzhi Wang, Yanyu Li, Sergey Tulyakov, Xian Liu

- 안정적인 확산을 기반으로 하는 텍스트-이미지 생성 모델들이 이미지 편집 및 비디오 합성과 같은 분야에서 상당한 진보를 이루었지만 여전히 입력 텍스트와 잘 맞는 이미지를 합성하는 것은 어려운 과제입니다.
- 사전 훈련된 확산 모델, 즉 UNet을 미세 조정하는 데 다양한 기술이 사용되었지만 텍스트 인코더를 미세 조정하여 텍스트-이미지 확산 모델의 성능을 향상시킬 수 있는지의 문제는 대부분 미개척 상태였습니다.
- 본 연구에서는 Stable Diffusion에 사용되는 CLIP 텍스트 인코더를 다른 대규모 언어 모델로 교체하는 대신 제안된 미세 조정 접근법, TextCraftor를 통해 향상시킬 수 있음을 발견하였으며 이는 정량적 벤치마크 및 인간 평가에서 큰 개선을 이루었습니다.
- 또한 우리의 기법은 다양한 보상으로 미세 조정된 다른 텍스트 인코더의 보간을 통해 제어 가능한 이미지 생성을 가능하게 합니다.
- TextCraftor는 UNet의 미세 조정과 직교적이며 결합하여 생성 품질을 더욱 향상시킬 수 있음을 보여줍니다.

### [Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field Representation and Generation](https://arxiv.org/abs/2403.19319)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19319.png)

Vote: 4

Authors: Matthias Nießner, Yinyu Nie, Yujin Chen, Michael Paulitsch, Reiner Birkl, Benjamin Ummenhofer, Matthias Müller

- Mesh2NeRF는 직접적인 메쉬 감독을 통해 3D 생성 작업을 위한 실제 방사형 필드를 파생시키는 접근법을 제시합니다.
- 기존의 3D 생성 방식들은 방사형 필드를 훈련을 위한 3D 장면의 대표로 사용하나, 대규모 합성 데이터셋에서 다중 시점 렌더링을 통해 추정된 방사형 필드는 종종 가려짐이나 미만적합으로 인해 아티팩트 문제가 발생합니다.
- Mesh2NeRF는 정의된 표면 두께를 가진 점유 함수로 밀도 필드를 나타내고, 메쉬와 환경 조명 모두를 고려하는 반사 함수를 통해 시점 종속 색을 결정하는 방법으로 3D 메시로부터 실제 방사형 필드를 직접 얻을 수 있는 해석적 해결책을 제안합니다.
- 이 방법은 정확한 방사형 필드를 추출하여, 생성적 NeRF 학습 및 단일 장면 표현을 위한 직접적인 감독을 제공합니다.
- Mesh2NeRF의 효율성은 여러 작업에서 검증되었으며, ABO 데이터셋에서 단일 장면 표현을 위한 시점 합성에서 PSNR에서 3.12dB 개선 효과, ShapeNet Cars의 단일 시점 조건부 생성에서 0.69 PSNR 향상, 그리고 Objaverse Mugs의 무조건적 생성에서 상당히 개선된 메시 추출 등 놀라운 결과를 달성했습니다.

