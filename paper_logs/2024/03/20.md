## Daily Papers (2024-03-20)

### [mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding](https://arxiv.org/abs/2403.12895)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12895.png)

Vote: 17

Authors: Jingren Zhou, Anwen Hu, Jiabo Ye, Fei Huang, Liang Zhang, Bo Zhang, Chen Li, Ming Yan, Qin Jin, Haiyang Xu, Ji Zhang

- 텍스트가 풍부한 이미지 문서의 의미를 이해하는 데는 구조 정보가 중요하며, 기존의 다중 모드 대형 언어 모델들은 텍스트 인식 기능은 갖추고 있지만 텍스트가 풍부한 문서 이미지의 일반 구조 이해 능력은 부족합니다.
- 이 연구에서는 시각적 문서 이해에서 구조 정보의 중요성을 강조하고, 다중 모드 대형 언어 모델의 성능을 향상시키기 위한 통합 구조 학습을 제안합니다.
- 통합 구조 학습은 문서, 웹 페이지, 표, 차트, 자연 이미지의 5개 영역에 걸쳐 구조 인식 파싱 과제와 다중 입자 텍스트 위치 확인 과제로 구성됩니다.
- 구조 정보를 더 잘 인코딩하기 위해, 레이아웃 정보를 유지하고 수평 인접 패치를 합치는 컨볼루션을 통해 시각적 특징의 길이를 줄이는 간단하면서도 효과적인 시각-텍스트 모듈 H-Reducer를 설계하였습니다.
- 공개적으로 사용 가능한 텍스트가 풍부한 이미지로부터 구조 인식 텍스트 시퀀스와 텍스트와 경계 상자의 다중 입자 쌍을 구성하여 구조 학습을 지원하는 종합적인 트레이닝 세트 DocStruct4M을 구축하였습니다.
- 마지막으로, 문서 도메인에서 상세한 설명 능력을 활성화하기 위해, 소규모이지만 고품질의 추론 튜닝 데이터 세트 DocReason25K를 구축했습니다.
- 우리의 모델 DocOwl 1.5는 10개의 시각적 문서 이해 벤치마크에서 최신 성능을 달성하여, 7B 대형 언어 모델로 10개 벤치마크 중 5개에서 기존 최고 성능을 10점 이상 향상시켰습니다.
- 우리의 코드, 모델, 데이터 세트는 https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5 에서 공개적으로 이용할 수 있습니다.

### [AnimateDiff-Lightning: Cross-Model Diffusion Distillation](https://arxiv.org/abs/2403.12706)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12706.png)

Vote: 11

Authors: Xiao Yang, Shanchuan Lin

- 본 논문에서는 번개같이 빠른 비디오 생성을 위해 AnimateDiff-Lightning 모델을 제시합니다.
- 이 모델은 진보된 적대적 확산 증류(적대적인 방법으로 확산 과정을 가속화하는 기법)를 활용하여 몇 단계의 비디오 생성에서 새로운 최고 기준을 세웁니다.
- 비디오 매체에 맞게 모델을 수정한 방법들에 대해 논의합니다.
- 게다가, 여러 기본 확산 모델들의 확률 흐름을 동시에 증류하여, 더 넓은 스타일 호환성을 가진 단일 증류된 동작 모듈을 제안합니다.
- 커뮤니티에서 사용할 수 있도록 우리가 증류한 AnimateDiff-Lightning 모델을 공개할 예정이라고 밝힙니다.

### [TnT-LLM: Text Mining at Scale with Large Language Models](https://arxiv.org/abs/2403.12173)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12173.png)

Vote: 10

Authors: Nagu Rangan, Longqi Yang, Mengting Wan, Jennifer Neville, Georg Buscher, Tara Safavi, Dhruv Joshi, Reid Andersen, Chirag Shah, Scott Counts, Yujin Kim, Sujay Kumar Jauhar, Siddharth Suri, Ryen W White

- 비구조화된 텍스트를 유용한 범주 레이블로 구성된 구조화된 형태로 전환하는 것은 텍스트 마이닝의 기본 단계이지만, 기존 방법들은 여전히 도메인 전문 지식과 수동 큐레이션에 크게 의존하는 비효율적인 점을 갖고 있습니다.
- 레이블 공간이 미확정되어 있고 대규모 데이터 주석이 없는 경우에 이러한 문제가 특히 도전적입니다.
- 본 논문은 대규모 언어 모델(LLM)의 프롬프트 기반 인터페이스를 활용하여 대규모 가짜 레이블을 유도하고 사용하는 TnT-LLM, 즉 두 단계 프레임워크를 제안하여 사람의 노력을 최소화하면서 라벨 생성 및 할당 과정을 자동화합니다.
- 첫 번째 단계에서는 LLM이 레이블 분류 체계를 반복적으로 생산하고 개선하는 것을 가능하게 하는 제로샷, 다단계 추론 접근법을 도입합니다.
- 두 번째 단계에서는 LLM을 데이터 레이블러로 사용하여 훈련 샘플을 만들고, 가벼운 지도 분류자를 믿을 수 있게 구축하여 대규모로 배치하고 서비스할 수 있습니다.
- 저자들은 개방형 도메인 기반의 검색 엔진인 Bing Copilot(이전의 Bing Chat)의 사용자 의도와 대화 도메인 분석에 TnT-LLM을 적용하였으며, 인적 및 자동 평가 지표를 사용한 광범위한 실험이 TnT-LLM이 최신 기준 모델에 비해 더 정확하고 관련성 있는 레이블 분류 체계를 생성하며, 고르게 정확도와 효율성의 균형을 이룩함을 보여줍니다.
- 또한, 실제 응용 프로그램에서 대규모 텍스트 마이닝을 위한 LLM 사용의 도전과 기회에 대한 실질적인 경험과 통찰을 공유합니다.

### [GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation](https://arxiv.org/abs/2403.12365)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12365.png)

Vote: 8

Authors: Qiangeng Xu, Wenchao Ma, Le Chen, Zhe Cao, Ulrich Neumann, Quankai Gao, Ben Mildenhall, Danhang Tang

- 3D 가우시안 스플랫팅을 이미지 또는 비디오로부터 4D 필드로 만드는 것은 그 본성상 어려운 작업입니다. 
- 이번 논문에서는 연속된 프레임 사이의 픽셀 속도와 3D 가우시안의 동적인 관계를 잇는 새로운 개념인 가우시안 플로우를 소개합니다.
- 가우시안 플로우는 이미지 공간으로 가우시안 동적을 스플래팅함으로써 효율적으로 얻을 수 있습니다.
- 이 차별화된 프로세스는 옵티컬 플로우로부터 직접적인 동적 감독을 가능하게 합니다.
- 가우시안 플로우는 특히 기존에 처리하기 어려웠던 풍부한 움직임이 있는 콘텐츠를 가진 4D 동적 콘텐츠 생성과 4D 새로운 시점 합성에 큰 이점을 제공합니다.
- 향상된 가우시안 동적으로 4D 생성에서 흔히 발생하는 색상 변형 문제도 해결할 수 있습니다.
- 광범위한 실험에서 우수한 시각적 품질로 이 방법의 효과성을 입증하였으며, 정량적 및 정성적 평가 모두에서 4D 생성 및 4D 새로운 시점 합성 작업에서 최첨단 결과를 달성했습니다. 
- 프로젝트 페이지: https://zerg-overmind.github.io/GaussianFlow.github.io/

### [LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression](https://arxiv.org/abs/2403.12968)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12968.png)

Vote: 8

Authors: Menglin Xia, Huiqiang Jiang, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Qingwei Lin, Qianhui Wu, Victor Rühle, Dongmei Zhang, Jue Zhang, Xufang Luo, Zhuoshi Pan

- 이 논문은 태스크에 독립적인 프롬프트 압축을 통해 일반화 및 효율성을 개선하는 데 중점을 두고 있다.
- 기존의 접근법은 LLaMa-7B와 같은 인과적 언어 모델에서 얻은 정보 엔트로피를 기반으로 토큰이나 어휘 단위를 제거함으로써 프롬프트를 압축하나, 이러한 정보 엔트로피가 최적의 압축 지표가 아닐 수 있음을 지적하고 있다.
- 특히, 정보 엔트로피는 단방향 컨텍스트만을 사용하며 프롬프트 압축에 필요한 모든 중요 정보를 포착하지 못할 수 있으며, 프롬프트 압축의 목표와도 일치하지 않을 수 있다.
- 이 문제를 해결하기 위해 저자들은 대형 언어 모델(LLM)로부터 지식을 추출하는 데이터 증류 절차를 제안하여 중요 정보 손실 없이 프롬프트를 압축할 수 있는 방법을 소개한다.
- 또한, 토큰 분류 문제로서 프롬프트 압축을 정의하여 압축된 프롬프트가 원본과의 충성도를 유지할 수 있도록 하고, 양방향 컨텍스트에서 필수적인 정보를 모두 포착하기 위해 Transformer 인코더 기반 아키텍처를 사용한다.
- 이 접근법은 XLM-RoBERTa-large 및 mBERT와 같은 작은 모델을 사용하여 압축 목표를 명시적으로 학습함으로써 응답 시간을 줄여준다.
- 저자들은 MeetingBank, LongBench, ZeroScrolls, GSM8K, BBH를 포함한 도메인 내외 데이터셋에서 해당 방법을 평가하고, 강력한 기준 모델들을 능가하는 중요한 성능 향상을 보여주며, 다른 LLM에 걸쳐 견고한 일반화 능력을 시연했다.
- 여기에 더해, 이 모델은 기존의 프롬프트 압축 방법보다 3배에서 6배 가까이 빠르며, 2배에서 5배의 압축 비율을 가지면서 종단간 응답 시간을 1.6배에서 2.9배 가속화한다.

### [Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs](https://arxiv.org/abs/2403.12596)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12596.png)

Vote: 8

Authors: Hassan Mansoor, Abhanshu Sharma, Victor Carbune, Jindong Chen, Fangyu Liu, Rahul Aralikatte, Gilles Baechler

- 비전-언어 모델(VLMs)이 멀티모달 작업에서 강력한 성능을 달성하고 있지만, 특히 작은 VLMs의 추론 능력은 제한적인 반면, 대형 언어 모델(LLMs)의 능력은 계속 향상되고 있습니다.
- 이 논문에서는 LLMs의 기능을 VLMs로 전달하는 기법을 제안하여 차트 기반 QA 작업에서 최신 성능을 달성합니다.
- 차트 표현을 개선하기 위해 차트에서 표로의 번역 작업을 개선된 버전으로 사전 훈련 단계를 지속하고, 20배 큰 데이터셋을 구축하여 일반 추론 능력을 향상시킵니다.
- 차트의 표현 형식을 사용하여 추론 트레이스를 합성함으로써, 수치 연산을 향상시킬 수 있습니다.
- 모델은 멀티태스크 손실 함수를 사용하여 미세 조정되며, 이 손실 함수는 Hsieh 등에 의해 2023년에 소개되었습니다.
- 제안된 모델인 ChartPaLI-5B는 상류 OCR 시스템을 사용하지 않고도 기존의 PaLIX-55B 모델보다 우수한 성능을 제공하며, PaLI3-5B 기준 모델에 비해 추론 시간은 변하지 않습니다.
- Chen 등이 2023년에 소개한 간단한 사고 프로그램 프롬프트를 사용하여 근거를 더 다듬으면, 최신 모델인 Gemini Ultra와 GPT-4V보다 더 우수한 성능을 낼 수 있습니다.

### [Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models](https://arxiv.org/abs/2403.12881)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12881.png)

Vote: 7

Authors: Jiangning Liu, Zehui Chen, Kai Chen, Wenwei Zhang, Dahua Lin, Feng Zhao, Kuikun Liu, Qiuchen Wang

- 공개된 대규모 언어 모델(Large Language Models, LLMs)은 다양한 자연어 처리(NLP) 작업에서 큰 성공을 거두었지만, 에이전트로 활동할 때는 API 기반 모델에 비해 여전히 열등합니다.
- 이 논문은 에이전트 능력을 일반 LLM에 통합하는 것이 절실하고 중요한 문제임을 제기하고 있습니다.
- 첫째, 현재 에이전트 훈련 코퍼스는 형식을 따르고 에이전트 추론을 혼합하고 있어서 그 분포가 사전 훈련 데이터의 분포와 상당히 달라진다는 점을 관찰했습니다.
- 둘째, LLM들은 에이전트 작업에 요구되는 능력을 다르게 학습하는 속도를 보였습니다.
- 셋째, 현재의 접근법은 환상(hallucinations)을 도입함으로써 에이전트 능력을 향상시키지만 부작용이 있다는 점을 발견했습니다.
- 이러한 발견을 바탕으로, 에이전트 훈련을 위한 언어 모델을 효과적으로 미세 조정하기 위한 Agent-FLAN을 제안합니다.
- 훈련 코퍼스를 신중하게 분해하고 재설계함으로써, Agent-FLAN은 Llama2-7B를 여러 에이전트 평가 데이터셋에서 이전의 최고 연구보다 3.5% 향상시켰습니다.
- 체계적으로 구축된 부정적인 샘플들을 통해, Agent-FLAN은 확립된 평가 벤치마크를 기반으로 환상 문제를 크게 완화합니다.
- 또한, 모델의 크기가 커짐에 따라 LLM의 일반 능력을 약간 개선하면서 LLM의 에이전트 능력을 일관되게 향상시킵니다.
- 관련 코드는 https://github.com/InternLM/Agent-FLAN 에서 확인할 수 있습니다.

### [Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers](https://arxiv.org/abs/2403.12943)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12943.png)

Vote: 6

Authors: Christine Chan, Pannag R Sanketi, Nikhil J Joshi, Quan Vuong, Igor Gilitschenski, Vidhi Jain, Ayzaan Wahid, Danny Driess, Maria Attarian, Stefan Welker, Pierre Sermanet, Yonatan Bisk, Debidatta Dwibedi

- 이 연구는 로봇이 인간을 관찰함으로써 직접 과제를 추론할 수 있는지에 대한 접근 방식을 탐구하며, 이를 위해 Vid2Robot이라는 새로운 비디오 기반 학습 프레임워크를 소개합니다.
- Vid2Robot은 조작 과제를 수행하는 비디오 데모와 현재의 시각적 관찰을 기반으로 로봇 동작을 직접 생성합니다.
- 대규모의 인간 비디오 및 로봇 궤적 데이터셋에 훈련된 통합 표현 모델을 사용하여 비디오의 프롬프트 기능을 로봇의 현재 상태와 결합하고 관찰된 과제를 모방하는 적절한 동작을 생성합니다.
- 비디오 조건부 정책에 비해 20% 개선된 성능을 실제 로봇에서 입증하였으며, 인간과 로봇 비디오 표현 간의 정렬을 강화하는 보조 대조 손실을 제안하여 정책 성능을 더욱 향상시킵니다.
- Vid2Robot은 한 객체에서 다른 객체로 관찰된 동작을 성공적으로 전달하고 장기적인 구성 등과 같은 실세계 응용에 대한 잠재력을 보여주는 신흥 기능을 보여 줍니다.
- 프로젝트 웹사이트는 vid2robot.github.io 에서 확인할 수 있습니다.

### [ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware Diffusion Guidance](https://arxiv.org/abs/2403.12409)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12409.png)

Vote: 6

Authors: Yongwei Chen, Kui Jia, Tong Wu, Tengfei Wang, Xingang Pan, Ziwei Liu

- 'ComboVerse'는 특정한 이미지로부터 고품질의 다중 객체가 포함된 3D 자산을 생성하는 새로운 프레임워크입니다.
- 기존의 피드포워드 단일 이미지 3D 생성 모델은 단일 객체 생성에는 유망한 결과를 보였으나 복잡한 3D 자산 모델링에는 한계가 있었습니다.
- 본 연구는 모델과 데이터 관점에서 이러한 '다중 객체 갭'에 대한 심층 분석을 수행하였습니다.
- 개별 객체의 3D 모델을 복원한 후, 주어진 이미지에 맞게 객체의 크기, 회전 각도, 위치를 조정하여 3D 자산을 생성합니다.
- 이 과정을 자동화하기 위해 사전 훈련된 확산 모델로부터 공간 인식 스코어 증류 샘플링(SSDS)을 적용하여 객체 배치를 안내합니다.
- 제안된 프레임워크는 표준 스코어 증류 샘플링에 비해 객체의 공간 정렬을 강조하여 보다 정확한 결과를 달성합니다.
- 광범위한 실험을 통해 ComboVerse가 복합 3D 자산 생성에서 기존 방법들보다 명확한 개선을 달성했음을 검증했습니다.

### [FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis](https://arxiv.org/abs/2403.12963)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12963.png)

Vote: 5

Authors: Hongsheng Li, Si Liu, Linjiang Huang, Guanglu Song, Aiping Zhang, Yu Liu, Rongyao Fang

- 이 연구에서는 학습된 해상도를 넘어서 적용될 때 나타나는 반복적 패턴과 구조적 왜곡과 같은 문제를 해결하고자 선행 학습된 확산 모델(Diffusion Model)을 이용하여 고해상도 이미지를 생성하는 것에 초점을 맞추었습니다.
- 고해상도 이미지 생성 시 구조적 일관성과 해상도 간의 규모 일관성을 각각 달성하기 위하여, 주파수 도메인 분석 관점에서 혁신적인 훈련이 필요 없는 접근 방식인 FouriScale을 소개합니다.
- 기존의 합성곱 계층을 대체하여 확장 기법과 저주파 통과 연산을 결합함으로써, 다양한 종횡비의 텍스트-이미지 생성을 유연하게 처리할 수 있는 패딩-후-자르기 전략을 사용합니다.
- FouriScale을 유도로 사용하며, 생성된 이미지의 구조적 무결성과 충실도를 성공적으로 균형있게 유지하여 임의의 크기, 고해상도, 고품질 생성 능력을 경이롭게 달성했습니다.
- 본 방법은 단순성과 호환성을 가지고 있으며, 초고해상도 이미지 합성 연구에 중요한 통찰력을 제공할 수 있습니다.
- 관련 코드는 https://github.com/LeonHLJ/FouriScale 에서 공개될 예정입니다.

### [GVGEN: Text-to-3D Generation with Volumetric Representation](https://arxiv.org/abs/2403.12957)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12957.png)

Vote: 4

Authors: Tong He, Wanli Ouyang, Yangguang Li, Xianglong He, Di Huang, Xiaoshui Huang, Junyi Chen, Sida Peng, Chun Yuan

- 본 논문에서는 텍스트 입력으로부터 3D 가우시안 표현을 효과적으로 생성하기 위한 새로운 확산 기반 프레임워크인 GVGEN을 소개한다.
- 무질서한 3D 가우시안 점을 구조화된 형태인 GaussianVolume으로 정렬하는 '구조화된 볼륨 표현(Structured Volumetric Representation)' 기술을 제안한다.
- 볼륨 상세 정보를 최적화하기 위해 세부 정보를 선택적으로 최적화함으로써 세부 피델리티를 향상시키는 'Candidate Pool Strategy'라는 독창적인 가지치기 및 밀집화 방법을 제안한다.
- GaussianVolume 생성을 단순화하고 모델이 자세한 3D 기하학적 인스턴스를 생성할 수 있도록 하기 위해 초기 기본 지오메트릭 구조를 구축한 다음 완전한 가우시안 속성을 예측하는 '단계별 생성 파이프라인(Coarse-to-fine Generation Pipeline)'을 제안한다.
- GVGEN는 기존의 3D 생성 방법들과 비교하여 질적, 양적 평가 모두에서 우수한 성능을 보여주며, 동시에 빠른 생성 속도(약 7초)를 유지하여 품질과 효율성 사이의 균형을 효과적으로 달성한다.

### [FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation](https://arxiv.org/abs/2403.12962)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12962.png)

Vote: 3

Authors: Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy

- 텍스트-이미지 확산 모델의 뛰어난 효과에 힘입어 비디오 분야에서의 잠재적 응용 탐구가 활발히 이루어지고 있다.
- 제로샷 방법은 모델 훈련을 필요로 하지 않으면서 이미지 확산 모델을 비디오로 확장하고자 한다.
- 최근 방법들은 주로 프레임 간 상관관계를 주의 기제(attention mechanisms)에 포함시키는 데 집중하고 있지만, 때때로 이러한 부드러운 제약이 시간적 일관성을 유지하는 데 충분하지 않을 수 있다.
- 본 논문에서는 FRESCO라는 프레임 내 상관관계를 프레임 간 상관관계와 함께 소개하여 더 강력한 공간-시간 제약을 설정함으로써 프레임 간 시맨틱 유사 콘텐츠의 일관된 변환을 보장한다.
- 단순한 주의 유도를 넘어서, 우리의 접근 방식은 입력 비디오와 높은 공간-시간 일관성을 달성하기 위해 명시적인 특성 업데이트를 포함한다.
- 실시한 광범위한 실험을 통해 우리가 제안한 프레임워크가 기존의 제로샷 방법보다 개선되어 고품질이고 일관된 비디오를 생산하는 효과를 입증한다.

### [TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation](https://arxiv.org/abs/2403.12906)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12906.png)

Vote: 2

Authors: Jiangning Zhang, Junwei Zhu, Weijian Cao, Chengjie Wang, Junshu Tang, Dongjin Huang, Shijie Zhang, Yunsheng Wu, Yufei Liu

- 3D 인간의 텍스쳐링은 양질의 UV 맵을 얻기 어렵기 때문에 여전히 도전 과제로 남아 있습니다.
- 최근의 텍스트-투-3D 발전에도 불구하고, 다중 시점 렌더링을 대규모 텍스트-투-이미지 모델을 사용하여 감독하는 것이 일반적이지만, 생성 속도, 텍스트 일관성, 텍스처 품질 문제로 데이터가 부족합니다.
- 저희는 TexDreamer, 최초의 제로샷 멀티모달 고해상도 3D 인간 텍스처 생성 모델을 제시합니다.
- 큰 T2I 모델을 의미 있는 UV 구조에 맞게 조율하면서 원래의 일반화 기능을 유지하도록 효율적인 텍스처 적응 미세조정 전략을 활용합니다.
- 새로운 특징 변환 모듈을 이용하여, 훈련된 모델은 몇 초 내에 텍스트나 이미지에서 고해상도 3D 인간 텍스처를 생성할 수 있습니다.
- 또한, 저희는 ArTicuLated humAn textureS (ATLAS)을 소개합니다. 이는 50k 고해상도(1024 X 1024) 텍스처와 텍스트 설명을 포함하는 가장 큰 3D 인간 텍스처 데이터셋입니다.

