## Daily Papers (2024-03-28)

### [ViTAR: Vision Transformer with Any Resolution](https://arxiv.org/abs/2403.18361)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.18361.png)

Vote: 26

Authors: Ran He, Yunzhe Tao, Yongfei Liu, Qihang Fan, Hongxia Yang, Huaibo Huang, Quanzeng You, Xiaotian Han

- 이 논문은 Vision Transformers (ViTs)가 다양한 이미지 해상도 간의 확장성에 대한 제한적인 문제를 해결한다.
- 기존의 ViTs는 훈련 중 보던 해상도와 다를 때 성능이 저하되는 경향이 있다.
- 우리는 하나의 Transformer block으로 설계된 동적 해상도 조정을 위한 새로운 모듈을 제안한다, 이는 효율적인 점진적 토큰 통합을 달성하기 위함이다.
- 또한, 다중 해상도에서 일관된 위치 인식을 제공하고 단일 훈련 해상도에 과적합되는 것을 방지하는 퍼지(fuzzy) 위치 인코딩을 Vision Transformer에 도입한다.
- ViTAR 모델은 1120x1120 해상도에서 83.3%의 top-1 정확도, 4032x4032 해상도에서는 80.4%의 정확도를 달성하면서 계산 비용을 줄이는 인상적인 적응성을 보여준다.
- 인스턴스 및 시맨틱 세분화(Semantic Segmentation)와 같은 다운스트림 작업에서도 뛰어난 성능을 나타내며, Masked AutoEncoder와 같은 자기지도 학습 기법과 쉽게 결합될 수 있다.
- 우리의 작업은 Vision Transformers의 해상도 확장성을 향상시키는 비용 효율적인 솔루션을 제공하며, 더 다재다능하고 효율적인 고해상도 이미지 처리 분야로의 길을 열어준다.

### [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/abs/2403.18814)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.18814.png)

Vote: 25

Authors: Shaoteng Liu, Yanwei Li, Jiaya Jia, Yixin Chen, Zhisheng Zhong, Yuechen Zhang, Chengyao Wang, Ruihang Chu

- 이 연구에서는 기존 시각 언어 모델(VLMs)을 보완하여 복잡한 시각 대화와 추론 능력을 강화하는 간단하고 효과적인 프레임워크인 Mini-Gemini를 소개합니다.
- GPT-4와 Gemini와 같은 고급 모델과의 성능 격차를 좁히기 위해 고해상도 시각 토큰, 고품질 데이터 및 VLM 가이드 생성의 세 가지 측면에서 VLM의 잠재력을 발굴합니다.
- 고해상도 시각 토큰 강화를 위해 시각 토큰 수를 증가시키지 않고 추가적인 시각 인코더를 사용하여 고해상도 정제를 제안합니다.
- 또한, 현재 VLM의 운영 범위를 확장하는 이미지 이해와 추론 기반 생성을 촉진하는 고품질 데이터셋을 구축합니다.
- Mini-Gemini는 시각 이해, 추론 및 생성을 동시에 강화하면서 VLM의 잠재력을 추가로 채굴하고 기존 프레임워크를 지원합니다.
- Mini-Gemini는 2B부터 34B에 이르는 다양한 밀집 및 MoE 대규모 언어 모델(LLMs)을 지원하며, 여러 제로샷 벤치마크에서 선도적인 성능을 달성하고 개발된 프라이빗 모델을 능가하는 것으로 입증되었습니다.
- 코드와 모델은 https://github.com/dvlab-research/MiniGemini에서 제공됩니다.

### [Long-form factuality in large language models](https://arxiv.org/abs/2403.18802)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.18802.png)

Vote: 14

Authors: Da Huang, Quoc V. Le, Yifeng Lu, Ruibo Liu, Jerry Wei, Xinying Song, Cosmo Du, Nathan Hu, Chengrun Yang, Dustin Tran, Daiyi Peng

- 대형 언어 모델(LLMs)은 종종 사실을 찾기 위한 프롬프트에 대응할 때, 오류를 포함한 내용을 생성합니다.
- 이 연구에서는 GPT-4을 사용하여 38개 주제에 걸친 수천 개의 질문을 포함하는 LongFact 프롬프트 집합을 생성하여 모델의 장문 형식 사실성을 벤치마킹합니다.
- 연구팀은 장문 응답 중 각 사실의 정확성을 평가하기 위해 구글 검색에 검색 쿼리를 보내는 다단계 추론 과정을 포함하는 방법, 즉 Search-Augmented Factuality Evaluator (SAFE)라고 하는 LLM 에이전트 사용을 제안합니다.
- 또한, 사용자가 선호하는 응답 길이를 나타내는 하이퍼파라미터에 대한 제공된 사실의 비율(재현율)과 응답 내 지원되는 사실의 비율(정밀도)을 균형잡기 위해 F1 점수를 장문 형식 사실성에 대한 종합 측정 기준으로 확장할 것을 제안합니다.
- 실험적으로, LLM 에이전트는 인간 평가자보다 뛰어난 평가 성능을 달성할 수 있으며 SAFE는 약 16k 개의 개별 사실에 대해 인간 어노테이터와 72% 일치하며, 무작위로 선정된 100건의 의견 불일치 사례 중 76%에서 SAFE가 우월합니다.
- SAFE는 인간 어노테이터보다 20배 이상 저렴한 비용으로 운영됩니다.
- 13개의 다양한 언어 모델들(Gemini, GPT, Claude, 및 PaLM-2 모델 가족 포함)을 LongFact에서 벤치마킹한 결과, 대체로 대형 언어 모델들이 더 나은 장문 형식 사실성을 달성하는 것으로 나타났습니다.
- LongFact, SAFE 및 모든 실험 코드는 https://github.com/google-deepmind/long-form-factuality 에서 이용 가능합니다.

### [ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion](https://arxiv.org/abs/2403.18818)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.18818.png)

Vote: 12

Authors: Daniel Winter, Shlomi Fruchter, Matan Cohen, Yael Pritch, Yedid Hoshen, Alex Rav-Acha

- 확산 모델을 이용한 이미지 편집은 혁명적이었으나 물리적 법칙을 위반하는 결과를 발생시키곤 했는데, 예를 들어 사물이 주변 장면에 미치는 영향인 가려짐, 그림자, 반사 등을 처리하는 데 어려움이 있었습니다.
- 기존 자기 지도(self-supervised) 방법론의 한계를 분석하고, 반사실적 데이터셋을 중심으로 한 실용적 해결책을 제시합니다.
- 우리의 방법은 사물을 제거하기 전후의 장면을 촬영함으로써, 사물의 제거뿐만 아니라 그 사물이 주변 장면에 미치는 영향도 제거할 수 있도록 확산 모델을 세밀하게 조정하는 것을 포함하고 있습니다.
- 그러나 사실적인 객체 삽입을 위해 이 방법을 적용하는 것은 실제로 매우 큰 데이터셋이 필요하며, 이는 비현실적입니다.
- 이 같은 도전을 해결하기 위해, 작은 반사실적 데이터셋에 기반한 객체 제거 모델을 통해, 인위적으로 데이터셋을 크게 확장하는 부트스트랩 감독(bootstrap supervision) 방법을 제안합니다.
- 우리의 접근 방식은 특히 사물의 장면 효과 모델링에 있어 이전 방법들보다 사실적인 객체 제거 및 삽입 측면에서 상당한 성능 향상을 보여줍니다.

### [BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text](https://arxiv.org/abs/2403.18421)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.18421.png)

Vote: 11

Authors: Betty Xiong, Christopher D. Manning, Michael Carbin, David Hall, Elliot Bolton, Abhinav Venigalla, Tony Lee, Percy Liang, Jonathan Frankle, Michihiro Yasunaga, Roxana Daneshjou

- GPT-4 및 Med-PaLM 2와 같은 모델들이 생물의학 NLP 작업에서 뛰어난 성능을 보여왔으나, 이러한 모델들은 수백억 개의 매개변수를 가지고 있고, 실행하는 데에 많은 계산 비용이 들며, 사용자의 데이터를 인터넷으로 전송해야 하고, 알려지지 않은 데이터 소스에서 훈련됩니다.
- 이러한 문제를 해결하기 위해, 우리는 BioMedLM이라는 27억 개의 매개변수를 가진 GPT 스타일의 자동 회귀 언어 모델을 PubMed 초록과 전문을 사용하여 훈련하였습니다.
- 세밀한 조정을 거친 후, BioMedLM은 MedMCQA (개발)에서 57.3%, MMLU Medical Genetics 시험에서 69.0%의 점수를 달성하는 등, 훨씬 큰 모델들과 경쟁할 수 있는 강력한 생물의학적 다중선택형 질문-응답 결과를 생성할 수 있습니다.
- 또한, BioMedLM은 의료 주제에 대한 환자 질문에 유용한 답변을 제공하기 위해 세밀하게 조정될 수 있습니다.
- 이는 더 작은 모델들이 특정 NLP 응용 분야에서 투명성, 개인정보 보호, 경제성 및 환경 친화성의 기반으로 작용할 수 있다는 것을 보여줍니다.
- 이 모델은 Hugging Face Hub(https://huggingface.co/stanford-crfm/BioMedLM)에서 사용할 수 있습니다.

### [Garment3DGen: 3D Garment Stylization and Texture Generation](https://arxiv.org/abs/2403.18816)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.18816.png)

Vote: 9

Authors: Rakesh Ranjan, Tuur Stuyck, Jovan Popovic, Nikolaos Sarafianos, Yilei Li, Xiaoyu Xiang

- Garment3DGen은 단일 입력 이미지를 바탕으로 하여 3D 의류 자산을 합성하는 새로운 방법을 소개합니다.
- 이 방법은 실제 및 합성 이미지를 모두 기반으로 하여 3D 질감 의류를 생성할 수 있으며, 이는 텍스트 프롬프트에 의해 생성된 이미지를 포함합니다.
- 생성된 3D 의류는 인간의 신체에 즉시 드레이핑되고 시뮬레이션될 수 있습니다.
- Image to 3D diffusion 방법의 최근 진보를 활용하여 3D 의류 기하학적 형상을 생성하고, 이러한 기하학적 형상을 가상의 지상 진실로 사용하여 기본 템플릿 메시를 목표 3D 형상에 맞도록 변형하는 메시 변형 최적화 절차를 제안합니다.
- 입력 기본 메시가 원하는 목표물에 자유롭게 변형되면서도 메시 품질과 위상을 유지하여 시뮬레이션할 수 있도록 설계된 손실함수를 도입합니다.
- 마지막으로, 텍스처 추정 모듈은 입력 가이드를 정확하게 포착하여 3D 자산을 생성할 수 있는 전역적이고 지역적으로 일관성 있는 고해상도 텍스처 맵을 생성합니다.
- Garment3DGen을 사용하면 아티스트의 개입 없이 원하는 질감의 3D 의류를 생성할 수 있으며, 시뮬레이션 준비가 된 3D 자산을 생성하기 위해 의류에 대한 텍스트 프롬프트를 제공할 수 있습니다.
- 다양한 실제 및 생성된 자산에 대한 많은 양적 및 질적 비교를 제시하고, 시뮬레이션 준비가 된 3D 의류를 생성하는 방법에 대한 사례를 제공합니다.

### [Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction](https://arxiv.org/abs/2403.18795)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.18795.png)

Vote: 7

Authors: Zike Wu, Pan Zhou, Xuanyu Yi, Hanwang Zhang, Qiuhong Shen, Xinchao Wang, Shuicheng Yan

- 단일 이미지로부터 효율적인 3D 자산을 재구축하는 문제에 대해, 자동화된 3D 콘텐츠 제작 파이프라인 수요가 증가함에 따라 이를 해결하기 위해 노력하고 있습니다.
- 이전 방법들은 주로 Score Distillation Sampling (SDS)과 Neural Radiance Fields (NeRF)에 의존했으나, 이러한 접근법들은 긴 최적화 시간과 상당한 메모리 사용으로 인해 실제적인 한계를 겪고 있습니다.
- 본 보고서에서는, (1) 효율적인 3D Gaussian 스플래팅 과정을 위해 대규모 3D Gaussian을 활용하는 3D 표현, (2) 문맥에 따른 추론을 촉진하고 시퀀스 길이와 비례하는 선형적 확장성을 갖춘 Mamba 기반 순차 네트워크 설계를 강조한, 단일 시점 이미지로부터의 종단간(end-to-end) 3D 재구성 모델인 Gamba를 소개합니다.
- Gamba는 데이터 전처리, 정규화 설계 및 교육 방법론에 있어서 중요한 발전을 포함하고 있으며, 실제 스캔된 OmniObject3D 데이터셋을 사용하여 현존하는 최적화 기반 및 피드포워드(feed-forward) 3D 생성 접근법과 비교되었습니다.
- 여기서 Gamba는 경쟁력 있는 생성 능력을 질적 및 양적으로 보여주며, 특히 단일 NVIDIA A100 GPU에서 약 0.6초라는 눈에 띄는 속도를 달성하였습니다.

### [EgoLifter: Open-world 3D Segmentation for Egocentric Perception](https://arxiv.org/abs/2403.18118)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.18118.png)

Vote: 5

Authors: Qiao Gu, Duncan Frost, Simon Green, Chris Sweeney, Julian Straub, Zhaoyang Lv

- 이 논문에서는 착용형 센서에서 캡처된 장면을 개별 3D 객체로 완전히 분해하여 자동으로 분할할 수 있는 새로운 시스템인 EgoLifter를 소개합니다.
- EgoLifter는 자연스러운(비스캐닝) 모션에서 수백 개의 객체를 캡처하는 이코센트릭 데이터를 위해 특별히 설계되었습니다.
- 3D 장면과 객체의 기본 표현으로 3D 가우시안을 채택하고, 특정 객체 분류 체계에 구애받지 않는 객체 인스턴스의 유연하고 즉각적인 정의를 학습하기 위해 Segment Anything Model (SAM)의 분할 마스크를 약한 감독으로 사용합니다.
- 이코센트릭 비디오에서 동적 객체의 문제를 다루기 위해, 동적 객체를 3D 재구성에서 필터링하는 방법을 학습하는 변화 예측 모듈을 설계했습니다.
- 결과적으로, EgoLifter는 전체 장면을 구성하는 3D 가우시안의 모음으로서 3D 객체 인스턴스를 재구성할 수 있는 완전 자동 파이프라인입니다.
- 새로운 벤치마크, 즉 Aria Digital Twin 데이터셋을 생성하여 자연스러운 이코센트릭 입력에 대한 오픈월드 3D 분할에서의 최첨단 성능을 정량적으로 입증했습니다.
- EgoLifter는 다양한 이코센트릭 활동 데이터셋에 적용되었으며, 이는 대규모 3D 이코센트릭 인식을 위한 방법의 잠재력을 보여줍니다.

### [FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image Editing](https://arxiv.org/abs/2403.18605)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.18605.png)

Vote: 5

Authors: Cuong Pham, Trong-Tung Nguyen, Anh Tran, Duc-Anh Nguyen

- 기존 객체 중심 편집 문제에서 발생하는 비현실적인 결과와 객체 교체 또는 삽입 시 제한된 제어 문제를 해결하기 위해 개발된 FlexEdit에 대해 소개합니다.
- FlexEdit는 테스트 시간에 지정된 객체 제약 조건과 일치하도록 레이턴트를 최적화하는 반복적인 조정 방식을 사용합니다.
- 우리의 프레임워크는 디노이징 과정에서 자동으로 추출된 적응형 마스크를 사용하여 새로운 콘텐츠를 타겟 이미지에 매끄럽게 통합하면서 배경을 보호합니다.
- 다양한 객체 편집 작업에서 FlexEdit의 다재다능성을 보여주고, 실제 및 합성 이미지 샘플을 포함한 평가 테스트 스위트를 만들며 객체 중심 편집을 위한 새로운 평가 메트릭을 설계합니다.
- 다양한 편집 시나리오에서 광범위한 실험을 수행하고, 최근 고급 텍스트 가이드 이미지 편집 방법보다 우리의 편집 프레임워크의 우수성을 입증합니다.
- 프로젝트 페이지는 https://flex-edit.github.io/ 에서 확인할 수 있습니다.

### [Towards a World-English Language Model for On-Device Virtual Assistants](https://arxiv.org/abs/2403.18783)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.18783.png)

Vote: 4

Authors: Youssef Oualil, Lyan Verwimp, Arturo Argueta, Rricha Jalota, Markus Nussbaum-Thom, Amr Mousa

- 가상 어시스턴트(Virtual Assistants, VAs)용 신경망 언어 모델(Neural Network Language Models, NNLMs)은 일반적으로 언어, 지역 또는 경우에 따라 기기에 따라 다르므로, 이를 확장하고 유지 관리하기 위한 노력이 늘어난다.
- 본 논문에서는 모바일 기기용 가상 어시스턴트를 위한 '월드 잉글리시(World English)' NNLM을 구축하기 위해 영어의 지역 변종을 결합하는 방법을 탐구한다.
- 특히, 생산 중인 NNLM에서 방언 특유의 특징을 모델링하기 위해 어댑터 병목현상(adapter bottlenecks)의 적용을 조사하고, 다양한 방언 기준을 강화하는 방법을 탐색한다.
- 어댑터 모듈이 전체 서브네트워크를 특화하는 것보다 방언을 모델링하는데 더 효과적임을 발견한다.
- 이러한 통찰을 바탕으로 하여, 단일 방언 모델의 정확성, 지연 시간 및 메모리 제약 조건을 충족시키는 새로운 월드 잉글리시 NNLM 구조를 제안한다.

