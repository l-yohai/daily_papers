## Daily Papers (2024-02-08)

### [Grandmaster-Level Chess Without Search](https://arxiv.org/abs/2402.04494)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ZvInJdCi1cLqCTsLal-xl.png)

Vote: 35

Authors: Grégoire Delétang, Elliot Catt, Jordi Grau-Moya, John Reid, Sourabh Medapati, Li Kevin Wenliang, Anian Ruoss, Tim Genewein

- 이 연구는 대규모 주의(Attention)-기반 구조와 데이터셋을 통해 스케일의 영향을 체스에 적용하여 조사하였다.
- 복잡한 휴리스틱이나 명시적 탐색 없이, 2억 7천만 개의 파라미터를 가진 트랜스포머 모델을 학습시켜 체스의 실력을 향상시켰다.
- 1천만 개의 체스 게임 데이터셋과 강력한 Stockfish 16 엔진으로 제공된 액션-값을 이용해 학습시켰으며, 대략 150억 개의 데이터 포인트를 생성하였다.
- 대규모 모델은 Lichess 블리츠 Elo에서 2895점을 달성하고, 도메인 특정 트윽이나 명시적 탐색 알고리즘 없이 복잡한 체스 퍼즐을 풀었다.
- 또한, AlphaZero의 정책 및 가치 네트워크(몬테카를로 트리 탐색 없이) 및 GPT-3.5-터보-인스트럭트보다 우수한 성능을 보임을 확인하였다.
- 모델과 데이터셋 크기에 대한 체계적인 조사를 통해 강력한 체스 성능은 충분한 스케일에서만 나타난다는 것을 발견하였다.
- 설계 선택 및 하이퍼파라미터의 다양한 변형 실험을 통해 본 연구결과의 타당성을 검증하였다.

### [BiLLM: Pushing the Limit of Post-Training Quantization for LLMs](https://arxiv.org/abs/2402.04291)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SVbBf7vhT-Dh266xo-B2J.png)

Vote: 21

Authors: Ying Li, Wei Huang, Yangdong Liu, Michele Magno, Shiming Zhang, Xiaojuan Qi, Haotong Qin, Xianglong Liu

- 사전 훈련된 대규모 언어 모델(LLMs)은 상당한 메모리와 계산 자원을 요구하나, 비너리제이션을 통한 가중치 1비트 압축으로 이를 감소시킬 수 있다.
- 기존의 양자화 기법들은 초저비트 폭에서 LLM의 성능을 유지하는데 한계가 있으나, BiLLM은 사전 훈련된 LLM들이 맞춤화된 1비트 사후 훈련 양자화 방식을 제시한다.
- LLMs의 가중치 분포를 기반으로, BiLLM은 중요 가중치를 구조적으로 선별하고 이진 잔여 근사 전략을 통해 압축 손실을 최소화한다.
- 비중요 가중치의 종 모양 분포를 고려하여 정확한 그룹화와 이진화를 위한 최적 분할 탐색을 제안한다.
- BiLLM은 다양한 LLM 가족 및 평가 메트릭을 통해 1.08비트 가중치로 고품질 추론(예: LLaMA2-70B에서 8.41 perplexity)을 달성하여 최신 양자화 방법을 크게 뛰어넘는다.
- 또한 BiLLM은 단일 GPU에서 7억 가중치가 있는 LLM을 0.5시간 이내에 이진화할 수 있어 시간 효율성이 뛰어남을 입증한다.

### [Direct Language Model Alignment from Online AI Feedback](https://arxiv.org/abs/2402.04792)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/CZwOWDgHGpSRJpyM60e7T.png)

Vote: 15

Authors: Misha Khalman, Tianqi Liu, Mathieu Blondel, Alexandre Rame, Thomas Mesnard, Tianlin Liu, Johan Ferret, Felipe Llinares, Shangmin Guo, Yao Zhao, Biao Zhang, Bilal Piot

- 선호도 기반 직접 정렬(DAP) 방법, 예를 들어 DPO는 별도의 보상 모델이 필요하지 않은 인간 피드백으로부터의 강화 학습(RLHF)에 비해 효율적인 대안으로 최근 등장하였습니다.
- 그러나 DAP 방법에서 사용되는 선호도 데이터셋은 주로 훈련 전에 수집되며 업데이트되지 않아 오프라인 피드백에 국한됩니다.
- 또한, 이러한 데이터셋들의 응답은 종종 정렬되고자 하는 모델과 다른 언어 모델에서 샘플링되며, 모델이 훈련 과정에서 발전함에 따라 정렬 단계는 필연적으로 오프-폴리시가 됩니다.
- 이 연구에서는, 온라인 피드백이 DAP 방법을 개선하는 데에 중요하다고 주장하며, 우리의 방법인 온라인 AI 피드백(OAIF)을 제시합니다.
- OAIF는 언어 모델을 주석자로 사용하여, 각 훈련 반복마다 현재 모델에서 두 개의 응답을 샘플링하고 주석자 LLM에게 선호하는 응답을 선택하게 함으로써 온라인 피드백을 제공합니다.
- 단순함에도 불구하고, 여러 작업에서의 인간 평가를 통해 OAIF가 오프라인 DAP 및 RLHF 방법보다 뛰어난 성능을 보임을 보여줍니다.
- OAIF에서 활용되는 피드백은 주석자 LLM에게 지시하는 프롬프트를 통해 쉽게 제어 가능함을 더 보여줍니다.

### [ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation](https://arxiv.org/abs/2402.04324)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0gUN7U2teA_vu_BywhOEX.png)

Vote: 14

Authors: Cong Wei, Ge Zhang, Stephen Huang, Harry Yang, Wenhu Chen, Weiming Ren, Xinrun Du

- 이미지-비디오(I2V) 생성은 초기 프레임과 텍스트 프롬프트를 사용하여 비디오 시퀀스를 만드는 것을 목표로 합니다.
- I2V 생성의 주된 도전 과제는 비디오 전반에 걸쳐 시각적 일관성을 유지하는 것이며, 기존 방법들은 첫 프레임의 주체, 배경 및 스타일의 일관성과 비디오 내러티브의 유동적이고 논리적인 진행을 보존하는데 종종 어려움을 겪습니다.
- 이러한 문제를 완화하기 위해, 우리는 시각적 일관성을 강화하기 위한 확산 기반 방법인 ConsistI2V를 제안합니다.
- 특히, 우리는 첫 프레임에 대한 시공간적 주의 집중을 통해 공간 및 동작 일관성을 유지하고, 첫 프레임의 저주파 대역에서의 노이즈 초기화로 레이아웃 일관성을 강화하는 두 가지 접근 방식을 소개합니다.
- 이러한 접근 방식을 통해 ConsistI2V는 높은 일관성을 가진 비디오를 생성할 수 있습니다.
- 또한, 자동-회귀적 장기 비디오 생성과 카메라 모션 제어의 일관성을 향상시키기 위해 제안된 접근 방식의 잠재력을 확장하여 보여줍니다.
- 우리의 방법의 효과성을 검증하기 위해 I2V-Bench, 즉 I2V 생성을 위한 포괄적인 평가 기준을 제안합니다.
- 자동 및 인간 평가 결과는 ConsistI2V가 기존 방법들보다 우수함을 보여줍니다.

### [LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation](https://arxiv.org/abs/2402.05054)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/slx-FbyoMFAfi7mbdV0xJ.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/slx-FbyoMFAfi7mbdV0xJ.mp4" muted="false"></video></div>

Vote: 9

Authors: Zhaoxi Chen, Gang Zeng, Xiaokang Chen, Ziwei Liu, Jiaxiang Tang, Tengfei Wang

- 이 연구에서는 문장 또는 단일 시점 이미지로부터 고해상도 3D 모델을 생성하기 위해 Large Multi-View Gaussian Model (LGM)이라는 새로운 프레임워크를 소개했다.
- LGM은 효율적이면서도 강력한 다중 시점 가우스 특징을 제안하며, 이는 차별화 렌더링을 위해 함께 융합될 수 있다.
- 연구팀은 텍스트 또는 단일 시점 이미지 입력으로부터 다중 시점 이미지를 생성하기 위해 다중 시점 확산 모델을 활용하는 고처리량 백본인 비대칭 U-Net을 제시했다.
- 실시된 광범위한 실험을 통해 접근법의 높은 정확성과 효율성이 입증되었다.
- 특히, 훈련 해상도를 512로 크게 향상시키면서도 3D 객체를 5초 내에 생성할 수 있는 빠른 속도를 유지하여 고해상도 3D 콘텐츠 생성을 달성했다.

### [ScreenAI: A Vision-Language Model for UI and Infographics Understanding](https://arxiv.org/abs/2402.04615)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/W9t0SLOVJAqfef7fTw81_.png)

Vote: 9

Authors: Hassan Mansoor, Vincent Etter, Jason Lin, Victor Cărbune, Jindong Chen, Abhanshu Sharma, Srinivas Sunkara, Fedir Zubach, Maria Wang, Gilles Baechler

- 스크린 사용자 인터페이스(UI)와 인포그래픽을 이해하는 전문 모델인 ScreenAI가 소개되었습니다.
- 이 모델은 패치 전략의 유연성을 갖춘 PaLI 아키텍처를 기반으로 개선되었고 다양한 데이터셋을 혼합하여 훈련되었습니다.
- 중심적인 데이터셋 중 하나는 모델이 UI 요소의 유형과 위치를 식별해야 하는 새로운 스크린 주석 작업입니다.
- 이 텍스트 주석을 사용하여 대규모 언어 모델에게 스크린을 설명하고 자동적으로 질문-응답(QA), UI 탐색, 요약 훈련 데이터셋을 대량으로 생성합니다.
- 연구진은 설계 선택이 미친 영향을 보여주기 위해 열거 연구(ablation studies)를 진행했습니다.
- 5B 매개변수 크기의 ScreenAI는 UI 및 인포그래픽 기반 작업(Multi-page DocVQA, WebSRC, MoTIF, Widget Captioning)에 있어 새로운 최신 성과를 달성했으며, 유사한 크기의 모델과 비교해 다른 작업(Chart QA, DocVQA, InfographicVQA)에서도 최고 수준의 성능을 보였습니다.
- 마지막으로, 스크린 주석 작업에 중점을 둔 데이터셋 하나와 질문 응답에 중점을 둔 두 개의 새로운 데이터셋이 공개되었습니다.

### [EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss](https://arxiv.org/abs/2402.05008)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xcoE_jXgFVLNcKyeUeQuG.png)

Vote: 8

Authors: Zhuoyang Zhang, Han Cai, Song Han

- 본 논문에서는 새로운 가속화된 'Segment Anything' 모델 패밀리인 EfficientViT-SAM을 제안합니다.
- EfficientViT-SAM은 가볍고 효율적인 SAM의 프롬프트 인코더와 마스크 디코더를 유지하면서, 무거운 이미지 인코더를 새로운 EfficientViT로 대체합니다.
- 훈련 과정에서는 SAM-ViT-H 이미지 인코더에서 EfficientViT로 지식 증류를 시작한 뒤, SA-1B 데이터셋에 대해 엔드-투-엔드 훈련을 진행합니다.
- EfficientViT의 효율성과 용량 덕분에, EfficientViT-SAM은 성능 저하 없이 A100 GPU에서 SAM-ViT-H 대비 48.9배의 측정된 TensorRT 가속을 달성합니다.
- 논문의 코드와 사전 훈련된 모델은 https://github.com/mit-han-lab/efficientvit 에서 공개되어 있습니다.

### [Fine-Tuned Language Models Generate Stable Inorganic Materials as Text](https://arxiv.org/abs/2402.04379)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/d-3uw-iuloX_PNj5ObW7c.png)

Vote: 5

Authors: Andrew Gordon Wilson, Andrea Madotto, Anuroop Sriram, C. Lawrence Zitnick, Zachary Ulissi, Nate Gruver

- 본 연구에서는 안정된 물질을 생성하기 위해 대규모 언어 모델을 미세 조정하는 방법을 제안합니다.
- 기존과는 다르게 텍스트로 인코딩된 원자 데이터에 대규모 언어 모델을 미세 조정하면 실행이 간단하면서 약 90%의 샘플 구조가 원자 위치와 전하에 대한 물리적 제약을 준수하는 것으로 나타났습니다.
- 에너지 최소화 계산을 통해 본 논문의 가장 강력한 모델인 미세 조정된 LLaMA-2 70B가 경쟁 모델인 CDVAE보다 대략 두 배 높은 비율(49% 대 28%)로 준안정 상태(metastable)의 물질을 생성할 수 있음을 보여줍니다.
- 텍스트 프롬프트의 유연성을 활용하여, 모델이 안정된 물질을 무조건 생성하거나 부분 구조를 채우거나 텍스트 조건부 생성에 동시에 사용될 수 있습니다.
- 마지막으로, 크리스탈 구조의 주요 대칭성을 포착하는 언어 모델의 능력이 모델 규모에 따라 향상되며, 사전 훈련된 LLM의 편향이 원자 데이터에 놀랍게도 잘 맞는다는 점을 보여줍니다.

### [The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry](https://arxiv.org/abs/2402.04347)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/uClxXWYMEayIgucmXY5mN.png)

Vote: 4

Authors: Christopher Ré, Michael Zhang, Kush Bhatia, Hermann Kumbong

- 선형 어텐션은 트랜스포머의 효율성을 개선하고 주목의 이차적 복잡성을 선형으로 줄여주는 잠재력을 보여주었습니다.
- 자체 학습된 선형 트랜스포머의 훈련, 특정 태스크의 트랜스포머를 선형 버전으로 "미세 조정 변환", 대규모 언어 모델과 같은 트랜스포머를 후속 태스크에 미세 조정 가능한 선형 버전으로 "사전 훈련 변환"하는 것에 대한 가능성이 있습니다.
- 그러나 선형 어텐션은 종종 표준 소프트맥스 어텐션의 성능에 미치지 못합니다.
- 성능 격차를 해소하기 위해, 저자들은 소프트맥스 어텐션의 주요 특성인 낮은 엔트로피(또는 "뾰족한") 가중치와 내적 단조성을 유지하는 단순한 특징 맵을 발견했습니다.
- 이러한 특성을 유지하면서도 선형 복잡성을 유지하는 학습 가능한 선형 어텐션인 Hedgehog를 제안합니다.
- Hedgehog는 단순한 트레이너블 MLP를 사용하여 소프트맥스 어텐션을 모방하는 주의 가중치를 생성합니다.
- 실험 결과, Hedgehog는 기존 트랜스포머 품질의 99% 이상을 회복하며, 기존 선형 어텐션보다 WikiText-103에서 GPT를 사용하여 최대 6개의 혼란 지점과 BERT의 양방향 미세 조정에서 최대 8.7 GLUE 점수를 향상시켰습니다.
- 선형 어텐션 변형으로 사전 훈련된 GPT-2를 변환하면 125M 서브쿼드라틱 디코더 모델에 대해 WikiText-103에서 최첨단 16.7 혼란 지점을 달성합니다.
- 최종적으로, 저자들은 선형 주의력을 가진 Llama2 7B를 실행 가능한 선형 어텐션 Llama로 전환합니다.
- 저차 적응과 함께, Hedgehog-Llama2 7B는 기존 선형 어텐션들이 16.5 포인트 감소를 초래하는 반면, 기준 표준 어텐션 모델보다 ROUGE-1 점수가 28.1 포인트 높습니다.

### [Fast Timing-Conditioned Latent Audio Diffusion](https://arxiv.org/abs/2402.04825)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/cLmX_wACRNCoTWksMJ1xO.png)

Vote: 4

Authors: Jordi Pons, Zach Evans, Josiah Taylor, Scott H. Hawley, CJ Carr

- 텍스트 프롬프트를 사용하여 고품질의 44.1kHz 스테레오 오디오 및 음악을 효율적으로 만드는 연구인 'Fast Timing-Conditioned Latent Audio Diffusion'을 제시합니다.
- 이 연구는 음악과 사운드 이펙트의 지속시간이 자연스럽게 달라지는 점을 고려하여, 텍스트 프롬프트에 기반한 가변 길이 스테레오 음악 및 사운드의 효율적 생성에 중점을 두고 있습니다.
- 'Stable Audio'라는 이름의 생성 모델은 완전 합성곱 변이형 자동 인코더에 기반한 잠재 확산을 이용하며, 내용과 길이를 정교하게 조절할 수 있는 타이밍 임베딩을 텍스트 프롬프트와 함께 조건화합니다.
- 이 모델은 A100 GPU에서 최대 95초 길이의 44.1kHz 스테레오 신호를 8초 만에 렌더링할 수 있을 정도로 계산 효율성이 뛰어나고 추론이 빠릅니다.
- Stable Audio는 공개된 두 텍스트-투-뮤직 및 오디오 벤치마크에서 최상위 성능을 보여주며, 기존 최고 수준 모델들과 달리 구조화된 음악과 스테레오 사운드를 생성할 수 있습니다.

### [Hydragen: High-Throughput LLM Inference with Shared Prefixes](https://arxiv.org/abs/2402.05099)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/sUTApi0eh0QQB4t7JOc76.png)

Vote: 4

Authors: Azalia Mirhoseini, Christopher Ré, Bradley Brown, Jordan Juravsky, Daniel Y. Fu, Ryan Ehrlich

- 변환 기반 대규모 언어 모델(LLM)은 수백 만 사용자에게 배포되며 일반적으로 접두사를 공유하는 시퀀스 배치에서 추론이 수행됩니다.
- 공유 접두사 시나리오에서의 디코딩은 큰 key-value (KV) 캐시를 메모리에서 읽고 배치의 각 시퀀스에 대해 비효율적인 행렬-벡터 곱셈을 계산하는 주목(attention) 연산에 의해 병목 현상이 발생할 수 있습니다.
- 이 연구에서 우리는 공유 접두사를 가진 주목 연산의 하드웨어-인식적인 정확한 구현인 Hydragen을 소개합니다.
- Hydragen은 공유 접두사와 독특한 접미사에 대한 주목을 별도로 계산하여, 시퀀스간에 쿼리를 효율적으로 배치하여 메모리 읽기를 줄이고 하드웨어 친화적인 행렬 곱셈을 사용할 수 있게 합니다.
- 이 방법은 배치 크기와 공유 접두사 길이가 커짐에 따라 경쟁 기준을 대비하여 최대 32배에 이르는 엔드-투-엔드 LLM 처리량 향상을 이끌어낼 수 있습니다.
- 또한, Hydragen은 매우 긴 공유 컨텍스트 사용을 가능하게 합니다: 높은 배치 크기로 접두사 길이를 1K에서 16K 토큰으로 늘릴 때, Hydragen의 처리량은 15% 미만으로 감소하지만, 기준선의 처리량은 90% 이상 감소합니다.
- Hydragen은 간단한 접두사-접미사 분해를 넘어서 일반화되어 있으며, 트리 기반 프롬프트 공유 패턴에 적용될 수 있어, 경쟁 프로그래밍 문제에 대한 추론 시간을 55% 추가로 줄일 수 있습니다.

### [TP-Aware Dequantization](https://arxiv.org/abs/2402.04925)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Ct1sF8cpW7EsZfHb-4OFd.png)

Vote: 2

Authors: Chih-Chieh Yang, Raghu Ganti, Adnan Hoque, Mudhakar Srivatsa

- 본 논문에서는 대규모 언어 모델(Large Language Models, LLMs)의 분산 배치 시 모델 추론 지연 시간을 줄이는 새로운 방법을 제시한다.
- 최적화된 추론 배치 방식을 통해 텐서 병렬(Tensor Parallel, TP) 사용 시 기존의 최신 양자화 커널의 한계를 해결한다.
- GPU 메모리 접근 패턴의 데이터 지역성을 유지하고 TP의 사전 지식을 활용하여 전역 커뮤니케이션을 감소시킨다.
- A100 및 H100 NVIDIA DGX 시스템에서 Llama-70B 모델에 대해 기존 방법보다 최대 1.81배, IBM WatsonX의 Granite-20B MLP 계층 문제 크기에 대해 최대 1.78배의 속도 향상을 입증한다.
- 다양한 TP 설정에 대하여 이 방법의 효과를 실험을 통해 확인하였다.

### [CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay](https://arxiv.org/abs/2402.04858)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/2NUakqIc5EFvBpKF85_CG.png)

Vote: 2

Authors: David Zhang, Corrado Rainone, Auke Wiggers, Blazej Manczak, Taco Cohen, Michaël Defferrard, Natasha Butt

- 대규모 언어 모델이 인간 수준의 추론 능력이 필요한 것으로 여겨지는 작업을 점점 더 해결하고 있음에도 불구하고, 이 모델들은 일반 지능 벤치마크인 추상화 및 추론 코퍼스(ARC)에서 아직 매우 낮은 성능을 보임.
- 이 논문에서는 ARC를 예시 기반 프로그래밍 문제로 접근하고, '코드 이터레이션'(CodeIt)이라는 새롭고 확장 가능한 언어 모델 자체 개선 방법을 소개함.
- CodeIt 방법은 1) 프로그램 샘플링 및 목표 수정이라는 후행 재레이블링과 2) 우선 순위 경험 재생을 통한 학습 사이에서 반복됨.
- 샘플링된 프로그램이 생성한 실제 출력으로 에피소드의 목표(즉, 입력에 대한 목표 프로그램 출력)를 재레이블함으로써, 프로그램 합성에서의 극단적인 보상의 희박함을 효과적으로 처리함.
- CodeIt를 ARC 데이터셋에 적용하여, 우선 순위 후행 재생과 사전 학습, 데이터 확장을 통해 작업 간 일반화를 성공적으로 달성함을 보임.
- CodeIt는 ARC 평가 전체 데이터셋에 확장 가능한 첫 번째 신경-기호 접근 방식이며, 15%의 ARC 평가 작업을 해결하여 최고의 성능을 도달하고 기존의 신경 및 기호 기반 베이스라인을 능가함.

### [Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers](https://arxiv.org/abs/2402.04744)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/r6FHVJ8gCAZ9tvuIAAnso.png)

Vote: 1

Authors: Amir Yazdanbakhsh, Abhimanyu Rajeshkumar Bambhaniya, Utku Evci, Sheng-Chun Kao, Tushar Krishna, Shivani Agrawal, Suvinay Subramanian

- N:M 구조적 스파시티는 비교적 적은 오버헤드와 향상된 효율성으로 주목받고 있으며, 메모리 사용량을 줄이는 데에도 유리한 방법입니다.
- 기존 N:M 스파시티 훈련 방법들은 저스파시티 영역(약 50%)에 초점을 맞추고 있지만, 고스파시티 영역(>80%)으로 갈수록 모델 성능이 저하되는 문제가 있습니다.
- 본 연구에서는 고스파시티 영역에서의 기존 스파스 훈련 레시피의 효과를 연구하고, 지속 가능한 모델 품질을 유지하지 못하는 이유가 그래디언트 크기에 높은 수준의 노이즈가 유입되기 때문임을 주장합니다.
- 저자들은 점진적으로 가지치기된 요소들로의 그래디언트 흐름을 제한하는 붕괴 메커니즘을 적용하여, 비효율적인 영향을 완화시키는 새로운 접근법을 제안합니다.
- 이 방법은 시각 및 언어 모델에서 고스파시티 영역에서 최대 2% 및 5%의 모델 품질 향상을 달성합니다.
- 또한, 동일한 훈련 계산 비용(FLOPs)을 기준으로 할 때, 본 방법은 기존 스파스 훈련 레시피보다 우수한 성능과 최대 2%의 정확도 향상을 보여줍니다.
- 관련 소스코드는 https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity 에서 제공됩니다.

