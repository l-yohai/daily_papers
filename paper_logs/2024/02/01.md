## Daily Papers (2024-02-01)

### [Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens](https://arxiv.org/abs/2401.17377)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/REHvHBoxCtCfDveoGyPbH.png)

Vote: 21

Authors: Sewon Min, Yejin Choi, Hannaneh Hajishirzi, Luke Zettlemoyer, Jiacheng Liu

- 현대의 대규모 신경망 언어 모델(Neural Large Language Models, LLMs) 시대에도 n-gram 언어 모델의 가치가 여전하다는 것을 우리는 주장하며, 텍스트 분석 및 신경망 LLMs 개선에 있어 그 유용성을 보여줍니다.
- n-gram 모델을 현대화하기 위해 두 가지 측면에서 개선을 도입했는데, 첫째로는 신경망 LLM과 동일한 데이터 규모인 1.4조 토큰을 대상으로 훈련시켰으며 이는 역대 가장 큰 n-gram 모델을 구축하였습니다.
- 둘째는 기존의 n-gram 모델이 작은 n 값을 사용하여 성능에 한계가 있었기에, 백오프(backoff)를 도입한 새로운 무한 n-gram(infty-gram) 모델을 소개하면서 n의 크기를 임의로 확장합니다.
- 사전에 n-gram 카운트 테이블을 계산하는 것이 매우 비용이 많이 들기 때문에, 저희는 infini-gram이라 불리는 엔진을 개발하여 무한 n-gram 뿐만 아니라 임의의 n에 대한 n-gram 확률을 밀리초 단위의 지연 시간으로 계산할 수 있도록 하였습니다.
- 인피니티-그램(infini-gram) 엔진과 무한 n-gram(infty-gram) 프레임워크를 이용하여, 인간이 작성하거나 기계가 생성한 텍스트의 많은 새롭고 흥미로운 분석을 수행할 수 있었습니다; 무한 n-gram 언어 모델은 다음 토큰 예측에 상당히 높은 정확도(47%)를 보였으며, 신경망 LLMs의 언어 모델링 복잡도를 크게 줄여줄 수 있는 보완 역할을 할 수 있음을 발견하였습니다.
- 기계 생성 텍스트를 분석할 때, 접미사 길이에 대한 기계-무한 n-gram 합의 수준에 비정상적인 변동을 관찰했는데, 이는 신경망 LLM의 프리트레이닝과 트랜스포머의 위치 임베딩에 결함이 있음을 시사합니다.
- 저희는 대규모 텍스트 코퍼스에서 문자 그대로의 정보를 검색하는 방법에 대한 추가 연구를 가능하게 하기 위해 인피니-그램 엔진을 오픈소스로 제공합니다.

### [LongAlign: A Recipe for Long Context Alignment of Large Language Models](https://arxiv.org/abs/2401.18058)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/T-q276B5baxugYNtpxmOG.png)

Vote: 14

Authors: Ji Qi, Yuxiao Dong, Lei Hou, Xin Lv, Yuze He, Jiajie Zhang, Juanzi Li, Yushi Bai, Jie Tang

- 대규모 언어 모델이 긴 컨텍스트를 효율적으로 처리하기 위해, 유사한 길이의 입력 시퀀스에 대한 지시 사항을 미세 조정하는 것이 필요합니다.
- LongAlign이라는 새로운 방법을 소개하며, 이는 긴 지시-수행 데이터셋 구축, 다양한 길이 분포의 데이터에 대해 빠른 지도학습을 위한 패킹 및 정렬된 배치 전략을 채택합니다.
- 패킹 교육 중 다른 시퀀스 간의 손실 기여도를 균형 있게 하기 위한 손실 가중치 방법을 개발하였습니다.
- 10,000자에서 100,000자 길이의 쿼리에 대한 지시-수행 능력을 평가하기 위한 LongBench-Chat 벤치마크를 도입했습니다.
- 실험 결과, LongAlign은 기존 대규모 언어 모델 대비 긴 컨텍스트 작업에서 최대 30% 향상된 성능을 보이며, 짧은 일반적인 작업 처리 능력도 유지합니다.
- 코드, 데이터 및 긴 컨텍스트에 맞춘 모델은 https://github.com/THUDM/LongAlign에서 오픈소스로 제공됩니다.

### [Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion](https://arxiv.org/abs/2401.17583)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wPvuTsirGEnHmXsInBr9j.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wPvuTsirGEnHmXsInBr9j.mp4" muted="false"></video></div>

Vote: 14

Authors: Wenli Xiao, Tairan He, Changliu Liu, Chong Zhang, Guanqi He, Guanya Shi

- 이 논문은 장애물이 많은 환경에서 빠르고 안전하게 움직일 수 있는 사지 로봇을 위한 학습 기반 제어 프레임워크인 Agile But Safe (ABS)를 소개합니다.
- ABS는 장애물이 있는 상황에서 민첩한 모터 기술을 수행하는 데 필요한 민첩한 정책과 실패를 방지하는 복구 정책을 결합하여 고속 및 충돌 없는 네비게이션을 달성합니다.
- 정책 스위치는 학습된 제어 이론을 바탕으로 한 리치-어보이드 값 네트워크에 의해 지배되고, 이는 다시 복구 정책을 목표 함수로서 안내하여 로봇을 폐쇄 루프로 보호합니다.
- 훈련 과정에는 시뮬레이션에서 민첩한 정책, 리치-어보이드 값 네트워크, 복구 정책 및 외각 지각 표현 네트워크의 학습이 포함됩니다.
- 이러한 훈련된 모듈들은 실제 세계에서 온보드 센싱 및 계산과 함께 직접 배치되어 고정 및 동적 장애물이 있는 실내외 공간에서 고속과 충돌 없는 네비게이션을 이끌어냅니다.

### [Anything in Any Scene: Photorealistic Video Object Insertion](https://arxiv.org/abs/2401.17509)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ahbbbrhXG_tNwXzjiEnlk.qt)

Vote: 11

Authors: Di Liang, Guoxiang Zhang, Zeman Shao, Xiaoyin Zheng, Yiqiao Qiu, Cheng Lu, Yichen Guan, Chen Bai, Jie Yang, Chengzhang Zhong, Zhuorui Zhang, Yujian Guo, Tao Wang, Zhendong Wang

- 실감나는 비디오 시뮬레이션은 가상 현실부터 영화 제작까지 다양한 응용 분야에서 중요한 가능성을 보여왔으며, 이는 특히 실제 환경에서 비디오를 촬영하는 것이 불가능하거나 비용이 많이 드는 시나리오에서 더욱 그러하다.
- 기존의 비디오 시뮬레이션 접근 방식은 종종 조명 환경을 정확히 모델링하거나 객체 기하학을 표현하거나 높은 수준의 사실주의를 달성하는데 실패한다.
- 본 논문에서는 'Anything in Any Scene'이라는 새롭고 범용적인 프레임워크를 제안하여, 어떠한 객체도 기존의 동적인 비디오에 신체적 사실주의를 강조하며 매끄럽게 삽입할 수 있다.
- 제안된 프레임워크는 기하학적 사실주의를 보장하기 위한 적절한 배치를 통한 실제적인 객체 통합, 조명의 실사를 향상시키기 위한 하늘 및 환경 조명 분포의 추정 및 실제적인 그림자 시뮬레이션, 최종 비디오 출력의 사진 사실주의를 극대화하기 위한 스타일 전송 네트워크 사용이라는 세 가지 핵심 프로세스를 포함한다.
- 실험적으로 'Anything in Any Scene' 프레임워크는 기하학적 사실주의, 조명 사실주의 및 사진 사실주의가 뛰어난 시뮬레이션 비디오를 생산한다는 것을 입증한다.
- 비디오 데이터 생성과 관련된 문제점들을 크게 완화시키는 해당 프레임워크는 고품질 비디오를 효율적으로 비용 효과적으로 확보할 수 있는 솔루션을 제시한다.
- 그 응용 범위는 비디오 데이터 확장을 훨씬 넘어 가상 현실, 비디오 편집 및 다양한 비디오 중심 응용 분야에서 유망한 잠재력을 보인다.
- 프로젝트 웹사이트 https://anythinginanyscene.github.io에서 프로젝트 코드와 고해상도 비디오 결과에 접근할 수 있다.

### [ReplaceAnything3D:Text-Guided 3D Scene Editing with Compositional Neural Radiance Fields](https://arxiv.org/abs/2401.17895)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Bwc53A-goG9yRDg99IU_F.png)

Vote: 10

Authors: Edward Bartrum, Douglas Lanman, Chris Xie, Zhengqin Li, Lei Xiao, Numair Khan, Thu Nguyen-Phuoc, Armen Avetisyan

- 'ReplaceAnything3D' (RAM3D)라는 새로운 텍스트 가이드 3D 장면 편집 기법이 소개되었습니다.
- 이 방법은 장면 내 특정 객체를 텍스트 설명으로 식별하고 새로운 객체의 텍스트 설명을 덧붙여 특정 객체를 교체 가능하게 합니다.
- 다각도 이미지를 기반으로 'Erase-and-Replace' 접근법을 통해 장면에서 객체를 효과적으로 교체하며 다양한 관점에서 3D 일관성을 유지합니다.
- RAM3D는 다양한 사실적 3D 장면에 적용되어, 장면의 전반적인 무결성에 영향을 주지 않으면서 잘 통합된 수정된 전경 객체 결과를 선보입니다.

### [Advances in 3D Generation: A Survey](https://arxiv.org/abs/2401.17807)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6VspBO3oIMyGdmBRLfBj-.png)

Vote: 9

Authors: Jingbo Zhang, Yan-Pei Cao, Di Kang, Xiaoyu Li, Qi Zhang, Ying Shan, Weihao Cheng, Zhihao Liang, Jing Liao, Yiming Gao

- 3D 모델 생성은 컴퓨터 그래픽의 핵심이며 수십 년 동안의 연구의 초점이었습니다.
- 고급 신경 네트워크 표현과 생성 모델의 등장으로 3D 콘텐츠 생성 분야가 급속도로 발전하여, 보다 고품질이고 다양한 3D 모델을 생성할 수 있게 되었습니다.
- 이 서베이는 3D 생성 방법론의 기초를 소개하며, 3D 표현, 생성 방법, 데이터셋, 관련 응용 분야를 아우르는 체계적인 로드맵을 제공하는 것을 목표로 합니다.
- 특히, 3D 모델 생성의 기반으로 사용되는 3D 표현들을 소개합니다.
- 또한, 피드포워드 생성, 최적화 기반 생성, 절차적 생성, 그리고 생성적 새로운 뷰 합성 같은 알고리즘 패러다임에 따라 분류된 생성 방법에 대한 광범위한 문헌 리뷰를 제공합니다.
- 마지막으로, 사용 가능한 데이터셋, 응용 분야, 그리고 열린 도전과제들에 대해 논의합니다.
- 이 서베이는 3D 콘텐츠 생성 분야에서의 추가적인 발전을 촉진하고, 이 흥미로운 주제를 탐구하는 데 도움이 되기를 바랍니다.

### [Scavenging Hyena: Distilling Transformers into Long Convolution Models](https://arxiv.org/abs/2401.17574)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/FjsCL2gUQRX8STXYykG8h.png)

Vote: 9

Authors: Tokiniaina Raharison Ralambomihanta, Shahrad Mohammadzadeh, Mohammad Sami Nur Islam, Laurence Liang, Wassim Jabbour

- 이 논문은 GPT-4와 같은 대규모 언어 모델의 신속한 발전이 자연어 처리 분야를 재편하고 있음을 언급하며 시작합니다.
- 연구진은 대규모 언어 모델의 사전 훈련과 관련된 효율성 문제를 해결하기 위한 선도적 접근법으로 지식 증류를 활용한 아키텍처 간 전달을 제안합니다.
- 효율적인 Hyena 메커니즘에 통찰을 받아, 트랜스포머 모델에서 주의 집중 헤드를 Hyena로 대체하는 방법을 도입함으로써, 전형적인 사전 훈련에 비해 비용 효율적인 대안을 제공합니다.
- 이 기법은 특히 긴 문맥 정보를 처리하는 데 내재된 이차(quadratic) 주의 메커니즘의 도전과제를 해결하는 데 집중합니다.
- 기존의 압축 중심 방법들과 달리, 이 기술은 추론 속도를 향상시킬 뿐만 아니라 정확도와 효율성 면에서도 사전 훈련을 능가합니다.
- 저자들은 계산력과 환경 영향 사이의 균형을 이루는 지속 가능한 AI 솔루션을 추구하는 현재 대규모 언어 모델의 진화하는 시대에 기여함을 강조합니다.

### [Efficient Tool Use with Chain-of-Abstraction Reasoning](https://arxiv.org/abs/2401.17464)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/4GMhSwUSbIcRXS7lpkjVu.png)

Vote: 8

Authors: Antoine Bosselut, Xiaoqing Ellen Tan, Olga Golovneva, Koustuv Sinha, Jane Dwivedi-Yu, Silin Gao, Ping Yu, Ramakanth Pasunuru, Asli Celikyilmaz, Tianlu Wang

- 사람 기대와 일치하는 충실한 추론을 달성하기 위해, 큰 언어 모델들(Large Language Models, LLMs)은 실제 세계 지식(예: 웹 사실, 수학 및 물리 규칙)에 근거를 둔 추론이 필요합니다.
- 도구를 활용하면 LLMs는 이 외부 지식에 접근할 수 있지만, 연결된 도구 호출이 필요한 다단계 추론 문제에서 도구를 활용하여 정교하고 효율적인 사용 계획을 세우는 것에 대한 도전이 남아있습니다.
- 본 연구는 다단계 추론에서 도구를 더 잘 활용할 수 있도록 새로운 방법을 제안합니다. 추상화 사슬(Chain-of-Abstraction, CoA) 방법은 LLMs가 추상적인 자리 표시자를 가진 추론 체인을 먼저 해독하고, 특정 지식으로 각 추론 체인을 구현하기 위해 도메인 도구를 호출하도록 훈련합니다.
- 이 추상 체인을 사용한 계획은 다양한 추론 질문에 해당하는 도메인 지식의 변화(예: 수학 결과)에 강인한 일반적인 추론 전략을 배우게 합니다.
- 또한, LLMs가 추론을 해독하고 외부 도구를 병렬로 호출하게 하여, 도구 응답을 기다리는 데에 따른 추론 지연을 피할 수 있습니다.
- 수학 추론 및 위키 QA 분야에서, 우리의 방법은 기존의 사고의 사슬(chain-of-thought) 및 도구 보강 기반을 일관되게 능가하며, 평균적으로 약 6%의 절대 QA 정확도 향상을 보여줍니다.
- 우리의 방법으로 훈련된 LLM 에이전트는 더 효율적인 도구 사용을 보여주며, 평균적으로 기준 도구 보강 LLM보다 추론 속도가 약 1.4배 빠릅니다.

### [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/u2Ehyac70HABru-yoBiLg.png)

Vote: 7

Authors: Anna Goldie, Christopher D. Manning, Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna

- 검색 기능을 향상된 언어 모델은 세계의 상태 변화에 더 잘 적응하고 장기 지식을 통합할 수 있지만, 기존의 방법들은 검색 코퍼스에서 짧고 연속된 청크만을 검색하여 전반적인 문서 컨텍스트의 전체적인 이해에 제한이 있습니다.
- 우리는 전체 텍스트 청크를 재귀적으로 임베딩하고, 클러스터링하고, 요약하여 하단부터 시작하는 요약 수준이 다른 트리를 구축하는 새로운 접근법을 소개합니다.
- 추론 시, RAPTOR 모델은 이 트리에서 검색을 하면서 다양한 추상화 수준에서 긴 문서에 걸친 정보를 통합합니다.
- 제어 실험은 재귀 요약을 사용한 검색이 전통적인 검색 보강 언어 모델보다 몇 가지 과제에서 상당한 향상을 보여준다는 것을 보여줍니다.
- 복잡한 다단계 추론이 포함된 질문-응답 작업에서, RAPTOR 검색을 GPT-4의 사용과 결합함으로써 QuALITY 벤치마크에서 절대 정확도로 20%의 성능 향상을 이루어 최고의 결과를 보여줍니다.

### [CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting](https://arxiv.org/abs/2401.18075)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vEVUJk-dn9Jpd_1pGCq1I.png)

Vote: 6

Authors: Nicholas Rhinehart, Khushi Desai, Charles Packer, Joseph Gonzalez, Rowan McAllister, Jiezhi Yang, Harshil Bhatia

- 본 논문에서는 과거의 관측 데이터를 바탕으로 미래 3D 장면을 예측하는 CARFF(Conditional Auto-encoded Radiance Field for 3D Scene Forecasting) 방법을 제안합니다.
- 이 방법은 확률적 인코더를 이용하여 이미지를 가능한 3D 장면 구성의 분포로 매핑하고 시간에 따른 장면의 변화를 예측합니다.
- 제안된 잠재 장면 표현은 전역 신경 방사선 필드(NeRF)를 조건화하여, 설명 가능한 예측과 다운스트림 어플리케이션을 단순화할 수 있는 3D 장면 모델을 표현할 수 있도록 합니다.
- 본 연구는 환경 상태와 역학의 복잡한 불확실성을 고려하여 이전의 신경 렌더링 작업을 넘어선 접근법을 취합니다.
- 저자들은 Pose-Conditional-VAE와 NeRF의 이 단계 훈련을 사용하여 3D 표현을 학습합니다.
- 또한, 혼합 밀도 네트워크를 사용하여 부분적으로 관찰 가능한 마르코프 결정 프로세스로 장면의 잠재적 표현을 자동 회귀 예측합니다.
- CARLA 자율주행 시뮬레이터를 사용한 실제 시나리오에서 CARFF의 유틸리티를 입증하며, CARFF는 시각적 가림 현상을 포함하는 복잡한 다중 에이전트 자율 주행 상황에서의 효율적인 경로 및 비상 계획 계획을 가능하게 합니다.

