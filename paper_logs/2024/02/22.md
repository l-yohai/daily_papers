## Daily Papers (2024-02-22)

### [LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens](https://arxiv.org/abs/2402.13753)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/VO-TU3OZvAP3C-BSvFMeI.png)

Vote: 50

Authors: Ning Shang, Yiran Ding, Li Lyna Zhang, Yuanyuan Xu, Fan Yang, Jiahang Xu, Mao Yang, Chengruidong Zhang

- 큰 언어 모델의 컨텍스트 윈도 크기는 매우 중요한 특징이지만, 현재의 확장된 컨텍스트 윈도는 대략 128k 토큰들로 제한되었습니다.
- 본 논문은 LongRoPE이라는 새로운 방법을 소개하여, 사전 훈련된 대규모 언어 모델의 컨텍스트 윈도를 최초로 놀랍도록 2048k 토큰까지 확장하고, 기존의 짧은 컨텍스트 윈도에서의 성능도 유지합니다.
- 이 방법은 (i) 비효율적인 위치보간법을 이용해 전 지점에서의 비균일성을 식별 및 활용하고, 기존의 훈련 길이를 8배로 확장하는 효율적인 검색을 제공, (ii) 256k 길이의 LLM을 먼저 미세조정한 후 확장된 LLM에 다시 위치보간법을 적용하여 2048k 컨텍스트 윈도를 달성하는 점진적 확장 전략을 도입, (iii) 짧은 컨텍스트 윈도 성능을 회복하기 위해 8k 길이에서 LongRoPE을 재조정합니다.
- 여러 가지 작업에 대한 LLaMA2 및 Mistral에서 광범위한 실험이 우리 방법의 효과를 입증합니다.
- LongRoPE을 통해 확장된 모델들은 위치 임베딩에 아주 작은 수정을 가한 원래의 구조를 유지하며, 대부분의 기존 최적화를 재사용할 수 있습니다.

### [Aria Everyday Activities Dataset](https://arxiv.org/abs/2402.13349)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/E0VdLERPAj7O_nwHJzKdi.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/E0VdLERPAj7O_nwHJzKdi.mp4" muted="false"></video></div>

Vote: 19

Authors: Vijay Baiyya, Jing Dong, Luis Pesqueira, Huixuan Tang, Pierre Moulon, Edward Miller, Mark Schwesinger, Yuyang Zou, Jeff Meissner, Alexander Gamino, Kiran Somasundaram, Chris Sweeney, Jakob Julian Engel, Qiao Gu, Nickolas Charron, Zhaoyang Lv, Shangyi Cheng, Renzo De Nardi, Omkar Parkhi, Cheng Peng, Richard Newcombe, +, Steve Saarinen

- 본 논문에서는 Project Aria 안경을 이용하여 기록된 일인칭 시점의 다모달 오픈 데이터셋인 'Aria Everyday Activities (AEA) Dataset'을 소개합니다.
- AEA 데이터셋은 다양한 착용자들이 지리적으로 다른 5개의 실내 장소에서 기록한 총 143개의 일상 활동 시퀀스를 포함하고 있습니다.
- 각 녹화에는 Project Aria 안경을 통해 기록된 다중 모달 센서 데이터가 포함되어 있습니다.
- 또한, AEA는 높은 빈도로 전역적으로 정렬된 3D 궤적, 장면 포인트 클라우드, 프레임별 3D 눈동자 이동 벡터 및 시간에 맞춘 음성 전사 데이터를 제공합니다.
- 이 논문에서는 AEA 데이터셋을 활용하여 가능한 몇 가지 연구 응용 예를 보여줍니다. 이에는 신경 장면 재구성 및 프롬프트 세분화가 포함됩니다.
- AEA는 프로젝트 웹사이트인 projectaria.com을 통해 다운로드할 수 있는 오픈 소스 데이터셋이며, 데이터셋 활용 방법을 보여주는 Project Aria Tools의 오픈 소스 구현체 및 예시도 제공하고 있습니다.

### [YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information](https://arxiv.org/abs/2402.13616)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/piUnHtfzRy_bD0pW1Ub26.png)

Vote: 17

Authors: Chien-Yao Wang, Hong-Yuan Mark Liao, I-Hau Yeh

- 현재 딥러닝 방법들은 모델의 예측 결과가 실제 데이터에 최대한 가깝도록 적절한 목적 함수를 설계하는 데 중점을 두고 있습니다.
- 이 논문은 데이터가 깊은 네트워크를 통해 전달될 때 발생하는 정보 손실, 즉 정보 병목현상과 가역적 함수에 대한 중요한 문제를 탐구합니다.
- 연구자들은 다양한 목표를 달성하기 위해 깊은 네트워크에 의해 요구되는 변화에 대처할 수 있는 프로그래머블 그래디언트 정보(PGI, Programmable Gradient Information) 개념을 제안했습니다.
- PGI를 사용하면 신뢰할 수 있는 그래디언트 정보를 얻어 네트워크 가중치를 업데이트할 수 있도록 목표 작업에 대한 완전한 입력 정보를 제공할 수 있습니다.
- 또한, 그래디언트 경로 계획을 기반으로 한 새로운 경량 네트워크 구조인 일반화된 효율적 계층 통합 네트워크(GELAN, Generalized Efficient Layer Aggregation Network)가 설계되었습니다.
- GELAN의 구조는 PGI가 경량 모델에 탁월한 결과를 달성했음을 입증합니다.
- 제안된 GELAN과 PGI는 MS COCO 데이터셋 기반 객체 감지 작업에서 검증되었으며, GELAN은 깊이별 컨볼루션을 기반으로 한 최신 방법보다 전통적인 컨볼루션 연산자를 사용하여 더 나은 매개변수 활용성을 달성했습니다.
- PGI는 경량부터 대형까지 다양한 모델에 사용될 수 있으며, 완전한 정보를 얻어 대규모 데이터셋을 사용하여 사전 훈련된 최신 모델보다 더 나은 결과를 달성할 수 있습니다.
- 해당 논문의 소스 코드는 https://github.com/WongKinYiu/yolov9 에서 확인할 수 있습니다.

### [SDXL-Lightning: Progressive Adversarial Diffusion Distillation](https://arxiv.org/abs/2402.13929)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/o7G-alChcy8rWE3fz1PSU.png)

Vote: 15

Authors: Xiao Yang, Shanchuan Lin, Anran Wang

- 본 논문은 SDXL 기반의 1024px 해상도에서의 텍스트-이미지 변환 분야에서 새로운 최고 성능을 달성한 확산 증류 방법을 제안합니다.
- 이 방법은 진보적이고 적대적인 증류를 결합함으로써 품질과 모드 커버리지 간의 균형을 달성합니다.
- 이론적 분석, 판별자 설계, 모델 수식화 및 훈련 기법에 관해 논의합니다.
- LoRA 및 전체 UNet 가중치로서, 증류된 SDXL-Lightning 모델들을 오픈소스로 제공합니다.

### [User-LLM: Efficient LLM Contextualization with User Embeddings](https://arxiv.org/abs/2402.13598)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/R6pYFuABiWEVbRXR1Xx-x.png)

Vote: 13

Authors: Jun Xie, Sushant Prakash, Bradley Green, Devora Berlowitz, Lin Ning, Jiaxing Wu, Shawn O'Banion, Luyang Liu, Neo Wu

- 대규모 언어 모델(LLM)은 자연어 처리 분야에 혁명을 가져왔으나, 복잡하고 잠재적으로 노이즈가 있는 사용자 상호작용 데이터를 효과적으로 통합하는 것은 여전히 도전적인 과제입니다.
- 이를 해결하기 위해, 사용자-LLM이라는 새로운 프레임워크를 제안하며, 이는 사용자 임베딩을 활용하여 LLM을 상황에 맞게 조정합니다.
- 이 사용자 임베딩은 자기 감독 학습을 통해 다양한 사용자 상호작용으로부터 추출되며, 사용자의 숨겨진 취향과 시간에 따른 변화를 포착합니다.
- 교차 주목(cross-attention)과 소프트-프롬프팅(soft-prompting)을 통해 LLM에 사용자 임베딩을 통합함으로써, LLM이 사용자 상황에 동적으로 적응할 수 있도록 합니다.
- MovieLens, Amazon Review, Google Local Review 데이터셋에서 실시한 광범위한 실험을 통해 다양한 작업에서 중요한 성능 향상을 입증했습니다.
- 특히, 본 접근법은 긴 시퀀스 작업 및 깊은 사용자 이해를 요구하는 작업에서 텍스트 프롬프트 기반 컨텍스트화를 능가하는 성능을 보였고 계산 효율성도 제공합니다.
- 또한, 사용자 인코더와 LLM 사이의 통합을 간소화하여 계산 요구 사항을 줄이는 Perceiver 계층을 추가로 통합하였습니다.

### [In deep reinforcement learning, a pruned network is a good network](https://arxiv.org/abs/2402.12479)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/TlV3U3eAuyIlwugk2YhKm.png)

Vote: 13

Authors: Aaron Courville, Johan Obando-Ceron, Pablo Samuel Castro

- 최근의 연구에 따르면, 딥 강화 학습 에이전트들이 그들의 네트워크 파라미터를 효과적으로 사용하는데 어려움을 겪고 있는 것으로 보여진다.
- 희소 훈련 기술의 이점에 대한 이전의 통찰을 활용하여 점진적인 크기 가지치기를 통해 에이전트들이 파라미터의 효과를 극대화할 수 있음을 보여준다.
- 이러한 접근은 전통적인 네트워크들을 능가하는 획기적인 성능 향상을 가져오는 네트워크를 형성하며, 전체 네트워크 파라미터의 아주 작은 부분만을 사용하면서도 일종의 "규모의 법칙"을 보여준다.

### [Coercing LLMs to do and reveal (almost) anything](https://arxiv.org/abs/2402.14020)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kY063XaHt9uccw74IYmn2.png)

Vote: 8

Authors: Tom Goldstein, Alex Stein, Jonas Geiping, Khalid Saifullah, Yuxin Wen, Manli Shu

- 최근 대규모 언어 모델(Large Language Models, LLMs)을 대상으로 하는 적대적 공격이 모델을 '탈옥시켜' 해로운 발언을 하게 만드는 것이 밝혀졌습니다.
- 이 연구에서는 LLMs에 대한 적대적 공격이 단순한 '탈옥'에 그치지 않고 훨씬 넓은 범위에 걸쳐있다고 주장합니다.
- 공격 표면과 목표에 대한 포괄적인 개요를 제공하고, 구체적인 예시를 통해 잘못된 행동을 강제하는 공격을 논의, 분류, 체계화합니다.
- 오도, 모델 제어, 서비스 거부, 데이터 추출 등 다양한 의도하지 않은 행동을 강제하는 공격이 제시됩니다.
- 이러한 공격들을 통제된 실험을 통해 분석하고, 많은 공격들이 코딩 능력을 포함한 LLMs의 사전 훈련 관행과 삭제되어야 할 '이상한' 글리치 토큰의 계속된 존재에서 비롯됨을 발견했습니다.

### [D-Flow: Differentiating through Flows for Controlled Generation](https://arxiv.org/abs/2402.14017)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/BUSROcVVb0CCGbOAGfKMk.png)

Vote: 4

Authors: Omri Puny, Brian Karrer, Uriel Singer, Itai Gat, Heli Ben-Hamu, Yaron Lipman

- 본 연구에서는, 특정 과제를 위해 모델을 재학습할 필요 없이 유명한 확산 및 흐름매칭(FM) 모델의 생성 결과를 제어하는 강력한 도구 역할을 하는 D-Flow, 즉 간단한 프레임워크를 소개한다.
- D-Flow는 흐름을 통해 차별화하고 소스(노이즈) 지점을 최적화함으로써 생성 과정을 제어한다.
- 가우시안 확률 경로로 훈련된 확산/FM 모델의 경우, 생성 과정의 차별화가 데이터 매니폴드에 그래디언트를 투영하고, 최적화 과정에 암묵적으로 사전 지식을 주입한다는 주요 관찰을 통해 이 프레임워크를 동기부여한다.
- 본 프레임워크는 이미지 및 오디오 역 문제와 조건부 분자 생성을 포함한 선형 및 비선형 제어 생성 문제에서 검증되었으며, 모든 분야에서 최고 수준의 성능에 도달했다.

### [ToDo: Token Downsampling for Efficient Generation of High-Resolution Images](https://arxiv.org/abs/2402.13573)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/7MwxpFCz-ptXi20ZQmBWM.png)

Vote: 4

Authors: Ethan Smith, Nayan Saxena, Aninda Saha

- 이미지 확산 모델에서 주의 기제(attention mechanism)는 중요하지만, 제곱에 비례하는 계산 복잡도로 인해 합리적인 시간과 메모리 제약 내에서 처리할 수 있는 이미지 크기에 한계가 있습니다.
- 이 논문은 생성적 이미지 모델에서 밀집된 주의가 종종 중복된 특성을 포함하고 있어, 더 희소한 주의 메커니즘이 적합할 수 있음을 탐구합니다.
- 저자들은 키(key) 및 값(value) 토큰의 다운샘플링에 의존하는 새로운 교육 없는(Training-free) 방법인 ToDo를 제안하여, 흔한 크기의 이미지에 대해서 최대 2배, 2048x2048과 같은 고해상도 이미지에 대해서는 최대 4.5배 이상의 빠른 안정적 확산 추론을 가능하게 합니다.
- 이 접근법은 효율적인 처리량과 충실도의 균형을 이전 방법들보다 우수하게 달성한다는 것을 보여줍니다.

### [Music Style Transfer with Time-Varying Inversion of Diffusion Models](https://arxiv.org/abs/2402.13763)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ksfAZsYtuC4yOx4I1Y9Uf.png)

Vote: 4

Authors: Weiming dong, Sifei Li, Fan Tang, Yuxin Zhang, Chongyang Ma, Changsheng Xu

- 확산 모델의 개발로 인해 텍스트 기반 이미지 스타일 변환에서 고품질의 제어 가능한 합성 결과가 가능해졌지만, 다양한 음악 스타일 전송에 텍스트를 활용하는 것은 일치하는 오디오-텍스트 데이터세트가 제한적이기 때문에 상당한 도전을 안고 있습니다.
- 음악은 추상적이고 복잡한 예술 형식으로, 심지어 같은 장르 내에서도 다양한 변화와 섬세함을 나타내어 정확한 텍스트 기술을 어렵게 만듭니다.
- 이 논문은 최소한의 데이터를 사용하여 음악적 속성을 효과적으로 포착하는 음악 스타일 전송 방법을 제시합니다.
- 시간에 따라 변하는 텍스트 인버전 모듈을 도입하여 다양한 수준에서 멜-스펙트로그램 기능을 정밀하게 포착할 수 있도록 합니다.
- 추론 중에는 안정적인 결과를 얻기 위한 편향 감소 스타일화 기술을 제안합니다.
- 실험 결과는 우리의 방법이 특정 악기의 스타일을 전송할 수 있을 뿐만 아니라 자연 소리를 포함하여 선율을 작곡할 수 있음을 보여줍니다.
- 샘플과 소스 코드는 https://lsfhuihuiff.github.io/MusicTI/에서 확인할 수 있습니다.

### [Ouroboros: Speculative Decoding with Large Model Enhanced Drafting](https://arxiv.org/abs/2402.13720)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/JxKS9DnwNvDOeNwuR0CTM.png)

Vote: 4

Authors: Maosong Sun, Zhiyuan Liu, Yuxiang Huang, Weilin Zhao, Chaojun Xiao, Xu Han

- 'Ouroboros'는 큰 언어 모델의 추론 과정을 가속화하기 위해 작은 모델을 이용하여 초안을 만든 후, 큰 모델로 비순차적으로 검증 및 수정하는 '초안 작성 후 검증(decoding)' 방법인 "speculative decoding"을 활용합니다.
- 이 방법은 먼저 효율적인 작은 모델로 초안을 생성한 뒤, 큰 모델을 사용하여 시간 지연을 최소화하기 위해 비순차적으로 검증 및 수정을 수행합니다.
- 긴 초안을 생성하면 검증이 성공할 경우 더 큰 속도 향상을 이끌어낼 수 있지만, 실패할 경우 시도와 오류에 상당한 비용이 발생합니다.
- 기존의 디코딩 방법들은 검증 실패 가능성이 높기 때문에 한 번에 너무 많은 내용을 초안으로 작성하여 검증할 수 없어, 최적의 추론 속도 향상을 달성하지 못했습니다.
- 본 논문에서 제시한 'Ouroboros'는 큰 모델의 검증 과정에서 구문 후보 풀을 구축하여 작은 모델의 초안 생성에 대한 후보를 제공함으로써, 초기 초안의 효율성과 효과성을 개선합니다.
- 실험 결과는 텍스트 생성 과제에서 'Ouroboros'가 lookahead decoding에 비해 최대 1.9배, speculative decoding에 비해서는 최대 2.8배의 속도 향상을 달성했음을 보여줍니다.
- 'Ouroboros'의 소스 코드는 https://github.com/thunlp/Ouroboros 에서 확인할 수 있습니다.

### [BBA: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models](https://arxiv.org/abs/2402.13577)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/_SKGpk5id8mEWnJ-6M0Qg.png)

Vote: 3

Authors: Wei Bi, Xueliang Zhao, Shansan Gong, Lingpeng Kong, Lemao Liu, Qintong Li, Xinting Huang, Tingchen Fu

- 멀티모달 논리 추론은 대규모 시각-언어 모델(LVLMs)에게 중요한 역량으로, 정교한 시각적 표현을 제공하는 도메인 특화 언어(DSL)와의 통합을 통해 복잡하고 전문적인 영역에서 보다 정확한 추론 실행이 가능해진다.
- 기존의 Chain-of-Thought(CoT) 프롬프팅 방식은 시각적 표현과 DSL 표현의 상이한 추론 메커니즘을 효과적으로 활용하는 데에 한계가 있으며, 다단계 추론 과제에서 중요한 단계들을 다루기에 충분하지 않다.
- 이러한 문제를 해결하기 위해, 복잡한 멀티모달 추론 과제에서 DSL의 잠재력을 극대화하기 위해 설계된 Bi-Modal Behavioral Alignment(BBA) 프롬프팅 방법을 소개한다.
- BBA 방법은 LVLMs가 시각적 및 DSL 표현에 대한 별도의 추론 체인을 생성하도록 유도한 다음, 이러한 체인들의 불일치를 해결함으로써 다양한 형태의 행동들을 통합하는 일관된 연결을 달성한다.
- 실험 결과, BBA는 GPT-4V(ision)의 기하학 문제 해결(28.34%에서 34.22%), 체스 위치적 우위 예측(42.08%에서 46.99%), 분자 특성 예측(77.47%에서 83.52%)에서 성능이 크게 향상된 것을 보여준다.

