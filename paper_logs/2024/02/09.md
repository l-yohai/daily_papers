## Daily Papers (2024-02-09)

### [More Agents Is All You Need](https://arxiv.org/abs/2402.05120)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0WVmL5UD5hH_OuMUiWCAf.png)

Vote: 16

Authors: Yangbin Yu, Deheng Ye, Qin Zhang, Junyou Li, Qiang Fu

- 해당 연구에서는 샘플링과 투표 방법을 통해 대규모 언어 모델(LLMs)의 성능이 인스턴스화된 에이전트의 수에 따라 확장될 수 있음을 발견하였다.
- 본 방법은 기존에 LLMs를 개선하기 위한 복잡한 방법들과는 독립적이며, 개선 정도는 과제 난이도와 상관관계를 가진다.
- 연구팀은 다양한 LLM 벤치마크에 대해 포괄적인 실험을 진행하여 이러한 발견을 확인하고, 그 발생을 촉진하는 속성들을 연구하였다.
- 연구에 사용된 코드는 https://anonymous.4open.science/r/more_agent_is_all_you_need 주소에서 공개적으로 이용할 수 있다.

### [WebLINX: Real-World Website Navigation with Multi-Turn Dialogue](https://arxiv.org/abs/2402.05930)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Bk0fc-EV224wB0_G-Qphs.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Bk0fc-EV224wB0_G-Qphs.mp4" muted="false"></video></div>

Vote: 15

Authors: Xing Han Lù, Siva Reddy, Zdeněk Kasner

- 웹브라우저를 제어하여 멀티턴 대화 방식으로 사용자의 지시에 따라 실제 작업을 해결하는 디지털 에이전트의 대화형 웹 탐색 문제를 제안합니다.
- 2,300개의 전문가 시연을 통해 100K 상호작용을 포함하는 대규모 기준인 WEBLINX를 도입하여 다양한 시나리오에서 에이전트를 훈련하고 평가할 수 있습니다.
- 우리의 벤치마크는 150개 이상의 실제 웹사이트에서 다양한 패턴을 다루며, 대규모 언어 모델(LLMs)은 실시간으로 전체 웹 페이지의 정보를 처리할 수 없습니다.
- 이러한 병목 현상을 해결하기 위해, HTML 페이지에서 관련 요소를 순위에 따라 효율적으로 선택하는 검색 영감 모델을 설계했습니다.
- 선택된 요소들을 스크린샷과 액션 이력과 함께 사용하여 다양한 모델들이 웹 탐색에서 인간 행동을 얼마나 잘 모방하는지 평가합니다.
- 실험은 작은 텍스트 전용 모델부터 저작권이 있는 멀티모달 LLMs에 이르기까지 포괄적으로 진행되었습니다.
- 발견에 따르면, 크기가 더 작은 미세 조정된 디코더가 GPT-4V를 포함한 최고의 제로샷 LLM들을 초과했지만, 스크린샷에 대해 명시적으로 미리 훈련된 더 큰 미세 조정된 멀티모달 모델들도 능가했습니다.
- 그러나 모든 미세 조정된 모델들은 보지 않은 웹사이트로 일반화하는 데 어려움을 겪습니다.
- 우리의 발견은 새로운 환경으로 일반화할 수 있는 대규모 멀티모달 모델의 필요성을 강조합니다.
- 연구를 위한 코드, 데이터, 모델은 다음 웹사이트에서 사용할 수 있습니다: https://mcgill-nlp.github.io/weblinx

### [$λ$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space](https://arxiv.org/abs/2402.05195)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/fIJ-6-DuDiL2yaOmC7_EB.png)

Vote: 11

Authors: Maitreya Patel, Chitta Baral, Sangmin Jung, Yezhou Yang

- 최근 개인화된 텍스트-이미지(T2I) 생성 모델의 발전에도 불구하고, 주제 중심의 T2I는 여전히 도전적인 과제로 남아 있으며, 이는 훈련에 많은 자원이 필요하고, 하이퍼파라미터에 대한 민감도가 높으며, 새로운 시각적 개념과 구성의 조화를 이루기 어렵기 때문이다.
- 이러한 한계를 극복하기 위해, 저자들은 잠재 공간 확산 모델(LDM)에 의존하는 대신, UnCLIP 기반 T2I 모델을 더 효율적으로 훈련시킬 수 있는 새로운 경로인 ECLIPSE를 소개하며, 그 위에 lambda-ECLIPSE를 제안한다.
- lambda-ECLIPSE는 확산 모델의 잠재 공간에 의존하지 않고도 효과적인 개인화된 T2I를 달성할 수 있음을 보여주며, 단일 객체, 다중 주제 및 가이드된 이미지 생성을 위해서 단 34M 파라미터와 74 GPU 시간을 사용하여 1.6M 이미지-텍스트 데이터로 훈련된다.
- 광범위한 실험을 통해 lambda-ECLIPSE가 자원 사용을 크게 줄이면서도 기존 베이스라인보다 구성 조화(Composition Alignment)에서 우수한 성능을 보이며, 개념 조화(Concept Alignment) 성능을 유지함을 입증한다.

### [Multilingual E5 Text Embeddings: A Technical Report](https://arxiv.org/abs/2402.05672)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/34hZOlCcUTJ8cOVKtQ9Ij.png)

Vote: 9

Authors: Linjun Yang, Liang Wang, Xiaolong Huang, Furu Wei, Rangan Majumder, Nan Yang

- 본 기술 보고서는 2023년 중반에 공개된 다국어 E5 텍스트 임베딩 모델의 학습 방법론과 평가 결과를 제시합니다.
- 세 가지 크기(소형 / 베이스 / 대형)의 임베딩 모델이 제공되며, 추론 효율성과 임베딩 품질 간의 균형을 제공합니다.
- 교육 절차는 10억 개의 다국어 텍스트 쌍에 대한 대조적 사전 훈련을 포함하며 영어 E5 모델 레시피를 따릅니다.
- 레이블이 지정된 데이터 세트 조합에 대한 미세 조정이 이어집니다.
- 새로운 설명 기반 임베딩 모델을 도입하는데, 이는 비슷한 크기의 최신 영어 전용 모델과 동등한 성능을 보입니다.
- 모델 출시 관련 정보는 https://github.com/microsoft/unilm/tree/master/e5 에서 확인할 수 있습니다.

### [An Interactive Agent Foundation Model](https://arxiv.org/abs/2402.05929)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/sxDP8LP1kgDak86hmEHEv.png)

Vote: 9

Authors: Zane Durante, Hoi Vo, Ehsan Adeli, Li Fei-Fei, Bidipta Sarkar, Ashley Llorens, Yusuke Noda, Arnold Milstein, Noboru Kuno, Katsu Ikeuchi, Rohan Taori, Jianfeng Gao, Kevin Schulman, Qiuyuan Huang, Ran Gong, Shrinidhi Kowshika Lakshmikanth, Naoki Wake, Demetri Terzopoulos, Paul Tang, Ade Famoti

- 인공지능 시스템 개발이 정적이고 과제 특화 모델에서 동적이고 에이전트 기반 시스템으로 전환되고 있는 가운데, 본 논문은 다양한 도메인, 데이터셋 및 과제에 걸쳐 AI 에이전트를 훈련시키기 위한 새로운 다중 과제 에이전트 훈련 패러다임을 제안합니다.
- 이 훈련 패러다임은 시각적 마스크 자동 인코더, 언어 모델링, 다음 행동 예측 등 다양한 사전 훈련 전략을 통합하여 다재다능하고 적응력 있는 AI 프레임워크를 가능하게 합니다.
- 로봇공학, 게이밍 AI, 그리고 건강 관리 등 세 개의 별개 도메인에서 프레임워크의 성능을 시연하며, 각 분야에서 의미 있고 문맥상 관련된 출력을 생성하는 능력을 보여줍니다.
- 접근 방식은 로봇 시퀀스, 게임 플레이 데이터, 대규모 비디오 데이터셋 및 텍스트 정보와 같은 다양한 데이터 소스를 활용하여 효과적인 멀티모달 및 다중 과제 학습에 그 강점이 있습니다.
- 이 연구는 일반적이고 다중 모달 시스템에 작용하는 과제 수행능력을 가진 AI 에이전트를 개발하기 위한 유망한 방향을 제시합니다.

### [SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models](https://arxiv.org/abs/2402.05935)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/g5OAdnmaTrKzMFkPTD-hG.png)

Vote: 8

Authors: Chris Liu, Longtian Qiu, Renrui Zhang, Shijie Geng, Yu Qiao, Junjun He, Siyuan Huang, Chao Xu, Pan Lu, Peng Gao, Hongsheng Li, Conghui He, Ziyi Lin, Peng Jin, Shitian Zhao, Kaipeng Zhang, Hao Shao, Wenqi Shao, Weifeng Lin

- SPHINX-X는 다중 모델 대규모 언어 모델(MLLM) 시리즈를 더 발전시키기 위한 연구로, 기존의 SPHINX 프레임워크를 기반으로 하고 있다.
- 이 연구에서는 SPHINX 프레임워크의 아키텍처와 학습 효율을 향상시키기 위해 불필요한 시각적 인코더를 제거하고, 전체 패딩된 하위 이미지를 스킵 토큰으로 대체하는 등의 변형을 제안한다.
- 또한, 다단계 학습을 한 단계의 '올인원(All-in-One)' 패러다임으로 단순화하여 효율성을 높였다.
- 언어, 시각, 그리고 시각-언어 작업에 대한 공개 자원을 포함한 광범위한 다-도메인 및 다중 모달 데이터셋을 구성하고, OCR(Intensive) 및 Set-of-Mark 데이터셋을 통해 이 모음을 풍부하게 확장하여 다양성과 일반성을 더하였다.
- TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, 그리고 Mixtral8x7B를 포함하는 다양한 기본 LLM을 통해 학습함으로써 다중 언어 능력과 파라미터 크기가 다른 MLLM 스펙트럼을 얻을 수 있었다.
- 종합적인 벤치마킹을 통해 다중 모달 성능이 데이터 규모와 파라미터 크기와 강한 상관관계를 가짐을 밝혔다.
- 코드와 모델들은 https://github.com/Alpha-VLLM/LLaMA2-Accessory 에서 공개하였다.

### [Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains](https://arxiv.org/abs/2402.05140)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/M0QTYzIFKScZmrmWWPA8d.png)

Vote: 8

Authors: Nicolo Fusi, Neil Tenenholtz, Junhong Shen, David Alvarez-Melis, James Brian Hall

- 대형 언어 모델(LLM)은 자연어 이해와 생성에서 뛰어난 능력을 보여주지만, 사전 훈련 데이터에 적게 포함된 전문 분야(예: 물리 및 생물의학 분야)에서는 성능이 떨어진다.
- 이 연구는 일반 LLM을 전문 분야의 효과적인 과제 해결사로 전환하는 방법을 탐색하며, 입력층에 추가되는 연속 벡터로 파라미터화된 맞춤형 입력 태그를 학습하는 모델 비종속적 프레임워크를 도입한다.
- 도메인 태그는 특수한 표현(예: 화학식)을 구분하고 관련 분야의 맥락을 제공하며, 기능 태그는 특정 기능(예: 분자 특성 예측)을 나타내고 기능 해결 지침을 압축하는 데 사용된다.
- 보조 데이터와 도메인 지식을 활용한 3단계 프로토콜을 개발하여 이들 태그를 학습하고, 과제 도메인과 과제 기능을 명확히 구분함으로써 볼 수 없는 문제들에 대한 제로샷 일반화를 가능하게 한다.
- 입력 태그의 다양한 조합을 통해, 특정 과제(예: 단백질 또는 화학 물질 특성 예측, 약물-타겟 상호작용 모델링)에 대한 LLM의 성능을 향상시키며, 이 분야에 맞춰진 전문 모델보다 우수한 성능을 보인다.

### [In-Context Principle Learning from Mistakes](https://arxiv.org/abs/2402.05403)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/1pJwyciaGXIuEEyOELDgo.png)

Vote: 8

Authors: Tianjun Zhang, Swaroop Mishra, Steven Zheng, Uri Alon, Niket Tandon, Aman Madaan, Yiming Yang, Luyu Gao

- 본 논문은 기존의 소수의 정확한 입력-출력 예시에서 학습하는 인-컨텍스트 학습(ICL, few-shot prompting) 방식에서 벗어나, 몇 가지 주어진 예를 통해 더 많이 학습하는 새로운 접근법을 소개합니다.
- 학습 원칙(LEAP)이라고 불리는 이 방법은, 일부러 모델이 실수를 범하게 한 후, 이러한 실수에서 교훈을 얻어 특정 작업에 대한 명시적인 원칙들을 학습하고, 이러한 원칙들을 적용해 유사한 문제를 해결하고 잦은 실수를 피하도록 합니다.
- LEAP는 다양한 벤치마크 테스트에서 평가되었으며, 여러 벤치마크에서 기존의 강력한 대규모 언어 모델들(GPT-3.5-turbo, GPT-4, 등)을 능가하는 성과를 보였습니다.
- 예를 들어, LEAP는 표준적인 소수 예시 프롬프팅을 사용하는 GPT-4와 비교하여 DROP에서 7.5%, HotpotQA에서 3.3%의 성능 향상을 보였습니다.
- 중요한 점은 LEAP이 표준적인 소수 예시 프롬프팅 설정보다 더 많은 입력이나 예시를 요구하지 않으면서 성능을 향상시켰다는 것입니다.

### [Memory Consolidation Enables Long-Context Video Understanding](https://arxiv.org/abs/2402.05861)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/BPf7k7C9X0mQu2N6sDTgh.png)

Vote: 7

Authors: Yuge Shi, Ivana Balažević, Skanda Koppula, Olivier J. Hénaff, Rahma Chaabouni, Pinelopi Papalampidi

- 대부분의 트랜스포머 기반 비디오 인코더는 그들의 제곱 복잡성으로 인해 짧은 시간적 컨텍스트에 제한이 있습니다.
- 이 컨텍스트를 확장하려는 여러 시도가 있었지만, 종종 개념적 및 계산적 복잡성의 증가를 초래했습니다.
- 연구진은 기존의 사전 훈련된 비디오 트랜스포머를 재사용하여 과거 활성화에서 파생된 비파라메트릭 메모리에 주의를 기울이도록 단순하게 미세 조정하는 방법을 제안합니다.
- 이 방법을 통해 메모리-결합 비전 트랜스포머(MC-ViT)는 중복 감소를 활용하여 과거로의 컨텍스트를 수월하게 확장하고 긴 비디오로부터 학습 시 탁월한 스케일링 성능을 발휘합니다.
- MC-ViT는 훨씬 더 많은 파라미터의 이점을 가진 방법들을 능가하며 EgoSchema, Perception Test 및 Diving48에서 장기간 컨텍스트 비디오 이해 분야에서 새로운 최고 기록을 세웠습니다.

### [Offline Actor-Critic Reinforcement Learning Scales to Large Models](https://arxiv.org/abs/2402.05546)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/dKVrTBUwJK0MR91Uo7XVQ.png)

Vote: 4

Authors: Jost Tobias Springenberg, Jingwei Zhang, Nicolas Heess, Steven Kapturowski, Martin Riedmiller, Abbas Abdolmaleki, Michael Bloesch, Oliver Groth, Sarah Bechtle, Philemon Brakel, Roland Hafner, Thomas Lampe

- 이 연구에서는 오프라인 액터-크리틱(Actor-Critic) 강화학습이 대규모 모델, 예를 들어 트랜스포머와 같은 크기까지 확대 적용될 수 있으며 지도 학습과 유사한 확장 법칙을 따른다는 것을 보여줍니다.
- 132개의 연속 제어 과업에 대해 최적이 아닌 행동 및 전문가 수준의 행동 데이터가 포함된 대규모 데이터셋에서 다중 과업 훈련에 대해 기존의 강력한 지도 학습 기반 행동 모방(behavioral cloning) 기준선을 오프라인 액터-크리틱 알고리즘으로 뛰어넘을 수 있는 능력이 있다는 것을 발견했습니다.
- 연구팀은 Perceiver 기반 액터-크리틱 모델을 소개하고 오프라인 강화학습에서 자기주의(self-attention)와 상호주의(cross-attention) 모듈을 효과적으로 활용하기 위한 핵심 모델 특징을 밝혀냈습니다.
- 보고된 결과를 통해 오프라인 액터-크리틱 알고리즘은 행동 모방에서 점진적으로 벗어나 광범위한 도메인을 동시에 마스터하는 다중 과업 정책을 배우는 새로운 방법으로 자리매김할 수 있으며, 또한 부족하거나 자가 생성된 데이터에서 실제 로봇 과업까지 학습이 가능함을 시사합니다.

### [Implicit Diffusion: Efficient Optimization through Stochastic Sampling](https://arxiv.org/abs/2402.05468)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vVZB-lbo_d9jhndg_flu9.png)

Vote: 3

Authors: Anna Korba, Felipe Llinares-López, Peter Bartlett, Mathieu Blondel, Courtney Paquette, Quentin Berthet, Arnaud Doucet, Valentin De Bortoli, Pierre Marion

- 이 논문에서는 매개 변수화된 확률적 확산(diffusions)에 의해 암시적으로 정의된 분포를 최적화하기 위한 새로운 알고리즘을 제시합니다.
- 이 알고리즘은 샘플링 과정의 결과 분포를 해당 과정의 매개 변수를 최적화함으로써 수정할 수 있게 합니다.
- 저자들은 샘플링과 최적화 단계를 하나의 루프에서 함께 수행하는 이 과정들의 일차 최적화를 위한 일반적인 틀을 도입하였습니다.
- 이 접근법은 샘플링을 확률 분포 공간에 대한 최적화 작업으로 보는 새로운 관점에 기초하며, 이중수준(bilevel) 최적화와 자동 암시적 미분의 최근 성과에 영감을 받았습니다.
- 연구진은 제안된 방법의 성능에 대한 이론적 보장을 제공하고, 실험을 통해 실제 세계의 설정에서 그 효과를 입증합니다.

### [SpiRit-LM: Interleaved Spoken and Written Language Model](https://arxiv.org/abs/2402.05755)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/FM461J9e_aEvcnXUqFT9Q.png)

Vote: 3

Authors: Maha Elbayad, Emmanuel Dupoux, Paul-Ambroise Duquenne, Itai Gat, Ruslan Mavlyutov, Tu Anh Nguyen, Robin Algayres, Benjamin Muller, Sravya Popuri, Juan Pino, Benoit Sagot, Marta R. Costa-jussa, Bokai Yu, Gabriel Synnaeve

- SPIRIT-LM이라는 새로운 다중 모드 언어 모델을 소개하며, 이 모델은 텍스트와 음성을 자유롭게 섞어 사용합니다.
- 기존에 사전 훈련된 텍스트 언어 모델을 기반으로 하여, 텍스트와 음성 단위에 연속적으로 훈련시켜 음성 모델로 확장합니다.
- 음성과 텍스트 시퀀스를 단일 토큰 세트로 연결하고, 작은 규모의 자동으로 큐레이트된 음성-텍스트 병렬 코퍼스를 사용하여 단어 수준에서의 인터리빙 방법으로 훈련합니다.
- SPIRIT-LM은 BASE 버전과 EXPRESSIVE 버전의 두 가지가 있으며, BASE는 음성 의미 단위를 사용하고, EXPRESSIVE는 의미 단위뿐만 아니라 톤과 스타일 단위를 사용하여 표현력을 모델링합니다.
- 두 버전 모두 텍스트는 BPE(subword byte-pair encoding) 토큰으로 인코딩됩니다.
- 결과적으로 이 모델은 텍스트 모델의 의미적 능력과 음성 모델의 표현력을 모두 보여줍니다.
- 추가로, SPIRIT-LM이 다양한 모달리티에 걸쳐 몇 가지 새로운 과제(예: 음성 인식(ASR), 텍스트-음성 변환(TTS), 음성 분류)를 소수의 예시를 통해 학습할 수 있음을 보여줍니다.

### [Question Aware Vision Transformer for Multimodal Reasoning](https://arxiv.org/abs/2402.05472)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Ythq1YUQ9FF-EfRwnf0SQ.png)

Vote: 3

Authors: Ron Litman, Aviad Aberdam, Elad Ben Avraham, Shai Mazor, Roy Ganz, Oren Nuriel, Yair Kittenplon

- 비전-언어(VL) 모델은 멀티모달 추론에서 중요한 연구 집중을 받으며 상당한 발전을 이루었습니다.
- 기존의 아키텍처는 비전 인코더, 대규모 언어 모델(LLM), 시각적 특징을 LLM의 표상 공간과 정렬시키는 프로젝션 모듈로 구성되어 있습니다.
- 그러나 비전 인코딩 과정이 사용자 쿼리, 종종 이미지 관련 질문의 형태로부터 분리되어 있어 이미지의 질문-특정 요소에 최적으로 조율되지 않는 한계가 있습니다.
- 이를 해결하기 위해, 우리는 비전 인코더 안에 질문 인식을 직접 내장하는 QA-ViT, Question Aware Vision Transformer 접근법을 소개합니다.
- 이 통합은 제시된 질문과 관련된 이미지 측면에 초점을 맞춘 동적인 시각적 특징들을 결과적으로 만듭니다.
- QA-ViT는 모델에 구애받지 않으며 효율적으로 어떤 VL 아키텍처에도 통합될 수 있습니다.
- 다양한 멀티모달 아키텍처에 우리의 방법을 적용하는 것의 효과를 보여주는 광범위한 실험을 통해, 다양한 과제에 걸쳐 일관된 개선을 이루고 시각적 및 장면 텍스트 이해를 향상시킬 잠재력을 보여줍니다.

### [InstaGen: Enhancing Object Detection by Training on Synthetic Dataset](https://arxiv.org/abs/2402.05937)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/26C8KEwqUQefJRNmx6ABe.png)

Vote: 2

Authors: Zequn Jie, Chengjian Feng, Yujie Zhong, Lin Ma, Weidi Xie

- 본 논문에서는 확산 모델로 부터 생성된 합성 데이터셋을 훈련에 활용함으로써 객체 감지 능력을 향상시키는 새로운 패러다임을 소개합니다.
- 사전 훈련된 생성적 확산 모델에 인스턴스 수준의 그라운딩 헤드를 통합하여 생성된 이미지 내 임의의 인스턴스를 국소화하는 기능을 강화합니다.
- 그라운딩 헤드는 기존 객체 감지기로부터의 지도와 새로운 셀프 트레이닝 기법을 사용하여 범주 명칭의 텍스트 임베딩을 확산 모델의 지역적 시각 특성과 맞추도록 훈련됩니다.
- 이러한 고급 확산 모델인 InstaGen은 객체 감지를 위한 데이터 생성기로 사용될 수 있습니다.
- InstaGen으로부터 생성된 합성 데이터셋으로 훈련시킨 객체 감지기가 개방형 어휘(+4.5 AP) 및 데이터 희소 시나리오(+1.2에서 5.2 AP)에서 기존의 최신 방법들보다 우수한 성능을 보여주는 것을 실험을 통해 증명합니다.

### [Driving Everywhere with Large Language Model Policy Adaptation](https://arxiv.org/abs/2402.05932)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ekZreZJL4eFxtkipwOG7A.png)

Vote: 2

Authors: Sushant Veer, Boyi Li, Marco Pavone, Boris Ivanovic, Jiageng Mao, Yue Wang, Karen Leung

- 자율 주행 차량(AVs)이 새로운 환경, 관습, 법규에 맞게 운전 행동을 적응하는 것은 오랫동안 문제되었으나, 본 논문에서는 LLaDA라는 간단하지만 강력한 도구를 소개하여 인간 운전자와 자율 주행 차량이 새로운 장소에서의 교통 규칙을 이해하고 임무와 운동 계획을 적응할 수 있도록 합니다.
- 대규모 언어 모델(LLMs)의 인상적인 제로샷 일반화 능력을 활용하여 지역 운전자 지침서에 기술된 교통 규칙을 해석함으로써, LLaDA는 새로운 위치에서 교통 규칙에 맞춰 운전을 가능하게 합니다.
- 광범위한 사용자 연구를 통해 LLaDA가 야생에서 예상치 못한 상황을 명확하게 하는 데 유용한 지침을 제공함을 보여주고 있습니다.
- 실제 세계 데이터셋에서 자율 주행 차량의 운동 계획 정책을 적응하는 LLaDA의 능력을 시연하였으며, 모든 측정 기준에 대해 기준선 계획 접근 방식을 능가함을 보여 줍니다.
- 더 자세한 정보를 위해 해당 연구의 웹사이트(https://boyiliee.github.io/llada)를 방문할 것을 권장합니다.

