## Daily Papers (2024-02-02)

### [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8pLMrK_lGs2SpPHsKfS3Z.png)

Vote: 42

Authors: William Merrill, Akshita Bhagia, +, Yuling Gu, David Atkinson, Jack Hessel, Khyathi Raghavi Chandu, Iz Beltagy, Pete Walsh, Yanai Elazar, Dirk Groeneveld, Shane Arora, Russell Authur, Hamish Ivison, Rodney Kinney, Ian Magnusson, Jennifer Dumas, Oyvind Tafjord, Arman Cohan, Ananya Harsh Jha, Yizhong Wang, Jacob Morrison, Tushar Khot

- 언어 모델(Language Models, LMs)이 NLP 연구 및 상업 제품에 널리 사용되면서 이들의 상업적 중요성이 급증하고, 가장 강력한 모델들은 독점적 인터페이스 뒤에 숨겨져 있어, 훈련 데이터, 아키텍처, 개발 과정의 중요 세부 사항이 공개되지 않고 있습니다.
- 이러한 모델들을 과학적으로 연구하는 것, 즉 그들의 편향과 잠재적 위험을 포함한 중요한 세부 사항을 공개하는 것은 매우 중요하여, 연구 커뮤니티가 강력하면서도 진정으로 개방된 언어 모델에 접근할 수 있도록 하는 데 필수적입니다.
- 이 기술 보고서는 최첨단, 진정으로 개방된 언어 모델인 OLMo와 언어 모델링의 과학을 구축하고 연구하는 프레임워크의 첫 번째 릴리스를 자세히 설명합니다.
- 대부분의 이전 노력이 모델 가중치 및 추론 코드만을 공개한 것과 달리, OLMo는 훈련 데이터, 훈련 및 평가 코드를 포함한 전체 프레임워크를 공개합니다.
- 이 릴리스가 개방형 연구 커뮤니티를 강화하고 새로운 혁신의 물결을 촉진하는 데 기여하기를 바랍니다.

### [Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/abs/2402.00159)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/eYrEOmWYkrbgPLjMkJ89w.png)

Vote: 31

Authors: Aakanksha Naik, Akshita Bhagia, Niklas Muennighoff, +, David Atkinson, Matthew E. Peters, Yanai Elazar, Valentin Hofmann, Nathan Lambert, Russell Authur, Rodney Kinney, Ian Magnusson, Luca Soldaini, Li Lucy, Xinxi Lyu, Jennifer Dumas, Khyathi Chandu, Ben Bogin, Dustin Schwenk, Ananya Harsh Jha, Sachin Kumar, Crystal Nam, Jacob Morrison

- 언어 모델은 다양한 자연어 처리 작업에 중요한 기술이 되었지만, 최고 성능 모델 개발에 관한 많은 세부사항은 보고되지 않았습니다.
- 특히 상업적 언어 모델은 데이터에 대한 정보를 거의 제공하지 않고, 오픈 모델도 훈련 데이터셋이나 정확한 재현 방법을 드물게 공개합니다.
- 이러한 상황은 트레이닝 데이터가 모델 능력에 어떻게 영향을 주고 제한을 형성하는지 이해하는 연구를 어렵게 만듭니다.
- 언어 모델 전처리 연구를 촉진하기 위해, 웹 콘텐츠, 과학 논문, 코드, 공공 도메인 책, 소셜 미디어, 백과사전 자료 등 다양한 출처를 혼합하여 제작된 3조 토큰 분량의 영어 코퍼스인 'Dolma'를 공개합니다.
- 추가적인 실험과 작업의 재현을 가능하게 하기 위해 데이터 큐레이션 툴킷도 오픈 소스로 제공합니다.
- 이 보고서에서는 Dolma의 설계 원칙, 구축 세부사항, 내용 요약을 문서화하고 있으며, 중간 단계의 Dolma를 사용하여 언어 모델을 훈련하는 데서 얻은 지식을 공유합니다.
- 콘텐츠 필터, 중복 제거, 다중 소스 혼합 등 중요한 데이터 큐레이션 관행에 대한 분석과 실험 결과가 보고서에 포함되어 있습니다.
- Dolma는 OLMo 훈련에 사용되었으며, OLMo는 언어 모델링의 과학을 구축하고 연구하기 위해 설계된 최신 오픈 언어 모델 및 프레임워크입니다.

### [Can Large Language Models Understand Context?](https://arxiv.org/abs/2402.00858)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/2vhiTAdCSsDrf-u-pmdMO.png)

Vote: 11

Authors: Joel Ruben Antony Moniz, Site Li, Dhivya Piraviperumal, Jiarui Lu, Yuan Zhang, Bo-Hsiang Tseng, Yilun Zhu, Shruti Bhargava, Hong Yu

- 대규모 언어 모델(Large Language Models, LLMs)이 인상적인 수준으로 인간 언어의 맥락 이해 능력을 보여주고 있지만, 맥락 이해에 관한 언어학적 능력을 탐구하는 연구는 제한적이다.
- 이 논문은 기존 데이터셋을 수정하여 생성 모델 평가에 적합하도록 맥락 이해 벤치마크를 도입하여, 네 가지 고유한 작업과 아홉 개의 데이터셋을 통해 모델의 맥락 이해 능력을 평가한다.
- 연구 결과, 사전 훈련된 밀집 모델들은 최신의 미세 조정(fine-tuned) 모델들에 비해 미묘한 맥락 특성을 이해하는 데 어려움을 겪는 것으로 나타났다.
- 실제로 중요한 LLM 압축의 맥락에서, 3비트 사후 훈련 양자화(quantization)가 벤치마크의 성능에 다양한 영향을 미친다는 점을 발견하였다.
- 이러한 시나리오들에 대한 광범위한 분석을 통해 실험 결과를 뒷받침한다.

### [AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning](https://arxiv.org/abs/2402.00769)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/y8gnjxdLaA-k_0uykjiBF.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/y8gnjxdLaA-k_0uykjiBF.mp4" muted="false"></video></div>

Vote: 11

Authors: Guanglu Song, Fu-Yun Wang, Hongsheng Li, Weikang Bian, Xiaoyu Shi, Yu Liu, Zhaoyang Huang

- 동영상 확산 모델은 높은 연결성과 고화질의 비디오를 생성할 수 있는 능력으로 점점 더 많은 관심을 받고 있지만, 순차적인 잡음 제거 과정이 계산 집약적이고 시간이 많이 소요되어 응용 프로그램에 제한을 두고 있다.
- 기존 이미지 확산 모델의 샘플링을 가속화하기 위한 일관성 모델(CM) 및 조건부 이미지 생성에 성공적으로 확장된 잠재 일관성 모델(LCM)에서 영감을 받아, AnimateLCM을 제안하여 최소한의 단계로 고화질 비디오 생성을 가능하게 한다.
- 원시 비디오 데이터 세트에 대한 직접적인 일관성 학습 대신, 이미지 생성 규범과 동작 생성 규범을 분리하여 증류하는 분리된 일관성 학습 전략을 제안하며, 이는 훈련 효율성을 향상시키고 생성된 비주얼 품질을 강화한다.
- 안정된 확산 커뮤니티의 플러그 앤 플레이 어댑터들을 결합하여 다양한 기능(예: 제어 가능한 생성을 위한 ControlNet)을 달성할 수 있도록, 샘플링 속도에 영향을 주지 않으면서 기존 어댑터를 증류된 텍스트 조건부 비디오 일관성 모델에 적응시키거나 처음부터 어댑터를 훈련시키는 효율적인 전략을 제안한다.
- 제안된 전략은 이미지 조건부 비디오 생성 및 레이아웃 조건부 비디오 생성 모두에서 최고 성능을 달성하여 검증되었다.
- 실험 결과는 제안된 방법의 효과를 검증한다. 코드와 가중치는 공개될 예정이며, 더 많은 세부 사항은 https://github.com/G-U-N/AnimateLCM에서 확인할 수 있다.

### [SymbolicAI: A framework for logic-based approaches combining generative models and solvers](https://arxiv.org/abs/2402.00854)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6eSoDV09FVhBzGIHQPG9v.png)

Vote: 10

Authors: Claudiu Leoveanu-Condrei, Markus Holzleitner, Sepp Hochreiter, Werner Zellinger, Marius-Constantin Dinu

- SymbolicAI는 개념 학습 및 생성 프로세스 흐름 관리에 대한 논리 기반 접근 방식을 사용하는 다재다능하고 모듈식 프레임워크를 소개합니다.
- 이 프레임워크는 대규모 언어 모델(Large Language Models, LLMs)을 의미 파서로 활용하여 자연어 및 형식 언어 지시사항에 기반한 작업을 수행하게 함으로써 기호 추론(symbolic reasoning)과 생성 AI 사이의 간극을 메웁니다.
- 확률 프로그래밍 원리를 활용하여 복잡한 과제를 해결하고, 각각의 장점을 가진 미분 가능한 및 클래식 프로그래밍 패러다임을 사용합니다.
- 데이터 스트림 조작을 위한 다형성(poly-morphic), 구성적(compositional), 자기 참조적(self-referential) 연산 세트를 도입하여 LLM 출력을 사용자 목표에 맞게 조정합니다.
- 이를 통해 제로-샷 및 퓨-샷 학습 능력을 갖춘 다양한 기초 모델(foundation models)과 특정 문제를 해결하는 데 능숙한 특수화된 모델이나 솔버 간의 능력을 전환할 수 있습니다.
- 프레임워크는 설명 가능한 계산 그래프를 생성하고 평가하는 데 도움이 됩니다.
- 연구진은 계산 그래프를 평가하기 위한 질적 척도인 'Vector Embedding for Relational Trajectory Evaluation through Cross-similarity' 또는 줄여서 VERTEX 점수와 함께 이를 평가한 경험적 점수를 도입했습니다.
- 프레임워크는 복잡한 워크플로우 세트에서 다양한 최신 LLM을 비교하는 벤치마크를 제안하며, 관련 코드베이스와 벤치마크는 아래에 링크되어 있습니다.

### [CroissantLLM: A Truly Bilingual French-English Language Model](https://arxiv.org/abs/2402.00786)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/X_aSP3szOsAkx8iM3YlXF.png)

Vote: 10

Authors: Pierre Colombo, Céline Hudelot, Manuel Faysse, François Yvon, Ricardo Rei, Duarte Alves, Caio Corro, André Martins, António Loison, João Alves, Antoni Bigata Casademunt, Nuno Guerreiro, Pedro Martins, Nicolas Boizard, Gautier Viaud, Patrick Fernandes

- 본 논문에서는 연구 및 산업 커뮤니티에 고성능의 오픈 소스 이중 언어 모델인 'CroissantLLM(크루아상LLM)'을 소개하며, 이 모델은 1:1 영어-프랑스어 사전 훈련 데이터 비율을 기반으로 1.3B 언어 모델을 사전 훈련함으로써 소비자급 로컬 하드웨어에서 빠르게 작동합니다.
- 우리는 특별히 선별된 고품질의 다양한 데이터 소스를 포함하는 프랑스어 분할 데이터셋을 공개하고, 프랑스어에서의 성능을 평가하기 위해 분류 및 생성 작업을 포함하는 새로운 벤치마크 'FrenchBench'를 개발하였습니다.
- 투명성을 기반으로 하여 대규모 언어 모델 연구를 촉진하기 위해 코드베이스, 다양한 모델 크기의 체크포인트, 훈련 데이터 분포, 훈련 단계, 그리고 세부적으로 조정된 챗봇 모델 및 강력한 번역 모델을 공개합니다.
- FMTI 프레임워크를 통해 평가된 모델은 투명성 기준의 81%를 충족함으로써 대부분의 오픈 이니셔티브의 점수를 크게 웃돌았습니다.
- 이 작업은 영어 중심의 이전 작업들로부터 벗어나, 언어 모델의 다국어 처리에 대한 우리의 이해를 강화하는 NLP 분야에 새로운 통찰력을 제공합니다.

### [Efficient Exploration for LLMs](https://arxiv.org/abs/2402.00396)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/9XJKQOA2xUxVgXyoOGtte.png)

Vote: 9

Authors: Vikranth Dwaracherla, Benjamin Van Roy, Botao Hao, Seyed Mohammad Asghari

- 대규모 언어 모델을 개선하기 위해 효율적인 탐색이 인간 피드백 수집에 상당한 이점을 제공한다는 증거를 제시합니다.
- 실험에서, 에이전트는 피드백을 기반으로 보상 모델에 적합하게 순차적으로 질의를 생성합니다.
- 가장 성능이 좋은 에이전트는 에피스테믹(epistemic) 신경망으로 표현된 불확실성을 가진 더블 톰슨 샘플링을 사용하여 질의를 생성합니다.
- 효율적인 탐색은 훨씬 적은 수의 질의로도 높은 수준의 성능을 가능하게 함을 결과가 증명합니다.
- 불확실성 추정과 탐색 방안의 선택이 모두 중요한 역할을 하며 이를 통해 효율적인 탐색의 중요성을 강조합니다.

### [Machine Unlearning for Image-to-Image Generative Models](https://arxiv.org/abs/2402.00351)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/tDZ3Uu0zbnKBbHpvTAfNA.png)

Vote: 8

Authors: Radu Marculescu, Guihong Li, Chun-Fu, Hsiang Hsu, Chen

- 기계 학습에서의 '잊기'는 모델로부터 특정 데이터 샘플을 의도적으로 삭제하여 엄격한 규제를 준수하는 새로운 패러다임으로 주목받고 있지만, 주로 분류 모델에 초점을 맞추고 있어 생성 모델의 학습된 데이터를 잊는 연구는 상대적으로 미개척 지역이다.
- 이 논문은 이미지 대 이미지 생성 모델용 기계 학습의 '잊기'에 대한 통합 프레임워크를 제공하고, 이를 토대로 성능 저하가 거의 없으면서도 '잊어야 할' 샘플에서 정보를 효과적으로 제거하는 계산 효율적인 알고리즘을 제안한다.
- 제안된 알고리즘은 이론적 분석을 통해 뒷받침되며, ImageNet-1K와 Places-365와 같은 대규모 데이터셋에서 '유지해야 할' 샘플의 가용성에 의존하지 않고, 데이터 보유 정책에 부합한다는 것을 실증적 연구를 통해 보여준다.
- 특히, 이미지 대 이미지 생성 모델에 특화된 기계 '잊기'에 관한 시스템적인 이론적 및 실증적 탐구는 이 연구가 처음이다.
- 연구 결과물과 코드는 https://github.com/jpmorganchase/l2l-generator-unlearning 에서 확인할 수 있다.

### [AToM: Amortized Text-to-Mesh using 2D Diffusion](https://arxiv.org/abs/2402.00867)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/17CgB_AgvZABcWx2mMOss.png)

Vote: 8

Authors: Chaoyang Wang, Guocheng Qian, Hsin-Ying Lee, Junli Cao, Yash Kant, Igor Gilitschenski, Peiye Zhuang, Michael Vasilkovsky, Aliaksandr Siarohin, Jian Ren, Kfir Aberman, Sergey Tulyakov, Ivan Skorokhodov, Bernard Ghanem, Yuwei Fang

- 'AToM'이라고 불리는 텍스트에서 메시로 변환하는 새로운 프레임워크를 소개합니다. 이는 다수의 텍스트 프롬프트를 동시에 최적화하며 속도를 향상시킨 방식입니다.
- 기존의 텍스트-3D 방법들이 개별 프롬프트 최적화에 많은 시간을 요구하고 다각형 메시 이외의 다른 형태를 출력하는 반면, AToM은 1초 미만으로 고품질의 질감 메시를 직접 생성하며 학습 비용을 약 10배 절감합니다.
- 새로운 개념의 삼면기반 텍스트-메시 아키텍처와 이 단계의 손실된 최적화 전략을 사용함으로써 안정적인 학습이 가능하고 확장성 역시 확보할 수 있습니다.
- 다양한 프롬프트 벤치마크에 대한 광범위한 실험을 통해 AToM은 기존의 손실된 접근 방식보다 4배 이상 높은 정확도(DF415 데이터세트 기준)를 달성하고 보다 구별 가능하며 고품질의 3D 출력물을 생산합니다.
- AToM은 뛰어난 일반화 능력을 보여주어, 기존의 프롬프트별 솔루션과 달리 추론 동안 추가적인 최적화 없이도 보지 못한 보간 프롬프트에 대하여 세밀한 3D 에셋을 제공합니다.

### [Transforming and Combining Rewards for Aligning Large Language Models](https://arxiv.org/abs/2402.00742)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/_S-dQdA05eQLF3cbp7P8G.png)

Vote: 6

Authors: Victor Veitch, Alex D'Amour, Jacob Eisenstein, Jonathan Berant, Sanmi Koyejo, Zihao Wang, Chirag Nagpal

- 인간의 선호도에 맞추어 언어 모델을 조정하는 일반적인 접근법은 우선 선호도 데이터에서 보상 모델을 학습한 뒤, 이 보상 모델을 사용하여 언어 모델을 업데이트하는 것입니다.
- 이 접근법에는 두 가지 밀접한 문제가 있습니다: 첫째, 보상 모델의 단조롭게 변환하는 것은 선호도 순위를 유지하지만, 다른 선택지보다 "더 나은" 변환 방법이 있는지의 문제, 둘째로, 언어 모델을 여러 특성에 맞추고자 할 때 여러 보상 모델을 어떻게 결합할 것인가 하는 문제입니다.
- 우리는 보상을 학습할 때 자주 쓰이는 Bradley-Terry 선호 모델로부터 확률적 해석을 사용하여 변환 방법에 대한 자연스러운 선택을 찾아냈습니다.
- 이러한 변환은 성능이 떨어지는 출력을 개선하는 데 집중하며, 이는 언더피팅(어떤 프롬프트가 개선되지 않는 상황)과 보상 해킹(모델이 보상 모델의 불완전성을 이용하는 상황)을 완화하는 데 도움이 됩니다.
- 또한, 변환된 보상의 합은 논리적인 접속사에 따라 연결되어, 출력이 모든 측정된 특성에 대해 "좋다"는 것에 대한 확률에 해당하는 것으로 고려됩니다.
- RLHF(Reward Learning from Human Feedback)를 사용하여 언어 모델을 도움이 되고 해가 되지 않도록 조정하는 실험에서, 변환되지 않은 기본 방식에 비해 상당한 개선이 있음을 보여줍니다.

### [EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models](https://arxiv.org/abs/2402.00518)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/QKRziDrqNV9s3yqHzJjA4.png)

Vote: 3

Authors: Yaliang Li, Jingren Zhou, Xuchen Pan, Yanxi Chen, Bolin Ding

- 본 연구는 조기 종료 기능을 갖는 대형 언어 모델(LLMs)을 학습/튜닝하기 위한 경제적이고 가벼운 EE-Tuning 방법을 소개합니다.
- EE-Tuning은 기존의 전체 매개변수 사전 교육 방식과 달리, 사전 교육된 표준 LLM에 추가적인 조기 종료 층을 효율적인 매개변수 방식으로 튜닝하여 계산 자원과 훈련 데이터를 현저하게 절감합니다.
- 큰 성능 최적화를 통해 뛰어난 트레이닝 효율성을 달성하며, 3D 병렬성과 완전 호환되는 확장성 덕분에 적용 범위가 넓습니다.
- 시스템적인 실험을 통해 EE-Tuning의 효과를 검증하였으며, 제한된 트레이닝 예산으로도 효과적인 조기 종료 LLM 추론이 가능함을 확인하였습니다.
- 조기 종료 LLM을 커뮤니티에 접근 가능하게 하기 위해, EE-Tuning의 구현 소스 코드를 https://github.com/pan-x-c/EE-LLM 에서 공개하였습니다.

