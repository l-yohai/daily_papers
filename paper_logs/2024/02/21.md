## Daily Papers (2024-02-21)

### [Neural Network Diffusion](https://arxiv.org/abs/2402.13144)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/e1_fO49BMGWUOSbjH3fui.png)

Vote: 56

Authors: Yukun Zhou, Zelin Zang, Kai Wang, Zhuang Liu, Zhaopan Xu, Trevor Darrell, Yang You

- 확산 모델이 이미지 및 비디오 생성에서 눈에 띄는 성공을 거둔 것처럼, 이 연구에서는 확산 모델이 고성능 신경망 파라미터도 생성할 수 있음을 보여줍니다.
- 이 접근법은 오토인코더와 표준 잠재 확산 모델을 사용하는 것으로 간단합니다.
- 오토인코더는 훈련된 신경망 파라미터의 부분 집합에 대한 잠재적 표현을 추출합니다.
- 확산 모델은 무작위 잡음으로부터 이 잠재 파라미터 표현을 합성하는 방법을 학습합니다.
- 그런 다음 새로운 표현을 생성하여 오토인코더의 디코더를 통과시키고, 그 출력은 새로운 신경망 파라미터 부분 집합으로 바로 사용됩니다.
- 다양한 아키텍처 및 데이터셋에 걸쳐, 우리의 확산 과정은 훈련된 네트워크보다 비슷하거나 향상된 성능을 지닌 모델을 일관적으로 생성하며, 추가 비용도 최소화합니다.
- 특히, 생성된 모델이 훈련된 네트워크와 다르게 작동한다는 것을 경험적으로 발견했습니다.
- 우리의 결과는 확산 모델의 다재다능한 사용에 대한 더 많은 탐구를 장려합니다.

### [Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models](https://arxiv.org/abs/2402.13064)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pV9CsBvcxsXZo13IMztIi.png)

Vote: 20

Authors: Haoyang Huang, Zeqiang Huang, Shaohan Huang, Dongdong Zhang, Zhengyang Tang, Benyou Wang, Xin Cheng, Haoran Li, Wai Lam, Li Dong, Si-Qing Chen, Xingxing Zhang, Furu Wei, Chaojun Wang, Xiaolong Huang, Qingxiu Dong, Wei Lu, Xun Wang, Yuxian Gu, Zhifang Sui

- '일반화된 지침 튜닝(GLAN)'은 대규모 언어 모델(LLMs)의 지침 튜닝을 위한 일반적이고 확장 가능한 방법을 소개합니다.
- 이 방법은 기존 데이터셋이나 예시를 기반으로 하지 않고, 인간 지식과 능력의 사전 정리된 분류체계만을 활용하여 모든 학문에 걸쳐 대규모 합성 지침 데이터를 생성합니다.
- 인간 교육 체계의 체계적 구조에 영감을 받아, 언어 모델을 활용하여 인간 지식과 능력을 여러 분야, 부문 및 구체적인 학문으로 분해하여 준자동적으로 분류체계를 구축합니다.
- 각 학문의 포괄적인 과목 리스트를 만든 후, 언어 모델을 이용하여 각 과목에 맞는 교육 과정을 설계합니다.
- 상세한 핵심 개념을 바탕으로, 다양한 학문과 기술 영역 전반에 걸쳐 광범위하게 다양한 지침을 생성할 수 있습니다.
- 대규모 언어 모델(Mistral 등)을 이용한 광범위한 실험을 통해, GLAN은 수학적 추론부터 코딩, 학문적 시험, 논리적 추론, 일반적인 지침 수행에 이르기까지 특정 작업 데이터 없이도 여러 면에서 뛰어난 성능을 보여줍니다.
- 또한, GLAN은 쉽게 맞춤화할 수 있으며, 새로운 노드를 분류체계에 추가함으로써 새로운 분야나 기술을 추가할 수 있습니다.

### [Instruction-tuned Language Models are Better Knowledge Learners](https://arxiv.org/abs/2402.12847)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8mfynPVJrF-DhLkflr77T.png)

Vote: 10

Authors: Chunting Zhou, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Xi Victoria Lin, Zhengbao Jiang, Srinivasan Iyer, Wen-tau Yih, Graham Neubig

- 대규모 언어 모델(LLM) 기반 어시스턴트가 정보 요구의 변화에 효과적으로 적응하도록 하기 위해서는 새로운 자료에 대한 지속적인 훈련을 통해 사실 지식을 업데이트할 수 있어야 합니다.
- 표준적인 훈련 방식은 새로운 문서에 대한 지속적인 사전 훈련과 질문-응답(QA) 쌍에 대한 지시사항 튜닝(instruction-tuning)을 포함하나, 이 방식으로 훈련된 LLM들은 질문에 답하는 데 어려움을 겪는 것으로 나타났습니다.
- 문서는 복잡하게 매듭진 방식으로 많은 사실 진술들을 엮어내는 반면, QA 쌍은 일반적으로 간단하기 때문에, 복잡한 문서에서 지식을 인코딩하는 과정이 질문을 통해 어떻게 지식에 접근하는지를 고려해야 할 필요가 있습니다.
- 이에 기반하여, 연구팀은 문서 훈련 이전에 QA 쌍에 대해 먼저 지시사항 튜닝을 진행하는 사전 지시사항 튜닝(pre-instruction-tuning, PIT) 방법을 제안합니다.
- PIT는 표준 지시사항 튜닝과 달리 문서 훈련 이후가 아닌 이전에 지식 추출 방법을 학습합니다.
- 광범위한 실험과 손실 연구를 통해 PIT가 새로운 문서로부터 지식을 흡수하는 LLM의 능력을 크게 향상시키며, 표준 지시사항 튜닝보다 17.8% 더 나은 성능을 보임을 입증하였습니다.

### [Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields](https://arxiv.org/abs/2402.13252)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0B3OkGkzG0KRfr1G_-pcu.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0B3OkGkzG0KRfr1G_-pcu.mp4" muted="false"></video></div>

Vote: 10

Authors: Bo-Yu Cheng, Wei-Chen Chiu, Yu-Lun Liu

- 본 논문에서는 2D 이미지만을 활용하여 카메라 포즈와 장면 기하학을 나타내는 분해된 저랭크 텐서를 동시에 정제하는 알고리즘을 제안한다.
- 1D 신호에 바탕을 둔 예비 연구를 통해, 표준적인 결합 포즈 최적화가 보편적인 기반을 둔 NeRF에서 쉽게 부적합한 해를 초래할 수 있음을 3D 상황에 연관시켜 제시한다.
- 또한, 주파수 스펙트럼 분석을 기반으로, 2D 및 3D 방사 영역(radiance fields)에 가우시안 필터를 적용하고 경우에 따라 조정된 학습 일정을 진행하여 카메라 포즈 최적화를 가능하게 한다.
- 분해된 저랭크 텐서의 분해 속성을 활용하여, 매우 적은 계산 비용으로 3D 컨볼루션의 동등한 효과를 달성한다.
- 조인트 최적화의 강인성과 안정성을 더욱 향상시키기 위해, 부드러운 2D 관찰 기법, 무작위로 조정된 커널 파라미터 및 경계 가이드 손실 마스크 기술을 제안한다.
- 광범위한 양적 및 질적 평가를 통해, 우리의 제안된 프레임워크가 새로운 시점 합성뿐만 아니라 최적화를 위한 빠른 수렴에서 우수한 성능을 달성함을 보여준다.

### [VideoPrism: A Foundational Visual Encoder for Video Understanding](https://arxiv.org/abs/2402.13217)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vPWIkmnRImesnlkqTVyAq.png)

Vote: 10

Authors: Ming-Hsuan Yang, Hao Zhou, Mikhail Sirotenko, David A. Ross, Rachel Hornung, Shen Yan, Rui Qian, Luke Friedman, Jennifer J. Sun, Nitesh B. Gundavarapu, Liangzhe Yuan, Florian Schroff, Long Zhao, Yue Zhao, Boqing Gong, Tobias Weyand, Huisheng Wang, Ting Liu, Hartwig Adam

- 'VideoPrism'이라는 새로운 범용 비디오 인코더를 소개하며, 이는 단일 고정 모델을 사용하여 다양한 비디오 이해 작업을 처리합니다.
- VideoPrism은 3천6백만 개의 고품질 비디오-자막 쌍과 5억8천2백만 개의 잡음이 섞인 병렬 텍스트(예: 자동 음성 인식(ASR) 전사본)가 포함된 이질적인 코퍼스에서 사전 학습됩니다.
- 사전 학습 접근법은 의미 있는 비디오 임베딩의 전역-지역 증류(distillation)와 토큰 순서 섞기 체계를 통해 개선되었으며, VideoPrism이 비디오와 관련된 귀중한 텍스트를 활용하면서 주로 비디오 모달리티에 집중하도록 합니다.
- 웹 비디오 질문 답변에서 과학을 위한 컴퓨터 비전에 이르기까지 네 가지 넓은 범주의 비디오 이해 작업에서 VideoPrism의 성능을 광범위하게 테스트했습니다.
- VideoPrism은 33개의 비디오 이해 벤치마크 중 30개에서 최고의 성능을 달성함으로써, 범용 비디오 인코더에 대한 새로운 기준을 제시합니다.

### [Video ReCap: Recursive Captioning of Hour-Long Videos](https://arxiv.org/abs/2402.13250)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Q3Ru9B88XuttYGZqcf3R_.png)

Vote: 10

Authors: Tushar Nagarajan, Ngan Ho, Lorenzo Torresani, Gedas Bertasius, Xitong Yang, Md Mohaiminul Islam

- 본 연구에서는 기존의 짧은 비디오 클립을 대상으로 한 캡셔닝 모델을 넘어, 몇 분에서 몇 시간에 이르는 긴 실세계 비디오를 처리하고 다양한 시간적 단계에 걸쳐 계층적 구조를 가진 내용에 대해 글을 생성하는 Video ReCap, 즉 재귀적 비디오 캡셔닝 모델을 제안합니다.
- Video ReCap 모델은 한쪽 끝에서는 1초 길이의 비디오부터 다른 한쪽 끝인 2시간짜리 비디오까지 극도로 다양한 길이의 비디오 입력을 처리할 수 있으며, 여러 계층에 걸쳐 비디오 캡션을 생성합니다.
- 이 재귀적 비디오-언어 구조는 다양한 비디오 계층 간의 시너지를 활용하여, 시간이 오래 걸리는 비디오도 효과적으로 처리할 수 있습니다.
- 동영상의 계층적 구조를 학습하기 위해 커리큘럼 학습 교육 방식을 사용하여, 원자적 동작을 설명하는 클립 수준 캡션부터 시작하여 세그먼트 수준의 묘사에 집중하고, 마지막으로 수시간에 걸친 비디오 요약을 생성합니다.
- 연구팀은 8,267개의 수작업으로 수집된 장거리 비디오 요약으로 Ego4D를 보충하여 Ego4D-HCap 데이터 세트를 소개합니다.
- 재귀 모델은 다양한 계층 수준에서 유연하게 캡션을 생성할 수 있을 뿐만 아니라 EgoSchema에서의 VideoQA와 같은 기타 복잡한 비디오 이해 작업에도 유용합니다.
- 데이터, 코드 및 모델은 https://sites.google.com/view/vidrecap에서 제공됩니다.

### [MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction](https://arxiv.org/abs/2402.12712)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-m9ie4V6MQQYiQcA_uvH9.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-m9ie4V6MQQYiQcA_uvH9.mp4" muted="false"></video></div>

Vote: 9

Authors: Shitao Tang, Dilin Wang, Vikas Chandra, Chengzhou Tang, Rakesh Ranjan, Jiacheng Chen, Fuyang Zhang, Yasutaka Furukawa, Yuchen Fan

- 본 논문에서는 카메라 포즈 없이 하나 또는 몇 개의 이미지를 통해 객체의 높은 해상도와 밀도 있는 시점들을 합성할 수 있는 신경 구조인 MVDiffusion++를 제시한다.
- '포즈 프리 아키텍처(pose-free architecture)'라 불리는 첫 번째 아이디어는, 카메라 포즈 정보를 명시적으로 사용하지 않고도 임의의 수의 조건부 및 생성된 시점들에 대한 3D 일관성을 학습하는 표준 2D 잠재 특징 간의 자체 주의(self-attention) 메커니즘에 기반한다.
- 두 번째 아이디어인 '시점 드롭아웃 전략(view dropout strategy)'은 훈련 중 상당한 수의 출력 시점을 버리는 방식이며, 이는 훈련 시 메모리 사용량을 감소시키고 테스트 시에 고밀도 및 고해상도 시점 합성을 가능하게 한다.
- MVDiffusion++은 Objaverse 데이터셋을 학습에, Google Scanned Objects 데이터셋을 표준 새로운 시점 합성과 3D 복원 지표에 대한 평가에 사용하며, 현재 최신 기술보다 현저히 우수한 성능을 달성한다.
- 또한, MVDiffusion++를 텍스트-이미지 생성 모델과 결합하여 텍스트-3D 어플리케이션 예시를 보여줌으로써 보다 확대된 활용 가능성을 시연한다.

### [A Touch, Vision, and Language Dataset for Multimodal Alignment](https://arxiv.org/abs/2402.13232)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/AfQoeNvvWP_Z5vGDmmU43.png)

Vote: 7

Authors: Huang Huang, Mustafa Mukadam, Joseph Ortiz, Roberto Calandra, Jaimyn Drake, Gaurav Datta, Letian Fu, Mike Lambeta, William Chung-Ho Panitch, Ken Goldberg

- 터치는 인간의 중요한 감각 모달리티이지만, 아직 다중 모달 생성 언어 모델에 통합되지 않았습니다.
- 이러한 격차를 메우기 위해, 연구진은 야생에서 수집한 44K의 시각-터치 쌍과 인간이 주석을 단 영어 언어 라벨(10%) 및 GPT-4V에서 생성된 텍스트 의사 라벨(90%)을 포함하는 새로운 데이터셋을 소개합니다.
- 이 데이터셋을 사용하여 개방형 어휘 분류를 위한 시각-언어 정렬 터치 인코더와 텍스트 생성을 위한 터치-비전-언어(TVL) 모델을 학습시켰습니다.
- 터치를 포함함으로써, TVL 모델은 기존의 어떤 모달리티 쌍에서도 훈련된 모델들에 비해 터치-비전-언어 정렬을 29% 향상시켰습니다.
- 전체 데이터셋 중 소수만이 인간에 의해 라벨링된 상태임에도 불구하고, TVL 모델은 새로운 터치-비전 이해 벤치마크에서 GPT-4V보다 12%, 오픈소스 비전-언어 모델보다 32% 개선된 시각-터치 이해력을 보여줍니다.
- 관련 코드와 데이터는 https://tactile-vlm.github.io에서 확인할 수 있습니다.

### [FlashTex: Fast Relightable Mesh Texturing with LightControlNet](https://arxiv.org/abs/2402.13251)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/CPo0eFkLMjUmG0IsYJLsx.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/CPo0eFkLMjUmG0IsYJLsx.mp4" muted="false"></video></div>

Vote: 6

Authors: Alexander Weiss, Deva Ramanan, Tinghui Zhou, Maneesh Agrawala, Timothy Omernick, Kangle Deng, Jun-Yan Zhu

- 3D 메시에 대한 텍스처를 수작업으로 생성하는 것은 시간이 많이 소요되므로, 본 논문에서는 사용자가 제공한 텍스트 프롬프트를 기반으로 3D 메시를 자동으로 텍스처링하는 빠른 접근 방식을 제안합니다.
- 제안된 방식은 결과 텍스처에서 조명과 표면 재질/반사율을 분리하여, 메시가 어떤 조명 환경에서도 적절하게 재조명하고 렌더링될 수 있게 합니다.
- LightControlNet이라는 새로운 텍스트-이미지 모델을 도입하여, 제어 가능한 네트워크(ControNet) 아키텍처를 바탕으로, 원하는 조명을 모델에 조건 이미지로 명시할 수 있습니다.
- 텍스트-텍스처 파이프라인은 두 단계로 텍스처를 구성하는데, 첫 번째 단계에서는 LightControlNet을 사용하여 메시의 시각적으로 일관된 희소 참조 뷰를 생성합니다.
- 두 번째 단계에서는 Score Distillation Sampling (SDS)에 기반한 텍스처 최적화를 적용해 텍스처 품질을 향상시키고 표면 재질과 조명을 분리합니다.
- 이 파이프라인은 이전의 텍스트-텍스처 방법보다 훨씬 더 빠르면서, 고품질이며 다시 조명할 수 있는 텍스처를 생성합니다.

### [The FinBen: An Holistic Financial Benchmark for Large Language Models](https://arxiv.org/abs/2402.12659)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/i2mCzy4HZyLUmOLnlU-vu.png)

Vote: 6

Authors: Dong Li, Yijing Xu, Kailai Yang, Zhiyang Deng, Duanyu Feng, Tianlin Zhang, Yueru He, Xiao Zhang, Ruoyu Xiang, Haoqiang Kang, Zheheng Luo, Zhiyuan Yao, Guojun Xiong, Yongfu Dai, Chenhan Yuan, Qianqian Xie, Ziyan Kuang, Zhiwei Liu, Zhengyu Chen, +, Yuechen Jiang, Mengxi Xiao, Weiguang Han

- 대규모 언어 모델(Large Language Models, LLMs)이 자연어 처리(NLP)를 변화시키고 다양한 분야에서의 가능성을 보여주었지만 금융 분야에서의 잠재력은 충분히 탐구되지 않았습니다.
- FinBen이라는 체계적인 금융 평가 벤치마크가 처음으로 소개되었으며, 이는 금융 영역에서 LLM들의 능력을 철저히 평가하기 위해 특별히 설계되었습니다.
- FinBen은 Cattell-Horn-Carroll 이론에서 영감을 받아 어려움에 따라 세 가지 스펙트럼으로 조직된 23개의 금융 작업을 포함하는 35개의 데이터셋을 포함하고 있습니다.
- 이 벤치마크는 유도 추리, 연합 기억, 양적 추론, 결정화된 지능 등 LLM의 인지 능력을 평가합니다.
- GPT-4, ChatGPT, 최신 Gemini를 포함한 15개의 대표적인 LLMs를 평가한 결과, 금융 분야 내에서의 강점과 한계에 대한 통찰력을 제공합니다.
- GPT-4는 수량화, 추출, 숫자 추론, 주식 거래 분야에서 선도적이었으며, Gemini는 생성 및 예측 분야에서 두각을 나타냈습니다.
- 두 모델 모두 복잡한 추출 및 예측 작업에서 어려움을 겪어, 목표 지향적인 개선이 필요함을 보여줍니다.
- 지시 튜닝(instruction tuning)은 간단한 작업의 성능을 향상시키는 데 도움이 되지만 복잡한 추론 및 예측 능력을 개선하는 데는 부족합니다.
- FinBen은 금융 분야에서 LLMs의 지속적인 평가를 추구하며, 작업 및 모델의 정기적인 업데이트를 통해 AI 발전을 촉진합니다.

### [TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization](https://arxiv.org/abs/2402.13249)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/PBfJ7ChEbHACyakpBwPSd.png)

Vote: 5

Authors: Siffi Singh, Jon Burnsky, Igor Shalyminov, Liyan Tang, Yu'an Yang, Song Feng, Saab Mansour, Hwanjun Song, Amy Wing-mei Wong, Kathleen McKeown, Lijia Sun, Jake W. Vincent, Hang Su, Yi Zhang

- 일대일 뉴스 요약 분야에서 사실 정확성에 대한 진전이 있었으나, 이러한 발전이 다른 텍스트 요약 영역에도 적용되는지에 대해 연구했습니다.
- 다양한 크기의 대규모 언어 모델(LLMs)이 생성한 주제 중심의 대화 요약에 대한 새로운 평가 벤치마크를 제안합니다.
- 이 요약들에 대해 사람이 판단한 이진 수준의 사실 일치성 주석과 사실과 일치하지 않는 문장에 대한 자세한 설명을 제공했습니다.
- LLM들이 대화 영역에서 주요한 사실 오류를 만들어내는 경향이 있으며, 이는 모델의 크기에 관계없이 나타나는 현상임을 분석을 통해 보여줍니다.
- GPT-4를 포함한 LLM이 이진 사실 평가자로써 활동할 때 성능이 낮았으며, 특수화된 사실 정확성 평가 지표들에 비해 뒤처졌다는 것을 발견했습니다.
- 또한, 정교한 오류 분류법을 사용한 환영 유형의 분석을 통해 모델이 생성한 요약본에서 다양한 오류 및 오류 분포가 있음을 발견했습니다.
- LLM 기반 평가자들 보다 비 LLM 기반 지표들이 모든 오류 유형을 더 잘 포착할 수 있음을 확인했습니다.

### [How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts](https://arxiv.org/abs/2402.13220)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/S2qy3Sw_vQjElbg1-hyt7.png)

Vote: 5

Authors: Yinfei Yang, Yusu Qian, Zhe Gan, Haotian Zhang

- 이 연구는 다양한 종류의 사기 정보에 대한 대응으로, Multimodal Large Language Models(MLLM)의 취약점을 정량적으로 분석합니다.
- 연구팀은 비존재하는 객체, 객체 수, 공간적 관계, 시각적 혼동 등 6가지 범주로 나뉘는 총 850개의 테스트 샘플을 담고 있는 MAD-Bench 벤치마크를 소개합니다.
- GPT-4V, Gemini-Pro, LLaVA-1.5, CogVLM 등 인기 있는 MLLMs에 대한 종합적인 분석을 제공하며, GPT-4V가 MAD-Bench에서 75.02%의 정확도를 달성하는 반면 다른 모델들은 5%에서 35%의 범위로 낮은 성능을 보입니다.
- 이전에 강인한 지시에 튜닝된 모델들인 LRV-Instruction 및 LLaVA-RLHF도 이 새로운 벤치마크에서 효과적이지 않음을 관찰했습니다.
- 사기적 질문에 대해 모델들이 다시 생각하도록 유도하는 추가적인 단락을 삽입하는 해결책을 제안함으로써 정확도가 두 배까지 상승할 수 있지만 절대적인 수치는 여전히 낮습니다.
- 연구팀은 MAD-Bench가 사기적 질문에 대한 모델들의 내성을 강화하기 위한 진일보한 연구를 자극하는 중요한 벤치마크로서의 역할을 할 것으로 기대합니다.

### [RealCompo: Dynamic Equilibrium between Realism and Compositionality Improves Text-to-Image Diffusion Models](https://arxiv.org/abs/2402.12908)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SOYjO6_xZICWp7jUMiibm.png)

Vote: 2

Authors: Ling Yang, Minkai Xu, Yong Tang, Bin Cui, Yaqi Cai, Xinchen Zhang, Zhaochen Yu, Yujiu Yang, Ye Tian, Jiake Xie

- 본 논문에서는 문자에서 이미지로의 변환에서 다중 객체 구성 생성에 어려움을 겪고 있는 기존 모델들의 한계를 극복하기 위한 새로운 훈련 없는, 전이 가능한 텍스트-이미지 생성 프레임워크인 RealCompo를 제안한다.
- RealCompo는 텍스트-이미지 및 레이아웃-이미지 모델의 장점을 활용하여 생성된 이미지의 리얼리즘과 구성성을 모두 향상시키는 것을 목표로 한다.
- 새롭고 직관적인 균형 장치가 도입되어 두 모델의 강점을 노이즈 제거 과정에서 동적으로 균형 조정할 수 있게 하며, 별도의 추가 훈련 없이도 모델을 플러그 앤 플레이 방식으로 사용할 수 있다.
- 다양한 실험을 통해 RealCompo가 다중 객체 구성 생성에서 최신 텍스트-이미지 모델 및 레이아웃-이미지 모델을 일관되게 능가하며, 생성된 이미지의 리얼리즘과 구성성을 만족스럽게 유지함을 보여주었다.
- 관련 코드는 https://github.com/YangLing0818/RealCompo 에서 확인할 수 있다.

