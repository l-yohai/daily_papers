## Daily Papers (2024-02-29)

### [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/MBjSFighU68b4s8iAx5Iv.png)

Vote: 291

Authors: Hongyu Wang, Li Dong, Lei Wang, Ruiping Wang, Shaohan Huang, Lingxiao Ma, Wenhui Wang, Jilong Xue, Furu Wei, Shuming Ma

- 최근 연구인 BitNet은 1비트 대규모 언어 모델(LLM)의 새로운 시대를 열고 있다.
- 본 논문에서는 각각의 파라미터가 삼원수{-1, 0, 1}인 1비트 LLM 변형체인 BitNet b1.58을 소개하고 있다.
- 이 모델은 동일한 모델 크기와 훈련 토큰을 가진 전체 정밀도(FP16 또는 BF16) 트랜스포머 LLM과 복잡도와 종단 태스크 성능 모두에서 동등하게 매치된다.
- 1.58비트 LLM은 대기 시간, 메모리, 처리량 및 에너지 소비와 관련해 훨씬 더 비용 효율적이다.
- 더 중요한 것은, 1.58비트 LLM이 고성능이면서도 비용 효율적인 새로운 세대의 LLM을 훈련하기 위한 새로운 확대 법칙과 레시피를 정의한다는 것이다.
- 또한, 새로운 계산 패러다임을 가능하게 하며 1비트 LLM에 최적화된 특정 하드웨어를 설계하기 위한 문을 연다.

### [EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions](https://arxiv.org/abs/2402.17485)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Jpuu71oibjBK1VrDwppG5.png)

Vote: 119

Authors: Liefeng Bo, Qi Wang, Linrui Tian, Bang Zhang

- 본 연구에서는 오디오 신호와 얼굴 움직임 사이의 동적이고 미묘한 관계에 집중하여, 현실적이고 표현력 있는 토킹 헤드 비디오 생성의 도전과제에 대응합니다.
- 기존 기술들이 인간의 다양한 표정과 개인의 독특한 얼굴 스타일을 포착하는데 종종 실패함을 지적하고, 이러한 문제점을 해결하기 위해 중간 3D 모형이나 얼굴 랜드마크 없이도 직접 오디오에서 비디오로의 합성 방식을 사용하는 새로운 프레임워크 EMO를 제안합니다.
- EMO 방법은 비디오 내에서 원활한 프레임 전환과 일관된 신원 유지를 보장하며, 매우 표현력 있고 생생한 애니메이션을 만들어냅니다.
- 실험 결과에 따르면 EMO는 설득력 있는 말하기 비디오뿐만 아니라 다양한 스타일의 노래하는 비디오도 생산할 수 있으며, 표현력과 현실성 면에서 기존 최고의 기술들을 크게 뛰어넘는다고 합니다.

### [Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](https://arxiv.org/abs/2402.17177)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/qkWMyZ3B6nFw9ScqbdkiM.png)

Vote: 64

Authors: Yuan Li, Yue Huang, Hanchi Sun, Lifang He, Lichao Sun, Zhiling Yan, Ruoxi Chen, Yixin Liu, Zhengqing Yuan, Chujie Gao, Kai Zhang, Jianfeng Gao

- 2024년 2월에 OpenAI에서 발표한 텍스트를 비디오로 생성하는 인공지능 모델인 'Sora'에 대한 심층적인 리뷰를 제공합니다.
- 모델은 실제 혹은 상상의 장면들을 텍스트 지시에 따라 비디오로 생성할 수 있는 능력과 물리 세계 시뮬레이션의 가능성을 보여줍니다.
- 공개된 기술 보고서와 역공학을 통해, Sora가 어떻게 "세계 시뮬레이터"로 만들어졌는지에 대한 기술적 배경을 탐구합니다.
- 영화 제작, 교육, 마케팅 등 다양한 산업 분야에서 Sora의 응용과 잠재적 영향을 자세히 설명합니다.
- 안전하고 편향되지 않은 비디오 생성을 보장하는 것과 같은 Sora의 대규모 배포를 위해 해결되어야 할 주요 도전과제와 한계를 논의합니다.
- 마지막으로, 향후 Sora와 비디오 생성 모델의 발전 방향을 논의하며, 영역의 진보가 새로운 인간-AI 상호작용 방식을 가능하게 하고 비디오 생성의 생산성과 창의성을 촉진할 수 있는 방안을 탐색합니다.

### [DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Model](https://arxiv.org/abs/2402.17412)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/G7DpUf36R0brB1zRVJ-If.png)

Vote: 18

Authors: Nityanand Mathur, Shyam Marjit, Harshit Singh, Sayak Paul, Pin-Yu Chen, Chia-Mu Yu

- 'DiffuseKronA'는 주제 기반의 텍스트-이미지 생성 모델의 파라미터 효율성과 이미지 합성 품질 사이의 균형을 개선하는 새로운 방법을 제시한다.
- DreamBooth와 BLIP-Diffusion의 선행 연구들이 뛰어난 결과를 보였지만, 많은 파라미터 요구량과 강한 파인튜닝 요구로 인해 제한된 측면이 있었다.
- 'DiffuseKronA'는 크로네커 곱을 기반으로 한 적응 모듈을 도입하여 LoRA-DreamBooth 대비 35%, 원래 DreamBooth 대비 99.947%의 파라미터를 감소시켰다.
- 이 방법은 높은 이미지 합성 품질을 유지하면서 하이퍼파라미터의 민감도 문제를 완화시키고, 광범위한 하이퍼파라미터 범위에서 일관된 고품질 이미지 생성을 가능하게 한다.
- 파라미터 효율성 뿐만 아니라 해석 가능한 더 조절 가능한 분해(decomposition)를 통해 최대 50%까지 파라미터 감소를 달성하면서도 LoRA-Dreambooth와 비슷한 결과를 제공한다.
- 다양하고 복잡한 입력 이미지와 텍스트 프롬프트에 대한 평가에서 'DiffuseKronA'는 기존 모델들을 넘어서는 높은 품질의 다양한 이미지를 생성하였으며, 향상된 충실도와 더 정확한 색상 분포를 달성했다.
- 제안된 방법은 개인화된 확산 모델을 위한 파라미터 효율적인 합성 방법의 중요한 발전을 나타내며, 해당 프로젝트 페이지에는 코드 및 미리 훈련된 체크포인트 링크를 포함하고 있다: https://diffusekrona.github.io/.

### [When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method](https://arxiv.org/abs/2402.17193)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/OHI5lrgnO3IsfEp2p9AJm.png)

Vote: 15

Authors: Biao Zhang, Orhan Firat, Colin Cherry, Zhongtao Liu

- 대규모 언어 모델 (LLM)의 미세조정 성능에 미치는 다양한 스케일링 요소의 영향력을 이해하기 위해, 모델 크기, 사전 훈련 데이터 크기, 새로운 미세조정 파라미터 크기 및 미세조정 데이터 크기 등을 포함하는 체계적인 실험을 수행하였습니다.
- 전체 모델 튜닝(Full-Model Tuning, FMT)과 파라미터 효율 튜닝(Parameter Efficient Tuning, PET, 프롬프트 튜닝 및 LoRA를 포함)의 두 가지 미세조정 방식의 스케일링 동작을 데이터가 제한된 상황에서 탐구하였습니다.
- 이중언어 대규모 언어 모델들을 사용한 실험을 통해, 미세조정 데이터 크기와 각각의 다른 스케일링 요인 사이에 멱법칙에 기반한 곱셈조인트 스케일링 법칙을 따르는 것을 발견하였습니다.
- 대규모 모델 스케일링에서의 이득이 사전 훈련 데이터 스케일링보다 클 뿐만 아니라, 일반적으로 PET 파라미터 스케일링은 비효과적이라는 점을 밝혔습니다.
- 영어-중국어 및 영어-독일어를 포함한 언어쌍에 대한 두 세트의 사전 훈련된 양방향 대규모 언어 모델을 바탕으로 이중언어 기계 번역 및 다국어 요약 벤치마크 실험을 통해, 최적의 미세조정 방법이 과제와 미세조정 데이터에 매우 의존적임을 식별하였습니다.
- 이 연구는 대규모 언어 모델의 미세조정 방법을 이해하고 선택 및 개발하는 데 있어 통찰력을 제공할 것으로 기대됩니다.

### [OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web](https://arxiv.org/abs/2402.17553)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/sVlGbt1C9fBO9qqYg8M-0.png)

Vote: 14

Authors: Waseem Alshikh, Ruslan Salakhutdinov, Raghav Kapoor, Kiran Kamble, Jing Yu Koh, Melisa Russak, Yash Parag Butala

- 이 연구에서는 컴퓨터 작업을 자동화 할 수 있는 가상 에이전트의 개발 가능성을 높이기 위해 OmniACT라는 첫 데이터셋과 벤치마크를 소개합니다.
- 전통적인 웹 자동화를 넘어 다양한 데스크탑 응용 프로그램을 커버하는 데이터셋은 "다음 노래 재생"과 같은 기본 작업부터 "John Doe에게 만남의 시간과 장소를 언급하여 이메일 보내기"와 같은 장기적인 작업까지 포함합니다.
- 데이터셋의 목표는 화면 이미지와 시각적으로 기반한 자연어 작업의 쌍을 주어, 이 작업을 완전히 수행할 수 있는 스크립트를 생성하는 것입니다.
- OmniACT 벤치마크에서 여러 강력한 기준 언어 모델 에이전트를 실행한 결과, 가장 강력한 기준인 GPT-4가 가장 좋은 성능을 보였지만, 작업을 완료할 수 있는 실행 가능한 스크립트 생성에서 인간의 숙련도의 15%에만 도달함으로써 과제의 도전적인 성격을 보여줍니다.
- 이 벤치마크는 컴퓨터 작업을 자동화하는 언어 모델 에이전트의 진행 상황을 측정하고 평가하는 플랫폼을 제공하며 컴퓨터 화면의 시각적 기반과 대규모 언어 모델을 연결하는 멀티모달 모델을 구축하기 위한 미래 연구를 동기화합니다.

### [Video as the New Language for Real-World Decision Making](https://arxiv.org/abs/2402.17139)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/HJmf2efYoD0IlWqmBWEou.png)

Vote: 14

Authors: Pieter Abbeel, Sherry Yang, Jake Bruce, Yilun Du, Jack Parker-Holder, Dale Schuurmans, Andre Barreto, Jacob Walker

- 인터넷에서 텍스트와 비디오 데이터는 모두 풍부하며, 다음 토큰이나 프레임 예측을 통한 대규모 자가 감독 학습을 지원합니다.
- 언어 모델은 실제 세계에 상당한 영향을 끼친 반면, 비디오 생성은 주로 미디어 엔터테인먼트에 국한되어 있습니다.
- 비디오 데이터는 언어로 표현하기 어려운 물리적 세계에 대한 중요 정보를 포착합니다.
- 이 논문은 비디오 생성을 실제 세계의 과제를 해결하기 위한 이용 모델로 확장할 수 있는 미개척 기회에 대해 논의합니다.
- 언어와 마찬가지로, 비디오는 인터넷 지식을 흡수하고 다양한 과제를 표현할 수 있는 통합 인터페이스로서 기능할 수 있음을 관찰합니다.
- 비디오 생성이 상황 인지 학습, 계획 및 강화 학습과 같은 기술을 통해 계획자, 에이전트, 컴퓨트 엔진, 환경 시뮬레이터로서의 역할을 수행할 수 있음을 보여줍니다.
- 로봇공학, 자율주행, 과학과 같은 분야에서 주요한 영향 가능성을 식별하며, 최근의 연구가 비디오 생성의 이와 같은 고급 기능이 도달 가능한 범위 내에 있다는 것을 입증합니다.
- 마지막으로, 비디오 생성에서 진보를 방해하는 주요 도전 과제를 식별합니다.
- 이러한 도전 과제를 해결하면, 비디오 생성 모델이 더 넓은 범위의 AI 응용 프로그램에서 언어 모델과 함께 독특한 가치를 입증할 수 있을 것입니다.

### [Towards Optimal Learning of Language Models](https://arxiv.org/abs/2402.17759)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pFZcsoaiudgYFU_Knk9qa.png)

Vote: 12

Authors: Li Dong, Qingxiu Dong, Yaru Hao, Yuxian Gu, Minlie Huang, Furu Wei

- 이 연구는 언어 모델(LMs)의 학습을 개선하는 일반 원칙에 대해 연구하며, 탁월한 성능을 달성하기 위해 필요한 훈련 단계를 줄이는 것을 목표로 합니다.
- 저자들은 "LM-training-as-lossless-compression" 관점에서 데이터 압축 비율을 최대화함으로써 LM 학습을 최적화하는 목표를 제안합니다.
- 연구는 이 목표 하에서 최적 학습 과정의 동태를 밝히는 "학습 법칙(Learning Law)"이라는 정리를 도출했습니다.
- 이 정리는 선형 분류 및 실제 언어 모델링 과제에서의 실험을 통해 검증되었습니다.
- 또한, LMs의 스케일링 법칙에서 계수의 개선을 통해 LMs의 최적 학습이 본질적으로 이루어짐을 실증적으로 확인했으며, 이는 실용적인 학습 가속 방법 설계에 있어 큰 약속과 중요성을 시사합니다.
- 연구에 사용된 코드는 https://aka.ms/LearningLaw 에서 확인할 수 있습니다.

### [Sora Generates Videos with Stunning Geometrical Consistency](https://arxiv.org/abs/2402.17403)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/210y4X6o_IOFY9KvT8Mqd.png)

Vote: 12

Authors: Daquan Zhou, Shaodong Wei, Ming-Ming Cheng, Qibin Hou, Xuanyi Li, Chenxu Zhang

- 최근 개발된 Sora 모델은 비디오 생성에서 주목할만한 능력을 보여주었으며, 실제 세계 현상을 시뮬레이션하는 능력에 관해 많은 논의가 있었습니다.
- 그러나 Sora 모델의 실제 물리 법칙에 대한 충실도를 양적으로 평가할 수 있는 확립된 메트릭이 부족합니다.
- 본 논문에서는 생성된 비디오의 실제 물리 법칙 준수 정도를 바탕으로 비디오 품질을 평가하는 새로운 벤치마크를 소개합니다.
- 생성된 비디오를 3D 모델로 변환하는 메소드를 사용하여, 비디오 품질이 3D 재구성의 정확성에 크게 의존한다는 전제를 활용했습니다.
- 3D 재구성의 관점에서, 구축된 3D 모델이 충족하는 기하학적 제약 조건의 충실도를 사용하여 생성된 비디오가 실제 세계 물리 규칙을 얼마나 준수하는지를 평가합니다.
- 프로젝트 페이지는 추가적인 정보와 결과를 공유하는 플랫폼으로 사용됩니다: https://sora-geometrical-consistency.github.io/.

### [Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners](https://arxiv.org/abs/2402.17723)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/aJGbhL62JRk5ZthHJN6zN.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/aJGbhL62JRk5ZthHJN6zN.mp4" muted="false"></video></div>

Vote: 11

Authors: Yingqing He, Yazhou Xing, Qifeng Chen, Zeyue Tian, Xintao Wang

- 영화 산업과 전문 사용자를 위한 핵심 기술인 비디오 및 오디오 콘텐츠 생성에 관련된 연구로, 기존의 확산 기반 방법들이 비디오와 오디오 생성을 별도로 다루는 한계를 극복하고자 한다.
- 이 연구는 시각-오디오 및 결합된 시각-오디오 생성을 위해 최적화 기반의 프레임워크를 신중하게 설계함으로써 학계와 산업 간의 기술 전달 격차를 메우려고 한다.
- 연구자들은 기존의 강력한 비디오 또는 오디오 생성 모델들이 가진 강력한 생성 능력을 확인하고, 이러한 거대 모델들을 처음부터 다시 학습하는 대신 공유된 잠재 표현 공간을 통해 모델들을 연결하는 방향을 제안한다.
- 특히, 사전 학습된 ImageBind 모델과 함께 사용되는 다중 모달리티 잠재 정렬자를 제안하며, 이는 추론 시간 동안 확산 노이즈 제거 과정을 안내하는 분류기 지도의 핵심과 유사하다.
- 주의 깊게 설계된 최적화 전략 및 손실 함수를 통해, 결합된 비디오-오디오 생성, 시각 주도 오디오 생성 및 오디오 주도 시각 생성 작업에서 본 방법의 우수한 성능을 입증한다.
- 프로젝트 웹사이트는 [https://yzxing87.github.io/Seeing-and-Hearing/](https://yzxing87.github.io/Seeing-and-Hearing/)에서 확인할 수 있다.

### [Evaluating Very Long-Term Conversational Memory of LLM Agents](https://arxiv.org/abs/2402.17753)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/qRmY1xwE4f4hm-L99ULtw.png)

Vote: 10

Authors: Francesco Barbieri, Mohit Bansal, Adyasha Maharana, Sergey Tulyakov, Yuwei Fang, Dong-Ho Lee

- 기존의 장기 오픈 도메인 대화 연구는 최대 다섯 차례의 채팅 세션을 넘지 않는 컨텍스트에서 모델 반응을 평가하는 데 초점을 맞췄다.
- 장기 맥락의 대형 언어 모델(LLMs)과 검색 증강 생성(RAG) 기술이 향상되었음에도 불구하고 매우 장기간 대화에서의 효과는 탐구되지 않았다.
- 연구 공백을 해소하기 위해 인물 설명(personas)과 시간 이벤트 그래프에 대화 내용을 기초하고 LLM 기반 에이전트 아키텍처를 활용하여 고품질의 매우 장기간 대화를 생성하는 기계-인간 파이프라인을 소개한다.
- 각 에이전트는 이미지 공유 및 반응할 수 있는 기능을 갖추고 있다.
- 생성된 대화는 사건 그래프에 대한 장거리 일관성 및 근거를 위해 사람의 어노테이터에 의해 검증 및 편집되었다.
- 이 파이프라인을 통해, 각각 300턴, 평균 9,000개의 토큰, 최대 35세션에 걸친 매우 장기간의 대화 데이터셋인 LoCoMo를 수집했다.
- LoCoMo를 기반으로, 모델에서 장기 기억 능력을 측정하기 위한 종합적인 평가 벤치마크를 제시하며, 이에는 질의응답, 이벤트 요약, 멀티모달 대화 생성 작업이 포함된다.
- 실험 결과, LLM은 긴 대화 이해와 대화 내 장거리 시간적 및 인과적 역학을 이해하는 데 어려움을 드러냄을 보여준다.
- 장기 맥락 LLMs 또는 RAG와 같은 전략을 사용하면 개선 효과가 있을 수 있으나, 이러한 모델은 여전히 인간의 성능에 크게 뒤쳐져 있다.

### [Disentangled 3D Scene Generation with Layout Learning](https://arxiv.org/abs/2402.16936)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/sqN59PrKFrDyvlXh3u7bi.png)

Vote: 9

Authors: Aleksander Holynski, Dave Epstein, Ben Poole, Alexei A. Efros, Ben Mildenhall

- 본 논문에서는 구성 요소 객체로 분리된 3D 장면을 생성하는 방법을 제안합니다.
- 이 분리는 사전 훈련된 대규모 텍스트-이미지 모델의 지식만을 활용한 비감독 방식으로 이루어집니다.
- 주요 통찰은 공간적으로 재배열되어도 여전히 유효한 장면 구성을 생성하는 3D 장면의 일부를 찾음으로써 객체를 발견할 수 있다는 것입니다.
- 구체적으로 우리의 방법은 각각 자체 객체를 나타내는 여러 NeRF(Neural Radiance Fields)를 처음부터 공동 최적화하고, 이 객체들을 장면으로 조합하는 레이아웃 세트와 함께 최적화합니다.
- 그런 다음, 이러한 구성된 장면들이 이미지 생성기에 의해 분포 내에 있는 것으로 간주되도록 장려합니다.
- 우리의 접근 방식이 단순함에도 불구하고 개별 객체로 분해된 3D 장면을 성공적으로 생성하여 텍스트-3D 콘텐츠 생성에서 새로운 가능성을 제시한다는 결과를 보여줍니다.
- 결과 및 대화형 데모는 저희 프로젝트 페이지 https://dave.ml/layoutlearning/ 에서 확인할 수 있습니다.

### [Training-Free Long-Context Scaling of Large Language Models](https://arxiv.org/abs/2402.17463)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/D6aUg6csG2bSP1w1DP9Uk.png)

Vote: 8

Authors: Shansan Gong, Lingpeng Kong, Chang Zhou, Jun Zhang, Chenxin An, Fei Huang, Xipeng Qiu

- 대규모 언어 모델(LLMs)은 사전 트레이닝 길이를 넘어서는 입력 토큰 수가 증가할 때 일관성 있는 텍스트 처리 및 생성 능력이 현저하게 약화된다.
- 긴 시퀀스를 가진 대규모 모델의 파인튜닝 비용이 높은 것을 감안하여, 우리는 지속적인 트레이닝 없이도 Llama2 70B가 100,000개가 넘는 토큰 컨텍스트 윈도우를 지원하게 해주는 Dual Chunk Attention(DCA)을 제안한다.
- DCA는 긴 시퀀스에 대한 주의 계산을 청크 기반 모듈로 분해하여, 같은 청크 내부(Intra-Chunk)와 서로 다른 청크 간(Inter-Chunk) 토큰의 상대적 위치 정보를 효과적으로 포착한다.
- DCA는 Flash Attention과도 원활하게 통합되며, 실제 긴 컨텍스트 작업에서 파인튜닝된 모델과 비교하여 동등하거나 더 나은 성능을 달성한다.
- 독점 모델과 비교할 때, 우리의 트레이닝-프리 70B 모델은 gpt-3.5-16k의 성능의 94%를 달성하며, 이는 실용적인 오픈 소스 대안이 될 수 있음을 나타낸다.
- 이 작업에 사용된 모든 코드와 데이터는 https://github.com/HKUNLP/ChunkLlama에서 공개되었다.

### [Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation](https://arxiv.org/abs/2402.17245)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/lfyxQFYDgnR95y7HvJaD5.png)

Vote: 8

Authors: Suhail Doshi, Ehsan Akhgari, Ali Sabet, Aleks Kamko, Linmiao Xu, Daiqing Li

- 본 연구에서는 텍스트-이미지 생성 모델의 미적 품질을 최고 수준으로 향상시키기 위한 세 가지 인사이트를 공유합니다.
- 모델 개선의 세 가지 핵심 요소로, 색상 및 대비 향상, 다양한 종횡비에서의 이미지 생성 개선, 인간 중심의 미세 디테일 개선에 초점을 맞춥니다.
- 먼저, 확산 모델 훈련에서 노이즈 일정의 중요성을 탐구하며, 이것이 리얼리즘과 시각적 정확도에 미치는 깊은 영향을 보여줍니다.
- 두 번째로, 다양한 종횡비의 이미지 생성에 따르는 도전을 해결하기 위해 균형 잡힌 버킷형 데이터셋의 준비의 중요성을 강조합니다.
- 마지막으로, 생성된 이미지가 인간의 인지적 기대에 부합하도록 모델 출력을 조정하는 것의 중요한 역할을 조사합니다.
- 광범위한 분석과 실험을 통해 Playground v2.5는 다양한 조건과 종횡비에서 미적 품질 측면에서 최고 수준의 성능을 입증하며 SDXL 및 Playground v2와 같은 널리 사용되는 오픈소스 모델들과 DALLE 3 및 Midjourney v5.2와 같은 폐쇄출처 상업 시스템을 능가합니다.
- Playground v2.5 모델은 오픈소스이며, 확산 기반 이미지 생성 모델의 미적 품질을 높이고자 하는 연구자들에게 유용한 가이드라인을 제공할 것으로 기대됩니다.

### [VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction](https://arxiv.org/abs/2402.17427)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/4iteIvMm0gDIxoA3tqKKh.png)

Vote: 5

Authors: Jiayue Liu, Shiyong Liu, Youliang Yan, Jiaqi Lin, Zhihao Li, Yangdi Lu, Songcen Xu, Wenming Yang, Xiaofei Wu, Xiao Tang, Jianzhuang Liu

- VastGaussian은 대규모 장면 재구성과 실시간 렌더링을 위한 최초의 고품질 방법으로, 3D Gaussian Splatting을 기반으로 합니다.
- 기존의 NeRF 기반 방법들이 시각적 품질과 렌더링 속도의 제한을 가진 것에 비해, 공간을 여러 셀로 나누는 점진적 파티셔닝 전략을 통해 대형 장면에도 확장합니다.
- 항공 공간 인식 가시성 기준을 활용해 트레이닝 카메라와 포인트 클라우드를 적절히 배분하고, 병렬 최적화를 거쳐 완전한 장면으로 통합합니다.
- 최적화 과정 중에는 렌더링된 이미지의 외관 변화를 줄이기 위해 분리된 외관 모델링을 도입합니다.
- VastGaussian 방법은 기존 NeRF 기반 방법들을 뛰어넘으며, 다수의 대규모 장면 데이터셋에서 최신 결과를 달성하여 빠른 최적화와 고품질 실시간 렌더링을 가능하게 합니다.

