## Daily Papers (2024-02-07)

### [Self-Discover: Large Language Models Self-Compose Reasoning Structures](https://arxiv.org/abs/2402.03620)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-JXIkLf1Pwi-5NdZ3rOyU.png)

Vote: 23

Authors: Heng-Tze Cheng, Quoc V. Le, Pei Zhou, Denny Zhou, Swaroop Mishra, Huaixiu Steven Zheng, Jay Pujara, Ed H. Chi, Xiang Ren, Xinyun Chen

- SELF-DISCOVER는 큰 언어 모델들(Large Language Models, LLMs)이 전형적인 프롬프팅 방법으로 해결하기 어려운 복잡한 추론 문제를 다룰 수 있도록 작업 본질적인 추론 구조를 자체 발견하는 일반적인 프레임워크를 소개합니다.
- 이 프레임워크의 핵심은 비판적 사고 및 단계별 사고와 같은 다양한 원자적 추론 모듈을 선택하여 LLM들이 해독하는 동안 따를 명확한 추론 구조로 구성하는 자체 발견 프로세스입니다.
- SELF-DISCOVER는 BigBench-Hard, 그라운드 에이전트 추론, MATH와 같은 도전적인 추론 벤치마크에서 GPT-4와 PaLM 2의 성능을 Thought (CoT)보다 최대 32%까지 높이는 등 상당한 성능 향상을 보여주었습니다.
- SELF-DISCOVER는 추론 집약적인 방법, 예를 들어 CoT-Self-Consistency보다 20% 이상 성능이 좋으며, 동시에 10-40배 적은 추론 컴퓨팅을 요구합니다.
- 마지막으로, SELF-DISCOVER에 의해 자체 발견된 추론 구조들이 모델 가족 간에 보편적으로 적용 가능하며(PaLM 2-L에서 GPT-4까지, GPT-4에서 Llama2까지), 인간의 추론 패턴과 공통점을 공유합니다.

### [Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks](https://arxiv.org/abs/2402.04248)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wjHOhC5IL0JB9W9bgqTPo.png)

Vote: 14

Authors: Jongho Park, Nayoung Lee, Jaeseung Park, Dimitris Papailiopoulos, Kangwook Lee, Samet Oymak, Jaewoong Cho, Zheyang Xiong

- 상태 공간 모델(SSM)은 언어 모델링에서 Transformer 네트워크의 대안으로 제안되었으며 Mamba Gu & Dao (2034) 또한 이러한 모델 중 하나로, 멀티 헤드 어텐션의 제곱 비용을 완화하기 위한 게이팅, 컨볼루션, 입력 의존 토큰 선택을 도입했습니다.
- SSM은 뛰어난 성능을 보이나, 모델교정 없이 작업을 실행할 수 있는 현대 언어 모델의 주목할만한 특성인 인-컨텍스트 학습(ICL) 능력에 대해선 Transformer와 비교해서 덜 탐구되었습니다.
- 본 연구에서는 다양한 작업에서 Mamba를 포함한 SSM의 ICL 성능을 Transformer 모델과 비교하였습니다.
- SSM은 표준 회귀 ICL 작업에서 Transformer와 비슷한 성능을 보였고, 희소 패리티 학습 같은 작업에서는 더 우수한 성능을 보였습니다.
- 그러나, SSM은 비표준 검색 기능을 포함한 작업에서는 부진한 성능을 보였습니다.
- 이러한 한계를 극복하기 위해, Mamba와 어텐션 블록을 결합한 하이브리드 모델, \variant를 소개했으며, 이는 개별 모델이 독자적으로 어려워하는 작업에서 우수한 성능을 보였습니다.
- 하이브리드 아키텍처는 언어 모델에서 ICL을 향상시키는데 유망한 방법을 제시합니다.

### [MusicRL: Aligning Music Generation to Human Preferences](https://arxiv.org/abs/2402.04229)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/2sJ3cQMXM-HByK5qLj3q3.png)

Vote: 10

Authors: Matthieu Geist, Mauro Verzetti, Damien Vincent, Neil Zeghidour, Victor Ungureanu, Matej Kastelic, Olivier Bachem, Olivier Pietquin, Brian McWilliams, Andrea Agostinelli, Sertan Girgin, Zalán Borsos, Léonard Hussenot, Geoffrey Cideron

- MusicRL은 인간의 피드백으로 세밀하게 조정된 최초의 음악 생성 시스템을 제안합니다.
- 이 시스템은 주관적인 음악 취향과 특정 문구의 의도를 반영하는 텍스트-음악 모델의 평가가 유저마다 다를 수 있음을 인식하고 있습니다.
- 전처리된 자동회귀 MusicLM 모델을 시퀀스 수준의 보상을 최대화하도록 강화 학습으로 미세 조정하여 MusicRL-R로 발전시켰습니다.
- MusicLM을 사용자들에게 배포하고 300,000쌍의 선호도 데이터를 수집하여 인간 피드백을 활용하는 MusicRL-U 모델을 훈련시켰습니다.
- 인간 평가에서는 MusicRL-R과 MusicRL-U 모델이 기본 모델보다 선호됩니다.
- 최종적으로 MusicRL-RU는 두 방법을 결합하여 인간 평가자에 의해 가장 좋은 모델로 평가받았습니다.
- 본 논문의 연구 결과는 텍스트 준수와 음질이 음악에 대한 인간의 선호에 일부만 영향을 미친다는 것을 보여주며 음악 생성 모델의 미세 조정에 인간 청자의 추가 참여를 요구합니다.

### [EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters](https://arxiv.org/abs/2402.04252)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/HvXmvjmGfTEetw0qDjbiX.png)

Vote: 8

Authors: Yufeng Cui, Jinsheng Wang, Fan Zhang, Xinlong Wang, Qiying Yu, Quan Sun, Xiaosong Zhang

- EVA-CLIP-18B 모델은 현재까지 공개된 것 중 가장 큰 규모(180억 파라미터)의 CLIP 모델로, 대규모의 시각 및 다중 모드 모델을 강화하는 데 초점을 맞추었습니다.
- 이 모델은 단 60억 개의 학습 샘플만으로도, 이미지 분류 벤치마크 27개에서 평균 80.7%의 제로샷 정확도를 달성하여, EVA-CLIP(50억 파라미터) 등 기존의 다른 오픈소스 CLIP 모델보다 월등한 성능을 보였습니다.
- LAION-2B와 COYO-700M으로 구성된 20억 개의 이미지-텍스트 쌍으로 구성된 공개 데이터셋으로 학습된 결과, 모델 크기가 커질수록 일관되게 성능 향상이 관찰되었습니다. 이는 비공개 데이터셋을 사용하는 다른 최첨단 CLIP 모델(예: DFN-5B, WebLI-10B)이 사용하는 것보다 훨씬 작습니다.
- EVA-CLIP-18B의 공개는 EVA 스타일의 약-강 시각 모델 스케일링 가능성을 보여주며, 비전 및 다중 모달 기반 모델 연구를 촉진할 것으로 기대됩니다.

### [Scaling Laws for Downstream Task Performance of Large Language Models](https://arxiv.org/abs/2402.04177)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0hOM6hyH6onuLhmYmp6yt.png)

Vote: 8

Authors: Sergei Vassilvitskii, Sanmi Koyejo, Natalia Ponomareva, Berivan Isik, Dimitris Paparas, Hussein Hazimeh

- 이 연구에서는 대규모 언어 모델(LLM)의 디자인을 안내할 수 있는 중요한 인사이트를 제공하는 스케일링 법칙에 대해 조사했습니다.
- 기존 연구들은 주로 사전 학습(상류) 손실에 대한 스케일링 법칙을 연구했지만, 이 연구는 미감독 데이터셋에서 사전 학습을 하고 그 후 하류 과제(기계 번역)에 미세조정을 하는 전이 학습 환경에서의 성능에 초점을 맞췄습니다.
- 사전 학습 데이터의 선택과 그 크기가 하류 성능(번역 품질)에 어떤 영향을 미치는지를 두 가지 지표인 하류 크로스 엔트로피와 BLEU 점수를 통해 조사하였습니다.
- 실험 결과, 미세조정 데이터셋의 크기와 사전 학습 데이터와 하류 데이터 간의 분포 정렬도가 스케일링 행태에 크게 영향을 미치는 것으로 나타났습니다.
- 충분한 정렬이 있을 때, 더 많은 사전 학습 데이터를 사용함에 따라 하류 크로스 엔트로피와 BLEU 점수가 모두 일관적으로 개선되며, 이러한 경우 BLEU 점수를 로그 법칙을 사용하여 잘 예측할 수 있음을 보여주었습니다.
- 그러나, 중간 정도의 불일치는 사전 학습 데이터가 늘어남에 따라 BLEU 점수가 변동하거나 나빠지는 반면, 하류 크로스 엔트로피는 일관되게 개선됨을 발견했습니다.
- 이러한 관찰을 분석함으로써, 적절한 사전 학습 데이터를 선택하는 데 있어 실질적인 통찰력을 제공하고 있습니다.

### [Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models](https://arxiv.org/abs/2402.03749)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ycTS7R2L3sornErr9e1Aq.png)

Vote: 8

Authors: Kai Han, Chengcheng Wang, Hanting Chen, Yunhe Wang, Jianyuan Guo, Chang Xu

- 최근 대규모 언어 모델의 큰 진전이 초인간적인 능력으로 관심을 불러일으키며, 이러한 능력을 평가하고 최적화하는 방법, 즉 'superalignment' 연구가 활발해졌습니다.
- 이 논문은 시각 기반 모델 분야에 초점을 맞추고 더 약한 모델을 이용해 강한 모델의 성능을 뛰어넘게 하려는 '약-강 일반화(weak-to-strong generalization)' 개념을 탐구합니다.
- 새롭고 적응 가능한 손실 함수를 도입해 약한 모델로부터 강한 모델을 감독하는 새로운 접근법을 소개합니다.
- 몇 개의 샘플 학습, 전이 학습, 잡음이 있는 레이블 학습, 일반적인 지식 증류 환경 등 다양한 시나리오에서 광범위한 실험을 진행했습니다.
- 실험 결과, 이 방법은 강-강 일반화 성능 벤치마크는 물론, 전체 데이터셋을 사용하여 강한 모델을 미세 조정하는 결과까지 초과하는 것으로 나타났습니다.
- 이러한 결과는 약-강 일반화의 중요한 잠재력을 강조하며, 시각 기반 모델의 성능을 크게 향상시킬 수 있는 능력을 보여줍니다.
- 코드는 https://github.com/ggjy/vision_weak_to_strong에서 제공됩니다.

### [Diffusion World Model](https://arxiv.org/abs/2402.03570)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/s87KJbBmwzRrwhm3dWLw0.png)

Vote: 6

Authors: Yuandong Tian, Zihan Ding, Qinqing Zheng, Amy Zhang

- 'Diffusion World Model (DWM)'이라는 새로운 조건부 확산 모델을 소개하며, 이 모델은 다단계 미래 상태와 보상을 동시에 예측할 수 있습니다.
- 기존의 일회성 동역학 모델과 달리, DWM은 재귀적인 쿼리 없이 단일 전달 패스로 장기간 예측을 제공합니다.
- 모델 기반 가치 추정에 DWM을 통합하여, DWM에서 샘플링된 미래 궤적으로 단기간 수익을 시뮬레이션합니다.
- 오프라인 강화 학습의 맥락에서 DWM은 생성 모델링을 통한 보수적인 가치 규제로 볼 수 있습니다.
- 또는, 합성 데이터를 이용한 오프라인 Q-학습을 가능하게 하는 데이터 소스로 볼 수도 있습니다.
- D4RL 데이터셋에 대한 실험을 통해, DWM이 장기간 시뮬레이션에 대한 강인함을 입증하였습니다.
- 절대 성능 측면에서 DWM은 일회성 동역학 모델을 44%의 성능 향상으로 크게 앞서며, 최신 성능을 달성합니다.

### [Multi-line AI-assisted Code Authoring](https://arxiv.org/abs/2402.04141)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/K8syd0VxoeQgEz3pxBKBd.png)

Vote: 5

Authors: Ali Tayyebi, Omer Dunay, Daniel Cheng, Nachiappan Nagappan, Vijayaraghavan Murali, Andy Chiu, Imad Ahmad, Arun Ganesan, Chandra Maddila, Peter C Rigby, Parth Thakkar, Adam Tait

- 'CodeCompose'는 대규모 언어 모델(LLMs)을 이용한 AI 보조 코드 작성 도구로, 메타의 수만 명의 개발자들에게 인라인 제안 기능을 제공합니다.
- 본 논문에서는 단일 줄 제안에서 다줄 제안으로 제품을 확장하는 과정에서 직면한 독특한 도전과제들을 극복하는 방법을 소개합니다.
- 다줄 제안은 개발자의 기존 코드 주변에서 연속적으로 변화하기 때문에 '충격적'인 효과를 낼 수 있으며, 이는 생산성과 만족도 하락으로 이어질 수 있습니다.
- 다줄 제안 생성에 더 많은 시간이 걸리기 때문에, 사용자가 느끼는 대기 시간을 줄이기 위한 다양한 혁신적인 투자를 소개합니다. 이러한 모델 호스팅 최적화는 다줄 제안 지연 시간을 2.5배 가량 개선했습니다.
- 수천 명의 엔지니어를 대상으로 다줄 제안이 사용자 경험에 미치는 영향을 이해하고 단일 줄 제안과 비교하는 실험을 진행했습니다.
- 실험 결과, 다줄 제안은 전체 수용된 문자의 42%를 차지했으며(표시된 제안의 16%에 불과함) 사용자의 키스트로크를 기존 9%에서 17%로 거의 두 배로 절약했습니다.
- 'Multi-line CodeCompose'는 메타의 모든 엔지니어에게 도입되었으며, 1% 미만의 엔지니어만이 다줄 제안 기능을 사용하지 않기로 선택했습니다.

### [MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/abs/2402.03766)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/mSoWqRTMm2HB8kjpTZQST.png)

Vote: 5

Authors: Xinyang Lin, Limeng Qiao, Fei Wei, Xiangxiang Chu, Yang Yang, Xiaofei Sun, Bo Zhang, Chunhua Shen, Shuang Xu, Yiming Hu, Xinyu Zhang

- MobileVLM V2는 기존 MobileVLM의 크게 향상된 비전 언어 모델로, 새로운 아키텍처 디자인, 모바일 VLM을 위한 개선된 훈련 방식, 그리고 풍부하고 고품질의 데이터셋 큐레이션이 VLM의 성능에 크게 기여할 수 있음을 증명합니다.
- MobileVLM V2 1.7B는 3B 규모의 훨씬 큰 VLM과 비교해서도 표준 VLM 벤치마크에서 우수하거나 동등한 성능을 달성했습니다.
- 특히, 이들의 3B 모델은 7B 이상 규모의 다양한 VLM을 뛰어넘는 성능을 보여줍니다.
- 연구진이 개발한 모델은 https://github.com/Meituan-AutoML/MobileVLM 에서 공개될 예정입니다.

### [IMUSIC: IMU-based Facial Expression Capture](https://arxiv.org/abs/2402.03944)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Qw6ZHpM_Rv9tHDJvbNX16.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Qw6ZHpM_Rv9tHDJvbNX16.mp4" muted="false"></video></div>

Vote: 3

Authors: Ruiqian Li, Yingsheng Zhu, Hengan Zhou, Yiwen Wu, Yingwenqi Jiang, Jingya Wang, Jingyi Yu, Youjia Wang, Guanpeng Long, Hongyang Lin, Lan Xu

- 본 논문에서는 개인 정보 보호를 보장하고 가리개에 강한 새로운 얼굴 표정 캡처 방법인 IMUSIC를 제안합니다.
- IMUSIC는 얼굴 움직임 캡처를 위해 특별히 제작된 마이크로-IMU(관성 측정 장치)와 해부학 기반의 IMU 배치 방식을 디자인하였습니다.
- 연구 팀은 IMU-ARKit 데이터셋을 구축하여 다양한 얼굴 표정과 퍼포먼스에 대한 풍부한 IMU/시각적 신호를 제공함으로써, IMU 기반 얼굴 행동 분석과 같은 미래 연구 방향성에 기여합니다.
- 또한, 본 데이터셋을 사용하여 얼굴 블렌드쉐이프(blendshape) 매개변수를 IMU 신호로부터 정확히 예측할 수 있는 강력한 기준 모델을 소개합니다.
- 특히, 이 트래킹 작업을 위한 트랜스포머 기반 확산 모델과 이중 단계 훈련 전략을 맞춤형으로 탑재한 것이 IMUSIC의 핵심 설계입니다.
- IMUSIC 프레임워크를 사용하면 시각적 방법이 실패하는 환경에서도 정확한 얼굴 캡처가 가능하며 사용자의 개인 정보를 동시에 보호할 수 있습니다.
- 다양한 잠재적이고 새로운 응용 프로그램, 예를 들어 개인 정보를 보호하는 얼굴 캡처, 가리개에 대한 하이브리드 캡처 또는 시각적 신호로는 종종 보이지 않는 미세한 얼굴 움직임을 감지할 수 있습니다.
- 실험을 통해 IMU 구성과 기술적 구성 요소의 효과성을 확인하였으며, IMUSIC의 데이터셋과 구현을 공개하여 우리 커뮤니티의 얼굴 캡처 및 분석 가능성을 더 풍부하게 할 예정입니다.

### [EscherNet: A Generative Model for Scalable View Synthesis](https://arxiv.org/abs/2402.03908)

![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2402.03908.png)

Vote: 3

Authors: Xiaojuan Qi, Marwan Taher, Shikun Liu, Xin Kong, Xiaoyang Lyu, Andrew J. Davison

- EscherNet은 다중 시점 조건부 확산 모델을 활용하여 뷰 생성을 위한 시스템으로, 3D 표현을 암시적이고 생성적으로 학습합니다.
- 이 모델은 전문적인 카메라 위치 인코딩을 사용하여 임의의 참조 뷰와 목표 뷰 간의 카메라 변환을 정밀하고 연속적으로 제어합니다.
- EscherNet은 한 번에 100개가 넘는 일관된 목표 뷰를 단일 소비자 등급의 GPU에서 생성할 수 있는 범용성, 유연성 및 확장성을 갖추고 있습니다.
- 단 3개의 참조 뷰에서 3개의 목표 뷰로 훈련된 상태임에도 불구하고, 이 모델은 제로샷 새로운 뷰 생성 문제를 해결할 뿐만 아니라 단일 이미지와 다중 이미지 3D 재구축을 통합합니다.
- EscherNet은 다양한 벤치마크에서 최첨단 성능을 달성하며, 개별 문제에 특화된 방법들과 비교해도 우수한 결과를 보입니다.
- 이러한 다목적성은 3D 비전을 위한 확장 가능한 신경 아키텍처 설계의 새로운 방향을 제시합니다.
- EscherNet에 대한 자세한 정보는 프로젝트 페이지에서 확인할 수 있습니다: https://kxhit.github.io/EscherNet/.

### [CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations](https://arxiv.org/abs/2402.04236)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/DHoMxwCZiubNNil-5FL8W.png)

Vote: 3

Authors: Yuxiao Dong, Bin Xu, Lei Hou, Weihan Wang, Wenyi Hong, Jie Tang, Ji Qi, Yushi Bai, Qingsong Lv, Ming Ding, Juanzi Li

- 본 논문에서는 비전-언어 모델(VLMs)이 시각적 지시사항을 답변에 맞추는 광범위한 훈련에 효과가 있음을 보이지만, 정교한 시각적 문제와 충실하지 못한 반응으로 인해 결정적인 시각적 추론을 간과하게 되는 문제를 지적합니다.
- 이에 따라, 저자들은 '조작의 체인(Chain of Manipulations)'이라는 메커니즘을 제안하여, VLMs가 사전 훈련을 통해 습득한 내재적 능력(예: 기반 찾기) 또는 인간과 유사한 행동 모방(예: 확대)과 같은 시각적 입력에 대한 연산을 통해 문제를 해결하도록 합니다.
- 이 메커니즘을 통해 VLMs는 증거 기반 시각적 추론을 바탕으로 믿을 수 있는 응답을 생성하고, 사용자는 해석 가능한 경로에서 오류 원인을 추적할 수 있게 됩니다.
- 연구팀은 이러한 추론 메커니즘을 적용한 17B 규모의 VLM인 CogCoM을 훈련시키고, 이를 통해 3개 카테고리에서 8개 벤치마크에 걸친 최신의 성능을 달성하였음을 보여줍니다.
- 또한, 제한된 학습 단계에서도 경쟁력 있는 성능을 신속하게 얻을 수 있음을 실험을 통해 증명합니다.
- 관련 코드와 데이터는 https://github.com/THUDM/CogCoM 에서 공개적으로 제공됩니다.

