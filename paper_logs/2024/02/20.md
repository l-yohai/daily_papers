## Daily Papers (2024-02-20)

### [Speculative Streaming: Fast LLM Inference without Auxiliary Models](https://arxiv.org/abs/2402.11131)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/osnH3_vEdXyOPsnZgeHOL.png)

Vote: 33

Authors: Nikhil Bhendawade, Irina Belousova, Mahyar Najibi, Mohammad Rastegari, Qichen Fu, Henry Mason

- 추측적 디코딩은 부가적인 드래프트 모델의 예측에 기반하여 대형 타겟 언어 모델의 추론 속도를 높이는 주요 기술입니다.
- 기존 방식은 효과적이면서도 다운스트림 태스크가 많아질수록 드래프트 모델들이 추론 시스템에 복잡성을 증가시키는 문제가 있었습니다.
- 저희는 단일모델을 사용하는 추측적 스트리밍 방식을 제안하여, 다음 토큰 예측에서 미래 n-그램 예측으로 미세 조정 목표를 변경함으로써 드래프팅을 타겟 모델에 통합합니다.
- 추측적 스트리밍은 요약, 구조화된 질의, 의미 표현 등 다양한 작업에서 1.8 - 3.1배의 디코딩 속도 향상을 달성하면서도 생성 품질을 저하시키지 않습니다.
- 추가적으로 추측적 스트리밍은 매개변수 효율적으로, 메두사 스타일 아키텍처와 비슷하거나 더 높은 속도 향상을 달성하면서 사용하는 추가 매개변수가 약 10000배 적습니다.
- 이러한 특징은 자원 제약이 있는 장치에 특히 적합합니다.

### [FiT: Flexible Vision Transformer for Diffusion Model](https://arxiv.org/abs/2402.12376)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/aIFYYHp38njZwRBvFQoMP.png)

Vote: 28

Authors: Chengyue Wu, Lei Bai, Zidong Wang, Zeyu Lu, Xihui Liu, Wanli Ouyang, Di Huang

- 자연은 무한한 해상도로 존재합니다. 현실 세계에서 이미지를 고정된 해상도에 구속하지 않고 생성하는 것을 목표로, 기존의 디퓨전 모델이 훈련된 해상도 범위를 벗어날 때 직면하는 어려움을 극복하기 위해 Flexible Vision Transformer (FiT)를 제시합니다.
- FiT는 정해진 해상도의 격자로 이미지를 인식하는 전통적인 방식과 달리 이미지를 동적 크기의 토큰 시퀀스로 개념화함으로써, 다양한 해상도 및 가로세로 비율에 쉽게 적응하며 훈련과 추론 단계 모두에서 유연성을 제공합니다.
- 이미지 자르기에 의한 편향을 없애고 해상도 일반화를 촉진하는 유연한 훈련 전략을 채택함으로써, FiT는 다양한 해상도에 대해 해상도 외삽 생성에서 놀라운 유연성을 보입니다.
- 정교하게 조정된 네트워크 구조와 트레이닝-프리 외삽 기술을 통합함으로써, FiT는 훈련 해상도 분포 내외의 광범위한 해상도에 걸쳐 탁월한 성능을 입증합니다.
- 연구의 소스 코드는 https://github.com/whlzy/FiT 주소에서 찾아볼 수 있습니다.

### [FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models](https://arxiv.org/abs/2402.10986)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wGT1thIAef7ggSrxrrVY2.png)

Vote: 28

Authors: Gagan Bhatia, Hasan Cavusoglu, El Moatez Billah Nagoudi, Muhammad Abdul-Mageed

- 'FinTral'은 금융 분석에 특화된 최신 다중 모드 대규모 언어 모델(LLMs) 모음을 소개하며, Mistral-7b 모델을 기반으로 제작되었습니다.
- 이 모델은 텍스트, 숫자, 표, 이미지 데이터를 통합하고 도메인 특화 사전 트레이닝, 엄선된 지침 미세 조정, RLAIF 트레이닝을 통해 향상됩니다.
- FinTral은 이 분야의 대규모 텍스트 및 시각 데이터셋을 활용하여 특화된 데이터를 사용합니다.
- 9가지 작업과 25개 데이터셋을 포함하는 광범위한 벤치마크를 도입하여, 금융 분야에서의 환영 효과를 평가합니다.
- 직접적인 선호 최적화(DPO), 고급 도구 및 검색 방법을 활용한 FinTral-DPO-T&R 모델은 뛰어난 제로샷 성능을 보여줍니다.
- FinTral-DPO-T&R은 ChatGPT-3.5를 모든 작업에서 능가하고, 9가지 작업 중 5개에서 GPT-4를 초과하는 성능을 나타내면서 금융 기술 분야의 AI 진보에 중요한 기여를 합니다.
- FinTral은 다양한 금융 상황에서 실시간 분석과 의사 결정에서 뛰어난 잠재력을 지닌 것으로 평가됩니다.

### [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/91qtWOTzD4TxxP1jvGIQG.png)

Vote: 15

Authors: Hang Yan, Jie Fu, Jun Zhan, Zhigeng Liu, Jiasheng Ye, Junqi Dai, Ruibin Yuan, Dong Zhang, Linyang Li, Yugang Jiang, Xin Zhang, Yunhua Zhou, Tao Gui, Xipeng Qiu, Ge Zhang, Tianxiang Sun

- AnyGPT란 음성, 텍스트, 이미지, 음악 등 다양한 모달리티를 통합 처리하기 위한 이산 표현을 활용하는 어떤 모달리티에서든 변환 가능한 멀티모달 언어 모델을 소개합니다.
- 이 모델은 현존하는 대규모 언어 모델의 아키텍처나 훈련 패러다임을 변경하지 않고도 안정적으로 훈련이 가능하며, 데이터 레벨의 전처리에만 의존합니다.
- 새로운 모달리티를 대규모 언어 모델에 쉽게 통합할 수 있도록 지원하며, 이는 새로운 언어를 추가하는 것과 유사합니다.
- 멀티모달 텍스트 중심 데이터셋을 구축하고, 최초의 대규모 어떤 모달리티에서든 변환 가능한 멀티모달 지시 데이터셋을 생성 모델을 사용하여 합성했습니다.
- 이 데이터셋은 다양한 모달리티가 복잡하게 얽혀 있는 10만 8천개의 멀티 턴 대화 샘플로 구성되어 있어 모델이 임의의 멀티모달 입력과 출력을 처리할 수 있도록 합니다.
- 실험 결과, AnyGPT는 모든 모달리티에서 특화된 모델과 비교할 수 있는 성능을 달성하며, 어떤 모달리티에서든 변환 가능한 멀티모달 대화를 용이하게 할 수 있음을 입증하고, 이산 표현이 언어 모델 내에서 다양한 모달리티를 효율적으로 통합할 수 있음을 보여줍니다.
- 데모는 https://junzhan2000.github.io/AnyGPT.github.io/ 에서 확인할 수 있습니다.

### [Learning to Learn Faster from Human Feedback with Language Model Predictive Control](https://arxiv.org/abs/2402.11450)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/e58pqxExygfwNeRLVbqAG.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/e58pqxExygfwNeRLVbqAG.mp4" muted="false"></video></div>

Vote: 11

Authors: Chuyuan Kelly Fu, Sean Kirmani, Montserrat Gonzalez Arenas, Maria Bauza, Matthew Bennice, Nimrod Gileadi, Adil Dostmohamed, Ben Jyenis, Tsang-Wei Edward Lee, Chase Kew, Fei Xia, Keerthana Gopalakrishnan, Jasmine Hsu, Alex Bewley, Maria Attarian, Wenhao Yu, +, Leonard Hasenclever, Marissa Giustina, Jacky Liang, Jan Humplik, Andy Zeng, Nikhil Joshi

- 본 연구에서는 언어 명령으로 로봇 코드를 작성하는 큰 언어 모델들이 비전문가들이 로봇 행동을 지시하고 피드백을 바탕으로 수정하거나 새로운 작업을 수행하도록 결합하는 능력을 보여주었으나, 이러한 능력은 LLM의 컨텍스트 크기 내에 사용자의 피드백이 유지되는 단기적 상호작용에 제한됩니다.
- 사용자가 작업을 성공적이라고 판단하기 전에 평균 몇 번의 수정이 필요한지를 재는 기준으로 LLM의 가르치기 용이성, 즉 인간의 입력에 얼마나 효율적으로 적응하는지를 향상시키기 위해 로봇 코드 쓰기를 위한 LLM을 미세 조정하는 방법을 조사하였습니다.
- 본 연구의 주요 관찰은 인간-로봇 상호작용을 부분적으로 관측 가능한 마르코프 결정 과정으로 형식화 할 때, 과거 상호작용을 완료하기 위해 LLM을 훈련시키는 것은 전이 동역학 모델을 훈련시키는 것으로 볼 수 있으며 이는 성공으로 가는 더 짧은 경로를 발견하기 위해 고전 로봇 기술인 모델 예측 제어(MPC)와 결합될 수 있습니다.
- 이러한 기법은 언어 모델 예측 제어(Language Model Predictive Control, LMPC)이라는 틀을 형성하여, 5가지 로봇 실체에서 78개의 작업에 대해 PaLM 2를 미세 조정함으로써 과거의 상호작용을 기억하고 가르치기 용이성을 향상시킬 수 있음을 제시합니다.
- 실시된 실험은 LMPC가 비전문가의 보이지 않는 작업에 대한 가르치기 성공률을 26.9% 향상시키고 평균적으로 필요한 인간 수정의 수를 2.4에서 1.9로 줄일 수 있음을 보여줍니다.
- 또한, LMPC는 강력한 메타 학습자를 만들어내어, 보이지 않는 로봇 실체 및 API에서 새로운 작업을 컨텍스트 안에서 학습할 때의 성공률을 31.5% 향상시킨다는 결과를 제시합니다.
- 추가 자료와 영상, 코드, 데모는 https://robot-teaching.github.io/ 웹사이트에서 확인할 수 있습니다.

### [OneBit: Towards Extremely Low-bit Large Language Models](https://arxiv.org/abs/2402.11295)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/65YtBbORH890uHj5Ti1J9.png)

Vote: 9

Authors: Qingfu Zhu, Yuzhuang Xu, Weidong Liu, Wanxiang Che, Zonghan Yang, Shuo Wang, Xu Han, Zhiyuan Liu

- 모델 양자화는 모델의 가중치 행렬을 저비트 폭 값으로 표현하여, 대형 언어 모델(LLMs) 배포 시 저장공간과 연산 오버헤드를 줄이는 유망한 접근 방식입니다.
- 기존의 양자화 방법들은 비트 폭을 극단적으로 줄일 경우 성능 저하가 심하여, 대체로 4비트 또는 8비트 값을 사용하지만, 본 논문은 LLM의 가중치 행렬을 1비트로 양자화함으로써 극도의 저비트 폭 배포를 가능하게 합니다.
- OneBit이라는 1비트 양자화 인식 훈련(QAT) 프레임워크를 소개하였으며, 이는 LLM을 더욱 효과적으로 양자화하기 위한 새로운 1비트 매개변수 표현 방법과 QAT 프레임워크의 수렴 속도를 개선하는 기반 매트릭스 분해 방식의 효과적인 매개변수 초기화 방법을 포함합니다.
- 충분한 실험 결과에 따르면 OneBit은 1비트 가중치 행렬만을 사용할 때도 견고한 훈련 과정을 유지하면서 높은 성능(비양자화 성능의 최소 83%)을 달성할 수 있음을 나타냅니다.

### [Reformatted Alignment](https://arxiv.org/abs/2402.12219)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/4ILagzVdG_LnwZS0lUEAp.png)

Vote: 8

Authors: Shwai He, Jiewen Hu, Haoyang Zou, Run-Ze Fan, Xuefeng Li, Ethan Chern, Pengfei Liu, Junlong Li

- 큰 언어 모델들(Large Language Models, LLMs)을 인간의 가치와 정렬시키는 데에는 미세조정 데이터의 품질이 매우 중요하다.
- 현재 데이터 품질을 개선하는 방법들은 인력이 많이 들거나 LLM의 환각으로 인한 사실 오류가 발생하기 쉽다.
- 본 논문에서는 기존 지시 데이터의 품질을 향상시켜 인간의 가치와 더 잘 조화시킬 수 있는 ReAlign이라는 간단하지만 효과적인 접근 방식을 탐구한다.
- ReAlign은 사람의 주석 필요성과 환각 및 확장성의 어려움을 최소화하면서 기존의 정렬 기술과 직교한다.
- 실험 결과, ReAlign은 LLM의 일반적인 정렬 능력, 수학적 추론, 사실성, 가독성을 크게 향상시켰다.
- 추가 데이터나 고급 훈련 기법 없이도, 응답을 단순히 포맷을 변경함으로써 LLaMA-2-13B의 GSM8K에 대한 수학적 추론 능력을 정확도 46.77%에서 56.63%로 향상시켰다.
- ReAlign 데이터의 5%만 사용하여도 알파카(Alpaca) 데이터셋에 의해 측정된 일반 정렬 능력이 67% 향상되었다.
- 이 연구는 LLM의 과학과 기계적 해석 가능성에 대한 추가 연구의 필요성을 강조하며 관련 코드와 데이터를 https://github.com/GAIR-NLP/ReAlign에서 공개하여 향후 연구를 지원한다.

### [LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration](https://arxiv.org/abs/2402.11550)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ALV8aFV_4Vd4S8j79CaFm.png)

Vote: 8

Authors: Can Zu, Wei He, Hao Xu, Qi Zhang, Yi Lu, Yiwen Ding, Jun Zhao, Tao Gui, Xuanjing Huang

- 대규모 언어 모델은 언어 이해와 복잡한 추론 작업 수행에서 인상적인 성능을 보였으나, 긴 문맥을 다루는 모델들은 훈련 비용과 추론 지연이 높다는 문제점을 가지고 있다.
- 기존 고급 모델들조차 100k 토큰을 넘는 입력을 처리할 때 오류를 내는 현상이 있으며, 이러한 현상을 '중간에서의 손실'이라고 한다.
- 본 논문에서는 다중 에이전트 협력에 기반한 LongAgent 방법을 제안하여 언어 모델들(예: LLaMA)을 128k 문맥으로 확장하고, GPT-4와 비교하여 장문의 텍스트 처리에서 잠재적인 우수성을 보여준다.
- LongAgent에서 리더 에이전트는 사용자의 의도를 이해하고 팀 멤버들이 문서에서 정보를 수집하도록 지휘한다.
- 팀 멤버들의 착각으로 인해 정확한 정보를 얻는 것은 쉬운 일이 아니며, 수십에서 수백 명의 멤버 반응으로부터 오는 응답의 충돌을 해결하기 위해 상호 멤버 간의 의사소통 메커니즘이 개발되었다.
- 실험 결과에 따르면 LongAgent는 장문 텍스트 처리를 위한 유망한 대안을 제공하며, LLaMA-7B로 구현된 에이전트 팀은 128k 장문 텍스트 검색, 멀티홉 질문 답변 등의 업무에서 GPT-4에 비해 상당한 개선을 보인다.

### [CoLLaVO: Crayon Large Language and Vision mOdel](https://arxiv.org/abs/2402.11248)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/t_sywR7VOSJIZuTXBOhEr.png)

Vote: 6

Authors: Beomchan Park, Chae Won Kim, Yong Man Ro, Byung-Kwan Lee

- 대규모 언어 모델(LLMs)과 지시 튜닝의 성공에 힘입어 비전 언어 모델(VLMs)이 범용 모델로의 발전을 추구하고 있지만, 현재 VLMs가 이미지 내의 객체에 대해 질문하는 '어떤 객체들이 그림속에 있는가?' 또는 '지정된 경계 상자에 해당하는 객체는 무엇인가?'와 같은 객체 수준의 이미지 이해 능력을 진정으로 갖추고 있는지는 아직 탐구되지 않았다.
- 연구 결과에 따르면 현재 VLMs의 이미지 이해 능력은 비전 언어(VL) 작업에 대한 제로샷 성능과 강한 상관관계를 가지고 있으며, 이는 VLM을 VL 작업에 통달하게 만들기 위해 기본 이미지 이해를 중시해야 함을 시사한다.
- 객체 수준의 이미지 이해를 향상시키기 위해, 우리는 판옵틱 색도표를 기반으로 하는 새로운 시각적 프롬프트 튜닝 방법인 크레용 프롬프트(crayon prompt)와 함께 지시 튜닝을 통합한 CoLLaVO(Crayon Large Language and Vision mOdel)를 제안한다.
- 시각적 지시 튜닝 동안 객체 수준의 이미지 이해를 잊어버리지 않고 보존하는 학습 전략인 Dual QLoRA를 제시하며, 이를 통해 다수의 VL 벤치마크에서의 제로샷 성능의 중대한 향상을 달성한다.

### [GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements](https://arxiv.org/abs/2402.10963)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-brTuoSBKqV8_yUdP8yE6.png)

Vote: 6

Authors: Alex Havrilla, Sharath Raparthy, Roberta Railneau, Jane Dwivedi-Yu, Christoforus Nalmpantis, Maksym Zhuravinskyi, Eric Hambro

- 최첨단 언어 모델은 수학, 과학, 코딩 작업 등에서 인상적인 추론 정제(refinement) 능력을 보여주지만, 외부 피드백 없이 추론을 언제, 어디에서 수정해야 할지 결정하는 데 어려움을 겪는 것으로 나타났습니다.
- 본 논문은 최종 답변의 정확성을 예측하여 개정할 시기를 알려주는 결과 기반 보상 모델(Outcome-based Reward Models, ORMs)과 중간 단계의 정확성을 예측하여 수정할 위치를 알려주는 과정 기반 보상 모델(Process Based Reward Models, PRMs)의 한계를 극복하는 새로운 모델인 단계별 ORMs(Stepwise ORMs, SORMs)를 제안합니다.
- SORMs는 인간의 주석 없이 합성 데이터만으로 훈련되며 현재 정책을 여러 번 샘플링할 때 최종 답변의 정확성을 예측하여, ORMs보다 부정확한 추론 단계를 더 정확하게 탐지하는데 도움을 줍니다.
- 또한, 질문과 초안 해결책만을 입력으로 사용하는 글로벌 정제 모델과 첫 번째 추론 오류 위치를 알려주는 비판을 추가로 입력받는 로컬 정제 모델을 훈련합니다.
- SORM으로 훈련된 데이터를 재사용하여 두 모델 모두에 대해 합성 훈련 데이터를 생성하였으며, ORM을 재순위 부여기로 사용하는 글로벌 및 로컬 정제의 결합은 각각 개별적으로, 그리고 최고의 세 가지 샘플 기준선보다 현저히 높은 성능을 보였습니다.
- 이러한 전략을 사용하여 이미 RL(Reward Learning)을 통해 미세 조정된 LLaMA-2 13B 모델의 GSM8K에 대한 정확도를 탐욕적 샘플링으로 53%에서 65%로 향상시킬 수 있었습니다.

### [DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation](https://arxiv.org/abs/2402.11929)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/aRiZlwe4dVmAAyASQ947y.png)

Vote: 6

Authors: Pieter Peers, Yue Dong, Youkang Kong, Chong Zeng, Xin Tong, Hongzhi Wu

- 본 논문은 텍스트 기반 확산 모델을 이용한 이미지 생성 과정에서 미세한 빛 조절을 가능하게 하는 새로운 방법을 제시한다.
- 기존의 확산 모델은 조명 조건을 변화하여 이미지를 생성할 능력은 있지만, 추가적인 가이드 없이는 이미지 내용과 조명을 상관시키는 경향이 있고, 텍스트 프롬프트는 세부적인 조명 설정을 기술하는 데 표현력이 부족하다.
- 컨텐츠 제작자가 이미지 생성 중 조명을 세밀하게 제어할 수 있도록, 우리는 텍스트 프롬프트에 목표 조명 아래에서 균질한 표준 재질로 렌더링된 장면 기하학의 시각화인 'radiance hints'(방사량 힌트)를 추가로 제공한다.
- 장면 기하학이 미리 알려져 있지 않기 때문에, 정확한 방사량 힌트는 필요하지 않으며, 확산 프로세스를 올바른 방향으로 유도하기만 하면 된다.
- 이미지 생성을 위한 조명 제어 방법으로 세 단계를 소개한다: 첫 단계에서는 표준 사전 훈련된 확산 모델을 사용하여 조명이 제어되지 않은 초기 이미지를 생성한다.
- 다음에, 두 번째 단계에서는 대상 조명과 추정된 주요 객체의 간략한 형태에 기반한 방사량 힌트를 사용하여 개선된 확산 모델인 'DiLightNet'으로 생성된 이미지의 전경 객체를 재합성하고 정제한다.
- 세 번째 단계에서는 전경 객체의 조명과 일치하는 배경을 재합성한다.
- 다양한 텍스트 프롬프트와 조명 조건에 대해 우리의 조명 제어 확산 모델을 시연하고 검증한다.

### [Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis](https://arxiv.org/abs/2402.12377)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/swTfRzGG-4tsL5oJ8ZPsu.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/swTfRzGG-4tsL5oJ8ZPsu.mp4" muted="false"></video></div>

Vote: 4

Authors: Richard Szeliski, Pratul P. Srinivasan, Ben Mildenhall, Stephan Garbin, Dor Verbin, Jonathan T. Barron, Andreas Geiger, Peter Hedman, Christian Reiser

- 본 연구는 표면 기반 뷰 합성 알고리즘의 저전력 요구 장점을 유지하면서도 세밀한 구조물의 재현에 어려움을 겪는 문제를 해결하고자 함.
- NeRF와 같은 고가의 볼륨 기반 방법들이 세밀한 기하학적 세부를 잘 재현하는 반면, 종종 "흐릿한" 방식으로 기하학적 구조를 표현하여 정확한 표면 위치 파악이 어렵다.
- 연구팀은 연속 밀도 필드 대신 이산 투명도 그리드를 사용하여 표면에서 투명도 값이 0에서 1로 불연속적으로 전환되도록 함으로써, 밀도 필드가 표면으로 수렴하도록 개선함.
- 픽셀 당 다중 광선을 캐스팅하는 앤티 앨리어싱 방식을 채택하여 반투명 복셀을 사용하지 않고도 가려짐 경계와 서브픽셀 구조를 모델링함.
- 훈련의 끝에 투명도 값이 이진화되도록 유도하여 표면 기하학을 추출하기 쉽게 만드는 이진 엔트로피를 최소화함.
- 마지막으로 퓨전 기반 메싱 전략을 개발하고 메시 단순화 및 외관 모델 피팅을 수행하여 모바일 기기에서 실시간 렌더링이 가능한 소형 메시를 생성함.
- 제안하는 모델은 기존 메시 기반 접근법에 비해 상당히 높은 뷰 합성 품질을 달성함.

### [Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning](https://arxiv.org/abs/2402.11690)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/JYvyHYXHmY7pVf71xYjsU.png)

Vote: 4

Authors: Rulin Shao, Di Jin, Lifu Huang, Chao Feng, Ying Shen, Yu Cheng, Qifan Wang, Trevor Ashby, Zhiyang Xu

- 시각-언어 모델(VLMs)의 다재다능한 시각 보조 기능에도 불구하고, 기존 VLM 프레임워크 내에서 사전 학습 및 시각적 지시 학습의 과제 다양성 부족, GPT-4로 합성된 학습 데이터의 주석 오류 및 편향과 같은 두 가지 주요 도전 과제가 있습니다.
- 이러한 도전 과제를 해결하기 위해 연구팀은 학술 데이터셋에서 출처를 찾아 187가지 다양한 과제와 1,664,261개의 인스턴스를 포함하는 현재까지 공개적으로 가장 다양한 시각적 지시 학습 데이터셋인 Vision-Flan을 구축하였고, 각 과제마다 전문가가 작성한 지시사항이 포함되어 있습니다.
- 또한 연구팀은 먼저 Vision-Flan에서 VLMs를 세밀하게 조정한 후 GPT-4로 합성된 데이터로 추가적으로 조정하는 두 단계 지시 학습 프레임워크를 제안합니다.
- 이러한 두 단계 튜닝 프레임워크는 전통적인 단일 단계 시각적 지시 학습 프레임워크보다 뛰어난 성능을 발휘하며, 다양한 멀티모달 평가 벤치마크에서 최신 성능을 달성합니다.
- 마지막으로, 시각적 지시 학습을 더 깊이 이해하기 위한 분석을 수행한 결과, GPT-4로 합성된 데이터는 VLMs의 능력을 크게 향상시키기보다는 모델의 반응을 인간이 선호하는 형식으로 조정하며, 소량(예: 1,000개)의 GPT-4 합성 데이터만으로도 VLM 반응을 인간 선호와 효과적으로 일치시킬 수 있음을 밝혀냈습니다.
- 시각적 지시 학습은 주로 대규모 언어 모델(LLMs)이 시각적 특성을 이해하는 데 도움이 됩니다.

### [Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability](https://arxiv.org/abs/2402.12225)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/FFFHOjH0KoMcNNLWAuvH0.png)

Vote: 3

Authors: Simian Luo, Xiangyang Xue, Tiejun Huang, Ying Tai, Yu Wang, Zhenyu Zhang, Bo Zhao, Yinda Zhang, Chengjie Wang, Yunsheng Wu, Xuelin Qian, Yanwei Fu

- 본 논문은 3D 영역에서 자동회귀 모델을 확장하여 용량 및 확장성 측면에서 3D 형상 생성 능력을 개선하고자 한다.
- 약 90만 개의 다양한 메쉬, 포인트, 복셀, 렌더링 이미지 및 텍스트 캡션을 포함하는 광범위한 3D 데이터셋 컬렉션인 Objaverse-Mix를 활용하여 대규모 모델 훈련을 촉진한다.
- 3D 자동회귀를 직접 적용하면 볼륨 그리드에서의 높은 계산 요구와 그리드 차원에 따른 모호한 자동회귀 순서로 인해 3D 형상 품질이 떨어짐을 직면한다.
- 이러한 문제를 해결하기 위해 대용량의 Argus3D 프레임워크를 제안하며, 이는 볼륨 그리드 대신 잠재 벡터 기반의 이산 표현 학습을 도입하여 계산 비용을 줄이고, 보다 다루기 쉬운 순서에서 합동 분포를 학습해 기하학적 세부 사항을 보존한다.
- 조건부 생성의 용량은 포인트 클라우드, 카테고리, 이미지 및 텍스트 같은 다양한 조건 입력을 잠재 벡터에 단순히 연결함으로써 실현된다.
- 모델 아키텍처의 단순함 덕분에 36억 개의 매개변수로 모델을 확장할 수 있어 다양한 3D 생성의 품질을 더욱 향상시킨다.
- 다양한 범주에서의 다양하고 정확한 형상을 합성하는 Argus3D의 능력을 강조하는 것으로, 네 가지 생성 작업에 대한 광범위한 실험을 통해 뛰어난 성능을 입증한다.

