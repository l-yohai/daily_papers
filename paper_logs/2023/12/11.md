## Daily Papers (2023-12-11)

### [SparQ Attention: Bandwidth-Efficient LLM Inference](https://arxiv.org/abs/2312.04985)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Vx2jSyMVBp3V5t0uIbYqg.png)

Vote: 27

Authors: Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas Orr

- 대규모 언어 모델(LLMs)은 다양한 새로운 가능성을 열었지만, 그들의 높은 계산 요구로 인해 보편적인 사용이 도전적으로 남아 있다.
- 많은 수의 샘플을 처리하고 긴 컨텍스트를 사용해야하는 가장 유용한 응용 프로그램들은 모델의 메모리 통신 부하를 크게 증가시킨다.
- SparQ Attention은 주목 블록 내의 메모리 대역폭 요구 사항을 줄임으로써 LLMs의 추론 처리량을 증가시키는 기술을 소개한다.
- 이 기술은 사전 훈련 설정을 수정하거나 추가 미세 조정을 요구하지 않고, 기존 LLMs에 추론 중에 바로 적용할 수 있다.
- SparQ Attention이 어떻게 Llama 2와 Pythia 모델을 다양한 다운스트림 작업에서 평가함으로써 정확성 손실 없이 주목 메모리 대역폭 요구 사항을 최대 8배까지 감소시킬 수 있는지 보여준다.

### [DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models](https://arxiv.org/abs/2312.05107)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/AJbMsPax_q2MX_w46LV3M.qt)

Vote: 18

Authors: Mengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng Hui, Xiefan Guo, Xianhui Lin, Haolan Xue, Chen Shi, Xiaowen Li, Aojie Li, Miaomiao Cui, Peiran Ren, Xuansong Xie

- 본 논문에서는 특정 인물과 포즈 시퀀스를 바탕으로 고품질의 맞춤형 인간 댄스 비디오를 생성할 수 있는 확산 기반의 제어 가능한 비디오 생성 프레임워크인 DreaMoving을 제시합니다.
- 목표 정체성 및 포즈 시퀀스가 주어지면, DreaMoving은 그 포즈 시퀀스에 따라 어디서나 목표 정체성이 춤추는 비디오를 생성할 수 있습니다.
- 이를 위해 본 논문은 움직임을 제어하는 비디오 컨트롤넷(Video ControlNet)과 정체성을 보존하는 콘텐츠 가이더(Content Guider)를 제안합니다.
- 제안된 모델은 사용이 간편하며, 다양한 결과물을 생성하기 위해 대부분의 스타일화된 확산 모델에 적응될 수 있습니다.
- 프로젝트 페이지는 https://dreamoving.github.io/dreamoving/ 에서 확인할 수 있습니다.

### [Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors](https://arxiv.org/abs/2312.04963)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/VBXy3qRK8Bpq_yQJi3vKP.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/VBXy3qRK8Bpq_yQJi3vKP.mp4" muted="false"></video></div>

Vote: 10

Authors: Lihe Ding, Shaocong Dong, Zhanpeng Huang, Zibin Wang, Yiyuan Zhang, Kaixiong Gong, Dan Xu, Tianfan Xue

- 종전의 3D 생성 연구는 주로 2D 기반 모델을 3D 공간으로 상향 프로젝션하는데 집중했으나, 이러한 방식은 명시적인 3D 선행 정보 없이 기하학적 비정상과 다시점 불일치를 유발할 수 있습니다.
- 연구자들은 직접 3D 데이터셋에서 훈련함으로써 3D 객체의 진정성을 향상시키려고 노력했지만, 3D 데이터셋의 제한된 텍스처 다양성 때문에 낮은 품질의 텍스처 생성이라는 문제에 직면했습니다.
- 본 논문에서는 3D 정확성과 2D 텍스처의 풍부함을 모두 보존하기 위해 3D와 2D 확산 과정을 모두 통합하는 통합 프레임워크인 Bidirectional Diffusion(BiDiff)를 제안합니다.
- 단순한 결합이 일관성 없는 생성 결과를 초래할 수 있으므로, 저자들은 새로운 양방향 지도 방법으로 두 과정을 연결합니다.
- 또한, 제안된 방법은 최적화 기반 모델의 초기화로 사용될 수 있으며, 3D 모델의 품질과 최적화의 효율성을 더욱 향상시켜, 생성 과정을 3.4시간에서 20분으로 단축시킬 수 있습니다.
- 실험 결과는 제안된 모델이 고품질, 다양하며 확장 가능한 3D 생성을 달성함을 보여줍니다.
- 프로젝트 웹사이트는 다음 주소에서 확인할 수 있습니다: https://bidiff.github.io/.

### [Customizing Motion in Text-to-Video Diffusion Models](https://arxiv.org/abs/2312.04966)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ErsTSwgRxP9FJurw2pDg8.png)

Vote: 6

Authors: Joanna Materzynska, Josef Sivic, Eli Shechtman, Antonio Torralba, Richard Zhang, Bryan Russell

- 텍스트-비디오 생성 모델에 사용자 지정 동작을 추가하는 접근 방식을 소개하여, 원래 훈련 데이터에 표시된 동작을 넘어서는 기능을 확장합니다.
- 특정 동작을 보여주는 몇 가지 비디오 샘플을 사용하여, 입력된 동작 패턴을 다양한 텍스트로 지정된 시나리오에 대해 학습하고 일반화합니다.
- 연구의 첫 번째 기여는 이미 있는 텍스트-비디오 모델을 미세조정하여 입력 예시에서 보여지는 동작과 새로운 고유 토큰 사이의 새로운 맵핑을 학습하는 것입니다.
- 새로운 맞춤 동작에 대한 과적합을 방지하기 위해 비디오에 대한 정규화 접근 방식을 도입했습니다.
- 미리 훈련된 모델의 동작 선험 지식을 활용하여, 우리의 방법은 사용자 지정 동작을 하는 여러 사람이 등장하는 새로운 비디오를 생성할 수 있으며 다른 동작과 결합하여 해당 동작을 구현할 수 있습니다.
- 또한 우리의 접근 방식은 동작과 외형의 다양한 사용자 정의에 확장되어 독특한 캐릭터와 구별되는 동작이 특징인 비디오 생성을 가능하게 합니다.
- 끝으로, 우리의 방법을 확인하기 위해 학습된 사용자 지정 동작을 정량적으로 평가하는 방법을 도입하고 체계적인 소거 연구를 수행했습니다.
- 우리는 우리의 방법이 동작 사용자 정의 작업으로 확장될 때 이전의 외형 기반 사용자 정의 접근법보다 현저하게 뛰어남을 보여줍니다.

### [PathFinder: Guided Search over Multi-Step Reasoning Paths](https://arxiv.org/abs/2312.05180)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/czhomgicWZGGL2IEhZOq7.png)

Vote: 5

Authors: Olga Golovneva, Sean O'Brien, Ramakanth Pasunuru, Tianlu Wang, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz

- 본 논문은 다단계 추론 작업에서 여러 단계의 추론이 필요한 과제에 대해 최신 모델들이 여전히 직면하고 있는 도전을 해결하고자 'PathFinder'라는 나무 탐색 기반의 추론 경로 생성 방법을 제안합니다.
- 이 방법은 다양한 샘플링 방법과 매개변수를 변화시키는 동적 디코딩을 통해 다양한 분기와 다단계 추론을 강화합니다.
- PathFinder는 새로운 품질 제약 조건, 가지치기와 탐색 방법을 통합하여 생성의 효율성과 품질을 개선합니다.
- 또한, 후보 선택을 개선하기 위해 점수 매기기 및 순위 매기기 기능을 포함합니다.
- 제안된 접근 방식은 복잡한 산술 및 상식 추론 작업에서 경쟁 기준을 평균 6% 앞서며, 긴 추론 체인에 대한 일반화 능력이 좋고, 특히 대규모 분기 요인의 경우 빔 탐색과 유사한 복잡성을 보여줍니다.

### [MVDD: Multi-View Depth Diffusion Models](https://arxiv.org/abs/2312.04875)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-HrkcnEiao48s2HpWljfS.png)

Vote: 3

Authors: Zhen Wang, Qiangeng Xu, Feitong Tan, Menglei Chai, Shichen Liu, Rohit Pandey, Sean Fanello, Achuta Kadambi, Yinda Zhang

- 노이즈 감소 확산 모델은 2D 이미지 생성에 뛰어난 결과를 보였지만, 3D 형태 생성에 그 성공을 재현하는 것은 여전히 도전적인 과제입니다.
- 본 논문에서는 복잡한 3D 형태를 노이즈 제거가 쉬운 2D 데이터 형식으로 표현하는 다시점 깊이(Multi-View Depth)를 활용하여, 고품질의 세밀한 상세를 포함하는 20K 이상의 점을 가진 밀집 포인트 클라우드를 생성할 수 있는 확산 모델인 MVDD를 제안합니다.
- 다시점 깊이에서 3D 일관성을 강화하기 위해, 이웃하는 뷰에 대한 노이즈 감소 단계를 조건화하는 에피폴라 선분 주의(epipolar line segment attention)를 도입합니다.
- 또한, 깊이 맵의 정렬을 보장하기 위해 확산 단계로 깊이 융합 모듈을 통합합니다.
- MVDD는 표면 재구성을 통해 고품질 3D 메시도 생성할 수 있으며, 깊이 완성(depth completion)과 같은 다른 작업에서도 뛰어난 성능을 보입니다.
- MVDD는 3D 사전정보로서 기능할 수 있으며, GAN 인버전과 같은 다운스트림 작업에 큰 향상을 제공합니다.
- 다양한 실험을 통한 최신 결과는 MVDD가 3D 형태 생성, 깊이 완성 및 다운스트림 작업을 위한 3D 사전정보로서의 잠재력에서 뛰어난 능력을 보여줍니다.

### [EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism](https://arxiv.org/abs/2312.04916)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/9oX4wgR6gmhFCqhIpgNVP.png)

Vote: 3

Authors: Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou

- EE-LLM은 대규모로 대형 언어 모델(LLMs)의 조기 종료(Early-Exit) 기능을 갖춘 훈련 및 추론을 위한 프레임워크를 제공합니다.
- 이 프레임워크는 3D 병렬 구조를 이용하여 대형 언어 모델의 스케일업을 지원하며, 조기 종료 기능을 위한 알고리즘 혁신과 성능 최적화를 실현합니다.
- EE-LLM은 파이프라인 병렬 구조를 사용하여 쉽게 역전파가 가능한 경량 메소드 및 이에 관련된 계산을 위해 파이프라인 일정에서 놀고 있는 자원들을 사용하는 기술을 포함합니다.
- 자동 회귀생성의 KV 캐싱과 호환되는 두 가지 방법을 통해서 조기 종료 추론을 진행합니다.
- 분석 및 실증 연구는 EE-LLM이 표준 LLM 교육에 비해 미미한 계산 오버헤드로 큰 훈련 효율성을 달성한다는 것과, 결과 품질을 저하시키지 않으면서도 뛰어난 추론 속도 향상을 이룰 수 있음을 보여줍니다.
- 연구와 채택을 촉진하기 위해 https://github.com/pan-x-c/EE-LLM에서 EE-LLM을 공개합니다.

### [Localized Symbolic Knowledge Distillation for Visual Commonsense Models](https://arxiv.org/abs/2312.04837)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/YvAuPStFyPBN8PhbZYa5r.png)

Vote: 1

Authors: Jae Sung Park, Jack Hessel, Khyathi Raghavi Chandu, Paul Pu Liang, Ximing Lu, Peter West, Youngjae Yu, Qiuyuan Huang, Jianfeng Gao, Ali Farhadi, Yejin Choi

- 시각-언어(VL) 모델의 지시사항을 이용하는 인터페이스는 다양한 멀티모달 작업을 제로샷 방식으로 지원하지만, 이미지 내 특정 영역을 지목하고 접근하는 기능이 직접적으로 가능하지 않은 문제점이 있습니다.
- 이를 해결하기 위해, 사용자가 입력으로 하나 이상의 영역을 지정할 수 있는 Localized Visual Commonsense 모델을 구축했습니다.
- 이 모델은 VL 모델 세트에 의해 자동 생성된 글로벌 이미지 기술과 로컬 영역 기술을 사용하여, 대규모 언어 모델(LLM)에서 상식 지식을 샘플링하여 훈련됩니다.
- 고품질 예시를 선택하는 별도로 훈련된 비평 모델(critic model)을 사용하여, 지역화된 상식 말뭉치에 대한 훈련이 기존 VL 모델에 참조 입력 인터페이스를 지원하는 기능을 통합할 수 있음을 발견했습니다.
- 제로샷 설정에서의 실제 결과와 인간 평가를 통해, 우리의 지식 증류 방법이 LLM에 생성된 참조 표현을 전달하는 기본선보다 더 정밀한 VL 모델 추론을 가능하게 함을 입증했습니다.
