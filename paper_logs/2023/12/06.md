## Daily Papers (2023-12-06)

### [FaceStudio: Put Your Face Everywhere in Seconds](https://arxiv.org/abs/2312.02663)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pcRGMysi0Rn6ElQxmaYVd.png)

Vote: 17

Authors: Yuxuan Yan, Chi Zhang, Rui Wang, Pei Cheng, Gang Yu, Bin Fu

- 본 연구는 주체의 정체성을 유지하면서 개인화된 스타일을 추가하는 매력적인 이미지 생성 작업인 정체성 보존 이미지 합성에 대해 조사합니다.
- 기존 방법들은 맞춤형 이미지 생성에 있어 진전을 보였으나, 많은 자원과 시간이 소요되는 미세 조정, 다중 참조 이미지가 필요한 등의 단점이 있습니다.
- 우리는 이러한 도전 과제를 극복하기 위하여, 특히 인간 이미지에 초점을 맞춘 정체성 보존 합성의 새로운 접근 방식을 소개합니다.
- 우리의 모델은 직접적인 전방향 메커니즘을 활용하여, 강도 높은 미세조정의 필요성을 피함으로써 빠르고 효율적인 이미지 생성을 촉진합니다.
- 혁신의 핵심은 스타일화된 이미지, 얼굴 이미지 및 텍스트 프롬프트를 결합한 하이브리드 유도 프레임워크를 사용하여 이미지 생성 과정을 안내하는 것입니다.
- 이 독특한 조합을 통해 우리의 모델은 예술적 초상화 및 정체성이 혼합된 이미지 등 다양한 어플리케이션을 생성할 수 있습니다.
- 양적 및 질적 평가를 포함한 실험 결과는 특히 놀라운 효율성과 주체의 정체성을 높은 정확도로 보존하는 능력에 있어서, 우리의 방법이 기존 베이스라인 모델 및 이전 작업보다 우수함을 보여줍니다.

### [ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation](https://arxiv.org/abs/2312.02201)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/d0VHat4w8gM400SkeWPP7.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/d0VHat4w8gM400SkeWPP7.mp4" muted="false"></video></div>

Vote: 16

Authors: Peng Wang, Yichun Shi

- "ImageDream"은 이미지를 사용하여 3D 객체를 생성하는 새로운 유형의 다면적 확산 모델을 소개합니다.
- 기존의 이미지 기반 방법보다 높은 품질의 3D 모델을 생성할 수 있는 능력이 뛰어납니다.
- 이 모델은 이미지 속 객체에 대한 표준 카메라 좌표를 이용하여 보다 정교한 시각적 기하학적 정확도를 달성합니다.
- 입력 이미지에 기반하여 확산 모델 내 각 블록에서의 다양한 수준의 제어를 설계하여, 전체 객체 레이아웃을 형성하는 글로벌 제어와 이미지 세부 사항을 미세 조정하는 로컬 제어를 가능하게 합니다.
- ImageDream의 유효성은 표준 프롬프트 목록을 사용한 광범위한 평가를 통해 입증되었습니다.
- 자세한 정보는 프로젝트 페이지(https://Image-Dream.github.io)에서 확인할 수 있습니다.

### [X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model](https://arxiv.org/abs/2312.02238)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/dJ1Z4zY9XfLcaBwF2aS4_.png)

Vote: 15

Authors: Lingmin Ran, Xiaodong Cun, JiaWei Liu, Rui Zhao, Song Zijie, Xintao Wang, Jussi Keppo, Mike Zheng Shou

- X-Adapter는 ControlNet, LoRA와 같은 플러그-앤플레이 모듈들이 재훈련 없이도 개선된 텍스트-이미지 확산 모델(예: SDXL)과 직접 작동할 수 있게 하는 범용 업그레이더를 소개합니다.
- 이 목표를 달성하기 위해 X-Adapter는 새로운 텍스트-이미지 데이터 쌍으로 동결된 업그레이드 모델을 제어하기 위한 추가 네트워크를 훈련합니다.
- X-Adapter는 다양한 플러그인의 연결부를 보존하기 위해 구 모델의 동결된 복사본을 유지합니다.
- 또한 X-Adapter는 기능 재매핑을 위해 서로 다른 버전의 모델 디코더를 연결하는 훈련 가능한 매핑 레이어를 추가합니다.
- 재매핑된 특성은 업그레이드된 모델에 대한 가이드로 사용됩니다.
- X-Adapter의 가이드 능력을 강화하기 위해 업그레이드된 모델을 위한 null-text 훈련 전략을 적용합니다.
- 훈련 후, X-Adapter와 업그레이드된 모델의 초기 노이즈를 맞추기 위한 2단계 덴로이징 전략을 도입합니다.
- 우리의 전략 덕분에 X-Adapter는 다양한 플러그인과의 범용 호환성을 시연하고, 서로 다른 버전의 플러그인이 함께 작동할 수 있게 하여 확산 커뮤니티의 기능을 확장합니다.
- 제안된 방법의 효과를 검증하기 위해 광범위한 실험을 실시하였으며, 결과는 X-Adapter가 업그레이드된 기본 확산 모델에서 더 넓은 적용을 촉진할 수 있음을 보여줍니다.

### [Describing Differences in Image Sets with Natural Language](https://arxiv.org/abs/2312.02974)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/zmpBLE85AvoxI2824gWDU.png)

Vote: 9

Authors: Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, Serena Yeung-Levy

- 이미지 세트 간의 차이를 식별하는 것은 모델 행동 이해와 데이터셋 분석에 중요하나 수천 개의 이미지를 수작업으로 검토하는 것은 비실용적입니다.
- 본 논문에서는 이미지 세트 D_A와 D_B 사이의 차이를 자동으로 기술하는 작업, '세트 차이 캡셔닝(Set Difference Captioning)'을 탐구합니다.
- 이 작업은 이미지 세트를 입력받아 D_A에 비해 D_B에서 더 자주 참인 설명을 출력합니다.
- 저자들은 먼저 이미지 세트로부터 후보 차이 설명을 제안하고, 두 세트를 구분할 수 있는 정도를 확인하여 후보들을 재순위하는 두 단계 접근법을 개요합니다.
- VisDiff는 먼저 이미지에 캡션을 달고 언어 모델에게 후보 설명을 제안하도록 요청한 다음, CLIP을 사용하여 이러한 설명을 재순위합니다.
- VisDiff를 평가하기 위해, 저자들은 참된 차이 설명이 있는 187 쌍의 이미지 세트를 포함하는 VisDiffBench 데이터셋을 수집합니다.
- VisDiff는 데이터셋(예: ImageNet과 ImageNetV2 비교), 분류 모델(예: 제로샷 CLIP과 감독된 ResNet 비교), 모델 실패 모드 요약(감독된 ResNet), 생성 모델 차이 특성화(예: StableDiffusionV1과 V2) 및 이미지 기억에 무엇이 영향을 주는지 발견하는 등 다양한 도메인에 적용됩니다.
- VisDiff를 사용하여 데이터셋과 모델에서 흥미롭고 이전에 알려지지 않은 차이를 찾아내어 섬세한 통찰력을 드러내는 데에 그 유용성을 입증하였습니다.

### [Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models](https://arxiv.org/abs/2312.02969)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wAE2R4bahtAx6Z_6M8pPo.png)

Vote: 9

Authors: Xinyu Zhang, Sebastian Hofstätter, Patrick Lewis, Raphael Tang, Jimmy Lin

- 기존의 리스트 재정렬기 연구는 모두 GPT 모델에 의존하여 과학적 재현성에 단일 실패 지점을 초래하며 현재 연구 결과가 GPT 모델에만 적용되는지에 대한 우려를 제기합니다.
- 이 연구에서는 GPT에 대한 의존도 없이 효과적인 리스트 재정렬기를 최초로 구축하며, 이는 대규모 언어 모델(LLM)에 대한 연구를 확장합니다.
- 수행한 문서 검색 실험에서 제시한 리스트 재정렬기는 GPT-3.5를 기반으로 한 재정렬기보다 13% 우수한 성능을 보이며, GPT-4 기반 재정렬기의 97% 효과를 달성했습니다.
- 기존의 점별 랭킹을 위해 구축된 훈련용 데이터셋은 리스트 재정렬기를 구축하는 데 부족하며, 높은 품질의 리스트 순위 데이터의 중요성을 강조하며, 인간 주석이 달린 리스트 순위 데이터 자원 구축에 대한 추가적인 작업이 필요함을 시사합니다.

### [Analyzing and Improving the Training Dynamics of Diffusion Models](https://arxiv.org/abs/2312.02696)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8k1uE8BhpEymJ69BJTLxh.png)

Vote: 8

Authors: Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, Samuli Laine

- 현재 대규모 데이터셋에서의 이미지 합성 분야를 장악하고 있는 확산 모델의 트레이닝 동력학을 분석하여 파악했으며, ADM 확산 모델 아키텍처의 효율적이지 않고 불균형한 트레이닝 원인을 규명하고 개선했습니다.
- 네트워크 활성화 및 가중치의 제어되지 않은 크기 변화와 불균형을 조사한 결과, 레이어를 재설계하여 활성화, 가중치, 그리고 업데이트 크기를 예상하에 유지시키는 데 초점을 맞췄습니다.
- 이 철학을 체계적으로 적용하여 관찰된 편차와 불균형을 제거함으로써, 동일한 계산 복잡도에서 훨씬 개선된 네트워크를 얻을 수 있었습니다.
- 수정된 모델을 통해 ImageNet-512 합성에서 이전 기록인 FID 2.41을 빠른 결정론적 샘플링을 사용하여 1.81로 개선하는 성과를 달성했습니다.
- 독립적인 기여로, 트레이닝이 완료된 후에 지수 이동 평균(EMA) 파라미터를 사후 설정하는 방법을 제시하였고, 이를 통해 여러 트레이닝 실행의 비용 없이 EMA 길이를 정밀하게 조정할 수 있음을 밝혔습니다.
- EMA 설정이 네트워크 아키텍처, 트레이닝 시간, 그리고 가이던스와 놀라운 상호 작용을 가지고 있음을 밝혔습니다.

### [Orthogonal Adaptation for Modular Customization of Diffusion Models](https://arxiv.org/abs/2312.02432)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/2kn-mn-ZCBO43tYMgPL5Z.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/2kn-mn-ZCBO43tYMgPL5Z.mp4" muted="false"></video></div>

Vote: 8

Authors: Ryan Po, Guandao Yang, Kfir Aberman, Gordon Wetzstein

- 텍스트-이미지 모델의 맞춤형 기술 발전으로 다양한 맥락과 스타일에서 특정 개념을 생성할 수 있는 새로운 애플리케이션을 가능하게 했지만, 기존 방법들은 개별 개념에 대한 고화질 맞춤화나 제한된 세트에 대해서만 효과적이며, 무한한 개념을 하나의 모델로 처리하는 확장성에는 부족함이 있습니다.
- 이 논문에서는 모듈러 맞춤화(Modular Customization)라는 새로운 문제를 해결하고자 합니다. 이는 개별 개념에 독립적으로 미세 조정된 맞춤화된 모델들을 효율적으로 병합하여 추가적인 계산 비용 없이 한 이미지에서 개념들을 함께 합성할 수 있도록 합니다.
- 본 논문에서는 미세 조정된 모델들이 서로 영향을 주지 않은 채로 직교 잔차 가중치를 갖도록 유도하는 새로운 방법인 Orthogonal Adaptation을 소개합니다. 이는 추론 시간에 맞춤화된 모델들을 최소한의 간섭으로 합칠 수 있게 해줍니다.
- 제안된 방법은 단순하면서도 범용적이며, 모델 아키텍처의 거의 모든 최적화 가능한 가중치에 적용될 수 있습니다. 양적 및 질적 평가를 통해 우리의 방법은 효율성과 정체성 보존 측면에서 기존 기준보다 일관되게 뛰어난 성과를 보여주며, 확장 가능한 디퓨전 모델의 맞춤화를 향한 중요한 진전을 보여줍니다.

### [LivePhoto: Real Image Animation with Text-guided Motion Control](https://arxiv.org/abs/2312.02928)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3wPuJhOuESma9yNupus1z.png)

Vote: 8

Authors: Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, Hengshuang Zhao

- 최근 텍스트를 이용한 비디오 생성 기술이 발전하고 있지만, 대부분의 연구들이 텍스트로 제어되는 영상의 공간적 내용에만 초점을 맞추고 임시적인 움직임을 간과하는 문제점이 있었다.
- 이에 대한 도전으로, 본 연구는 사용자가 흥미로운 이미지를 텍스트 설명으로 애니메이션화 할 수 있는 실용적인 시스템인 LivePhoto를 제안한다.
- 안정적인 텍스트-이미지 생성기(Stable Diffusion)를 기반으로 하여 이미지를 추가 입력으로 받아들일 수 있는 강력한 베이스라인을 구축하였다.
- 개선된 생성기에 시간적 모델링을 위한 모션 모듈을 장착하고 텍스트와 움직임을 더 잘 연결할 수 있는 학습 파이프라인을 제안한다.
- 텍스트가 움직임을 대략적으로만 설명할 수 있고(예: 움직이는 속도 무시), 내용과 움직임 모두를 설명할 수 있다는 점을 고려하여, 모션 강도 추정 모듈 및 텍스트 재가중치 모듈을 도입하여 텍스트-모션 매핑의 모호성을 줄였다.
- 실증적 증거에 따르면, 저희 접근방식은 비디오로 움직임 관련 텍스트 지시를 잘 디코딩할 수 있는 능력이 있으며, 이는 동작, 카메라 이동 또는 공중에서 새로운 내용을 생성하는 것과 같은 동작들을 포함한다.
- 제안된 강도 학습 메커니즘 덕분에, 시스템은 텍스트 외에도 비디오 사용자 정의를 위한 추가적인 컨트롤 신호(즉, 움직임 강도)를 사용자들에게 제공한다.

### [MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures](https://arxiv.org/abs/2312.02963)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/BbFQn3qhh2qCCJVStEdO1.png)

Vote: 7

Authors: Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao Hu, Junyi Zhu, Shuliang Ning, Lingteng Qiu, Chongjie Wang, Shijie Wang, Shuguang Cui, Xiaoguang Han

- 대규모 데이터셋들이 언어 모델 및 이미지 생성 모델의 발전에 기여해왔으나, 사람 중심의 3D 비전 분야에서는 이러한 진보가 관찰되지 않았습니다.
- 고품질의 3D 인간 데이터를 대규모로 수집하는 데에는 중대한 도전이 있기 때문에 기존의 인간 캡처 데이터셋들은 중간 규모로 제한되어 있습니다.
- 이 차이를 극복하기 위해, 우리는 4,500명의 인간 정체성을 포함하는 멀티뷰 인간 행동 시퀀스로 구성된 MVHumanNet 데이터셋을 제시합니다.
- 본 연구는 다양한 정체성과 일상복을 착용한 인간 데이터를 멀티뷰 인간 캡쳐 시스템을 이용해 쉽게 확장 가능한 방법으로 수집하는 데 중점을 두고 있습니다.
- MVHumanNet에는 9,000개의 일상복 아웃핏, 60,000개의 동작 시퀀스, 6억 4500만 개의 프레임과 함께 인간 마스크, 카메라 파라미터, 2D 및 3D 키포인트, SMPL/SMPLX 파라미터, 그리고 해당하는 텍스트 설명과 같은 방대한 어노테이션이 포함되어 있습니다.
- 이 데이터셋의 다양한 2D 및 3D 시각적 작업에서의 잠재성을 탐구하기 위해, 뷰 일관성 동작 인식, 인간 NeRF 재구축, 텍스트 기반의 뷰 비제약 인간 이미지 생성, 2D 뷰 비제약 인간 이미지 및 3D 아바타 생성 등에 대한 선행 연구를 수행하였습니다.
- 광범위한 실험을 통해 MVHumanNet이 제공하는 규모로 인해 성능 개선 및 유효한 응용 프로그램이 가능함을 보여줍니다.
- 현재까지 가장 큰 규모의 3D 인간 데이터셋인 MVHumanNet의 데이터 및 어노테이션 공개가 대규모에서 사람 중심의 3D 작업 분야에서의 혁신을 더욱 촉진할 것을 기대합니다.

### [Fine-grained Controllable Video Generation via Object Appearance and Context](https://arxiv.org/abs/2312.02919)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/eHDmu-m-mUCF_umhAQUnq.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/eHDmu-m-mUCF_umhAQUnq.mp4" muted="false"></video></div>

Vote: 6

Authors: Hsin-Ping Huang, Yu-Chuan Su, Deqing Sun, Lu Jiang, Xuhui Jia, Yukun Zhu, Ming-Hsuan Yang

- 본 연구에서는 객체의 외관 및 맥락을 포함하여 텍스트 프롬프트와 결합하여 위치와 범주를 조절함으로써 세밀한 제어를 달성하는 fine-grained controllable video generation(FACTOR)를 제안하였다.
- FACTOR는 통합된 프레임워크를 통해 기존의 텍스트-투-비디오 모델에 조절 신호를 공동으로 주입하여 세부적으로 제어하는 것을 목표로 한다.
- 모델은 공동 인코더 및 조정 가능한 크로스-어텐션 레이어로 구성되며, 이를 최적화하여 텍스트 프롬프트 및 세밀한 제어와 일치하는 비디오를 생성한다.
- 기존 방법들이 밀집된 제어 신호(예: 엣지 맵)에 의존하는 반면, 본 연구는 사용자 친화적 인터페이스를 통해 객체 수준의 세밀한 제어를 제공한다.
- 우리의 방법은 사전 튜닝 없이 객체의 외관 제어 가능성을 달성하며, 이는 사용자의 주제별 최적화 노력을 감소시킨다.
- 표준 벤치마크 데이터셋과 사용자 제공 입력에 대한 광범위한 실험을 통해 우리 모델이 기존 경쟁 기준보다 제어 가능성 메트릭에서 70% 향상됨을 검증하였다.

### [DragVideo: Interactive Drag-style Video Editing](https://arxiv.org/abs/2312.02216)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/MYkOHmThtBtu912MjUhn_.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/MYkOHmThtBtu912MjUhn_.mp4" muted="false"></video></div>

Vote: 5

Authors: Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, Chi-Keung Tang

- 비디오 내 시각적 콘텐츠 편집은 직접적이고 쉬운 사용자 제어 및 모양, 표현, 레이아웃 변화 후 자연스러운 편집 결과를 달성하는 두 가지 주요 문제에 직면해 있다.
- 이미지 기반 드래그 스타일 편집 기술인 DragGAN에 영감을 받아, 연구진은 시간적 일관성을 유지하면서 드래그 스타일 사용자 상호작용을 채택한 DragVideo를 제안한다.
- DragVideo는 최근의 확산 모델(DragDiffusion)을 활용하여, 원하는 제어를 달성하기 위해 비디오 U-Net에 의해 생성된 확산된 비디오 잠재 변수들을 최적화하는 새로운 Drag-on-Video U-Net(DoVe) 편집 방법을 포함한다.
- 비디오에서 DoVe 방법에 의한 충실한 재구성을 보장하기 위해, 연구진은 Sample-specific LoRA 미세조정과 Mutual Self-Attention 제어를 사용한다.
- 연구진은 드래그 스타일 비디오 편집을 위한 일련의 테스트 예제를 제시하고, 모션 편집, 스켈레톤 편집 등 다양한 도전적인 편집 작업에서 DragVideo의 다재다능함과 일반성을 강조하는 광범위한 실험을 수행한다.
- 연구진은 DragVideo 웹 사용자 인터페이스를 포함한 코드를 공개할 예정이다.

### [ReconFusion: 3D Reconstruction with Diffusion Priors](https://arxiv.org/abs/2312.02981)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/w7GUOQ6vX2MB-Z8DS-GFK.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/w7GUOQ6vX2MB-Z8DS-GFK.mp4" muted="false"></video></div>

Vote: 5

Authors: Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, Aleksander Holynski

- 이 논문은 소수의 사진만을 사용하여 실세계 장면을 재구성하는 ReconFusion 방법을 제시합니다.
- ReconFusion은 합성 및 다시점 데이터셋에서 훈련된 확산 전반적 소개를 활용하여, 적은 수의 입력 이미지로부터 NeRF 기반 3D 재구성 파이프라인을 정규화합니다.
- 제안된 방법은 감시되지 않은 지역에서의 현실적인 기하학 및 질감을 합성하는 동시에 관찰된 영역의 외형을 보존합니다.
- 전방향 및 360도 장면을 포함한 다양한 실세계 데이터셋에서 방대한 평가를 수행하여, 기존의 소수-뷰 NeRF 재구성 기법보다 상당한 성능 개선을 입증합니다.

### [Training Chain-of-Thought via Latent-Variable Inference](https://arxiv.org/abs/2312.02179)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/1t997t65ca_cQBCJTWB-y.png)

Vote: 5

Authors: Du Phan, Matthew D. Hoffman, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, Rif A. Saurous

- 큰 언어 모델(Large language models, LLMs)은 ‘사고의 연쇄(chain-of-thought, CoT)’ 프롬프트를 사용하여 단계적으로 답변을 작업함으로써 더 정확하고 해석 가능한 문제 해결이 가능하다.
- 이러한 LLM의 성능은 지도 학습을 통해 개선될 수 있지만, 정확한 답변뿐만 아니라 그 답변으로 가는 자세한 근거를 수작업으로 제공해야 하므로 비용이 많이 든다.
- 저자들은 가능한 모든 근거를 대략 평균내어 정확한 답변을 생성하도록 CoT 프롬프트 사용의 한계 로그우도를 최대화하는 전략을 제안한다.
- 이 방법의 핵심은 정확한 답변에 조건을 걸고 근거에 대한 후반 분포로부터 표본을 추출하는 것이며, 여기에는 간단한 마르코프 체인 몬테 카를로 (MCMC) 기대값 최대화 (EM) 알고리즘이 사용된다.
- 이 알고리즘은 모델이 개선됨에 따라 그라디언트 추정의 분산을 제로로 만드는 새로운 제어 변화 기법을 포함한다.
- GSM8K와 BIG-Bench Hard의 작업에 기술을 적용한 결과, MCMC-EM 세밀 조정 기술은 예시를 들어 놓고 본 모델의 정확성을 STaR이나 프롬프트 조정, CoT 사용 유무와 관계없이 향상시키는 것으로 나타났다.

### [Alchemist: Parametric Control of Material Properties with Diffusion Models](https://arxiv.org/abs/2312.02970)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/lT_nHGz-Nz3MRmC98rNgW.png)

Vote: 4

Authors: Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia, Dmitry Lagun, Fredo Durand, William T. Freeman, Mark Matthews

- 본 논문에서는 진짜 이미지 속 객체들의 재질 속성(거칠기, 금속성, 알베도, 투명도 등)을 제어하는 방법을 제안한다.
- 이 방법은 사진 리얼리즘으로 알려진 텍스트-이미지 모델의 생성적 선험을 활용하여 스칼라 값과 지시사항을 사용해 저차원 재질 특성을 변경한다.
- 제어된 재질 속성을 가진 데이터셋의 부재를 해결하기 위해 물리 기반의 재질로 된 객관 중심의 합성 데이터셋을 만들었다.
- 이 합성 데이터셋에서 미세 조정된 수정된 사전 훈련된 텍스트-이미지 모델을 사용함으로써, 다른 속성은 유지하면서 실제 세계 이미지의 재질 특성을 편집할 수 있다.
- 모델의 잠재적 적용 사례로서 재질이 편집된 NeRFs(Neural Radiance Fields)를 제시한다.

### [Axiomatic Preference Modeling for Longform Question Answering](https://arxiv.org/abs/2312.02206)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/L0zRFDtxgtR07TMU6lZxl.png)

Vote: 4

Authors: Corby Rosset, Guoqing Zheng, Victor Dibia, Ahmed Awadallah, Paul Bennett

- 대규모 언어 모델(GPT-4와 같은)의 성능은 RLHF(Reinforcement Learning from Human Feedback)와 같은 후처리 과정에서 인간 선호도를 코딩한 보상 모델에 부분적으로 기인한다.
- 기존 보상 모델은 선호 어노테이션이 왜, 어떤 원칙에 의해 이루어졌는지 직접적인 지식이 부족하다.
- 이 연구에서는 인간 선호도와 더 잘 일치하도록 가이드하는 원칙을 식별하고, 이를 준수하는 다양한 선호 신호를 생성하는 공리적 프레임워크를 개발했다.
- 공리적 신호를 사용하여 장문의 질문에 대한 답변을 점수화하기 위한 모델을 훈련했으며, 이 모델은 220M 매개변수로 구성되어 있다.
- 이 공리적 선호 모델은 골드 인간 어노테이션 선호 라벨과 더 자주 일치하며, GPT-4보다 선호 점수화 작업에서 더 나은 성능을 보여주었다.
- 본 연구의 기여로는 LLM 및 인간이 생성한 답변을 동일한 척도로 점수화 할 수 있는 독립 선호 모델 훈련, 특정 원칙에 맞춰 훈련 데이터 쌍을 생성하는 공리적 프레임워크 개발, 소량의 공리적 신호로 소형 모델이 GPT-4를 선호도 점수화에서 능가할 수 있음을 보여주는 것이 포함된다.
- 연구 모델은 huggingface에서 공개되어 있다: https://huggingface.co/corbyrosset/axiomatic_preference_model

### [Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions](https://arxiv.org/abs/2312.02772)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ngmG_-Bmj-bcFTkrcpuVT.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ngmG_-Bmj-bcFTkrcpuVT.mp4" muted="false"></video></div>

Vote: 3

Authors: Xu Shi, Chuanchen Luo, Junran Peng, Hongwen Zhang, Yunlian Sun

- 최근 텍스트 기반 인간 모션 생성 분야가 발전하여 텍스트 설명에 부합하는 다양하고 고품질의 인간 모션을 생성할 수 있게 되었으나, 세부적인 텍스트 설명이 포함된 데이터셋의 부족으로 인해 미세한 또는 스타일화된 모션 생성은 여전히 어려운 과제로 남아있다.
- 저자들은 대규모 언어 모델(GPT-3.5)을 활용하여 기존의 모호한 텍스트 주석을 다양한 신체 부위에 대한 세밀한 설명으로 파싱하는 전략을 채택함으로써 'Fine-Grained Human Motion Diffusion Model (FG-MDM)'이라는 새로운 프레임워크를 제안한다.
- 이 세밀한 설명들을 변환기 기반 확산 모델을 이용하여 인간 모션 생성에 가이드로 사용함으로써, FG-MDM은 훈련 데이터 분포 밖에서도 세밀하고 스타일화된 모션을 생성할 수 있다.
- 실험 결과는 FG-MDM이 이전 방법들보다 우수하며, 특히 강력한 일반화 능력을 보여준다는 점을 입증한다.
- 연구팀은 HumanML3D 및 KIT에 대한 세밀한 텍스트 주석 데이터를 공개할 예정이다.

### [StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D](https://arxiv.org/abs/2312.02189)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/tR_sQVz6GXb5B51ladCgI.png)

Vote: 3

Authors: Pengsheng Guo, Hans Hao, Adam Caccavale, Zhongzheng Ren, Edward Zhang, Qi Shan, Aditya Sankar, Alexander G. Schwing, Alex Colburn, Fangchang Ma

- 텍스트를 이용한 3D 생성 분야에서, 2D 확산 모델을 사용하는 점 증류 샘플링(SDS)은 종종 흐릿한 외관과 다중면 기하구조 문제를 발생시킨다.
- 이러한 문제의 핵심은 2D 확산 과정의 노이즈 수준, 확산 네트워크의 구조, 그리고 3D 모델 표현 간 상호작용에 있다.
- 본 논문은 세 가지 혁신을 통합한 새로운 방법론인 StableDreamer를 제시한다.
- 첫째, InstructNeRF2NeRF에서 영감을 받아 SDS 생성 사전과 간단한 감독된 L2 재구성 손실의 동등성을 명시하였다.
- 이 발견은 SDS의 영향을 디버깅하는 새로운 도구를 제공하여 시간에 따른 노이즈 레벨을 조절하여 다중면 기하구조를 감소시킨다.
- 둘째, 이미지 공간의 확산은 기하학적 정밀도에 기여하고, 잠재 공간의 확산은 생생한 색상 구현에 중요함을 분석이 보여준다.
- StableDreamer는 이 두 가지를 효과적으로 결합하는 두 단계 훈련 전략을 소개하여 고해상도의 3D 모델을 결과로 한다.
- 셋째, 전반적인 품질을 향상시키고, 훈련 중 메모리 사용을 줄이며, 렌더링 속도를 가속화하고, 반투명 객체를 더 잘 포착하기 위해 Neural Radiance Fields (NeRFs)를 대체하여 이방성 3D 가우시안 표현을 채택한다.
- StableDreamer는 다중면 기하구조를 줄이고, 세밀한 디테일을 생성하며, 안정적으로 수렴한다.

### [GPT4Point: A Unified Framework for Point-Language Understanding and Generation](https://arxiv.org/abs/2312.02980)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/EH61IYni6rfIV1KenKJc0.png)

Vote: 3

Authors: Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, Hengshuang Zhao

- 'GPT4Point'는 3D 세계에 대한 이해를 향상시키기 위해 고안된, 점-언어(point-language) 이해 및 생성에 특화된 혁신적인 다중모드 모델입니다.
- 이 모델은 점 구름 캡셔닝(point-cloud captioning) 및 질문응답(Q&A)과 같은 다양한 점-텍스트 참조 작업을 능숙하게 수행할 수 있습니다.
- GPT4Point는 3D 생성 작업에서도 고급 기능을 갖추고 있어, 낮은 품질의 점-텍스트 특성을 유지하면서 고품질 결과물을 도출할 수 있습니다.
- 100만 개가 넘는 다양한 텍스트 세분성 수준을 가진 객체의 대규모 데이터베이스를 구축하기 위해 'Pyramid-XL'이라는 점-언어 데이터셋 주석 엔진을 개발하였습니다.
- 이 데이터셋은 GPT4Point 훈련을 위해 필수적이며, Objaverse-XL 데이터셋을 기반으로 합니다.
- 3D 점-언어 이해 능력을 평가하기 위한 종합적인 벤치마크도 제안되었습니다.
- 광범위한 평가에서 GPT4Point는 3D 이해 및 생성 분야에서 우수한 성능을 보였음이 입증되었습니다.

### [LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models](https://arxiv.org/abs/2312.02949)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/YKqWtJIw9PanBW7hsv7ba.png)

Vote: 3

Authors: Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, Jianwei Yang

- 최근 대규모 다중 모달 모델(LMMs)의 눈에 띄는 발전으로 시각적 채팅에서 이들의 '그라운딩'(바탕 지식 활용) 능력이 갈수록 중요해지고 있다.
- 그러나 이러한 모델들이 그라운딩 지원을 가능하게 하는 최근의 노력에도 불구하고, 그라운딩과 채팅 능력이 보통 분리되어 있으며, 그라운딩 요구 시 채팅 성능이 크게 낮아진다.
- 기존의 그라운딩 데이터셋은 짧은 캡션만 포함하고 있어 '그라운디드 비주얼 채팅'(GVC)에 대한 데이터셋 부족이 문제로 지적되었다.
- 이 문제를 해결하기 위해, 그라운딩과 채팅 능력의 결합이 가능한 GVC 데이터를 우리는 생성했다.
- 또한 GVC 능력을 더 잘 평가하기 위해 'Grounding-Bench'라는 벤치마크를 도입했다.
- 우리는 세분화 모델과 언어 모델을 연결하여 GVC 및 다양한 시각적 프롬프트를 지원할 수 있는 모델 디자인을 제안했다.
- 실험 결과, 우리의 모델은 Grounding-Bench에서 다른 LMM들을 능가하는 성능을 보였다.
- 우리 모델은 동시에 RefCOCO/+/g와 Flickr30K Entities와 같은 클래식 그라운딩 벤치마크에서도 경쟁력 있는 성능을 달성했다.
- 우리의 코드는 https://github.com/UX-Decoder/LLaVA-Grounding 에서 공개될 예정이다.

### [WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words](https://arxiv.org/abs/2312.02931)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/UJ47sKMObKeehQAC4KGV4.png)

Vote: 3

Authors: Lukas Wolf, Klemen Kotar, Greta Tuckute, Eghbal Hosseini, Tamar Regev, Ethan Wilcox, Alex Warstadt

- 이 연구는 다양한 입력 모달리티를 통한 언어 모델 학습이 시스템의 품질과 효율성을 향상시킬 수 있는지를 탐색합니다.
- 연구팀은 텍스트-오디오에 초점을 맞추고, 플라바(FLAVA)의 텍스트-이미지 접근법에서 영감을 받아 Whisbert를 소개합니다.
- Whisbert는 '피플스 스피치' 데이터셋의 단어와 해당 오디오가 포함된 1억 개의 단어로 사전 훈련됩니다.
- 연구는 텍스트만 사용한 모델과 텍스트 및 오디오를 동시에 사용한 모델을 비교하여 다중 모달리티의 영향을 평가합니다.
- 결과적으로 Whisbert는 다중모달 가리기 모델링에서 뛰어난 성능을 보이고 대부분의 벤치마크 작업에서 Babylm 기준을 초과하지만, 복잡한 목표를 최적화하고 텍스트 전용 기준보다 더 나은 성과를 내는 데 어려움을 겪습니다.

