## Daily Papers (2023-12-20)

### [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](https://arxiv.org/abs/2312.11514)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/F6ltgoYqFf1nra_qzoeR3.png)

Vote: 136

Authors: Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, Mehrdad Farajtabar, Mehrdad Farajtabar

- 대규모 언어 모델(LLM)은 다양한 과제에서 뛰어난 성능을 낼 수 있지만, 높은 컴퓨팅 및 메모리 요구 사항으로 인해 DRAM 용량이 제한된 기기에서는 도전적입니다.
- 본 논문은 사용 가능한 DRAM 용량을 초과하는 LLM을 플래시 메모리에 저장하고 필요시 DRAM으로 불러와 효율적으로 실행하는 문제를 다룹니다.
- 플래시 메모리의 동작과 조화를 이루는 추론 비용 모델을 구축하고, 데이터 전송량을 줄이면서 데이터를 더 큰 연속 덩어리로 읽어 들이는 데 초점을 맞춰 최적화합니다.
- "윈도잉(windowing)" 기술은 이전에 활성화된 뉴런을 재사용하므로 데이터 전송량을 전략적으로 줄이며, "행-열 묶음(row-column bundling)" 기술은 플래시 메모리의 순차적 데이터 접근 강점에 맞춰 플래시 메모리에서 읽는 데이터 덩어리 크기를 증가시킵니다.
- 이러한 방법들은 사용 가능한 DRAM 크기의 두 배에 달하는 모델을 실행할 수 있게 하며, CPU와 GPU에서 각각 평범한 로딩 방식에 비해 추론 속도를 4-5배, 20-25배 증가시킵니다.
- 희소성 인식, 맥락 적응형 로딩, 하드웨어 지향적 설계를 통합함으로써, 메모리가 제한된 기기에서 LLM을 효과적으로 추론할 수 있는 길을 열었습니다.

### [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/d6zq2Tr31g2RI-XyBpRPh.png)

Vote: 22

Authors: Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jean-Baptiste Alayrac, Jiahui Yu, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, +

- 이 보고서는 이미지, 오디오, 비디오 및 텍스트 이해에서 뛰어난 능력을 보여주는 새로운 다중모달(Gemini) 모델 패밀리를 소개합니다.
- Gemini 모델 패밀리에는 복잡한 추론 작업부터 장치 내 메모리 제약이 있는 사용 사례에 이르기까지 적용 가능한 Ultra, Pro, Nano 크기가 포함되어 있습니다.
- 폭넓은 벤치마크에서의 평가 결과, 가장 능력이 뛰어난 Gemini Ultra 모델이 32개의 벤치마크 중 30개에서 최신 기술의 발전을 이룩하였으며, 특히 잘 알려진 시험 벤치마크 MMLU에서 인간 전문가 성능을 달성한 최초의 모델이 되었습니다.
- 또한 우리가 살펴본 20개의 다중모달 벤치마크 각각에서 기술의 최신 기준을 향상시켰습니다.
- 저자들은 Gemini 모델의 교차 모달 추론 및 언어 이해 능력이 다양한 사용 사례를 가능하게 할 것이라고 믿으며, 이를 사용자에게 책임감 있게 배포하는 접근 방식을 논의합니다.

### [StarVector: Generating Scalable Vector Graphics Code from Images](https://arxiv.org/abs/2312.11556)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GLisf7QA2THNYPochj6i5.png)

Vote: 14

Authors: Juan A. Rodriguez, Juan A. Rodriguez, Shubham Agarwal, Shubham Agarwal, Issam H. Laradji, Issam H. Laradji, Pau Rodriguez, David Vazquez, Christopher Pal, Marco Pedersoli, Marco Pedersoli

- 확장 가능한 벡터 그래픽스(SVG)는 무한한 해상도 확장성, 다용도 사용성 및 편집 가능성으로 인해 최신 이미지 렌더링 애플리케이션에서 필수적인 요소가 되었습니다.
- 기존의 딥러닝을 이용한 SVG 모델링 방법들은 복잡한 SVG 생성에 어려움을 겪으며 단순화된 SVG에 국한되어 많은 처리를 요구합니다.
- 본 논문에서는 코드 생성 대형 언어 모델(CodeLLMs)과 비전 모델을 효과적으로 통합하는 다중 모달 SVG 생성 모델인 StarVector를 소개합니다.
- CLIP 이미지 인코더를 사용하여 픽셀 기반 이미지에서 시각적 표현을 추출하고, 어댑터 모듈을 통해 이를 시각 토큰으로 변환합니다.
- 생성된 시각 토큰들은 SVG 토큰 임베딩 앞에 추가되며, StarCoder 모델은 다음 토큰 예측을 사용해 시각적 및 코드 토큰을 맞추는 방식으로 시퀀스를 모델링합니다.
- 이를 통해 StarVector는 픽셀 이미지를 정확하게 나타내는 제한 없는 SVG 생성이 가능하게 됩니다.
- StarVector의 성능을 평가하기 위해 다양한 데이터셋들과 관련 메트릭을 포함하는 종합적인 벤치마크인 SVG-Bench를 제시합니다.
- 벤치마크 내에 실제 SVG 예시들의 대규모 데이터셋인 SVG-Stack을 포함한 새로운 데이터셋을 소개하고, 이를 통해 SVG를 위한 대형 기본 모델로 StarVector를 사전 훈련합니다.
- 우리의 결과는 현재의 방법들 보다 시각적 품질과 복잡성 처리에서 상당한 개선을 보이며, SVG 생성 기술에서 눈에 띄는 진보를 표시합니다.
- 코드와 모델은 https://github.com/joanrod/star-vector 링크에서 확인할 수 있습니다.

### [Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model](https://arxiv.org/abs/2312.12423)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/BMUO83mzzuZUJUCpgfblA.png)

Vote: 8

Authors: Shraman Pramanick, Shraman Pramanick, Guangxing Han, Guangxing Han, Rui Hou, Rui Hou, Sayan Nag, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Qifan Wang, Rama Chellappa, Amjad Almahairi, Amjad Almahairi

- 대규모 언어 모델을 사용하여 다양한 시각-언어(VL) 작업을 통합하는 일반적인 목적의 시각 시스템의 등장으로, 시각 입력을 처리하는 능력이 강화되었습니다.
- 기존의 모델들이 이미지 세그멘테이션 및 다중 이미지 입력을 고려하지 못하는 단점을 극복하기 위해, VistaLLM이라는 통합 프레임워크로 단일 및 다중 입력 이미지에 대한 조림-세밀한 VL 작업을 수행합니다.
- VistaLLM은 작업 설명을 사용하여 전역 임베딩을 필터링하는 지시-가이드 이미지 토크나이저를 사용하여 다수의 이미지로부터 압축되고 정제된 특징을 추출합니다.
- 이 모델은 이진 세그멘테이션 마스크를 시퀀스로 표현하는 데 있어 기존의 균일 샘플링보다 개선된 경사-인식적인 적응 샘플링 기술을 적용합니다.
- VistaLLM의 기능을 강화하기 위해, 조림-세밀 조정 데이터셋인 CoinIt을 6.8M 샘플과 함께 제작하였습니다.
- 또한, 다중 이미지의 근거를 결여한 데이터셋의 부족을 해결하기 위해 새로운 작업인 AttCoSeg (속성 수준 공동 세그멘테이션)을 도입하여 모델의 추론과 다중 입력 이미지 위의 근거 능력을 향상시킵니다.
- 넓은 범위의 V 및 VL 작업에 대한 광범위한 실험을 통해, VistaLLM이 모든 하류 작업에서 강력한 기초 모델에 대한 일관된 최첨단 성능을 달성함을 입증합니다.
- 프로젝트 페이지는 https://shramanpramanick.github.io/VistaLLM/에서 확인하실 수 있습니다.

### [3D-LFM: Lifting Foundation Model](https://arxiv.org/abs/2312.11894)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/IsUWknJY_ArbqUcyxhpFd.png)

Vote: 5

Authors: Mosam Dabhi, Mosam Dabhi, Laszlo A. Jeni, Laszlo A. Jeni, Simon Lucey

- 2D 랜드마크에서 3D 구조와 카메라를 추출하는 것은 컴퓨터 비전 전체 분야의 핵심적인 도전 과제임.
- 기존 방법들은 특정한 강체(rigid) 객체들, 예를 들어 Perspective-n-Point (PnP) 문제에서 다루는 것들로 한정되어 있으나, 딥러닝은 다양한 객체 클래스의 재구성능력을 확장하고, 잡음, 가림, 원근 왜곡에 강한 모델들(C3PDO 및 PAUL 등)을 개발하게 함.
- 그러나 이러한 기법들은 3D 트레이닝 데이터 간의 대응점(correspondence)을 설정하는 기본적인 필요성에 의해 제한되며, 이는 풍부한 "대응점이 있는" 3D 데이터가 있는 애플리케이션에만 유용함을 의미함.
- 본 연구의 접근 방식은 변형자(transformers)의 내부 순열 동등성(permutation equivariance)을 활용하여 3D 데이터 인스턴스별로 다양한 수의 포인트를 관리하고, 가림을 견딜 수 있으며, 보지 못한 범주로 일반화가 가능함.
- 2D-3D 구조 추출 작업 벤치마크에 걸쳐 최고 수준의 성능을 보여줌.
- 이 접근법은 매우 다양한 종류의 구조에서 훈련이 가능하므로, 간단히 말해서 이것은 3D Lifting Foundation Model (3D-LFM)로, 최초의 예시로 언급됨.

### [A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise](https://arxiv.org/abs/2312.12436)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Gf59Q44VuJjj2U84AMXAm.png)

Vote: 5

Authors: Chaoyou Fu, Chaoyou Fu, Renrui Zhang, Haojia Lin, Haojia Lin, Zihan Wang, Timin Gao, Yongdong Luo, Yubo Huang, Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang Shen, Yunhang Shen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Sirui Zhao, Xiawu Zheng, Shaohui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Xing Sun, Xing Sun, Rongrong Ji

- GPT-4V(ision) 가시적 이해능력을 갖춘 다중모달 대규모 언어 모델(MLLM)에 대한 관심이 학계 및 산업계에서 두드러지고 있다.
- 구글이 멀티모달을 위해 처음부터 개발한 가장 신기능 MLLM인 Gemini를 출시하면서 GPT-4V의 선도적 위치를 도전할 수 있는 가능성이 제기되었다.
- 본 논문은 Gemini Pro가 기본 감각, 고급 인지, 어려운 시각 작업 및 다양한 전문 기능 등 네 영역에서 시각 이해 능력을 평가하였다.
- 연구 팀은 비교를 위해 Gemini Pro와 최첨단 GPT-4V를 비교 평가하였고, 수동 작업과 블랙박스 시스템 사이의 격차를 보여주는 최신 오픈 소스 MLLM인 Sphinx도 검토했다.
- 질적 샘플 분석에 따르면 GPT-4V와 Gemini는 다른 답변 스타일과 선호도를 보이면서도 유사한 시각적 추론능력을 나타냈고, Sphinx는 그들에 비해 도메인 일반화에 있어서 뒤쳐진다.
- 특히 GPT-4V는 자세한 설명과 중간 단계를 선호하는 반면, Gemini는 직접적이고 간결한 대답을 우선시하는 경향이 있다.
- 인기있는 MME 벤치마크에 대한 정량적 평가는 Gemini가 GPT-4V의 강력한 도전자가 될 가능성을 보여준다.
- 이러한 초기 조사를 통해 MLLM에 일반적인 문제점이 있음을 확인하고, 아직 인공 일반 지능에 도달하기 위한 상당한 거리가 남아 있음을 시사한다.
- MLLM의 진행 상황을 추적하는 프로젝트는 https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models에서 공개되었다.

### [HAAR: Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles](https://arxiv.org/abs/2312.11666)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/x384PuTeJwlUPEYefjxvM.png)

Vote: 4

Authors: Vanessa Sklyarova, Vanessa Sklyarova, Egor Zakharov, Egor Zakharov, Otmar Hilliges, Michael J. Black, Michael J. Black, Justus Thies

- HAAR는 텍스트 입력을 기반으로 현대 컴퓨터 그래픽 엔진에서 프로덕션 수준의 자산으로 사용될 수 있는 3D 인간 헤어스타일을 생성하는 새로운 모델이다.
- 기존의 AI 기반 생성 모델은 강력한 2D 사전 지식을 활용하였으나, 시각적인 부분만을 복구하는 데 한계가 있고, 매우 가려진 머리카락 구조를 재구성하거나 물리 기반 렌더링 또는 시뮬레이션 파이프라인에 사용할 준비가 안 된 '외피'만을 모델링한다.
- HAAR는 강력한 3D 헤어 스트랜드 표현을 사용하여 첫 번째 텍스트 가이드 생성 방법론을 제안한다.
- 2D 시각 질문-응답(VQA) 시스템을 이용하여 아티스트가 만든 소수의 헤어스타일로부터 생성된 합성 머리 모델에 자동으로 주석을 단다.
- 이를 통해 공통 헤어스타일 UV 공간에서 작동하는 잠재적 확산 모델을 훈련할 수 있다.
- 제안된 모델의 능력을 질적 및 양적 연구로 입증하며, 기존 헤어스타일 생성 접근법과 비교한다.

### [Tracking Any Object Amodally](https://arxiv.org/abs/2312.12433)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/OfhRXMG3lRMr7tckZr6pw.qt)

Vote: 4

Authors: Cheng-Yen Hsieh, Cheng-Yen Hsieh, Tarasha Khurana, Achal Dave, Achal Dave, Deva Ramanan, Deva Ramanan

- 부분 가시성에서 전체 객체 구조를 이해하는 아모달 인식은 기본 기술로, 영유아에게도 중요합니다.
- 이 기술은 자율 주행과 같은 응용 분야에서, 심하게 가려진 객체의 명확한 이해가 필수적입니다.
- 대부분의 데이터셋에서 모달 주석이 많기 때문에 현대의 검출 및 추적 알고리즘에서 아모달 능력을 종종 간과합니다.
- 아모달 데이터 부족 문제를 해결하기 위해, 우리는 다양한 비디오 시퀀스에 있는 880개의 다양한 카테고리를 포함한 TAO-Amodal 벤치마크를 소개합니다.
- 우리의 데이터셋은 시각적으로 가려진 객체와 프레임 밖으로 부분적으로 벗어난 객체를 포함한 아모달 및 모달 바운딩 박스를 제공합니다.
- 객체 영구성을 갖춘 아모달 추적을 강화하기 위해, 우리는 표준 모달 추적기를 몇 백 개의 비디오 시퀀스에서 데이터 증강을 통해 미세 조정하여 아모달 추적기로 변환하는 경량의 플러그인 모듈인 아모달 확장기를 활용합니다.
- 우리는 TAO-Amodal에서 가려진 객체의 탐지와 추적을 3.3% 및 1.6% 향상시켰습니다.
- 사람들에 대한 평가에서 우리의 방법은 최신의 모달 기준보다 2배 개선된 결과를 보여줍니다.

### [MixRT: Mixed Neural Representations For Real-Time NeRF Rendering](https://arxiv.org/abs/2312.11841)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0vwzyY_-HQEfseeOrDn42.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0vwzyY_-HQEfseeOrDn42.mp4" muted="false"></video></div>

Vote: 4

Authors: Chaojian Li, Chaojian Li, Bichen Wu, Bichen Wu, Peter Vajda, Peter Vajda, Yingyan, Lin

- 신경 복셀 필드(NeRF)가 새로운 시점 합성을 위한 주요 기술로 자리잡았지만, 대규모 장면에서 실시간 NeRF 렌더링을 달성하는 것에는 도전이 따랐습니다.
- 대규모 삼각형 메쉬를 이용한 복잡한 베이크된 메쉬 표현이나, 리소스를 많이 소모하는 레이 마칭 방법을 사용하는 것 대신, 고품질 지오메트리가 반드시 필요하지 않다는 것을 발견하였습니다.
- 이에 본 논문에서는 저품질 메쉬, 시점 의존적 디스플레이스먼트 맵, 압축된 NeRF 모델을 포함하는 새로운 NeRF 표현인 MixRT를 제안합니다.
- 이 방식을 통해 기존 그래픽 하드웨어의 능력을 효과적으로 활용하고, 엣지 디바이스에서 실시간으로 NeRF 렌더링을 가능하게 합니다.
- 최적화된 WebGL 기반 렌더링 프레임워크를 이용해 개발된 MixRT는 엣지 디바이스(예: MacBook M1 Pro 랩톱)에서 1280x720 해상도로 초당 30 프레임 이상의 렌더링 속도를 달성합니다.
- MixRT는 기존의 최신 방법들에 비해 더 나은 렌더링 품질(실내 장면에서 PSNR 0.2점 상승)과 더 적은 저장 공간(80% 미만)을 제공합니다.

### [FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple Super-Resolution Pipeline](https://arxiv.org/abs/2312.11537)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8tC5rBj4geGL_HLvISL4F.png)

Vote: 4

Authors: Chien-Yu Lin, Chien-Yu Lin, Qichen Fu, Qichen Fu, Thomas Merth, Thomas Merth, Karren Yang, Anurag Ranjan, Anurag Ranjan

- 본 논문에서는 추가적인 입력 특성, 손실 함수 및 비용이 많이 드는 교육 절차(예: 지식 증류)을 사용하지 않고도 수퍼 해상도(SR)를 활용하여 훈련 또는 아키텍처에 대한 비용이 많이 드는 변경 없이 효율적인 이점을 얻기 위해 노력한다.
- 저자들은 기존 모듈을 직접 결합하여 간단한 NeRF + SR 파이프라인을 구축했으며, 훈련을 위한 가벼운 증강 기술인 랜덤 패치 샘플링을 제안한다.
- 이 파이프라인은 기존 NeRF + SR 방법에 비해 SR 컴퓨팅 오버헤드를 완화시키고 애플 맥북 같은 소비자 기기에서도 실행이 가능할 정도로 최대 23배 빠르게 훈련될 수 있다.
- 실험 결과 본 파이프라인은 NeRF 출력을 2-4배까지 업스케일할 수 있으며 높은 품질을 유지하고, NVIDIA V100 GPU에서 최대 18배, M1 Pro 칩에서 12.8배까지 추론 속도를 증가시킬 수 있는 것으로 나타났다.
- 결론적으로, SR은 소비자 기기용 NeRF 모델의 효율성을 개선하기 위한 간단하지만 효과적인 기술이 될 수 있다.

### [TIP: Text-Driven Image Processing with Semantic and Restoration Instructions](https://arxiv.org/abs/2312.11595)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/uDKosm3lpDkRPwX68B_xJ.png)

Vote: 3

Authors: Chenyang Qi, Chenyang Qi, Zhengzhong Tu, Zhengzhong Tu, Keren Ye, Mauricio Delbracio, Mauricio Delbracio, Peyman Milanfar, Qifeng Chen, Qifeng Chen, Hossein Talebi

- 이미지 편집 작업을 위해 점점 인기가 높아지고 있는 텍스트 기반 확산 모델들이 존재하지만, 더 세밀한 이미지 처리 작업에 이러한 언어-비전 패러다임을 적용하는 것은 여전히 개방적인 연구 문제이다.
- 본 논문에서는 자연 언어를 사용하는 인터페이스를 통해 이미지 복원 과정을 제어할 수 있는 TIP(Text-driven Image Processing) 프레임워크를 개발하였다.
- 연구진은 텍스트 정보를 두 가지 차원에서 고려하였는데, 첫째로, 내용 관련 프롬프트를 사용하여 의미적 정렬을 강화하고 복원 결과에서의 정체성 모호성을 효과적으로 완화시켰다.
- 둘째로, 언어 기반의 정량적인 복원 강도 명세를 통해 명시적인 과제별 디자인 없이도 세밀한 수준의 지시를 지원하는 첫 번째 프레임워크를 제시했다.
- 또한 연구진은 생성적 사전을 재조정하도록 학습하는 새로운 융합 메커니즘을 도입하여 기존의 ControlNet 아키텍처를 보완하는 동시에 더 나은 복원 충실성을 달성했다.
- TIP은 체계적인 실험을 통해 최신 기술 대비 우수한 복원 성능을 입증하였으며, 텍스트에 기반한 제어를 통한 복원 효과의 유연성 또한 제공한다.

### [Customize-It-3D: High-Quality 3D Creation from A Single Image Using Subject-Specific Knowledge Prior](https://arxiv.org/abs/2312.11535)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/n-WLWEFFJmEj3I3ySC6VR.qt)

Vote: 3

Authors: Nan Huang, Ting Zhang, Ting Zhang, Yuhui Yuan, Yuhui Yuan, Dong Chen, Dong Chen, Shanghang Zhang

- 본 논문에서는 참조 이미지에서 제공되는 정보를 최대한 활용하여 이미지에서 3D 생성을 위한 맞춤형 지식 사전을 설정하는 새로운 두 단계 접근법을 제시합니다.
- 기존 방법들이 일반적인 확산 사전에 주로 의존하는 반면, 저희는 특정 주제에 맞는 멀티모달 확산 모델을 제안합니다.
- 이 모델은 NeRF 최적화를 통해 개선된 기하학적 구조를 고려함으로써 음영 모드를 고려하고, 조잡한 결과의 텍스처를 향상시켜 뛰어난 세밀화를 달성합니다.
- 이러한 측면들은 3D 콘텐츠를 주제와 정확하게 일치시키는 데 기여합니다.
- 광범위한 실험을 통해 저희 방법인 Customize-It-3D가 이전 작업들을 큰 차이로 뛰어넘는 것을 입증하고, 인상적인 시각적 품질로 신뢰할 수 있는 360도 재구성을 생성합니다.
- 이것은 텍스트에서 3D 생성을 포함한 다양한 응용 프로그램에 적합합니다.

### [Text-Conditioned Resampler For Long Form Video Understanding](https://arxiv.org/abs/2312.11897)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/eGPB2KsAGIlxuwQWknRk4.png)

Vote: 3

Authors: Bruno Korbar, Bruno Korbar, Yongqin Xian, Alessio Tonioni, Alessio Tonioni, Andrew Zisserman, Federico Tombari

- 본 논문에서는 긴 비디오 시퀀스를 처리하기 위해 사전 학습된 시각 인코더와 대규모 언어 모델(LLM)을 활용하는 텍스트 조건부 비디오 리샘플러(TCR) 모듈을 제시한다.
- TCR은 텍스트 조건에 따라 비디오에서 관련 시각적 특징을 지역화하고 이를 LLM에 제공하여 텍스트 응답을 생성한다.
- 경량 설계와 크로스 어텐션 사용으로 인해 TCR은 100프레임 이상을 처리할 수 있어 이전 연구들에 비해 훨씬 긴 비디오 구간을 사용할 수 있다.
- TCR은 사전 학습된 시각 및 언어 모델 간의 연결을 가능하게 하는 변형기 기반 샘플링 아키텍처 및 훈련 방법을 설계하는 등 다음과 같은 기여를 제공한다.
- NextQA, EgoSchema, EGO4D-LTA 챌린지에서 새로운 최고 성능을 달성함으로써 평가 작업의 다양한 종류에 대해 TCR의 효과를 실험적으로 검증한다.
- 장기 비디오 컨텍스트가 필요한 작업을 결정하고 이러한 작업을 사용하여 장거리 비디오 모델의 추가적인 평가를 효과적으로 수행할 수 있음을 규명한다.

### [Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation](https://arxiv.org/abs/2312.11532)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8_ZaPppPb9y0NulfK-Q4N.png)

Vote: 3

Authors: YoungJoon Yoo, YoungJoon Yoo, Jongwon Choi

- 이 논문은 사전 학습된 언어 모델과 같은 풍부한 정보를 이산적으로 봉인하는 Vector-Quantized Variational Auto-Encoder(VQ-VAE)의 잠재 코드북을 활용한 새로운 토픽 모델링 방법을 제시합니다.
- 잠재 코드북과 임베딩을 개념적 단어 가방으로 해석함으로써, 원래 문서를 거꾸로 생성하는 새로운 생성적 토픽 모델인 Topic-VQ-VAE(TVQ-VAE)를 제안합니다.
- TVQ-VAE는 전통적인 BoW 분포와 자동 회귀 이미지 생성을 포함한 다양한 생성 분포를 통해 토픽을 시각화할 수 있습니다.
- 문서 분석과 이미지 생성에 대한 실험 결과는 TVQ-VAE가 데이터셋의 내재된 구조를 드러내는 주제 맥락을 효과적으로 포착하고, 다양한 형태의 문서 생성을 지원한다는 것을 보여줍니다.
- 제안된 TVQ-VAE의 공식 구현은 https://github.com/clovaai/TVQ-VAE 에서 확인할 수 있습니다.

### [Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method](https://arxiv.org/abs/2312.12030)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Di8SQ2jeCEPSL84mAWIlM.png)

Vote: 3

Authors: Jiachun Pan, Hanshu Yan, Hanshu Yan, Jun Hao Liew, Jun Hao Liew, Jiashi Feng, Vincent Y. F. Tan

- 확산 모델에서 미리 학습된 네트워크를 사용하여 트레이닝 없는 유도 샘플링을 진행하는 과정에서, 초기 단계의 이미지 생성 품질을 저해하는 정확하지 않은 유도 에너지 함수 문제에 대응하여 Symplectic Adjoint Guidance (SAG)를 제안합니다.
- SAG는 첫 번째로, 이미지 품질 요구사항에 맞게 조정할 수 있는 유연한 하이퍼파라미터 n을 사용하여 깨끗한 이미지를 n번의 함수 호출로 추정합니다.
- 두 번째로, SAG는 메모리 요구 사항 측면에서 효율적이면서도 정확한 그래디언트 유도를 얻기 위해 심플렉틱 수반 방법을 사용합니다.
- 다양한 실험을 통해 SAG는 이미지 및 비디오 생성 작업에서 기존 방식보다 품질이 높은 이미지를 생성함을 보여줍니다.

