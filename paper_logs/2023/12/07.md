## Daily Papers (2023-12-07)

### [Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis](https://arxiv.org/abs/2312.03491)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/nPYjMzM1DDtfmacULALMR.png)

Vote: 23

Authors: Zehua Chen, Guande He, Kaiwen Zheng, Xu Tan, Jun Zhu

- 글자-음성(TTS) 합성에서 확산 모델이 고품질의 생성을 성취했음에도 불구하고, 사전 정의된 데이터-잡음 확산 과정으로 인해 생성 대상의 정보가 적은 잡음이 많은 표현에 국한됩니다.
- 본 연구에서는 기존의 확산 기반 TTS 방법에서 확률론적인 가우시안 사전을 구조적 정보가 강한 깨끗하고 결정론적인 것으로 대체하는 새로운 TTS 시스템인 Bridge-TTS를 제안합니다.
- 특히, 텍스트 입력에서 얻은 잠재 표현을 사전으로 사용하며, 이것과 실제 멜-스펙트로그램 사이에 완전히 추적 가능한 슈뢰딩거 브릿지를 구축해 데이터-데이터 과정을 이끕니다.
- 이 형식의 추적 가능성과 유연성은 노이즈 일정 등의 설계 공간을 경험적으로 연구하고 확률론적 및 결정론적 샘플러를 개발할 수 있게 합니다.
- LJ-Speech 데이터셋에서의 실험 결과는 제안된 방법이 합성 품질 및 샘플링 효율성 측면에서 확산 대응 모델인 Grad-TTS를 50-단계/1000-단계 합성과 몇 단계 시나리오에서 강력한 빠른 TTS 모델들을 상당히 뛰어넘는 것을 시사합니다.
- 프로젝트 페이지: [https://bridge-tts.github.io/](https://bridge-tts.github.io/)

### [Relightable Gaussian Codec Avatars](https://arxiv.org/abs/2312.03704)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/O2W3iEkfNl6qHdOoO3HD_.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/O2W3iEkfNl6qHdOoO3HD_.mp4" muted="false"></video></div>

Vote: 15

Authors: Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam

- 해당 연구에서는 신규 표정을 생성할 수 있는 고품질의 조명 조절이 가능한 머리 아바타를 구축하는 메소드인 Relightable Gaussian Codec Avatars를 제안합니다.
- 3D 가우시안을 기반으로 한 지오메트리 모델은 머리카락과 모공 같은 하위 밀리미터 세부 사항을 3D 일관된 방식으로 포착합니다.
- 인간 머리의 다양한 재질을 통합적으로 지원하기 위해, 학습 가능한 광선 전달에 기반한 새로운 조명 조절 가능한 외양 모델을 제시합니다.
- 확산 구성요소에 대해 전역 조명을 인지하는 구형 조화 성분과 함께, 우리는 구형 가우시안을 사용하여 실시간 렌더링에서 공간적으로 모든 주파수 반사를 달성합니다.
- 이 외양 모델은 점광원과 지속적인 조명 하에서 효율적으로 재조명될 수 있습니다.
- 눈의 반사 품질을 향상시키고 명시적인 시선 제어를 가능하게 하는 조명 조절 가능한 명확한 눈 모델을 소개함으로써, 기존 접근 방식보다 더 뛰어난 성능을 제공합니다.
- 연구팀은 소비자용 VR 헤드셋에서 실시간 재조명 아바타를 시연함으로써, 우리 아바타의 효율성과 높은 품질을 보여줍니다.

### [Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians](https://arxiv.org/abs/2312.03029)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/13qlJCPF1Zc6wYe9QbtHD.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/13qlJCPF1Zc6wYe9QbtHD.mp4" muted="false"></video></div>

Vote: 12

Authors: Yuelang Xu, Benwang Chen, Zhe Li, Hongwen Zhang, Lizhen Wang, Zerong Zheng, Yebin Liu

- 이 연구에서는 통제 가능한 3D 가우시안을 사용하여 고해상도 3D 헤드 아바타 모델링을 위한 '가우시안 헤드 아바타'를 제안합니다.
- 제안된 방법은 중립적인 3D 가우시안과 복잡한 표현을 포획하는 완전히 학습된 MLP 기반 변형 필드를 최적화하여 서로 상호 작용합니다.
- 이 과정을 통해 표정의 정확성을 보장하면서도 미세한 동적 디테일을 모델링할 수 있습니다.
- 또한, 학습 절차의 안정성과 수렴을 위해 암묵적 SDF 및 Deep Marching Tetrahedra를 기반으로 한 잘 설계된 기하학적 안내 초기화 전략을 고안했습니다.
- 실험 결과, 제안된 방법은 과장된 표현 하에서도 2K 해상도에서 초고해상도 렌더링 품질을 달성하며, 다른 최첨단의 희소 뷰 방법들을 능가하는 성능을 보여줍니다.

### [OneLLM: One Framework to Align All Modalities with Language](https://arxiv.org/abs/2312.03700)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/QASoBA0CXGj2LuUTApKq_.png)

Vote: 11

Authors: Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue

- 본 논문에서는 다양한 모달리티를 언어와 통합하는 통일된 프레임워크인 OneLLM(Multimodal large language models, MLLMs)을 소개하였다.
- 기존 작업들이 주로 모달리티 특화 인코더에 의존하고 있는 반면, OneLLM은 통일된 멀티모달 인코더 및 단계적 멀티모달 정렬 파이프라인을 통해 8가지 모달리티를 언어와 정렬한다.
- 이미지 프로젝션 모듈을 처음으로 학습시켜 비전 인코더와 LLM을 연결한 다음, 다중 이미지 프로젝션 모듈과 동적 라우팅을 혼합하여 범용 프로젝션 모듈(UPM)을 만든다.
- 이러한 UPM을 사용하여 점진적으로 더 많은 모달리티를 LLM과 정렬한다.
- OneLLM의 지시사항을 따르는 능력을 최대화하기 위해, 이미지, 오디오, 비디오, 포인트 클라우드, 깊이/노멀 맵, IMU, fMRI 뇌 활동을 포함하는 200만 개 항목의 포괄적인 멀티모달 지시 데이터셋도 함께 개발하였다.
- OneLLM은 멀티모달 캡셔닝, 질문 응답 및 추론과 같은 다양한 작업을 포함하는 25개의 다양한 벤치마크에서 우수한 성능을 보여주었다.
- 해당 코드, 데이터, 모델 및 온라인 데모는 https://github.com/csuhan/OneLLM 에서 확인할 수 있다.

### [Cache Me if You Can: Accelerating Diffusion Models through Block Caching](https://arxiv.org/abs/2312.03209)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Xb74HBAxNb_56geIiUb5S.png)

Vote: 10

Authors: Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, Christian Rupprecht, Daniel Cremers, Peter Vajda, Jialiang Wang

- 확산 모델은 그들이 광학적으로 사실적인 이미지를 생성하는 능력으로 인해 최근 이미지 합성 분야에서 혁명을 일으켰지만, 이미지 생성 과정이 비용이 많이 드는 것이 주요 단점 중 하나입니다.
- 기존의 연구들은 대부분 denoising 네트워크를 블랙박스로 취급하며 필요한 단계 수를 줄이는 기술을 제안하지만, 이 연구에서는 네트워크 내부 레이어의 동작을 조사하여 출력이 시간에 따라 부드럽게 변하고, 레이어마다 변화 패턴이 뚜렷하며, 단계마다 변화가 종종 매우 작다는 점을 발견했습니다.
- 저자들은 denoising 네트워크 내 많은 레이어 연산이 중복되어 불필요하다고 가설을 세웠고, 이를 활용하여 이전 단계의 레이어 블록 출력을 재사용하여 추론 속도를 높이는 블록 캐싱을 도입했습니다.
- 추가로, 각 블록의 시간 단계별 변화에 근거하여 캐싱 일정을 자동으로 결정하는 기술을 제안합니다.
- 연구에서의 실험을 통해 FID, 인간 평가, 질적 분석을 통해 블록 캐싱이 동일한 계산 비용으로 더 높은 시각적 품질의 이미지를 생성할 수 있음을 보였으며, 다양한 최신 모델(예: LDM, EMU)과 솔버(예: DDIM, DPM)에 대해 이를 입증했습니다.

### [MotionCtrl: A Unified and Flexible Motion Controller for Video Generation](https://arxiv.org/abs/2312.03641)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0jYtAhZVV3MbcTNcrqN_U.png)

Vote: 9

Authors: Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, Ying Shan

- 비디오 생성을 위해 카메라 움직임과 객체 움직임을 정확히 제어하는 것이 필수적이지만, 기존 연구들은 한 종류의 움직임에 주로 집중하거나 두 움직임을 명확히 구분하지 않아 제어 능력과 다양성에 한계가 있었습니다.
- 이에 본 논문에서는 카메라 움직임과 객체 움직임을 효과적이고 독립적으로 제어할 수 있는 통합적이고 유연한 비디오 생성용 모션 컨트롤러인 MotionCtrl을 제안하였습니다.
- MotionCtrl은 카메라 움직임과 객체 움직임의 내재적 특성과 불완전한 훈련 데이터를 고려해 설계되었으며, 기존 방법에 비해 세 가지 주요 장점을 제공합니다.
- 첫째, 카메라 움직임과 객체 움직임을 효과적으로 독립적으로 제어함으로써 더 세밀한 모션 제어를 가능하게 하고, 두 종류의 움직임의 유연하고 다양한 조합을 촉진합니다.
- 둘째, 모션 조건이 카메라 포즈와 궤적에 의해 결정되며, 이는 외형에 대한 자유롭고 생성된 비디오 내 객체의 외형이나 모양에 최소한의 영향을 미칩니다.
- 셋째, 한 번 훈련되면 다양한 카메라 포즈와 궤적에 적응할 수 있는 상대적으로 일반화 가능한 모델입니다.
- 광범위한 정성적 및 정량적 실험을 통해 MotionCtrl의 우수성을 입증하였습니다.

### [Kandinsky 3.0 Technical Report](https://arxiv.org/abs/2312.03511)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GKdHw3iIqQu0miCXIK9Y2.png)

Vote: 8

Authors: Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov, Julia Agafonova, Andrey Kuznetsov, Denis Dimitrov

- Kandinsky 3.0은 잠재 확산에 기반한 대규모 텍스트-이미지 생성 모델로, Kandinsky 모델 시리즈의 최신 진행 상황을 반영하여 이미지 생성의 품질과 리얼리즘을 높였습니다.
- 이전 버전인 Kandinsky 2.x와 비교하여, Kandinsky 3.0은 두 배 더 큰 U-Net 백본과 열 배 더 큰 텍스트 인코더를 사용하며, 확산 매핑을 제거하였습니다.
- 모델 아키텍처, 데이터 수집 절차, 훈련 기술, 사용자 상호 작용의 프로덕션 시스템에 대해 설명합니다.
- 대규모 실험을 통해 확인한 다른 모델들과 비교하여 우리 모델의 품질 개선에 가장 큰 영향을 미친 핵심 요소들에 초점을 맞추고 있습니다.
- Kandinsky는 텍스트 이해력이 향상되었고 특정 도메인에서 더 나은 성능을 보이는 것으로 양측 비교를 통해 확인되었습니다.
- 프로젝트 페이지(https://ai-forever.github.io/Kandinsky-3)에서 더 자세한 내용을 확인할 수 있습니다.

### [HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting](https://arxiv.org/abs/2312.03461)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/5De14jqqWb_dwBhNz5xMP.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/5De14jqqWb_dwBhNz5xMP.mp4" muted="false"></video></div>

Vote: 7

Authors: Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, Lan Xu

- HiFi4G는 밀도 높은 영상에서 고해상도 인간 퍼포먼스 렌더링을 위한 콤팩트한 가우시안 기반 접근 방식을 제시한다.
- 이 방식은 3차원 가우시안 표현을 비정형 추적과 결합하여 압축이 용이하고 간결한 표현을 달성한다.
- 두 가지 그래프 메커니즘을 이용하여 모션 사전 지식을 획득하고, 대략적인 형태 변형 그래프를 사용하여 효율적인 초기화를 달성한다.
- 자세한 가우시안 그래프를 통해 추후 제약을 강제하는 동시에, 4D 가우시안 최적화 방식과 적응형 시공간 정규화를 사용하여 비정형 사전 지식과 가우시안 업데이트 사이의 균형을 효과적으로 조절한다.
- 다양한 플랫폼에서 몰입도 높은 경험을 위한 동반 압축 방식과 잔여 보상을 제공하여, 프레임 당 2MB 미만의 저장 공간으로 약 25배의 압축률을 달성한다.
- 광범위한 실험을 통해 우리의 접근 방식이 최적화 속도, 렌더링 품질, 저장 오버헤드 측면에서 기존 방식보다 우수함을 입증하였다.

### [Context Diffusion: In-Context Aware Image Generation](https://arxiv.org/abs/2312.03584)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ZIRK_oWv2WzfHHgoo44Lq.png)

Vote: 7

Authors: Ivona Najdenkoska, Animesh Sinha, Abhimanyu Dubey, Dhruv Mahajan, Vignesh Ramanathan, Filip Radenovic

- 연구팀은 이미지 생성 모델이 문맥 속의 시각적 예시들로부터 학습할 수 있는 'Context Diffusion'이라는 diffusion 기반 프레임워크를 제안합니다.
- 최근 이미지 생성에 있어서 문맥 학습과 관련된 작업은 문맥 예시들과 텍스트 프롬프트를 함께 제공하는 방법으로 진행되었지만, 프롬프트가 없을 때 생성된 이미지의 품질과 충실도가 떨어지는 것으로 나타나 시각적 문맥으로부터의 진정한 학습이 이루어지지 않았습니다.
- 이 문제를 해결하기 위해 연구팀은 시각적 문맥의 인코딩을 분리하고 질의 이미지의 구조를 유지하는 새로운 프레임워크를 제안했으며, 이를 통해 시각적 문맥과 텍스트 프롬프트 뿐만이 아니라 둘 중 하나로부터도 학습할 수 있는 능력을 갖게 되었습니다.
- 또한, 연구팀은 모델이 few-shot 설정을 다룰 수 있게 하여 다양한 문맥 학습 시나리오에 효과적으로 접근할 수 있도록 했습니다.
- 실험 및 사용자 연구를 통해, Context Diffusion은 도메인 내 및 도메인 외 작업 모두에서 이미지 질과 충실도 면에서 기존 모델들에 비해 우수함을 보였습니다.

### [LooseControl: Lifting ControlNet for Generalized Depth Conditioning](https://arxiv.org/abs/2312.03079)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/lkdqC2VxSvnwwz1KedA0A.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/lkdqC2VxSvnwwz1KedA0A.mp4" muted="false"></video></div>

Vote: 7

Authors: Shariq Farooq Bhat, Niloy J. Mitra, Peter Wonka

- 본 논문에서는 확산 기반 이미지 생성을 위한 일반화된 깊이 조절 기능을 가능하게 하는 LooseControl을 제시합니다.
- 기존의 상태 기술인 ControlNet은 정밀한 깊이 맵을 기반으로 뛰어난 이미지 생성 결과를 도출하지만, 많은 상황에서 정확한 깊이 맵을 만드는 것은 어려운 일입니다.
- LooseControl은 사용자가 경계 조건만으로 대략적인 장면을 지정하거나(경계 제어), 대상 객체의 정확한 형태와 외관이 아닌 레이아웃 위치를 지정할 수 있도록(3D 상자 제어) 깊이 조건을 일반화시켜 새로운 콘텐츠 생성 워크플로우를 가능하게 합니다.
- 사용자는 텍스트 지침과 함께 LooseControl을 사용하여, 장면의 경계와 주요 객체의 위치만을 지정하여 복잡한 환경(예: 방, 거리 전망 등)을 창조할 수 있습니다.
- 추가적으로, 이미지를 세부적으로 다듬기 위해 (E1) 3D 상자 편집과 (E2) 속성 편집의 두 가지 편집 메커니즘을 제공합니다.
- 3D 상자 편집은 스타일을 고정한 상태에서 상자 변화를 통해 이미지를 미세 조절할 수 있게 해주며, 속성 편집은 장면의 특정 측면(예: 전체 객체 밀도 또는 특정 객체)을 변경할 수 있는 가능한 편집 방향을 제안합니다.
- 폭넓은 테스트와 기초 모델과의 비교를 통해 우리의 방법론의 일반성을 입증하였습니다.
- LooseControl은 복잡한 환경을 쉽게 창조하는 중요한 디자인 도구로서 기능하며 기타 가이던스 채널로 확장될 수 있을 것으로 기대됩니다.
- 코드와 추가 정보는 https://shariqfarooq123.github.io/loose-control/ 에서 확인할 수 있습니다.

### [DreamComposer: Controllable 3D Object Generation via Multi-View Conditions](https://arxiv.org/abs/2312.03611)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/D-XD8ahBk3r-cn9v0R7LL.png)

Vote: 5

Authors: Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu

- 최근 연구들은 사전 학습된 대규모 2D 생성 모델을 이용하여 단일 야생 이미지에서 고품질의 새로운 시점 이미지들을 생성할 수 있으나, 다중 시점 정보의 부족으로 인해 제어 가능한 새 시점 이미지 생성에 어려움을 겪고 있다.
- 본 논문에서는 다중 시점 조건을 주입하여 기존 시점 인식 확산 모델들을 향상시킬 수 있는 유연하고 확장 가능한 프레임워크인 DreamComposer를 제안한다.
- DreamComposer는 먼저 시점 인식 3D 리프팅 모듈을 사용하여 다중 시점에서 객체의 3D 표현을 얻은 후, 다중 시점 특징 융합 모듈을 통해 3D 표현에서 목표 시점의 잠재 특징을 렌더링한다.
- 생성된 목표 시점의 특징들은 사전 학습된 확산 모델에 주입되어, 제어 가능한 3D 객체 재구성과 다양한 다른 응용 분야에 적합한 고품질의 새로운 시점 이미지들을 생성하는데 기여한다.
- 실험 결과는 DreamComposer가 최첨단 확산 모델들과 호환되며, 다중 시점 조건을 이용하여 제로샷 새 시점 합성을 강화할 수 있음을 보여준다.

### [Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia](https://arxiv.org/abs/2312.03664)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/XNOb1PP4kljLm_KAaZ-WK.png)

Vote: 5

Authors: Alexander Sasha Vezhnevets, John P. Agapiou, Avia Aharon, Ron Ziv, Jayd Matyas, Edgar A. Duéñez-Guzmán, William A. Cunningham, Simon Osindero, Danny Karmon, Joel Z. Leibo

- 에이전트 기반 모델링(agent-based modeling)은 수십 년 동안 사회 및 자연 과학 분야에서 널리 적용되어 왔으며, 대규모 언어 모델(Large Language Models, LLM)의 새로운 기능을 통합함으로써 연구 영역이 대폭 확대될 것으로 예상됩니다.
- 생성적 에이전트 기반 모델(Generative Agent-Based Models, GABM)은 단순히 에이전트들이 서로 대화하는 전통적인 에이전트 기반 모델(ABM)이 아니라, LLM을 사용하여 상황에 대한 상식 적용, "합리적" 행동, 공통 의미 지식의 회상, 디지털 기술 제어를 위한 API 호출 생성, 시뮬레이션 내외부와의 의사소통 등을 구성합니다.
- '콩코디아(Concordia)'라는 라이브러리는 GABM 구축 및 작업을 용이하게 하도록 제시되었으며, 물리적 혹은 디지털적 환경을 기반으로 한 언어 매개 시뮬레이션을 쉽게 구성할 수 있습니다.
- 콩코디아 내의 에이전트는 LLM 호출과 연관 기억 검색이라는 두 가지 기본 작업 사이의 매개체로서 유연한 컴포넌트 시스템을 사용하여 행동을 생성합니다.
- 게임 마스터(Game Master, GM)라 불리는 특별한 에이전트는 테이블탑 롤플레잉 게임에서 영감을 받아 에이전트들이 상호 작용하는 환경을 시뮬레이션하는 역할을 담당합니다.
- 에이전트들은 자연어로 원하는 행동을 설명하고, GM은 이를 적절한 구현으로 번역합니다. 실제 물리 세계 시뮬레이션에서 GM은 에이전트의 행동이 물리적으로 타당한지 확인하고 그 영향을 설명합니다.
- 앱과 서비스와 같은 디지털 환경을 시뮬레이션 할 때, GM은 예를 들어 일반 AI 도우미(Google's Bard, ChatGPT 등), 디지털 앱(캘린더, 이메일, 검색 등)과 같은 외부 도구와 통합하기 위해 API 호출을 처리할 수 있습니다.
- 콩코디아는 과학 연구에서의 여러 응용 프로그램에 대한 지원뿐만 아니라, 사용자 시뮬레이션 또는 합성 데이터 생성을 통한 실제 디지털 서비스의 성능 평가에도 도움을 줄 수 있도록 설계되었습니다.

### [HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces](https://arxiv.org/abs/2312.03160)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/tfsh-ovITZfkiqOsjCKH6.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/tfsh-ovITZfkiqOsjCKH6.mp4" muted="false"></video></div>

Vote: 4

Authors: Haithem Turki, Vasu Agrawal, Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder, Deva Ramanan, Michael Zollhöfer, Christian Richardt

- 뉴럴 레디언스 필드(Neural Radiance Fields)는 최고의 뷰 합성(view synthesis) 품질을 제공하지만, 렌더링하는 데 시간이 오래 걸립니다.
- 볼륨 렌더링을 사용하기 때문에, 레이당(ray per) 많은 샘플과 모델 쿼리가 필요합니다.
- 실제 세계의 객체들은 대개 볼륨이 아닌 표면으로 좀 더 효과적으로 모델링될 수 있으며, 이는 레이당 적은 샘플이 필요합니다.
- 하이브리드NeRF(HybridNeRF)는 대부분의 객체들을 표면으로 렌더링하는 한편, 도전적인 영역들을 볼륨으로 모델링함으로써 두 표현 방식의 강점을 활용합니다.
- HybridNeRF는 아이풀 타워(Eyeful Tower) 데이터셋을 포함한 다양한 뷰 합성 데이터셋에서 평가되었으며, 최신의 래스터라이제이션(rasterization) 기반 접근법을 포함한 최신 베이스라인과 비교했을 때 15-30% 오류율을 개선합니다.
- 또한, HybridNeRF는 가상현실 해상도(2Kx2K)에 대하여 실시간 프레임레이트(최소 36 FPS)를 달성합니다.

### [MagicStick: Controllable Video Editing via Control Handle Transformations](https://arxiv.org/abs/2312.03047)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/_gWEIMVantEe4bXOZoy8S.png)

Vote: 4

Authors: Yue Ma, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, Qifeng Chen

- 텍스트 기반 동영상 편집에 대한 관심이 증가하고 있는 가운데, 본 연구에서는 스타일 변경이나 비슷한 구조의 객체 대체를 넘어 동영상의 형태, 크기, 위치, 모션 등의 속성을 편집할 수 있음을 제시합니다.
- 핵심 인사이트는 특정 내부 특징(예: 객체의 에지 맵이나 인간 자세)의 키프레임 변환을 다른 프레임으로 쉽게 전파하여 생성을 가이드할 수 있다는 것입니다.
- MagicStick은 추출된 내부 제어 신호에 변환을 적용하여 동영상 속성을 편집하는 제어 가능한 동영상 편집 방법을 제안합니다.
- 모양을 유지하기 위해 사전 훈련된 이미지 확산 모델과 ControlNet을 시간 차원으로 확장하고 특정 장면에 적합하도록 저순위 조정(LORA) 층을 훈련합니다.
- 편집 시 공간적 주의 맵의 주의 리믹스(attention remix)를 제안한 바 있는 세밀하게 조정된 ControlNet을 역변환과 생성 과정에서 모두 도입하여 주의 유도를 수행합니다.
- 이 방법은 사전 훈련된 텍스트-이미지 모델에서 동영상 속성 편집 능력을 보여주는 첫 번째 방법으로, 다양한 예제에 대한 실험 결과와 모양 인식 텍스트 기반 편집 및 수공예 모션 동영상 생성과 비교하여 우수한 시간적 일관성과 편집 능력을 입증합니다.
- 코드와 모델은 공개될 예정입니다.

### [Language-Informed Visual Concept Learning](https://arxiv.org/abs/2312.03587)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Uv48CBVgV6oTYysdMuQ8Q.png)

Vote: 4

Authors: Sharon Lee, Yunzhi Zhang, Shangzhe Wu, Jiajun Wu

- 시각 세계의 이해는 다양한 개념 축을 중심으로 이루어지며, 이 개념 축들은 언어로 쉽게 지정할 수 있지만, 각 축에 따른 시각적 뉘앙스는 언어의 표현 한계를 종종 초과합니다.
- 본 연구의 목표는 대규모 사전 훈련된 시각-언어 모델을 간단히 정제하여 언어로 알려진 개념 축에 정보를 인코딩하는 시각 개념 표현을 학습하는 것입니다.
- 특정 개념 축에 대한 정보를 인코딩하는 일련의 개념 인코더를 훈련시켜, 사전 훈련된 Text-to-Image (T2I) 모델을 통해 입력 이미지를 재생산하려는 것이 핵심입니다.
- 서로 다른 개념 인코더의 분리를 장려하기 위해, 사전 훈련된 Visual Question Answering (VQA) 모델로부터 얻은 텍스트 임베딩의 세트에 개념 임베딩을 고정합니다.
- 추론 시, 모델은 새로운 테스트 이미지에서 다양한 축에 따른 개념 임베딩을 추출할 수 있으며, 이를 재조합하여 시각적 개념의 새로운 구성을 가진 이미지를 생성할 수 있습니다.
- 가벼운 테스트 시간 미세 조정 절차를 통해, 훈련 중에 본 적이 없는 새로운 개념으로 일반화하는 것도 가능합니다.

### [Self-conditioned Image Generation via Generating Representations](https://arxiv.org/abs/2312.03701)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/PwcTr5GEBXzAswKDSthU5.png)

Vote: 3

Authors: Tianhong Li, Dina Katabi, Kaiming He

- 이 논문은 클래스 비조건부 이미지 생성에서 새로운 벤치마크를 설정하는 간단하지만 효과적인 이미지 생성 프레임워크인 Representation-Conditioned image Generation (RCG)을 제시합니다.
- RCG는 인간 주석에 의존하지 않고, 사전 훈련된 인코더를 사용하여 이미지 분포로부터 매핑된 자기 감독(representation) 분포에 의거하여 조건을 부여합니다.
- 생성 과정에서 RCG는 representation diffusion model (RDM)을 사용하여 해당 representation 분포에서 샘플링하고, 이를 바탕으로 픽셀 생성기가 이미지 픽셀을 정교하게 만듭니다.
- 이러한 설계는 생성 과정 동안 상당한 지도를 제공하여 고품질의 이미지 생성에 기여합니다.
- ImageNet 256x256에서 실험한 결과, RCG는 3.31의 Frechet Inception Distance (FID)와 253.4의 Inception Score (IS)를 달성하여, 클래스 비조건부 이미지 생성 분야의 최신 기술 수준을 대폭 향상시켰으며, 클래스 조건부 이미지 생성 분야의 선도적 방법들과 경쟁 가능한 수준에 이르게 했습니다.
- 이 논문에서 사용된 코드는 https://github.com/LTH14/rcg 에서 제공됩니다.

### [Multimodal Data and Resource Efficient Device-Directed Speech Detection with Large Foundation Models](https://arxiv.org/abs/2312.03632)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/L8ZO-N7BpSKIWdRFfFyEH.png)

Vote: 3

Authors: Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi

- 이 연구는 가상 조수와의 상호작용을 자연스럽게 하기 위해 트리거 구문 없이 사용자가 가상 조수를 호출했는지를 결정하는 가능성을 탐구합니다.
- 자동 음성 인식 시스템의 1-최선 가설과 디코더 신호를 오디오 인코더의 음향 표현과 함께 대규모 언어 모델(Large Language Model, LLM)에 입력 기능으로 결합하여 이 작업을 수행합니다.
- 연구팀은 훈련 데이터가 적고 단일 동결된 LLM만 사용하는 시나리오에서 작동할 수 있는 데이터 및 자원 효율적인 시스템에 관심이 있습니다.
- 이 모델은 80k 이하의 다중 모드 데이터 예제에 대해 저랭크 적응(low-rank adaptation)과 접두사 튜닝(prefix tuning)의 조합을 사용하여 훈련됩니다.
- 제안된 시스템은 단일 모드 기준과 비교하여 더 낮은 동등 오류율(EERs)을 달성하면서 훈련 데이터의 일부만을 사용한다는 것을 보여줍니다.
- 저차원의 전문 오디오 표현이 고차원의 일반 오디오 표현보다 더 낮은 EERs를 이끌어낸다는 것을 보여줍니다.

