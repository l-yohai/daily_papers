## Daily Papers (2023-12-26)

### [WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation](https://arxiv.org/abs/2312.14187)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pz-KPFiU6hTwBg-9IkHUf.png)

Vote: 22

Authors: Zhaojian Yu, Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, Qiufeng Yin

- 본 논문에서는 고품질 지시어 데이터 세트에 대한 튜닝을 통해 다양한 작업을 수행할 능력을 갖춘 모델을 제작하는 새로운 방법을 제시합니다.
- 기존 지시어 데이터 생성 방법의 한계를 극복하고자 4가지 코드 관련 작업에 대한 지시어 데이터 분류와 LLM(대규모 언어 모델) 기반 생성-감별 프레임워크를 이용해 다양하고 고품질의 새로운 지시어 데이터를 생성합니다.
- 이를 통해 제안된 CodeOcean 데이터셋은 4가지 범용 코드 관련 작업에 걸쳐 20,000개의 지시어 인스턴스를 포함하며 지시어 튜닝의 효과를 증가시키고 모델의 일반화 능력을 개선하기 위해 고안되었습니다.
- 더 나아가 WaveCoder라는 이름의 Code LLM을 선보이며 이는 다양하고 범용적으로 향상된 지시어 튜닝을 위해 특별히 설계되었습니다.
- 실험을 통해 WaveCoder 모델은 동일한 수준의 튜닝 규모에서 다른 오픈 소스 모델들에 비해 다양한 코드 관련 작업에서의 일반화 능력이 뛰어남을 보였습니다.
- 또한, WaveCoder는 기존의 코드 생성 작업에서 효율성이 높은 것으로 나타났습니다.
- 결론적으로, 이 논문은 지시어 데이터 생성 및 모델 튜닝 분야에 중요한 기여를 하며 코드 관련 작업의 성능 향상을 위한 새로운 통찰과 도구를 제공합니다.

### [Reasons to Reject? Aligning Language Models with Judgments](https://arxiv.org/abs/2312.14591)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ZYbDeZwoJNB4mPqMJmrQX.png)

Vote: 10

Authors: Weiwen Xu, Weiwen Xu, Deng Cai, Zhisong Zhang, Wai Lam, Shuming Shi

- 자연어로 된 피드백을 받는 인간의 상호작용을 바탕으로, 대규모 언어 모델(LLMs)을 언어 피드백을 통해 학습하고자 하는 새로운 연구를 소개합니다.
- 기존 연구와 달리, 보상이나 선호도 데이터를 사용하는 것이 아닌, 판단(judgment)을 통한 언어 모델의 정렬을 체계적으로 탐색한 최초의 연구입니다.
- 판단을 기반으로 한 언어 모델 학습에 적합한 여러 방법들을 심층적으로 조사하고, 기존 방법들이 판단을 충분히 활용하지 못한다는 점을 발견했습니다.
- 더 효과적인 판단의 활용을 돕기 위해, 판단에 근거한 부적절한 내용 감지 및 수정이 가능한 새로운 프레임워크인 Contrastive Unlikelihood Training (CUT)을 제안합니다.
- CUT을 적용한 LLMs (LLaMA2-13b)는 약 1317개의 준비된 판단 데이터만으로도 175B DaVinci003을 능가하고 AlpacaEval에서 최고의 기준 모델을 52.34점 차이로 앞섰다는 오프라인 정렬 결과를 보여 줍니다.
- 온라인 정렬 결과, CUT는 모델 특정 판단 데이터를 사용하여 LLMs (LLaMA2-chat-13b)를 반복적으로 정렬하며, AlpacaEval에서 81.09점에서 91.36점으로 꾸준한 성능 향상을 보여 줍니다.
- 분석을 통해 판단이 보상보다 LLM 정렬에 더 큰 잠재력을 가지고 있으며, 향후 연구가 필요함을 시사합니다.

### [DreamDistribution: Prompt Distribution Learning for Text-to-Image Diffusion Models](https://arxiv.org/abs/2312.14216)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pJDKAco0t5U_tI66hXLDL.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pJDKAco0t5U_tI66hXLDL.mp4" muted="false"></video></div>

Vote: 7

Authors: Brian Nlong Zhao, Yuhang Xiao, Jiashu Xu, Xinyang Jiang, Yifan Yang, Dongsheng Li, Laurent Itti, Vibhav Vineet, Yunhao Ge

- 텍스트에서 이미지로의 확산 모델을 이용해 고품질 이미지를 생성하는 기술이 대중화되고 있으나, 참조 시각 속성을 가진 다양한 맞춤형 이미지를 생성하는 것은 여전히 도전적임.
- 본 연구는 텍스트에서 이미지로의 확산 모델을 개인화하여 추상적 개념 또는 카테고리 수준에서 참조 이미지 집합의 공통점을 적응하고 충분한 변형을 가진 새로운 인스턴스를 생성하는 데 초점을 맞춤.
- 사전 학습된 텍스트에서 이미지로의 확산 모델이 학습된 분포에서 프롬프트를 샘플링하여 새로운 이미지를 생성할 수 있게 하는 소프트 프롬프트 집합을 학습하는 솔루션을 소개함.
- 이 프롬프트들은 텍스트 가이드 분산 생성 능력을 제공하며, 다중 분포 간의 변동성을 컨트롤하는 추가적인 유연성을 제공함.
- 학습된 프롬프트 분포의 적응성을 텍스트에서 3D로의 다른 작업에도 적용 가능함을 보임.
- 양적 분석뿐만 아니라 자동 평가 및 인간 평가를 포함한 비교 연구를 통해 본 연구의 접근법의 효과성을 입증함.
- 프로젝트 웹사이트: https://briannlongzhao.github.io/DreamDistribution

### [PlatoNeRF: 3D Reconstruction in Plato's Cave via Single-View Two-Bounce Lidar](https://arxiv.org/abs/2312.14239)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vTZszFFaLYbDAsX5Wdl1J.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vTZszFFaLYbDAsX5Wdl1J.mp4" muted="false"></video></div>

Vote: 7

Authors: Tzofi Klinghoffer, Xiaoyu Xiang, Siddharth Somasundaram, Yuchen Fan, Christian Richardt, Ramesh Raskar, Rakesh Ranjan

- 단일 시점에서의 3D 재구성은 단안 큐(monocular cues)의 모호성과 가려진 영역에 대한 정보 부족으로 인해 어렵습니다.
- 신경 복사 분야(Neural radiance fields, NeRF)는 시점 합성(view synthesis)과 3D 재구성에 인기 있으나, 일반적으로 다중 시점 이미지에 의존합니다.
- 기존 단일 시점 3D 재구성 방법들은 가려진 영역의 이미지를 가정하는 데이터 사전(data priors)에 의존하거나, 불분명한 주변광과 저 알베도(albedo) 배경에서는 탐지하기 어려운 RGB 카메라로 관찰된 그림자에 의존합니다.
- 우리는 단일 광자 적산 다이오드(single-photon avalanche diode)에 의해 캡처된 비행 시간(time-of-flight) 데이터를 사용하여 이러한 한계를 극복하는 방법을 제안합니다.
- 우리의 방법은 NeRF를 사용하여 두 번의 광학 경로(two-bounce optical paths)를 모델링하고, 리다(lidar) 트랜지언트 데이터(transient data)를 통제로 사용합니다.
- NeRF와 리다가 측정한 두 번의 바운스 빛의 이점을 활용함으로써, 데이터 사전이나 조절된 주변광, 장면 알베도에 의존하지 않고도 보이는 기하학적 구조(visible geometry)와 가려진 기하학적 구조(occluded geometry)를 재구성할 수 있다는 것을 입증했습니다.
- 또한, 센서 공간적 및 시간적 분해능(spatial- and temporal-resolution)에 대한 현실적인 제약에서 일반화(generalization)가 향상되었음을 보여줍니다.
- 단일 광자 리다(single-photon lidars)가 휴대폰, 태블릿, 헤드셋과 같은 소비자 장치에서 보편화됨에 따라, 우리의 방법이 유망한 방향임을 믿습니다.

### [InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks](https://arxiv.org/abs/2312.14238)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/7F6Pbd_hiG2l-BCHVUPtF.png)

Vote: 7

Authors: Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai

- 언어 모델의 빠른 성장과 달리, 시각 및 시각-언어 기반 모델의 진보는 더딘 편이었다.
- 연구에서는 웹 규모의 이미지-텍스트 데이터를 사용하여 60억 개의 매개변수를 가진 시각-언어 기반 모델(InterVL)을 설계, 크기를 늘리고 언어 모델과의 정렬을 진행하였다.
- 이미지 인식, 픽셀 수준 인식과 같은 시각 인식 과제와 제로샷 이미지/비디오 분류, 제로샷 이미지/비디오-텍스트 검색 등의 시각-언어 작업에 두루 적용 가능하다.
- 또한, InterVL 모델은 대규모 언어 모델과 연계하여 다중 모달 대화 시스템을 형성하는 데 사용될 수 있다.
- 이 연구가 다중 모달 대규모 모델의 발전에 기여하길 기대하며, 관련 코드와 모델은 제공된 링크를 통해 접근할 수 있다.

### [VCoder: Versatile Vision Encoders for Multimodal Large Language Models](https://arxiv.org/abs/2312.14233)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Uw4kwONUyJE5oQlEe8s_g.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Uw4kwONUyJE5oQlEe8s_g.mp4" muted="false"></video></div>

Vote: 7

Authors: Jitesh Jain, Jianwei Yang, Humphrey Shi

- 사람은 육안으로 본 것을 이해하고 사유하는 시각 인식이라는 놀라운 능력을 가지고 있으며, 최근 다양한 시각-언어 작업에서 인상적인 성능을 달성한 다모드 큰 언어 모델(MLLM)들이 등장했습니다.
- 그러나 기존 MLLM 시스템은 주어진 이미지 내의 개체를 식별하거나 개수를 셀 때(인식할 때) 실패한다는 문제점이 있었습니다.
- 우리는 이러한 인식과 사유를 위한 정확한 MLLM 시스템을 개발하기 위해, 다모드 LLM을 위한 시각 인식의 눈으로 다재다능한 시각 인코더(VCoder)를 사용할 것을 제안합니다.
- 세그먼테이션 또는 깊이 맵과 같은 인식 모달리티를 VCoder에 공급함으로써, MLLM의 인식 능력을 향상시키고자 합니다.
- COCO 이미지와 상용 시각 인식 모델의 출력을 활용해 MLLM의 객체 인식 작업에 대한 교육 및 평가를 위한 COCO Segmentation Text (COST) 데이터 세트를 생성했습니다.
- MLLM의 객체 인식 능력을 평가하기 위한 지표를 소개하고, COST 데이터 세트에서 MLLM의 객체 인식 능력을 평가할 수 있습니다.
- VCoder가 GPT-4V를 포함한 기존의 다모드 LLM에 비해 객체 수준 인식 기술이 향상되었음을 입증하는 광범위한 실험적 증거를 제공합니다.
- 연구 촉진을 위해 데이터 세트, 코드 및 모델을 오픈소스로 제공하며, 우리의 코드는 https://github.com/SHI-Labs/VCoder 에서 오픈소스로 제공됩니다.

### [Exploiting Novel GPT-4 APIs](https://arxiv.org/abs/2312.14302)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/14VZRk6MWW-SQ9MaUtAZ8.png)

Vote: 6

Authors: Kellin Pelrine, Mohammad Taufeeque, Michał Zając, Euan McLean, Adam Gleave

- 언어 모델 공격은 일반적으로 모델 가중치에 대한 전체 화이트박스 접근 또는 텍스트 생성 API에 대한 블랙박스 접근 중 하나를 가정한다.
- 그러나 실제 API는 텍스트 생성만을 넘어서 더 유연하며, 이로 인해 '그레이박스' 접근이 가능하고 새로운 위협 요소가 생긴다.
- GPT-4 API의 새로운 기능인 파인튜닝, 함수 호출, 지식 검색을 테스트하여 새로운 취약점을 평가하였다.
- 연구에 따르면, 15개의 해로운 예시나 100개의 해롭지 않은 예시로 모델을 파인튜닝할 때 GPT-4의 핵심 보호 기능을 제거할 수 있으며 다양한 해로운 출력을 가능하게 한다.
- 또한, GPT-4 어시스턴트가 함수 호출 스키마를 쉽게 노출하고 임의의 함수 호출을 실행할 수 있음을 발견하였다.
- 지식 검색 기능은 검색 문서 내에 지시사항을 주입함으로써 왜곡될 수 있다는 것도 밝혀졌다.
- 이러한 취약점들은 API에 의해 제공되는 기능의 추가가 새로운 보안 취약점을 만들어낼 수 있음을 강조한다.

### [Parrot Captions Teach CLIP to Spot Text](https://arxiv.org/abs/2312.14232)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/9x1DA0Meowcii7eoolmpA.png)

Vote: 5

Authors: Yiqi Lin, Conghui He, Alex Jinpeng Wang, Bin Wang, Weijia Li, Mike Zheng Shou

- CLIP이라는 비전-언어 모델이 이미지 내 포함된 텍스트를 그대로 '앵무새처럼' 반복하며 진짜 시각적 의미는 무시하는 심각한 텍스트 스폿팅 편향을 갖고 있음이 밝혀졌습니다.
- 인기 있는 이미지-텍스트 데이터셋 LAION-2B를 분석한 결과, 이미지 중 50%가 시각적 텍스트를 포함하고 있으며, 해당 캡션의 90%가 더 많거나 적게 이미지의 텍스트를 달팽이처럼 따라쓰고 있음이 드러났습니다.
- 이러한 관찰을 바탕으로, 다양한 버전의 CLIP 모델을 조사하여 LAION 스타일의 이미지-텍스트 유사성 측정에 있어 시각적 텍스트가 주요 인자로 작용함을 확인했습니다.
- 특히, 다양한 '앵무새 캡션' 기준에 따라 선별된 LAION 부분집합으로 CLIP 모델을 훈련시키면서 이러한 캡션들이 텍스트 스폿팅 편향을 형성하는지 조사했습니다.
- '앵무새 캡션'을 이용한 훈련은 쉽게 텍스트 스폿팅 편향을 형성하지만, CLIP 모델에서 기대되는 시각-언어 표현 학습에 해를 끼침을 보여주었습니다.
- 이는 CLIP과 같은 모델의 설계나 기존의 이미지-텍스트 데이터셋 큐레이션 파이프라인을 다시 검토해야 함을 시사합니다, 특히 CLIP 점수 필터링에 기반을 둔 것에 대해 문제를 제기합니다.

### [YAYI 2: Multilingual Open-Source Large Language Models](https://arxiv.org/abs/2312.14862)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ZuAwlda44hLaOZ0DkRCq3.png)

Vote: 4

Authors: Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu, Chenyang Zhao, Donglei Zhang, Fan Feng, Feifei Zhao, Hailong Sun, Hanxuan Yang, Haojun Pan, Hongyu Liu, Jianbin Guo, Jiangtao Du, Jingyi Wang, Junfeng Li, Lei Sun, Liduo Liu, +

- 자연어 처리의 최신 발전으로, 대규모 언어 모델(LLMs)은 다양한 현실 세계의 작업에서 인간 수준의 언어 이해 및 생성 능력을 달성했으며 인공 일반 지능으로 가는 잠재적인 길로 여겨져 왔습니다.
- 연구를 용이하게 하기 위해 여러 오픈 소스 LLMs, 예를 들어 Llama 2와 Falcon,이 제안되었고, 해당 모델들은 독점 모델들과 비슷한 성능을 보였으나, 주로 영어 시나리오를 위해 설계되었고 중국어 문맥에서는 성능이 떨어집니다.
- 본 기술 보고서에서는 YAYI 2를 제안하는데, 이는 기본 및 챗 모델을 포함하며, 300억 개의 매개변수를 가지고 있습니다.
- YAYI 2는 2.65조 토큰을 포함하는 다국어 말뭉치에서 처음부터 사전 훈련되었으며, 우리의 사전 훈련 데이터 처리 파이프라인을 통해 필터링되었습니다.
- 기본 모델은 수백만 개의 지침으로 감독된 미세 조정을 통해 인간의 가치에 맞추어졌고, 인간의 피드백으로부터 강화 학습을 통해 조정되었습니다.
- MMLU 및 CMMLU와 같은 여러 벤치마크에서 실시한 광범위한 실험을 통해, 제안된 YAYI 2는 다른 비슷한 크기의 오픈 소스 모델들을 일관적으로 능가하는 성능을 보여주었습니다.

### [Parameter Efficient Tuning Allows Scalable Personalization of LLMs for Text Entry: A Case Study on Abbreviation Expansion](https://arxiv.org/abs/2312.14327)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/qbUWCWrxQ-DL_X9wb2pve.png)

Vote: 4

Authors: Katrin Tomanek, Shanqing Cai, Subhashini Venugopalan

- 약어 확장을 통하여 의사소통 속도를 높이기 위해, 연구진들은 대규모 언어 모델(LLM)이 이전 대화를 기반으로 개인화된 제안을 하는 방법을 탐구했습니다.
- 이 연구에서는 사용자 데이터가 소량일 때(~1000개 샘플) 개인화가 예측의 관련성을 향상시킬 수 있는지에 초점을 맞췄습니다.
- 연구진은 미세조정, 프롬프트 튜닝, 검색 보강 생성 방식을 비교하여 약어 입력에 대한 확장된 텍스트 제안을 검토했습니다.
- ALS를 앓고 있는 실제 사용자를 대상으로 한 80억 파라미터 LLM의 사례 연구와 영화 캐릭터 개인화 실험 결과에서 (1) 몇몇 상황에서 맞춤화가 필요하며 프롬프트 튜닝이 잘 일반화되는 것을 보여주었습니다.
- (2) 도메인 내 데이터에 대한 미세조정은 600개의 샘플로도 어느 정도 성능 향상을 보이지만, (3) 검색 보강 몇 샷 선택이 미세조정보다 더 우수한 성능을 보였습니다.
- (4) 파라미터 효율적인 튜닝은 개인화를 효율적으로 확장 가능하게 할 수 있습니다.
- 프롬프트 튜닝의 경우, 배운 "소프트-프롬프트"를 사용자 관련 개념 토큰에 초기화하는 것이 무작위 초기화보다 더 높은 정확도에 도달하는 것으로 나타났습니다.

### [Generative AI Beyond LLMs: System Implications of Multi-Modal Generation](https://arxiv.org/abs/2312.14385)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/p92_VOZzvh9QOeoVRusGG.png)

Vote: 3

Authors: Alicia Golden, Samuel Hsia, Fei Sun, Bilge Acun, Basil Hosmer, Yejin Lee, Zachary DeVito, Jeff Johnson, Gu-Yeon Wei, David Brooks, Carole-Jean Wu

- 대규모 생성 인공지능(AI) 모델의 발전이 텍스트(1D) 생성을 넘어 이미지(2D) 및 비디오(3D) 생성으로 확장됨에 따라, 공간 및 시간 정보 처리는 품질, 성능 및 효율성에 있어 독특한 도전 과제를 제시합니다.
- 이 연구는 텍스트-이미지(TTI) 및 텍스트-비디오(TTV) 생성 모델을 위한 다중 모달 모델의 새로운 시스템 설계 공간을 이해하기 위한 첫 번째 작업을 제시합니다.
- 현재 모델 아키텍처 설계는 확산 기반과 트랜스포머 기반 모델 두 가지 범주로 나뉩니다.
- 여덟 가지 대표적인 TTI/TTV 모델에 대한 체계적 성능 특성 분석 결과 Flash Attention과 같은 최신 최적화 기술을 적용한 후 확산 기반 TTI 모델에 대해 Convolution이 실행 시간의 최대 44%를 차지하는 반면, 트랜스포머 기반 모델에서는 Linear 층이 최대 49%를 차지합니다.
- 확산 기반 TTI 모델은 LLM 추론의 Prefill 단계와 유사하고 Flash Attention에서 1.1-2.5배 더 큰 속도 향상을 보이며, 트랜스포머 기반 TTI 모델은 Decode 단계와 유사합니다.
- LLM용으로 설계된 최적화가 TTI/TTV 모델에 직접 적용되지 않기 때문에 새로운 최적화 기회를 얻기 위해 이러한 워크로드를 철저하게 특성화해야 합니다.
- TTI/TTV 모델의 맥락에서 시퀀스 길이를 정의하고, 확산 모델 추론에서 시퀀스 길이가 최대 4배까지 변할 수 있다는 것을 관찰했습니다.
- TTV 워크로드의 시간적 측면은 시스템 병목 현상을 독특하게 야기하며, Temporal Attention이 전체 Attention 시간의 60% 이상을 차지합니다.
- 전반적으로, 이 깊이 있는 시스템 성능 특성 분석은 다가오는 TTI/TTV 워크로드에 대한 효율적이고 배포 가능한 시스템 설계를 위한 중요한 첫 걸음입니다.

### [Shai: A large language model for asset management](https://arxiv.org/abs/2312.14203)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/LAtV_SCKOzazF3wRiGt8C.png)

Vote: 3

Authors: Zhongyang Guo, Guanran Jiang, Zhongdan Zhang, Peng Li, Zhefeng Wang, Yinchun Wang

- 이 논문은 "Shai" 라고 불리는 100억 급 대규모 언어 모델을 소개하는데, 이는 자산 관리 산업을 위해 특별히 고안되었으며, 개방형 기반 모델을 기반으로 구축되었습니다.
- 타깃된 코퍼스를 이용한 연속적인 사전 훈련 및 미세 조정을 통해, Shai는 해당 도메인에서 중요한 작업들을 더욱 향상된 성능으로 수행하며, 기준선 모델들을 뛰어넘는 성과를 보여줍니다.
- 연구자들은 Shai의 능력을 종합적으로 평가하기 위해 전문가 자격 시험, 맞춤형 작업, 개방형 질의응답, 안전성 평가를 통합한 혁신적인 평가 프레임워크를 개발하였습니다.
- 또한, 자산 관리 분야에서 GPT-4와 같은 대규모 언어 모델을 사용할 때의 도전 과제와 시사점에 대해 논의하며, 자동 평가와 인간 판단의 조합을 제안합니다.
- 100억 급 대규모 언어 모델의 잠재력과 금융 부문에서의 다양성을 시연한 Shai의 개발은 업계 동료들이 유사하게 시도하는 과정에서 실용적인 통찰과 방법론을 제공하기를 기대하며, 중요한 성능 향상과 함께 비교적 적은 컴퓨팅 요구 사항을 갖추고 있습니다.

### [ZeroShape: Regression-based Zero-shot Shape Reconstruction](https://arxiv.org/abs/2312.14198)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/NI7FpXDCzv_BqePHzhzTQ.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/NI7FpXDCzv_BqePHzhzTQ.mp4" muted="false"></video></div>

Vote: 2

Authors: Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani, James M. Rehg

- 'ZeroShape' 연구는 단일 이미지 기반 제로샷 3D 형태 복원 문제를 다룬다.
- 최근 연구들은 3D 자산의 생성 모델을 통해 제로샷 형태 복원을 학습하지만, 이러한 모델은 훈련 및 추론 시 계산 비용이 높다.
- 전통적인 회귀 기반 접근법은 확률적인 생성 방법보다 월등히 높은 계산 효율성을 지니고, 동일한 문제에 직접적으로 형태를 추론하도록 모델을 훈련한다.
- 이 연구는 회귀 기반 접근법이 아직도 경쟁력을 갖고 있는지, 또는 생성 모델링이 높은 성능을 위해 꼭 필요한 것인지에 대한 질문을 제기한다.
- 다양한 새로운 통찰과 이 분야의 지속적인 발견을 바탕으로, 우리는 강력한 회귀 기반 모델인 'ZeroShape'를 설계했다.
- 실제 세계의 3D 데이터셋 세 개로부터 가져온 물체들을 포함하는 대규모 실제 세계 평가 벤치마크를 조성했다.
- 이 평가 벤치마크는 기존 연구들이 사용하는 것보다 더 다양하고 한 자릿수 크기로, 우리 분야에서 평가 변동성을 줄이는 것을 목표로 한다.
- ZeroShape는 최첨단 방법들 보다 우수한 성능을 달성할 뿐만 아니라, 높은 계산 및 데이터 효율성을 보여준다.

### [MACS: Mass Conditioned 3D Hand and Object Motion Synthesis](https://arxiv.org/abs/2312.14929)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/DTAfHbZAP_ny0N4XB-JXz.png)

Vote: 2

Authors: Soshi Shimada, Franziska Mueller, Jan Bednarik, Bardia Doosti, Bernd Bickel, Danhang Tang, Vladislav Golyanik, Jonathan Taylor, Christian Theobalt, Thabo Beeler

- 객체의 질량과 같은 물리적 특성은 우리가 손으로 조작할 때 중요한 역할을 하지만, 이전의 3D 동작 합성 연구에서는 이 요소를 간과했습니다.
- 본 연구에서 제안하는 MACS는 첫 번째 물체 질량 조건부 3D 손과 물체 동작 합성 방법으로, 객체의 질량과 상호작용 유형을 기반으로 적절하게 조정되는 상호작용을 생성합니다.
- MACS는 수동으로 그린 3D 물체 궤적을 입력으로 받아 객체 질량에 따라 자연스러운 3D 손 동작을 합성합니다.
- ML 작업을 위한 합성 훈련 데이터 생성, 그래픽 워크플로에 손의 빠른 애니메이션 생성, 컴퓨터 게임 내 캐릭터 상호작용 생성 등 여러 하류 응용 프로그램에 MACS를 사용할 수 있습니다.
- 실험적으로 나타난 바에 따르면, MACS는 훈련 중에 보지 못한 내삽 및 외삽된 객체 질량에 합리적으로 일반화할 수 있으며, ConNet이 생성하는 질량 조건부 접촉 라벨 덕분에 보지 못한 물체에 대해서도 적당한 일반화를 보입니다.
- 종합적인 사용자 연구를 통해 합성된 3D 손-물체 상호작용이 매우 설득력 있고 현실적임을 확인했습니다.

### [Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning](https://arxiv.org/abs/2312.14878)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/N5_6wFfyo8gH0xn5OI88P.png)

Vote: 2

Authors: Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, Zheng Xiong, Yicheng Luo, Jianye Hao, Kun Shao, Haitham Bou-Ammar, Jun Wang

- 이 논문은 개선된 인공지능(AI) 에이전트를 생성하기 위한 주요 방법으로 강화 학습(RL)에 초점을 맞추고 있습니다.
- 기존의 RL 정책은 직접적인 인식과 행동의 매핑에 있어 다양한 과제의 일반성 결여 및 대량의 훈련 데이터 필요로 하는 문제점을 가지고 있습니다.
- 대규모 언어 모델(LLMs)은 AI 에이전트에 걸쳐 도메인 지식을 통합하는 데 중요하지만, 특정 결정 문제에 대한 학습과 적응에는 부족함이 있습니다.
- 이 논문은 인공지능 에이전트의 정책에 구조화된 추론을 통합하고 학습하는 일반적 프레임워크를 제시합니다.
- 인간 두뇌에서 발견되는 모듈성에 의해 동기를 부여받은 이 방법론은 인지 과정의 모듈 구조와 일관되는 방식으로 각 모듈이나 함수 내부에서 모델들을 학습하는 적응 능력을 제공합니다.
- 논문은 프레임워크를 심도 있게 설명하고 다른 AI 파이프라인 및 기존 프레임워크들과 비교합니다.
- 실제 응용 가능성을 탐구하며, 이 방법의 효과를 보여주는 실험들을 다룹니다.
- 연구 결과는 AI 에이전트들이 조직적인 추론과 사전 지식이 내장될 때 훨씬 더 낫게 수행하고 적응한다는 것을 나타내며, 이는 더욱 강건하고 일반적인 AI 에이전트 시스템으로 이어지는 문을 엽니다.

### [LLM4VG: Large Language Models Evaluation for Video Grounding](https://arxiv.org/abs/2312.14206)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/UKUnLkvZX9ayWspCIcVIm.png)

Vote: 1

Authors: Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, Zihan Song, Yuwei Zhou, Wenwu Zhu

- 최근 연구자들은 LLMs의 비디오 처리 능력을 조사하고 여러 비디오 LLM 모델들을 제안하였지만, 텍스트 쿼리와 일치하는 비디오 내 시간적 순간의 시작과 끝 타임스탬프를 정확히 찾아내는 중요한 시간 관련 비디오 작업인 비디오 그라운딩(VG) 능력은 아직도 불명확하고 연구되지 않았다.
- 이 공백을 메우기 위해 본 논문에서는 다양한 LLM을 비디오 그라운딩 작업에 대한 성능을 체계적으로 평가하는 LLM4VG 벤치마크를 제안한다.
- LLM4VG를 바탕으로, 비디오 그라운딩에 대해 평가하기 위해 두 가지 그룹의 비디오 LLM 모델을 광범위한 실험을 설계하였다: (i) 텍스트-비디오 쌍에 대해 훈련된 비디오 LLM(VidLLM), (ii) 비디오/이미지 캡셔닝 모델 같은 사전 훈련된 비주얼 설명 모델이 결합된 LLMs.
- VG의 지시를 통합하고 다양한 제너레이터에서 설명을 생성하는 프롬프트 방법을 제안하는데, 이는 직접적 비주얼 설명을 위한 캡션 기반 생성기와 정보 강화를 위한 VQA 기반 생성기를 포함한다.
- 또한, 다양한 VidLLMs의 비교와 비주얼 모델, LLM, 프롬프트 설계 등의 다른 선택이 미치는 영향에 대한 종합적인 비교를 제공한다.
- 실험 평가는 다음 두 가지 결론으로 이어진다: (i) 현재 VidLLMs는 비디오 그라운딩 성능에 대해 만족스러운 결과를 달성하기에는 아직 먼 길이며, 이 모델들을 더욱 미세 조정하기 위해 더 많은 시간 관련 비디오 작업이 포함되어야 한다, (ii) LLMs와 비주얼 모델의 결합은 비디오 그라운딩에 대한 초기 능력을 보여주고 있으며, 더 신뢰할 수 있는 모델과 추가적인 프롬프트 지시의 가이드를 통해 개선 가능성이 높다.

