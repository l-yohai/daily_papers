## Daily Papers (2023-12-15)

### [StemGen: A music generation model that listens](https://arxiv.org/abs/2312.08723)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/MXokcBs1a515R_qRZ2ejz.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/MXokcBs1a515R_qRZ2ejz.mp4" muted="false"></video></div>

Vote: 25

Authors: Julian D. Parker, Julian D. Parker, Janne Spijkervet, Katerina Kosta, Katerina Kosta, Furkan Yesiler, Boris Kuznetsov, Ju-Chiang Wang, Matt Avent, Matt Avent, Jitong Chen, Duc Le

- 딥러닝 기술을 이용하여 음악 오디오를 처음부터 끝까지 생성하는 연구가 최근 활발해졌습니다만, 대부분의 모델들은 추상적 조건 정보에 반응하여 완전히 혼합된 음악을 생성하는 데 집중하고 있습니다.
- 본 연구에서는 음악적 맥락에 귀를 기울이고 반응할 수 있는 음악 생성 모델을 만들기 위한 대안적 패러다임을 제안합니다.
- 비자기회귀적이면서 변환기(Transformer) 기반의 모델 아키텍처를 사용하여 이러한 모델을 구축하는 방법을 설명하며, 몇 가지 새로운 아키텍처 및 샘플링 개선 사항을 제시합니다.
- 이 연구에서 설명하는 아키텍처는 오픈 소스 데이터셋과 독점 데이터셋 모두에서 훈련되었습니다.
- 표준 품질 지표와 음악 정보 검색 기술을 기반으로 한 새로운 접근 방식을 사용하여 생성된 모델을 평가했습니다.
- 결과적으로 생산된 모델은 텍스트 조건 모델과 동등한 오디오 품질에 도달했을 뿐 아니라 기존 음악 맥락과 강력한 음악적 일관성을 보여줍니다.

### [CogAgent: A Visual Language Model for GUI Agents](https://arxiv.org/abs/2312.08914)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/N17ZGsrw6cMNLqWMHGiky.png)

Vote: 11

Authors: Wenyi Hong, Wenyi Hong, Weihan Wang, Qingsong Lv, Qingsong Lv, Jiazheng Xu, Jiazheng Xu, Wenmeng Yu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Yuxiao Dong, Ming Ding, Jie Tang, Jie Tang

- 본 논문에서는 GUI(그래피컬 유저 인터페이스) 이해 및 탐색을 전문으로 하는 180억 개의 파라미터를 가진 시각 언어 모델인 CogAgent를 소개하고 있습니다.
- CogAgent는 저해상도 및 고해상도 이미지 인코더를 사용함으로써 1120*1120 해상도의 입력을 지원하고, 작은 페이지 요소 및 텍스트를 인식할 수 있습니다.
- 이 모델은 텍스트가 풍부한 다섯 가지 및 일반적인 네 가지 VQA(비주얼 질문 응답) 벤치마크에서 현재 최고의 성능을 달성했습니다.
- CogAgent는 HTML 텍스트를 추출하는 LLM(대규모 언어 모델) 기반 방법을 능가하며, 오직 스크린샷만을 입력으로 사용하여 PC 및 Android GUI 탐색 작업에서 현재 기술의 최고 수준의 성과를 보여줍니다.
- 모델과 코드는 https://github.com/THUDM/CogVLM 에서 제공되고 있습니다.

### [TinyGSM: achieving >80% on GSM8k with small language models](https://arxiv.org/abs/2312.09241)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xNNBTtwf9JHvTLdEq5JRe.png)

Vote: 10

Authors: Bingbin Liu, Sebastien Bubeck, Sebastien Bubeck, Ronen Eldan, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Yuanzhi Li, Anh Nguyen, Rachel Ward, Yi Zhang

- 소규모 모델의 계산상 이점은 다양하지만, 문제 해결 능력에 크기가 얼마나 중요한지는 여전히 미해결된 질문입니다.
- 초등학교 수학 문제를 해결하는 데 있어서, GSM8K 벤치마크에서 80% 이상을 달성하기 위해 지금까지 필요한 최소 모델 크기는 34B였습니다.
- 본 연구는 고품질 데이터셋이 소규모 언어 모델이 수학적 추론을 습득하는 데 중요한 열쇠가 될 수 있다는 점을 탐구합니다.
- TinyGSM, 즉 GPT-3.5에 의해 완전히 생성된 1230만 개의 초등학교 수학 문제와 파이썬 솔루션이 짝지어진 합성 데이터셋을 소개합니다.
- TinyGSM에서 미세 조정 후, 1.3B 생성 모델과 1.3B 검증 모델로 구성된 듀오가 81.5% 정확도를 달성하여 규모가 훨씬 큰 기존 모델들보다 우수한 성능을 보였습니다.
- 이는 우리 모델의 교육 데이터가 생성된 GPT-3.5 "선생님" 모델(77.4%)의 성능과 맞먹기도 합니다.
- 접근 방식은 간단하며 두 가지 주요 구성 요소가 있습니다: 1) 고품질 데이터셋 TinyGSM, 2) 여러 후보 생성물 중 최종 출력을 선택하는 검증기 사용.

### [A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions](https://arxiv.org/abs/2312.08578)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/T_awjvIfM5FuOaHJ97wEY.png)

Vote: 9

Authors: Jack Urbanek, Jack Urbanek, Florian Bordes, Florian Bordes, Pietro Astolfi, Pietro Astolfi, Mary Williamson, Vasu Sharma, Adriana Romero-Soriano

- 대규모 시각-언어 데이터셋의 큐레이션 방식은 데이터셋의 크기와 품질 사이에서 절충을 이루고 있으나, 심지어 가장 높은 품질의 큐레이션된 캡션이라 할지라도 이미지의 풍부한 시각적 세부사항을 충분히 포착하기에는 너무 짧습니다.
- 본 연구에서는 인간이 주석을 단 8012개의 자연 이미지로 구성된, 마스크와 정렬된 설명을 평균 1000단어 이상 포함하는 Densely Captioned Images (DCI) 데이터셋을 수집하여, 밀집하고 높은 정렬의 이미지-텍스트 쌍의 가치를 입증합니다.
- 특정 이미지 부분에 연관된 정확하고 신뢰할 수 있는 캡션을 통해 비전-언어 모델(VLM)이 이미지 내용을 얼마나 잘 이해하는지 평가하는 새로운 작업을 도입하였으며, 각 캡션을 해당 부분 이미지와 매칭합니다.
- 현재 모델들은 대개 77개의 텍스트 토큰으로 제한되므로, 본 연구팀은 각 캡션 길이가 제한된 sDCI 요약 버전을 소개하고 이를 기반으로 한 벤치마크에서 표준 벤치마크에서의 진보하는 현대 기술이 중요한 개선과 관련이 없음을 보여줍니다.
- 또한, sDCI를 사용해 CLIP을 세부 조정하고 작은 훈련 세트에도 불구하고 기준선에 비해 상당한 향상을 보여줍니다.
- 이 연구는 다음 세대의 VLM 개발을 위한 새로운 벤치마크 또는 세부 조정 레시피를 가능하게 할 첫 번째 인간 주석이 달린 밀집 이미지 캡션 데이터셋을 공개함으로써, 그들의 평가 및 향상에 기여하고자 합니다.

### [VideoLCM: Video Latent Consistency Model](https://arxiv.org/abs/2312.09109)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vizXyl0RKESNPHHvIl14e.png)

Vote: 6

Authors: Xiang Wang, Shiwei Zhang, Shiwei Zhang, Han Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, Nong Sang

- 일관성 모델은 효율적인 이미지 생성에서 강력한 능력을 발휘하였으며, 확산 모델에서의 높은 계산 비용을 줄이기 위하여 몇 번의 샘플링 단계만으로 합성을 가능하게 하였습니다.
- 그러나 더 도전적이고 리소스를 많이 소모하는 비디오 생성 분야에서의 일관성 모델은 아직 충분히 탐구되지 않았습니다.
- 이 보고서에서는 이미지 생성에서 일관성 모델의 개념을 활용하여 최소한의 단계로 고품질의 비디오를 효율적으로 합성할 수 있는 VideoLCM 프레임워크를 소개합니다.
- VideoLCM은 기존의 잠재적 비디오 확산 모델을 기반으로 하여 훈련을 위한 일관성 증류 기술을 도입합니다.
- 실험 결과는 VideoLCM의 계산 효율성, 충실도 및 시간적 일관성 측면에서의 효과를 입증합니다.
- 특히 VideoLCM은 단 네 번의 샘플링 단계로 고해상도 및 부드러운 비디오 합성을 달성하여 실시간 합성의 잠재력을 보여줍니다.
- VideoLCM이 향후 연구를 위한 간단하면서도 효과적인 기준선으로 활용되기를 기대합니다.
- 소스 코드와 모델은 공개적으로 사용할 수 있게 될 예정입니다.

### [LIME: Localized Image Editing via Attention Regularization in Diffusion Models](https://arxiv.org/abs/2312.09256)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/V0gr5z_QSVGNBgcTJ0hzf.png)

Vote: 5

Authors: Enis Simsar, Enis Simsar, Alessio Tonioni, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari

- 확산 모델(DMs)은 텍스트-이미지 생성에 있어서 고화질 및 다양한 이미지들을 생성할 수 있는 능력으로 인기를 얻었으며, 이제 연구는 DMs의 조작 가능성으로 이동하고 있다.
- 본 논문에서는 사용자가 지정한 관심 영역(RoI) 또는 추가적인 텍스트 입력 없이도 확산 모델에서 지역별 이미지 편집이 가능한 LIME을 소개한다.
- 저희 방법은 사전 훈련된 방법에서 특징을 사용하고 간단한 클러스터링 기술을 이용하여 정확한 의미 구분 맵을 얻는다. 
- 그런 다음, 크로스-어텐션 맵을 활용하여 이러한 세그먼트를 지역적 편집을 위해 세분화한다.
- 마지막으로, 노이즈 감소 단계에서 관련 없는 크로스-어텐션 점수에 대한 패널티를 부여하는 새로운 크로스-어텐션 규제 기법을 제안하여, 지역적 편집이 이루어지도록 한다.
- 재훈련 및 파인튜닝(re-training and fine-tuning) 없이도 저희 접근 방식은 다양한 편집 벤치마크에서 기존 방법들의 성능을 일관되게 향상시킨다.

### [Vision-Language Models as a Source of Rewards](https://arxiv.org/abs/2312.09187)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/NG6q-hH37jWXczH2x5jSq.png)

Vote: 5

Authors: Kate Baumli, Satinder Baveja, Feryal Behbahani, Feryal Behbahani, Harris Chan, Harris Chan, Gheorghe Comanici, Sebastian Flennerhag, Maxime Gazeau, Kristian Holsheimer, Dan Horgan, Michael Laskin, Clare Lyle, Clare Lyle, Hussain Masoom, Kay McKinney, Volodymyr Mnih, Alexander Neitz, Fabio Pardo, Jack Parker-Holder, John Quan, Tim Rocktäschel, Himanshu Sahni, Tom Schaul, Yannick Schroecker, +

- 강화 학습을 사용하여 다양한 목표를 달성할 수 있는 범용 에이전트를 구축하는 것은 연구 분야의 최전선 중 하나입니다.
- 다양한 목표를 달성하기 위한 다수의 보상 함수가 필요하다는 점이 범용 에이전트 구축의 주요 제한 요소가 되었습니다.
- 연구 팀은 기존의 시각-언어 모델(VLMs)을 이용하여 강화 학습 에이전트에 대한 보상의 원천으로 활용하는 가능성을 탐구했습니다.
- CLIP 모델 가족으로부터 얻어진 보상을 통해 언어 목표를 달성하는 시각적인 작업들을 수행할 수 있는 강화 학습 에이전트를 훈련시킬 수 있음을 보여줍니다.
- 이 접근 방식은 두 가지 독특한 시각 도메인에서 시연되었으며, 더 큰 VLMs가 시각적 목표 달성을 위한 더 정확한 보상을 이끌어내며, 이는 더 능력 있는 강화 학습 에이전트를 생산한다는 스케일링 경향을 보여줍니다.

### [Mosaic-SDF for 3D Generative Models](https://arxiv.org/abs/2312.09222)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/aD_lDYp3xaLXMjR4LyC-O.qt)

Vote: 4

Authors: Lior Yariv, Lior Yariv, Omri Puny, Natalia Neverova, Oran Gafni, Oran Gafni, Yaron Lipman

- 현재 3D 형상 생성 모델은 사전 훈련된 2D 이미지 확산 모델을 정제하거나 3D 형상으로 직접 훈련하는 두 가지 구분으로 나뉩니다.
- 3D 형상에 대한 확산이나 유동 모델을 훈련할 때 중요한 설계 요소는 형상 표현 방법입니다.
- 효과적인 형상 표현은 큰 3D 데이터셋을 표현 형태로 효율적으로 변환할 수 있어야 하며, 근사능력과 파라미터 수의 좋은 균형을 제공해야 하고, 기존 강력한 신경 아키텍처와 호환 가능한 간단한 텐서 형태를 가져야 합니다.
- 전통적인 3D 형상 표현 방법인 볼륨 그리드와 포인트 클라우드는 이러한 원칙을 동시에 충족하지 못하는 반면, 본 논문에서 제안하는 새로운 표현 방식은 이러한 원칙을 동시에 충족합니다.
- 저희는 Mosaic-SDF(M-SDF)라는 새로운 3D 형상 표현을 소개하며, 이는 주어진 형상의 부호 거리 함수(SDF)를 형상 경계 근처에 퍼져 있는 여러 로컬 그리드를 사용하여 근사합니다.
- M-SDF 표현은 각 형상에 대해 빠르게 계산할 수 있으며 병렬 처리가 용이하고, 형상의 경계 주변 공간만을 커버하기 때문에 파라미터 효율성이 뛰어나며, 변압기 기반 아키텍처와 호환되는 간단한 행렬 형태를 가집니다.
- 본 논문은 3D Warehouse 데이터셋을 사용하여 범주 조건 생성 및 약 60만 개의 캡션-형상 쌍을 포함하는 데이터셋을 사용한 텍스트-3D 생성을 포함하여 M-SDF 표현을 사용하여 3D 생성 흐름 모델을 훈련함으로써 M-SDF의 효과를 입증합니다.

### [FineControlNet: Fine-level Text Control for Image Generation with Spatially Aligned Text Control Injection](https://arxiv.org/abs/2312.09252)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/KEmxkYLhj3vWR62B0TXSk.png)

Vote: 4

Authors: Hongsuk Choi, Hongsuk Choi, Isaac Kasahara, Selim Engin, Selim Engin, Moritz Graule, Moritz Graule, Nikhil Chavan-Dafle, Volkan Isler

- 최근 제안된 ControlNet은 인간 2D 포즈 또는 에지 기능과 같은 기하학적 입력을 통해 텍스트 기반 이미지 생성 과정을 제어하는 능력을 가지고 있습니다.
- ControlNet이 생성된 이미지 내 인스턴스의 기하학적 형태에 대한 제어는 가능하지만 각 인스턴스의 시각적 외모를 지시하는 능력에는 한계가 있습니다.
- 정교한 포즈 제어 기능을 유지하면서 각 인스턴스의 외모에 대한 미세 조정을 제공하기 위해 FineControlNet을 제시합니다.
- 구체적으로, 인간 포즈 이미지와 인스턴스 수준 텍스트 프롬프트를 통한 기하학적 제어 및 외모 제어를 위한 FineControlNet을 개발하고 시연합니다.
- 인스턴스별 텍스트 프롬프트와 2D 포즈의 잠재 공간에서의 공간적 정렬은 FineControlNet의 미세 제어 기능을 가능하게 합니다.
- FineControlNet의 성능은 최신 포즈 조건의 텍스트-이미지 확산 모델과의 엄격한 비교를 통해 평가됩니다.
- FineControlNet은 사용자가 제공한 인스턴스별 텍스트 프롬프트와 포즈를 따르는 이미지를 생성하는 데 있어 기존 방법들에 비해 우수한 성능을 달성합니다.
- 프로젝트 웹페이지: https://samsunglabs.github.io/FineControlNet-project-page

### [SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance](https://arxiv.org/abs/2312.08889)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/M5h7UDZ3Ca1kd1wo0wEDM.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/M5h7UDZ3Ca1kd1wo0wEDM.mp4" muted="false"></video></div>

Vote: 4

Authors: Yuanyou Xu, Zongxin Yang, Zongxin Yang, Yi Yang

- 대규모 텍스트-이미지 생성 모델을 기반으로 텍스트로부터 3D 아바타를 생성하는 부문은 발전을 거듭하고 있지만, 현재까지의 방법들은 대체로 정밀하지 않은 모양과 낮은 품질의 외모로 인해 포토리얼리스틱한 결과물을 생성하지 못한다.
- 실용적인 아바타 생성을 위해, 우리는 기하학과 외모에 대해 분리된 구속조건을 가진 SElf-Evolving(자가진화)를 통해 텍스트로부터 포토리얼리스틱한 3D 아바타를 생성하는 새로운 방법인 'SEEAvatar'를 제안한다.
- 기하학적인 부분에 있어서는, 템플릿 아바타를 인간에 대한 선험적 지식과 함께 초기화하고 이를 통해 정확한 글로벌 형태를 유지하며, 최적화된 아바타로 주기적으로 업데이트함으로써 보다 유연한 형태 생성을 가능하게 한다.
- 또한, 얼굴과 손 같은 지역적인 부분에서는 정교한 구조를 유지하기 위해 정적인 인간 선험적 지식에 의해 기하학을 제약한다.
- 외모 생성 부분에서는 프롬프트 엔지니어링으로 강화된 확산 모델을 사용하여 실제적인 질감을 생성하는 물리적 기반 렌더링 파이프라인을 안내한다.
- 알베도 질감에 적용된 밝기 제약을 통해 잘못된 조명 효과를 억제한다.
- 실험들은 우리의 방법이 글로벌 및 로컬의 기하학적 및 외모 품질 측면에서 이전의 방법들보다 크게 뛰어남을 보여준다.
- 우리의 방법은 높은 품질의 메쉬와 질감을 생성할 수 있어, 이러한 자산들은 어떠한 조명 상태에서도 사실적인 렌더링을 위한 고전적인 그래픽 파이프라인에 직접 적용될 수 있다.
- 프로젝트 페이지는 https://seeavatar3d.github.io 에서 확인할 수 있다.

### [Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention](https://arxiv.org/abs/2312.08618)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3vHubMNUGPAgjHgYl_JXD.png)

Vote: 4

Authors: Kaiqiang Song, Kaiqiang Song, Xiaoyang Wang, Sangwoo Cho, Xiaoman Pan, Dong Yu

- 본 논문에서는 대규모 언어 모델(Large Language Models, LLMs)이 방대한 텍스트 시퀀스를 처리하고 이해하는 능력을 향상시키는 새로운 접근 방법, Zebra를 소개한다.
- 기존 트랜스포머(Transformer) 아키텍처 기반 LLM의 문맥 윈도우를 확장하는 데 있어서의 복잡성 문제를 해결하기 위해 그룹화된 지역-전역(local-global) 주의력 레이어를 적용한 새로운 모델 구조로써 Zebra를 제안한다.
- Zebra는 얼룩말의 번갈아 나타나는 줄무늬와 같이 지역 및 전역 주의력 레이어를 균형 있게 조정하여 계산 요구 사항과 메모리 소비를 대폭 감소시킨다.
- 제안된 Zebra 모델의 성능을 평가하기 위해 처음부터 프리트레이닝하기, 긴 문맥 적응 트레이닝 계속하기, 그리고 긴 지시문 튜닝하기 등의 포괄적인 실험이 수행되었다.
- 실험 결과, Zebra는 짧은 시퀀스뿐만 아니라 긴 시퀀스 벤치마크에서도 비교 가능하거나 우수한 성능을 달성하는 것으로 나타났으며, 트레이닝 및 추론 효율성을 높였다.

### [VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation](https://arxiv.org/abs/2312.09251)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/UcOi8Yt9V8dA6UnVslC4J.png)

Vote: 3

Authors: Jinguo Zhu, Xiaohan Ding, Xiaohan Ding, Yixiao Ge, Yixiao Ge, Yuying Ge, Sijie Zhao, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, Xiaohua Wang, Ying Shan

- 이 연구에서는 이미지와 언어 데이터를 동시에 인지하고 생성하는 데 능숙한 트랜스포머 모델인 Vision-Language Generative Pre-trained Transformer (VL-GPT)을 소개합니다.
- VL-GPT는 이미지와 텍스트를 자연 언어 모델이 텍스트를 처리하는 것처럼 원활하게 처리할 수 있게 하는 자동 회귀 목적을 사용하여 통합된 사전 훈련 접근법을 달성합니다.
- 새로운 이미지 토크나이저-디토크나이저 프레임워크를 제안하여 이미지를 연속된 임베딩 시퀀스로 변환하고 이를 재구성할 수 있게 하며, 이는 트랜스포머 모델에 입력될 수 있는 다중모달 시퀀스로 이미지-텍스트 데이터를 인코딩할 수 있게 합니다.
- 사전 훈련을 완료한 후 VL-GPT는 제로샷(Zero-shot)과 퓨샷(Few-shot) 성능으로 이미지 캡셔닝, 시각적 질문 응답, 텍스트-이미지 생성 등 다양한 시각 및 언어 이해 및 생성 작업에서 뛰어난 성과를 보입니다.
- 이 모델은 다중모달 프롬프트가 제공될 때 맥락 학습 능력을 유지합니다.
- VL-GPT에 대한 추가적인 인스트럭션 튜닝을 통해 다중모달 지원에 대한 뛰어난 잠재력을 강조합니다.
- 소스 코드와 모델 가중치는 공개될 예정입니다.

### [Pixel Aligned Language Models](https://arxiv.org/abs/2312.09237)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ROrbOWbTegN012Md8OmVj.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ROrbOWbTegN012Md8OmVj.mp4" muted="false"></video></div>

Vote: 3

Authors: Jiarui Xu, Xingyi Zhou, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, Anurag Arnab, Chen Sun, Xiaolong Wang, Cordelia Schmid

- 최근 고성능을 달성한 대규모 언어 모델에 착안하여, 본 연구에서는 이미지 내 위치(점이나 상자 등)를 입력 혹은 출력으로 처리할 수 있는 시각-언어 모델을 개발하고자 하였습니다.
- 입력으로 위치를 받을 때, 모델은 해당 위치에 대한 캡션을 생성하는 위치 조건부 캡셔닝을 수행합니다.
- 출력으로 위치를 생성할 때는 언어 모델이 생성한 각 단어에 대해 픽셀 좌표를 추정하여 밀도가 높은 단어 그라운딩을 수행합니다.
- 사람의 주의를 반영한 픽셀-단어 정렬 캡셔닝이 있는 Localized Narrative 데이터셋에서 모델을 사전 훈련하였습니다.
- 모델은 참조 정렬, 위치 조건부 캡셔닝, 밀도 높은 객체 캡셔닝 등 다양한 위치 인식 비전-언어 작업에 적용될 수 있으며, RefCOCO와 Visual Genome에서 최첨단 성능을 달성하였음을 보여줍니다.
- 프로젝트 페이지는 https://jerryxu.net/PixelLLM 에서 더 많은 정보를 제공합니다.

### [General Object Foundation Model for Images and Videos at Scale](https://arxiv.org/abs/2312.09158)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/DqpAmUBf1VCpW8obojeKw.png)

Vote: 3

Authors: Junfeng Wu, Junfeng Wu, Yi Jiang, Qihao Liu, Qihao Liu, Zehuan Yuan, Xiang Bai, Xiang Bai, Song Bai

- 본 논문에서는 이미지 및 비디오에서 객체를 찾고 식별하는 객체 수준 기반 모델인 GLEE를 제시합니다.
- GLEE는 통합 프레임워크를 통해, 다양한 객체 인식 작업에 대하여 탐지, 분할, 추적, 기반화 및 식별을 수행합니다.
- 다양한 데이터 소스로부터 다양한 수준의 감독 하에 지식을 습득하며 일반 객체 표현을 형성하여, 새로운 데이터와 작업에 대한 제로샷 전이에 뛰어난 성능을 발휘합니다.
- 이미지 인코더, 텍스트 인코더 및 시각적 프롬프터를 사용하여 다중 모달 입력을 처리하고, 동시에 다양한 객체 중심의 다운스트림 작업을 해결하면서 최신 성능을 유지합니다.
- 500만 개 이상의 다양한 벤치마크 이미지로 대규모 훈련을 통해, GLEE는 다운스트림 작업을 특정 적응 없이도 효율적으로 처리하는 뛰어난 범용성과 향상된 일반화 성능을 보여줍니다.
- 자동으로 레이블이 지정된 대량의 데이터를 통합함으로써, 제로샷 일반화 능력을 더욱 강화합니다.
- GLEE는 대규모 언어 모델에 통합될 수 있으며, 다중 모달 작업을 위한 보편적인 객체 수준 정보를 제공하는 기반 모델로서 기능할 수 있습니다.
- 저희 방법의 다재다능함과 보편성이 AGI 시스템용 효율적인 시각 기반 모델 개발의 중요한 발전이 될 것임을 희망합니다.
- 모델 및 코드는 https://glee-vision.github.io 에서 공개될 예정입니다.

### [ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks](https://arxiv.org/abs/2312.08583)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/aPs6t7Oxu76S5D1CN7HmE.png)

Vote: 3

Authors: Xiaoxia Wu, Xiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Arash Bakhtiari, Michael Wyatt, Michael Wyatt, Yuxiong He, Olatunji Ruwase, Olatunji Ruwase, Leon Song, Leon Song, Zhewei Yao, Zhewei Yao

- 이 연구에서는 GPTQ 같은 4비트 양자화 방법을 대규모 언어 모델(LLMs)에 적용하면서, 주로 제로샷(Zero-Shot) 작업에서 GPTQ의 과적합과 개선의 한계를 지적한다.
- 선행 연구가 제로샷 작업만을 측정에 집중한 반면, 본 연구는 코드 생성 및 추상적 요약과 같이 더 다양한 생성 범주로 작업 범위를 확장하며, INT4 양자화가 현저하게 성능이 떨어질 수 있음을 발견했다.
- 높은 정밀도 포맷인 FP6으로의 전환은 이전에 열악한 성능 때문에 복잡한 통합과 시스템 가속화 전략이 부재한 현재 AI 하드웨어에서는 특히 도전적이었다.
- 실험 결과, 단순화된 양자화 방식임에도 FP6가 다양한 알고리즘과 작업에서 강건한 성능을 보여, 정확성과 다양성에서의 우수성을 입증했다.
- 특히, FP6 양자화를 적용한 \codestar-15B 모델은 코드 생성에서 FP16 버전과 비슷한 성능을 보였으며, 406M과 같은 작은 모델에서는 요약 작업에서 그 기준치에 가깝게 도달했다.
- 이는 INT4로는 달성할 수 없는 결과다.
- 다양한 AI 하드웨어에 잘 맞도록 하고 최상의 시스템 성능을 달성하기 위해, 본 논문에서는 최신 INT4 미세 양자화와 유사한 지연 시간을 가진 FP6을 위한 새로운 4+2 설계를 제안한다. 
- 이 설계를 통해, FP6는 현재 대규모 언어 모델에 사용되는 4비트 양자화 방법에 대한 유망한 솔루션이 될 수 있다.

### [Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent](https://arxiv.org/abs/2312.08926)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/PeV3XPpjoA0UMsydWaNm2.png)

Vote: 3

Authors: Haoran Liao, Haoran Liao, Qinyi Du, Shaohua Hu, Shaohua Hu, Hao He, Yanyan Xu, Yanyan Xu, Jidong Tian, Jidong Tian, Yaohui Jin, Yaohui Jin

- 복잡한 수학 문제를 해결하기 위해 대규모 언어 모델(LLMs)이 직면하는 도전 과제들을 다루고자, 이 논문에서는 수학적 추론 과정을 세분화하고 모델링하여 LLMs를 강화하는 에이전트의 잠재력을 탐구합니다.
- 제안된 Planner-Reasoner-Executor-Reflector (PRER)라는 제로 샷 프레임워크는 수학 문제 해결을 위한 공식적인 설명을 제공하고, 수학적 추론 과정을 확장합니다.
- 두 가지 유형의 MathAgent를 구현하는데, MathAgent-M은 LLM에 맞는 행동을 적응시키고, MathAgent-H는 인류와 일치하는 방식으로 설계됩니다.
- miniF2F와 MATH 데이터셋에서 PRER 및 제안된 MathAgents가 효과적임을 실험을 통해 입증하였으며, MiniF2F에서 12.3% (53.9%에서 66.2%로), MATH에서 9.2% (49.8%에서 59.0%로), 그리고 MATH 레벨-5 문제에서 GPT-4 대비 13.2% (23.2%에서 35.4%로)의 성능 향상을 달성했습니다.
- 추가적인 분석 결과는 LLMs가 에이전트로서의 행동을 활용하는 데 있어서 더 심도 깊은 관점을 제공합니다.

### [Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate Reward Hacking](https://arxiv.org/abs/2312.09244)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/DJhfjSg3pxntj5lF10QBc.png)

Vote: 2

Authors: Jacob Eisenstein, Chirag Nagpal, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Ahmad Beirami, Alex D'Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, Peter Shaw, Jonathan Berant, Jonathan Berant

- 보상 모델은 언어 모델 애플리케이션을 인간의 선호도에 맞추는 데 중요한 역할을 하지만, 이러한 설정은 언어 모델이 높은 추정 보상을 달성하기 위해 보상 모델의 오류를 이용하려는 현상, 즉 '보상 해킹'을 유도할 수 있습니다.
- 다양한 보상 모델로 구성된 앙상블을 훈련하는 것은 더 견고한 보상 추정치를 얻기 위한 자연스러운 완화 방법이며, 우리는 이러한 보상 앙상블의 적용을 학습 시간(강화 학습을 통해)과 추론 시간(리랭킹을 통해) 에서 조사했습니다.
- 보상 모델이 명세화되지 않았다는 것을 보여주며, 내부 분포에서 비슷한 성능을 보이는 보상 모델들은 분포 이동으로 인해 매우 다른 보상을 산출할 수 있고, 이는 조정에 영향을 미칩니다.
- 명세화되지 않음은 하나의 보상 모델로 정렬할 때 다른 보상 모델이 측정한 보상이 향상되지 않는 과잉 최적화로 이어집니다.
- 과잉 최적화는 보상 앙상블의 사용으로 완화될 수 있으며, 사전 트레이닝된 씨앗이 다른 앙상블이 미세 조정된 씨앗만 다른 앙상블보다 일반화 성능이 더 좋은 것으로 나타났으며, 이 두 경우 모두 개별 보상 모델보다 우수합니다.
- 그러나, 심지어 사전 트레이닝된 보상 앙상블조차도 보상 해킹을 완전히 제거하지는 못하며, 앙상블에 있는 모든 보상 모델이 유사한 오류 패턴을 보이기 때문에 앙상블에 의해 완화되지 않는 여러 가지 질적인 보상 해킹 현상을 보여줍니다.

### [Holodeck: Language Guided Generation of 3D Embodied AI Environments](https://arxiv.org/abs/2312.09067)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6n0IHNnI4V-a28SrsmR4j.png)

Vote: 2

Authors: Yue Yang, Fan-Yun Sun, Luca Weihs, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Chris Callison-Burch, Mark Yatskar, Mark Yatskar, Aniruddha Kembhavi, Christopher Clark

- 홀로데크(Holodeck)는 전문지식이나 대규모의 수작업 없이도 사용자가 제공한 질문에 따라 자동으로 3D 환경을 생성하는 시스템입니다.
- 다양한 장면(아케이드, 스파, 박물관 등)을 생성하고 스타일에 맞춰 설계를 조정할 수 있으며, "고양이를 가진 연구자의 아파트"나 "스타워즈 팬인 교수의 사무실"과 같은 복잡한 질의의 의미도 파악할 수 있습니다.
- 홀로데크는 장면이 어떻게 보일지에 대한 상식적인 지식을 위해 대형 언어 모델(GPT-4)을 활용하고, 다양한 객체로 장면을 채우기 위해 Objaverse의 대량의 3D 자산을 사용합니다.
- 객체를 올바르게 배치하는 문제를 해결하기 위해, GPT-4에게 객체 간의 공간적 관계 제약 조건을 생성하도록 요청한 후, 이러한 제약 조건을 만족시키는 레이아웃을 최적화합니다.
- 대규모 인간 평가에서, 평가자들은 주거 장면에서 수동으로 디자인된 절차적 기준보다 홀로데크를 선호하는 것으로 나타났으며, 홀로데크는 다양한 유형의 고품질 장면을 생성할 수 있습니다.
- 또한, 홀로데크는 음악실이나 어린이집과 같은 새로운 장면에서 인간이 만든 데이터 없이 탐색할 수 있는 AI 에이전트를 훈련시키는 흥미로운 응용 분야를 보여주며, 이는 일반 목적의 실체화된 에이전트를 개발하는 데 중요한 진전입니다.

### [SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds](https://arxiv.org/abs/2312.09246)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wpbmjk3yppJPPNnB4NdnW.png)

Vote: 1

Authors: Minghao Chen, Minghao Chen, Junyu Xie, Iro Laina, Andrea Vedaldi

- 이 논문은 'Shap-Editor'라는 새로운 피드포워드 3D 편집 프레임워크를 제안합니다.
- 기존의 3D 객체 편집 연구는 2D 이미지 편집 네트워크를 사용하여 개별 객체들을 편집하는 데 중점을 두었습니다.
- 이 과정은 2D 네트워크에서 3D 자산으로 지식을 전달하는 증류(distillation) 과정을 필요로 하며, 하나의 자산을 만족스럽게 편집하는 데 적어도 몇십 분이 소요되어 비실용적입니다.
- 반면, 본 연구는 테스트 시간 최적화를 사용하지 않고 3D 편집을 직접 수행할 수 있는지 여부를 탐구합니다.
- 특히, 적절한 잠재 공간에서 3D 객체를 인코딩함으로써 편집을 크게 단순화할 수 있다는 가설을 세웠습니다.
- 이 가설을 검증하기 위해, Shap-E의 잠재 공간을 바탕으로 하고, 이 공간에서 직접적이고 효율적인 3D 편집이 가능함을 보여줍니다.
- 편집당 약 1초만 필요한 피드포워드 편집 네트워크를 구축하여 효율성을 입증했습니다.
- 실험을 통해 Shap-Editor가 다양한 프롬프트를 가진 인-디스트리뷰션 및 아웃-오브-디스트리뷰션 3D 자산에 대해 잘 일반화되며, 각 편집 인스턴스마다 테스트 시간 최적화를 수행하는 방법과 비교할 만한 성능을 보여준다는 것을 보여줍니다.

### [UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation](https://arxiv.org/abs/2312.08754)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/zF51N_Bbp19dsA1wLU_G6.png)

Vote: 1

Authors: Zexiang Liu, Yangguang Li, Yangguang Li, Youtian Lin, Youtian Lin, Xin Yu, Sida Peng, Sida Peng, Yan-Pei Cao, Yan-Pei Cao, Xiaojuan Qi, Xiaoshui Huang, Xiaoshui Huang, Ding Liang, Ding Liang, Wanli Ouyang

- 글씨로부터 3D 객체를 생성하는 현대 기술은 상상력이 풍부한 정교한 기하학적 구조와 섬세한 텍스처를 가진 3D 모델을 만드는 데 크게 진전되었습니다.
- 그러나 기존 모델들은 RGB 데이터를 사용함으로써 자연스러운 조명과 그림자 효과가 없어 리얼리즘을 저하시키고, 정확한 재조명이 요구되는 응용 프로그램에서의 사용성을 제한합니다.
- 이러한 한계를 해결하기 위해, 우리는 통합된 확산 모델을 사용하여 텍스트로부터 3D 생성 프레임워크인 UniDream을 제시합니다.
- 이 접근 방식은 (1) 알베도-노말 정렬된 멀티뷰 확산 및 재구성 모델을 얻기 위한 이중 페이즈 트레이닝 과정, (2) 훈련된 재구성 및 확산 모델을 사용하여 기하학 및 알베도-텍스처 생성을 위한 점진적 생성 절차, (3) 안정적인 알베도 유지 기반의 PBR 생성을 위한 SDS의 혁신적 적용과 같은 세 가지 주요 구성 요소를 포함합니다.
- 광범위한 평가를 통해 UniDream이 명확한 알베도 텍스처, 부드러운 표면, 향상된 리얼리즘 및 상위의 재조명 능력으로 3D 객체를 생성하는 데 있어 기존 방법들을 능가함을 보여줍니다.

### [TigerBot: An Open Multilingual Multitask LLM](https://arxiv.org/abs/2312.08688)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vco2qsEVTcJdbDtj7Gp79.png)

Vote: 1

Authors: Ye Chen, Ye Chen, Wei Cai, Wei Cai, Liangmin Wu, Xiaowei Li, Zhanxuan Xin, Zhanxuan Xin, Cong Fu, Cong Fu

- TigerBot 대규모 언어 모델(LLM) 시리즈를 출시하였으며, 이는 7억, 13억, 70억, 그리고 1800억 개의 파라미터를 가진 기본 및 대화 모델로 구성되어 있다.
- 이 모델들은 Llama-2와 BLOOM을 기반으로 개발되었으며, 데이터, 학습 알고리즘, 인프라 및 응용 프로그램 도구의 경계를 더욱 확장하였다.
- TigerBot 모델군은 특히 영어에서 6%, 중국어에서 20%의 성능 향상을 달성하여 기존의 최신 오픈소스 모델들보다 우수한 성능을 보였다.
- 아울러 TigerBot은 주요한 학술 및 산업 벤치마크와 리더보드에서 선두적인 성과를 달성했다.
- 이 논문은 급속도로 발전하는 LLM 오픈소스 커뮤니티에서의 한 시점을 대표하며, 저자들은 모델 공개와 그 뒤에 있는 접근 방식을 공유함으로써 공동체에 기여하고자 한다.
- 이 연구는 민주화된 방식으로 최신 LLM을 구축하고 실제 세계 응용 프로그램에서 LLM을 유용하게 활용하는 데 중점을 둔다.

