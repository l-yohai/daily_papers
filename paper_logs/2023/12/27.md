## Daily Papers (2023-12-27)

### [SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling](https://arxiv.org/abs/2312.15166)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/v4N9U0WSFwJmVG51yg0SE.png)

Vote: 26

Authors: Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, Sunghun Kim

- 연구팀은 기존 대규모 언어 모델(LLMs)을 효율적이고 효과적으로 스케일 업하기 위한 새로운 기법인 깊이 업스케일링(DUS)을 소개합니다.
- DUS는 복잡한 학습이나 추론 과정 변경 없이 간단한 방법으로 사용할 수 있으며, 이를 통해 107억 개의 매개변수를 가진 SOLAR 10.7B를 구축했습니다.
- SOLAR 10.7B는 다양한 자연어 처리(NLP) 작업에서 뛰어난 성능을 보이며, Llama 2 및 Mistral 7B와 같은 기존의 오픈 소스 사전 훈련된 LLM보다 우수한 성능을 나타냈습니다.
- 지시에 따른 수행 능력을 강화하기 위해 파인튜닝된 SOLAR 10.7B-Instruct 버전은 Mixtral-8x7B보다 우수한 성능을 보였습니다.
- SOLAR 10.7B는 아파치 2.0 라이선스 하에 공개되어 대규모 언어 모델 분야에서의 광범위한 접근성과 적용을 촉진합니다.

### [Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4](https://arxiv.org/abs/2312.16171)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/emIM3ddZolV3WYWxdej7d.png)

Vote: 11

Authors: Sondos Mahmoud Bsharat, Aidar Myrzakhan, Zhiqiang Shen

- 이 논문은 대규모 언어 모델을 질문하고 프롬프트하는 과정을 간소화하기 위해 설계된 26개의 지침 원칙을 소개합니다.
- 목표는 다양한 규모의 대규모 언어 모델들에게 질문을 구성하고, 그들의 능력을 평가하며, 다양한 프롬프트에 대한 모델들의 행동을 사용자가 이해하기 쉽도록 개념을 단순화하는 것입니다.
- 실험은 LLaMA-1/2 (7B, 13B, 70B) 및 GPT-3.5/4 모델을 대상으로 실시하여 제안된 지침 원칙들의 효과를 검증합니다.
- 이 연구로 인해 대규모 언어 모델의 프롬프팅 작업에 종사하는 연구자들에게 보다 나은 가이드를 제공할 수 있기를 기대합니다.
- 프로젝트 페이지는 https://github.com/VILA-Lab/ATLAS에서 확인할 수 있습니다.

### [Make-A-Character: High Quality Text-to-3D Character Generation within Minutes](https://arxiv.org/abs/2312.15430)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/S3AZgppBywJMNMqLKZEyl.png)

Vote: 10

Authors: Jianqiang Ren, Chao He, Lin Liu, Jiahao Chen, Yutong Wang, Yafei Song, Jianfang Li, Tangli Xue, Siqi Hu, Tao Chen, Kunkun Zheng, Jianjing Xiang, Liefeng Bo

- 인공지능 에이전트와 메타버스의 등장으로 맞춤형 및 표현력이 풍부한 3D 캐릭터에 대한 수요가 증가하고 있지만, 전통적인 컴퓨터 그래픽 도구를 사용한 3D 캐릭터 제작은 복잡하고 시간이 많이 소요되는 작업입니다.
- 이에 대한 도전을 해결하기 위해, "Make-A-Character (Mach)"라는 사용자 친화적인 프레임워크를 제안하여 텍스트 설명에서 생생한 3D 아바타를 생성합니다.
- 이 프레임워크는 대규모 언어 및 비전 모델을 활용하여 텍스트 의도를 이해하고 중간 이미지 생성을 돕고, 이어서 인간 지향적인 시각 인식 및 3D 생성 모듈을 통해 프로세스를 진행합니다.
- 사용자는 직관적인 방법으로 2분 내에 제어 가능하고, 현실적이며, 완전히 현실화된 3D 캐릭터를 자신의 기대에 맞게 제작할 수 있으며, 동적 표현력을 위해 기존 CG 파이프라인과 쉽게 통합할 수도 있습니다.
- 프로젝트에 대한 자세한 정보는 https://human3daigc.github.io/MACH/ 프로젝트 페이지에서 확인할 수 있습니다.

### [UniRef++: Segment Every Reference Object in Spatial and Temporal Spaces](https://arxiv.org/abs/2312.15715)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/5kf3VRahLthzzRSyyYWu9.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/5kf3VRahLthzzRSyyYWu9.mp4" muted="false"></video></div>

Vote: 9

Authors: Jiannan Wu, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan, Ping Luo

- 참조 기반 객체 분할 작업(참조 이미지 분할, 적은 수의 이미지 분할, 참조 비디오 객체 분할 및 비디오 객체 분할)은 언어 또는 주석이 달린 마스크와 같은 참조를 활용하여 특정 객체를 분할하는 것을 목표로 합니다.
- 기존 방법들은 각각의 과제에 맞춰 특수하게 설계되고 개발되어 있어, 다중 과제 수행 능력의 활성화가 제한되는 상황이 있었습니다.
- 이 연구에서는 UniRef++를 제안하여 단일 아키텍처로 네 가지 참조 기반 객체 분할 작업을 통합합니다.
- UniFusion 모듈을 도입해 각 과제에 대한 활용 지정 참조에 맞춰 다방향 융합을 수행하는 것이 이 접근법의 핵심입니다.
- 이어서, 인스턴스 수준 분할을 달성하기 위해 통합된 트랜스포머 아키텍처가 채택되었습니다.
- 통합 디자인을 통해 UniRef++는 다양한 벤치마크에서 공동 훈련을 수행할 수 있으며, 해당 참조를 지정함으로써 다중 과제를 유연하게 완료할 수 있습니다.
- 여러 벤치마크에서 통합 모델을 평가한 결과, UniRef++는 RIS와 RVOS에서 최첨단 성능을 달성하고 FSS와 VOS에서 공유 네트워크로 경쟁력 있는 성과를 보여줍니다.
- 또한, 제안하는 UniFusion 모듈은 현재 고급 기초 모델 SAM에 쉽게 통합되고 파라미터 효율적인 파인튜닝을 통해 만족스러운 결과를 얻을 수 있음을 보여줍니다.
- 관련 코드와 모델은 https://github.com/FoundationVision/UniRef 에서 확인할 수 있습니다.

### [A Recipe for Scaling up Text-to-Video Generation with Text-free Videos](https://arxiv.org/abs/2312.15770)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/idx1BUNnmhsIGj7ODn92N.gif)

Vote: 8

Authors: Xiang Wang, Shiwei Zhang, Hangjie Yuan, Zhiwu Qing, Biao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, Nong Sang

- 본 연구에서는 텍스트 없는 동영상을 바로 학습할 수 있는 신규 텍스트-비디오 생성 프레임워크인 TF-T2V를 제안합니다.
- 비디오 캡셔닝의 높은 비용을 고려할 때, YouTube와 같은 플랫폼에서 레이블이 없는 클립을 수집하는 것이 훨씬 용이할 수 있음을 인식하였습니다.
- TF-T2V는 텍스트 디코딩 과정과 시간 모델링 과정을 분리하는 것을 근거로 하여, 내용 분기와 운동 분기를 공유 가중치로 함께 최적화합니다.
- 학습 세트의 규모를 두 배로 늘릴 때(FID는 9.67에서 8.19로, FVD는 484에서 441로 개선)와 텍스트 레이블을 다시 도입한 후의 성능 향상(FID는 8.19에서 7.64로, FVD는 441에서 366으로 개선)을 연구함으로써 접근 방식의 확장성을 입증했습니다.
- 이 모델이 토착 텍스트-비디오 생성과 구성 비디오 합성 패러다임 모두에서의 효과와 일반화 가능성을 검증합니다.
- 연구 결과를 바탕으로 코드와 모델은 https://tf-t2v.github.io/ 를 통해 공개될 예정입니다.

### [Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases](https://arxiv.org/abs/2312.15011)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3PkWmoon-v41VaD6GtDVq.png)

Vote: 7

Authors: Zhangyang Qi, Ye Fang, Mengchen Zhang, Zeyi Sun, Tong Wu, Ziwei Liu, Dahua Lin, Jiaqi Wang, Hengshuang Zhao

- 본 논문은 구글의 젬니니(Gemini)와 오픈AI의 GPT-4V(ision)라는 두 선도적인 다중 모달 대형 언어 모델(MLLM)의 심층적인 비교 연구를 제시합니다.
- 두 모델의 성능을 시각-언어 능력, 인간과의 상호 작용, 시간적 이해력, 지능 및 감정 지수 등 주요 차원에서 평가하였습니다.
- 이 연구의 핵심은 각 모델의 고유한 시각적 이해 능력을 분석하는 데 초점을 맞추고 있으며, 다양한 산업 적용 시나리오에서 그 성능을 평가하기 위한 일련의 구조화된 실험을 통해 수행되었습니다.
- 간단하고 명확한 반응에 있어 GPT-4V가 두각을 나타내는 반면, 젬니니는 관련 이미지와 링크를 동반한 상세하고 넓은 범위의 대답으로 뛰어납니다.
- 두 모델의 장점과 틈새를 조명하는 동시에 다중모드 기반 모델의 발전된 풍경을 강조하고, 향후 진전을 위한 길을 제시합니다.
- 비교 분석 후에는 두 모델을 결합하여 더 나은 결과를 얻기 위한 시도가 이루어졌습니다.
- GPT-4V와 젬니니 뒤에 있는 팀들의 기여에 대한 깊은 감사를 표하며, 'Dawn'이라는 양 등의 종합적인 정성적 분석이 본 분석에 기초를 제공하였음을 밝힙니다.

### [LangSplat: 3D Language Gaussian Splatting](https://arxiv.org/abs/2312.16084)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/FT6T3on3Rre1uIMW9gphI.qt)

Vote: 6

Authors: Minghan Qin, Wanhua Li, Wanhua Li, Jiawei Zhou, Haoqian Wang, Hanspeter Pfister

- 본 논문은 인간이 자연어를 사용하여 3D 장면과 상호작용하는 것을 모델링하기 위해 LangSplat이라는 새로운 방법을 제시하고 있습니다.
- LangSplat은 CLIP 언어 임베딩을 활용하여 공간에 대한 개방형 어휘 질의가 가능한 3D 언어 필드를 구축합니다.
- 기존의 NeRF 기반 방법과 달리, 3D 가우시안을 사용하여 언어 특징을 표현함으로써, NeRF의 고비용 렌더링 과정을 우회합니다.
- LangSplat은 CLIP 임베딩을 직접 학습하는 대신, 장면별 언어 오토인코더를 훈련시킨 후 장면 특정 잠재 공간에서 언어 특징을 학습함으로써 기억 용량의 부담을 줄입니다.
- 3D 언어 필드에서 객체 간의 명확한 경계를 구분하지 못했던 기존 방법의 한계를 SAM을 사용한 계층적 의미 학습을 통해 개선합니다.
- 개방형 어휘 기반의 3D 객체 위치 파악 및 의미 분할에 대한 광범위한 실험을 통해, LangSplat이 기존의 최고 방법인 LERF를 크게 능가하는 성능을 보임을 입증했습니다.
- 특히, LangSplat은 LERF에 비해 1440x1080 해상도에서 매우 높은 속도의 성능 향상을 달성하였습니다.
- 연구 결과의 시각적 결과물은 https://langsplat.github.io에서 확인할 수 있습니다.

### [Audiobox: Unified Audio Generation with Natural Language Prompts](https://arxiv.org/abs/2312.15821)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/U5DYEUYMsV-wkxjmxWGzm.png)

Vote: 6

Authors: Apoorv Vyas, Bowen Shi, Matthew Le, Andros Tjandra, Yi-Chiao Wu, Baishan Guo, Jiemin Zhang, Xinyue Zhang, Robert Adkins, William Ngan, Jeff Wang, Ivan Cruz, Bapi Akula, Akinniyi Akinyemi, Brian Ellis, Rashel Moritz, Yael Yungster, Alice Rakotoarison, Liang Tan, Chris Summers, Carleigh Wood, Joshua Lane, +

- 오디오는 우리 생활의 중요한 부분이지만, 오디오를 만들기 위해서는 전문 지식이 요구되며 시간도 많이 소모됩니다.
- 최근 연구 커뮤니티는 강력한 생성 모델을 채택하고 데이터를 확장함으로써 단일 모달리티(음성, 소리, 음악)에 대한 대규모 오디오 생성 모델의 성능을 크게 향상시켰습니다.
- 그러나 이러한 모델들은 여러 면에서 조절 가능성이 부족하며, 음성 생성 모델들은 텍스트 설명에 기반한 새로운 스타일을 합성하는 데 한계가 있고, 특정 분야나 야외 환경 등의 커버리지에 제한이 있습니다.
- 소리 생성 모델들은 "사람이 말하는 것" 같은 대략적인 설명을 기반으로 조배된 제어만 제공하고 사람의 목소리를 흉내 내는 정도의 오디오만을 생성할 수 있습니다.
- 이 논문은 다양한 오디오 모달리티를 생성할 수 있는 통합 모델인 Audiobox를 제시합니다.
- 우리는 설명 기반 및 예제 기반 프롬프팅을 설계하여 조절 가능성을 향상시키고, 음성과 소리 생성 패러다임을 통합했습니다.
- 음성을 생성할 때 대본, 보컬, 그리고 다른 오디오 스타일들을 독립적으로 제어할 수 있도록 허용합니다.
- 제한된 레이블로 모델의 일반화를 개선하기 위해, 레이블이 없는 대량의 오디오 데이터에 사전 훈련을 위한 자기 감독 인페일링 목표를 적용합니다.
- Audiobox는 음성 및 소리 생성에 대한 새로운 벤치마크를 설정했으며(리브리스피치에서 제로샷 TTS에 대해 0.745 유사성; 오디오캡스에서 텍스트-투-사운드에 대해 0.77 FAD), 새로운 보컬 및 음향 스타일로 오디오를 생성하는 새로운 방법을 제시합니다.
- 우리는 흐름 일치를 위한 기본적인 ODE 솔버에 비해 생성 속도를 25배 이상 향상시키는 Bespoke Solvers를 통합했으며, 이는 여러 작업에 대한 성능 손실 없이 제공됩니다.
- 우리의 데모는 https://audiobox.metademolab.com/에서 확인할 수 있습니다.

### [HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D](https://arxiv.org/abs/2312.15980)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/k4lZHvQL5g9tBWFkXsk1M.png)

Vote: 5

Authors: Sangmin Woo, Byeongjun Park, Byeongjun Park, Hyojun Go, Jin-Young Kim, Changick Kim

- 최근 단일 이미지로부터 3D 생성 분야의 발전은 다중 시점 일관성의 중요성을 강조하며 인터넷 규모의 이미지에 대해 사전 훈련된 대규모 확산 모델의 3D 사전 지식을 활용합니다.
- 하지만 2D 이미지를 3D 콘텐츠로 변환하는 과정에서 발생할 수 있는 여러 가지 잠재적 형태들의 다양성은 연구 분야에서 상대적으로 덜 탐구된 상태입니다.
- 본 연구는 일관성과 다양성을 동시에 다루는 것을 목표로, 이 두 가지 측면 사이의 균형을 맞추는 것은 내재된 상충 문제로 인해 상당한 도전이 됩니다.
- HarmonyView는 단일 이미지 3D 생성에서 일관성과 다양성이라는 두 복잡한 측면을 분해하는 간단하지만 효과적인 확산 샘플링 기술을 소개합니다.
- 이 접근법은 샘플링 과정 내에서 두 중요한 차원을 보다 섬세하게 탐색할 수 있는 길을 열어줍니다.
- 또한, 생성된 시점들의 다양성을 종합적으로 평가하기 위한 새로운 평가 지표로 CLIP 이미지 및 텍스트 인코더를 기반으로 하며 이는 인간 평가자의 판단과 밀접하게 일치합니다.
- 실험에서 HarmonyView는 일관성과 다양성 모두에서 조화로운 균형을 달성하는데, 이러한 접근은 양 측면에서 모두 윈-윈 시나리오를 실현합니다.

### [One-dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications](https://arxiv.org/abs/2312.16145)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WN-s0CSMKO4QMgubnQiPD.png)

Vote: 5

Authors: Mengyao Lyu, Yuhong Yang, Haiwen Hong, Hui Chen, Xuan Jin, Yuan He, Hui Xue, Jungong Han, Guiguang Ding

- 이 연구는 텍스트에서 이미지로 생성하는 상업적 및 오픈 소스 확산 모델(DMs)의 사용에 따른 위험을 줄이기 위한 개념 제거 방법에 초점을 맞추고 있습니다.
- 기존의 개념 제거 방법은 모두 전체 파라미터 또는 사양 기반의 미세 조정에 의존하며, 대상 제거 중 발생하는 파라미터 이동으로 인해 모든 생성물에 대한 변형과 잠재적 변형이 발생할 수 있음을 지적합니다.
- 특히 여러 개념을 제거할 때 다른 개념에 영향을 끼칠 수 있는 현상이 더욱 두드러집니다.
- 이전의 모델 특정 제거 방식은 개념의 유연한 조합과 다른 모델로의 교육 없는 전환을 방해하여 배포 시나리오가 증가함에 따라 비용이 선형적으로 증가합니다.
- 비침습적이고 정확하며 맞춤형이며 전달 가능한 제거를 달성하기 위해 이 논문은 다양한 제거 애플리케이션에 걸쳐 대부분의 DM에서 한 번에 여러 개념을 제거하는데 1차원 어댑터를 기반으로 한 제거 프레임워크를 제안합니다.
- 새로운 라텐트 앵커링 미세 조정 전략을 통한 대상 제거 학습은 SemiPermeable Membrane(SPM)을 DM에 주입 함으로써 변형 및 침식 현상을 효과적으로 완화합니다.
- 한번 얻어진 SPM은 다른 DM에 구체적인 재조정 없이 유연하게 결합되고 플러그 앤 플레이 할 수 있어 다양한 시나리오에 신속하고 효율적으로 적응할 수 있습니다.
- 생성 중에는 Facilitated Transport 메커니즘이 다양한 입력 텍스트에 따라 각 SPM의 투과성을 동적으로 조정함으로써 다른 개념에 대한 영향을 최소화합니다.
- 약 40개의 개념, 7개의 DM 및 4개의 제거 애플리케이션에 걸쳐 양적 및 질적 결과가 SPM의 우수한 제거 능력을 입증합니다.
- 연구팀의 코드 및 사전 조정된 SPM은 프로젝트 페이지에서 공개 될 예정입니다. https://lyumengyao.github.io/projects/spm

### [Human101: Training 100+FPS Human Gaussians in 100s from 1 View](https://arxiv.org/abs/2312.15258)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vfIzbgCJ8gMGTNq9QQAex.png)

Vote: 3

Authors: Mingwei Li, Jiachen Tao, Zongxin Yang, Yi Yang

- 단일 시점 비디오에서 인간의 몸을 재구성하는 것은 가상 현실 분야에서 중요한 역할을 합니다.
- Human101은 하나의 시점에서 고화질 동적 3D 인간 재구성을 100초 만에 3D 가우스를 훈련하여 100+ FPS로 렌더링하는 새로운 프레임워크를 소개합니다.
- 이 방법은 3D 인간을 명시적이고 효율적으로 표현하는 3D 가우스 스플래팅의 장점을 활용합니다.
- Human101은 이전 NeRF 기반 파이프라인과 달리, Human-centric Forward Gaussian Animation 방법을 적용하여 3D 가우스의 매개변수를 변형시키고 렌더링 속도를 향상시킵니다 (예: 1024 해상도의 이미지를 60+ FPS, 512 해상도의 이미지를 100+ FPS로 렌더링).
- 실험 결과에 따르면, 우리의 접근 방식은 현재 방법보다 최대 10배 빠른 프레임 속도를 기록하고, 비교할 수 있거나 우수한 렌더링 품질을 제공합니다.
- 코드와 데모는 https://github.com/longxiang-ai/Human101에서 공개 될 예정입니다.

### [Supervised Knowledge Makes Large Language Models Better In-context Learners](https://arxiv.org/abs/2312.15918)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/o4HaLRmFSwkL6sVVP3wUp.png)

Vote: 3

Authors: Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, Yue Zhang

- 대규모 언어 모델(Large Language Models, LLMs)은 프롬프트 엔지니어링을 통해 컨텍스트 내 학습 능력을 발휘합니다.
- 최근 대규모 생성 모델의 발전은 실제 언어 응용 프로그램에서의 사용을 확대하고 있습니다.
- LLM의 자연어 이해 및 질문 응답 분야에서 일반화와 사실 정확성을 향상시키는 과제는 충분히 탐구되지 않았습니다.
- 기존 연구는 사용자의 구체적인 지시에 따르고 품질 기대치를 충족시키는 모델 향상에 초점을 맞췄지만, 추론 단계에서 LLM의 컨텍스트 내 학습을 개선하기 위해 특정 과제에 맞춰 미세 조정된 언어 모델(SLMs) 사용은 거의 다루어지지 않았습니다.
- 이 연구의 주요 기여는 신뢰성 있는 LLM을 만들기 위한 간단하면서도 효과적인 틀을 제시하는 것입니다: 1) 분포 외 데이터의 일반화, 2) 판별 모델로부터 LLM이 이익을 얻는 방법을 밝힘, 3) 생성 과업에서 환각의 최소화.
- 제안된 플러그인 방법을 사용한 Llama 2 및 ChatGPT의 향상된 버전은 일반화와 사실 정확성 측면에서 원래 버전을 능가합니다.
- 16개의 선별된 데이터셋, 프롬프트, 모델 체크포인트, 9가지 과제에 대한 LLM 출력을 포함하는 포괄적인 자원 스위트를 제공합니다.
- 이 실증적 분석은 판별 모델을 LLM에 통합하는 이점을 밝히고, 더 신뢰할 수 있는 LLMs를 육성하는 우리 방법론의 잠재력을 강조합니다.

