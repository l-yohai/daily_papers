## Daily Papers (2023-12-08)

### [Alpha-CLIP: A CLIP Model Focusing on Wherever You Want](https://arxiv.org/abs/2312.03818)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-mUdJkMcJXi7in9HnHrY-.png)

Vote: 15

Authors: Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang

- 대조적 언어-이미지 사전학습(CLIP)은 이미지 내 다양한 과제에서 중요한 정보를 추출하는 필수적인 역할을 합니다.
- CLIP은 텍스트와 시각적 모달리티를 맞추어 이미지 전체와 모든 세부 사항을 이해하지만, 특정 과제와 관련 없는 부분까지 포함합니다.
- 이미지의 특정 영역에 더 세밀하게 집중하고 제어하기 위해, 우리는 Alpha-CLIP을 도입했으며 이는 조력 지점으로 주목 영역을 선택하는 보조 알파 채널을 갖추고 있습니다.
- Alpha-CLIP은 수백만 개의 RGBA 영역-텍스트 쌍으로 미세 조정되어, CLIP의 시각적 인식 능력을 유지하면서 이미지 내용에 대한 정밀한 제어를 가능하게 합니다.
- 이 모델은 오픈월드 인식, 다중모달 대규모 언어 모델, 조건부 2D / 3D 생성 등 다양한 작업에서 효과를 입증하였습니다.
- Alpha-CLIP은 이미지 관련 작업에 다재다능한 도구로서의 강력한 잠재력을 가지고 있습니다.

### [AnimateZero: Video Diffusion Models are Zero-Shot Image Animators](https://arxiv.org/abs/2312.03793)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wUclbarQ5flurbYkde1vl.png)

Vote: 9

Authors: Jiwen Yu, Xiaodong Cun, Chenyang Qi, Yong Zhang, Xintao Wang, Ying Shan, Jian Zhang

- 최근 텍스트-비디오(T2V) 확산 모델들이 비주얼 품질, 움직임, 시간적 일관성 면에서 크게 발전하였지만, 모든 속성(외형, 움직임 등)이 정확한 제어 없이 텍스트 설명만으로 생성되는 문제점이 있음.
- 이미지 애니메이션에서 영감을 받아 AnimateZero는 특정 외형과 해당 움직임을 분리하고, 기존 T2V 확산 모델, 즉 AnimateDiff를 공개하여 더욱 정밀한 외형 및 움직임 제어 능력을 제공함.
- 외형 제어를 위해서는 텍스트-이미지(T2I) 생성 과정에서 중간 레이턴트(latents)와 그 특징을 빌려, 생성된 첫 프레임이 주어진 생성 이미지와 동일하게 유지되도록 함.
- 시간적 제어를 위해 원래 T2V 모델의 글로벌 시간적 주의(global temporal attention)를 제안된 위치 수정 창 주의(positional-corrected window attention)로 대체하여 다른 프레임들이 첫 프레임과 잘 맞도록 확보함.
- 제안된 방법을 통해 AnimateZero는 추가 훈련 없이 생성 과정을 성공적으로 제어할 수 있으며, 주어진 이미지에 대해 제로샷 이미지 애니메이터로서 작동함.
- 대화형 비디오 생성과 실제 이미지 애니메이션을 포함하는 여러 새로운 응용 프로그램을 가능하게 함.
- 자세한 실험을 통해 제안된 방법이 T2V 및 관련 애플리케이션에서의 효과성을 입증함.

### [PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding](https://arxiv.org/abs/2312.04461)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/r1PzfCkV8dny0oReHy5yk.png)

Vote: 9

Authors: Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, Ying Shan

- 최근 텍스트 기반의 사실적인 인간 사진 합성 기술이 주목할 만한 발전을 이루었지만, 고효율성, ID 정확성 및 유연한 텍스트 제어를 동시에 충족시키는 개인화 생성 방법은 부족하다.
- 본 연구에서는 다수의 입력 ID 이미지를 ID 정보를 보존하기 위해 스택 ID 임베딩으로 인코딩하는 효율적인 개인화 텍스트-이미지 생성 방법인 PhotoMaker를 소개한다.
- 제안된 스택 ID 임베딩은 동일한 입력 ID의 특성을 종합적으로 포괄할 뿐만 아니라, 서로 다른 ID의 특성을 통합할 수 있는 통일된 ID 표현으로 작동한다.
- 이를 토대로 PhotoMaker는 테스트 시간에 미세 조정 기반 방법보다 우수한 ID 보존 능력을 보여주면서, 속도 향상, 고품질 생성 결과, 강력한 일반화 능력 및 광범위한 응용 분야를 제공한다.
- PhotoMaker의 학습을 위해, ID 중심 데이터 구축 파이프라인을 제안하여 훈련 데이터를 조립한다.
- PhotoMaker 프로젝트 페이지는 https://photo-maker.github.io/ 에서 확인할 수 있다.

### [Chain of Code: Reasoning with a Language Model-Augmented Code Emulator](https://arxiv.org/abs/2312.04474)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3nMQLI1L1Y5C-GGaxdwak.gif)

Vote: 9

Authors: Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, Brian Ichter

- 코드는 복잡한 프로그램을 구축하고 해석기와 결합할 때 정확한 계산을 수행하는 일반적인 구문 구조를 제공하며, 언어 모델은 논리 및 산술 작업뿐만 아니라 언어적 작업(특히 둘의 혼합인 경우)에 대한 사고 연쇄 추론을 향상시키기 위해 코드 작성을 활용할 수 있다는 가설이 제시됩니다.
- 예를 들어, 언어 모델에게 에세이에서 비꼼을 감지하는 횟수를 계산하는 코드를 작성하도록 요청할 때, 'detect_sarcasm(string)'의 실행 가능한 구현을 작성하는 데 어려움을 겪을 수 있지만, 이를 코드 작성 뿐만 아니라 해석을 "모방"하여 예상 출력을 생성함으로써 유효한 해결책을 제시할 수 있습니다.
- 이 연구에서는 언어 모델이 언어적 하위 작업을 컴파일러가 명시적으로 정의되지 않은 행동을 포착하고 언어 모델이 "LMulator"로 시뮬레이션 할 수 있도록 유연한 의사 코드로 포맷할 것을 권장하는 단순하면서도 놀랍도록 효과적인 연장 작업인 Chain of Code (CoT)를 제안합니다.
- 실험 결과 Chain of Code는 다양한 벤치마크에서 사고 연쇄 및 기타 베이스 라인을 능가하며, BIG-Bench 하드에서 사고 연쇄보다 12% 높은 84%를 달성합니다.
- CoT는 대형 및 소형 모델 모두에 잘 적용되며, 언어 모델이 '코드로 사고'함으로써 올바르게 답변할 수 있는 추론 질문의 범위를 넓힙니다.
- 프로젝트 웹페이지: https://chain-of-code.github.io/.

### [Beyond Surface: Probing LLaMA Across Scales and Layers](https://arxiv.org/abs/2312.04333)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ou-GmKXztRsdn5d3a2xtA.png)

Vote: 9

Authors: Nuo Chen, Ning Wu, Shining Liang, Ming Gong, Linjun Shou, Dongmei Zhang, Jia Li

- 이 연구논문은 자연어 처리 분야에서 주목받고 있는 오픈소스 기본 모델인 LLaMA의 대규모 언어 모델을 심층적으로 분석합니다.
- LLaMA의 생성적 출력을 평가하는 대신, 추론 및 계산과 같은 고차원 작업에서 본질적인 이해를 탐구하기 위해 객관식 과제를 설계하였습니다.
- 모델을 수평적으로 비교하여 다양한 크기를 평가하고, 수직적으로는 다른 레이어를 평가합니다.
- 설계된 과제를 기반으로 몇 가지 주요하고 드문 발견을 밝히는데, 수평적으로, 모델 크기 확대는 추가 지식이나 계산 능력을 자동적으로 부여할 수 없다는 것을 발견했습니다.
- 대신, 특히 수학 문제 해결에서 추론 능력을 향상시키고 환각을 줄일 수 있지만, 특정한 크기의 임계값을 넘어서야 합니다.
- 수직적 분석에서는 LLaMA의 하위 레이어들이 뚜렷한 산술적 및 사실적 지식을 부족하며 논리적 사고, 다국어 및 인식 능력을 보여줍니다.
- 상위 레이어들이 대부분의 계산력 및 현실 세계 지식을 포함하고 있다는 것이 밝혀졌습니다.

### [Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models](https://arxiv.org/abs/2312.04410)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WCWffMZE0NB1rcVR1qL9h.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WCWffMZE0NB1rcVR1qL9h.mp4" muted="false"></video></div>

Vote: 8

Authors: Jiayi Guo, Xingqian Xu, Yifan Pu, Zanlin Ni, Chaofei Wang, Manushree Vasu, Shiji Song, Gao Huang, Humphrey Shi

- 최근 텍스트에서 이미지로 변환하는(T2I) 생성을 위한 확산 모델이 고화질과 다양한 콘텐츠의 이미지를 만드는 데 큰 진전을 이루었습니다.
- 그러나 확산 모델 내에서의 매끄러운 잠재 공간의 중요성이 대부분 미개척 상태로, 이는 이미지 보간, 역변환, 편집 등의 후속 작업에 유용합니다.
- 본 연구에서는 확산 모델의 잠재 공간에서 미세한 변화로 인한 눈에 띄는 시각적 변동을 관찰함으로써 잠재 공간의 비매끄러움을 드러냅니다.
- 이 문제를 해결하기 위해, 고성능과 매끄러움을 동시에 갖춘 새로운 카테고리인 Smooth Diffusion을 제안합니다.
- 구체적으로, 임의의 입력 잠재 변수와 결과 이미지의 변동이 확산 훈련의 어떤 단계에서든 일정 비율을 유지하도록 Step-wise Variation Regularization을 도입했습니다.
- 또한, 확산 모델의 잠재 공간의 매끄러움을 효과적으로 평가하기 위해 interpolation standard deviation (ISTD) 지표를 개발하였습니다.
- 광범위한 정량적 및 정성적 실험을 통해 Smooth Diffusion이 T2I 생성뿐만 아니라 다양한 후속 작업에서도 더 바람직한 솔루션이라는 것을 입증합니다.
- Smooth Diffusion은 다양한 커뮤니티 모델과 함께 작동할 수 있는 plug-and-play Smooth-LoRA로 구현되었습니다.
- 관련 코드는 https://github.com/SHI-Labs/Smooth-Diffusion 에서 확인할 수 있습니다.

### [GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation](https://arxiv.org/abs/2312.04557)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pt7hvTcaYF_S-Vfjn2ZfV.png)

Vote: 8

Authors: Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, Juan-Manuel Perez-Rua

- 이 연구에서는 이미지 및 비디오 생성을 위한 변압기 기반 확산 모델을 탐구합니다.
- 변압기 아키텍처는 다양한 분야에서의 유연성과 확장성으로 주목받고 있으나 시각적 생성 분야에서는 주로 CNN 기반 U-Net 아키텍처가 우세합니다.
- 우리는 이격을 메워주기 위하여 변압기 기반 확산을 사용하는 생성 모델인 GenTron을 소개합니다.
- 클래스에서 텍스트 조건화까지 확장하기 위해 Diffusion Transformers (DiTs)의 조건화 메커니즘에 대해 철저한 실증적 탐구를 수행했습니다.
- GenTron은 약 900M에서 3B 이상의 매개변수로 확장되면서 시각적 품질이 중대하게 향상된 것을 관찰했습니다.
- 또한, 움직임 없는 안내를 통해 비디오 품질을 향상시키는 새로운 텍스트-비디오 생성 기능을 GenTron에 도입하였습니다.
- 인간 평가에서 SDXL과의 비교에서 GenTron은 시각적 품질에서 51.1%의 승률(무승부 19.8%)과 텍스트 정렬에서 42.3%의 승률(무승부 42.9%)을 달성했습니다.
- GenTron은 또한 T2I-CompBench에서 두각을 나타내며 구성적 생성에서의 강점을 강조했습니다.
- 우리는 이 작업이 미래 연구에 유의미한 통찰력과 귀중한 참고 자료를 제공할 것이라고 믿습니다.

### [HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image](https://arxiv.org/abs/2312.04543)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wftFb84XALhsb_ZIWPoQb.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wftFb84XALhsb_ZIWPoQb.mp4" muted="false"></video></div>

Vote: 7

Authors: Tong Wu, Zhibing Li, Shuai Yang, Pan Zhang, Xinggang Pan, Jiaqi Wang, Dahua Lin, Ziwei Liu

- 단일 이미지에서 3D 콘텐츠를 만드는 것은 오랫동안 지속되어온 매우 바람직한 작업으로, 최근에는 2D 확산 선행이 도입되어 합리적인 결과를 도출했습니다.
- 기존 방법들은 전체 범위에서 3D 콘텐츠를 볼 수 없으며, 렌더링하거나 편집하기에 충분히 하이퍼리얼리스틱하지 않다는 단점이 있습니다.
- HyperDreamer는 여러 가지 핵심 디자인과 유용한 속성들을 도입하여 이러한 도전 과제들을 해결합니다: 1) 전체 관측 범위에서 시각적으로 매력적인 3D 모델을 만들 수 있는 360도 메시 모델링과 고해상도 텍스처가 가능합니다.
- 2) 세부적인 의미론적 분할과 데이터 기반 사전 정보를 사용하여 재료의 albedo, 거침, 그리고 반짝임 특성을 학습할 수 있고, 의미론적으로 인식 가능한 재질 추정이 가능합니다.
- 3) 사용자는 생성된 모델이나 자신의 데이터에 대해 몇 번의 클릭으로 대상 영역을 선택하고 텍스트 기반 안내로 텍스처를 효율적으로 편집할 수 있습니다.
- HyperDreamer의 효과는 광범위한 실험을 통해 검증되었으며, 지역 인식 재료 모델링과 고해상도 텍스처를 가능하게 하고 사용자 친화적인 편집을 지원하는 데 성공하였습니다.
- 연구팀은 HyperDreamer가 3D 콘텐츠 생성을 발전시키고 다양한 분야에서 응용될 가능성을 가지고 있다고 믿습니다.

### [Controllable Human-Object Interaction Synthesis](https://arxiv.org/abs/2312.03913)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/30hPwTcq1Cfj64j-aGAws.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/30hPwTcq1Cfj64j-aGAws.mp4" muted="false"></video></div>

Vote: 7

Authors: Jiaman Li, Alexander Clegg, Roozbeh Mottaghi, Jiajun Wu, Xavier Puig, C. Karen Liu

- 3D 장면에서 언어 설명에 의해 인도되는 동기화된 객체 모션과 인간 모션을 생성하는 문제를 다루는 연구로, 현실적인 인간 행동을 시뮬레이션하는데 중요합니다.
- 'Controllable Human-Object Interaction Synthesis (CHOIS)' 접근 방식을 제안하여, 언어 설명, 초기 객체 및 인간 상태, 그리고 희소한 객체 웨이포인트가 주어진 조건에 따라 객체 모션과 인간 모션을 동시에 생성하는 조건부 확산 모델을 사용합니다.
- 언어 설명은 스타일과 의도를 전달하고, 웨이포인트는 장면 내에서 모션을 묶어 이를 효과적으로 추출하는 고급 계획 방법이 필요합니다.
- 확산 모델을 단순히 적용하는 것은 입력 웨이포인트와 일치하는 객체 모션을 예측하지 못하고, 정교한 손-객체 접촉과 적절한 바닥에 의해 구현된 접촉을 보장할 수 없습니다.
- 이 문제들을 극복하기 위해, 생성된 객체 모션과 입력 객체 웨이포인트 사이의 일치를 향상시키기 위한 추가적인 감독으로 객체 기하학 손실을 도입합니다.
- 훈련된 확산 모델의 샘플링 과정에서 접촉 제약을 강제하기 위한 지도 조건을 설계합니다.

### [DreamVideo: Composing Your Dream Videos with Customized Subject and Motion](https://arxiv.org/abs/2312.04433)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WbvQGxkK73a0DiDW3OT3_.png)

Vote: 7

Authors: Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, Hongming Shan

- 확산 모델을 이용한 맞춤형 생성은 이미지 생성에서 인상적인 발전을 이루었지만, 주체(subject)와 움직임(motion)을 모두 제어해야 하는 비디오 생성과제에서는 여전히 미흡하다.
- 이를 해결하기 위해, 원하는 주체의 몇 장의 정지 이미지와 목표 움직임의 몇 개의 비디오로 개인화된 비디오를 생성하는 새로운 접근 방식인 DreamVideo를 제시한다.
- DreamVideo는 사전 훈련된 비디오 확산 모델을 활용하여 주체 학습과 움직임 학습이라는 두 단계로 이 문제를 분리한다.
- 주체 학습은 제공된 이미지로부터 주체의 미세한 외모를 정확하게 포착하는 것을 목표로 하며, 이는 텍스트 인버전과 정체성 아답터(identity adapter)의 세심한 조정으로 달성된다.
- 움직임 학습에서는 주어진 비디오에 대해 효과적으로 목표 움직임 패턴을 모델링하기 위해 움직임 아답터(motion adapter)를 설계하고 이를 조정한다.
- 이 두 가지 가벼우면서도 효율적인 아답터들을 결합함으로써, 어떠한 주제든지 어떠한 움직임이든지 자유롭게 맞춤 설정이 가능하다.
- 광범위한 실험 결과는 맞춤형 비디오 생성을 위한 최신 기술 방법들을 능가하는 DreamVideo의 뛰어난 성능을 입증한다.
- 프로젝트 페이지는 https://dreamvideo-t2v.github.io 에서 확인할 수 있다.

### [Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation](https://arxiv.org/abs/2312.04483)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/I6ezTGSxdAM7T0NgAYz4S.png)

Vote: 6

Authors: Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, Nong Sang

- 확산 모델이 고화질 이미지 생성에 강력한 능력을 보여준 반면, 현실적이고 다양한 비디오 생성은 아직 초기 단계에 머물러 있는데, 이는 기존 방법들이 공간적 내용과 시간적 동태를 혼합해 복잡도가 증가한 텍스트-비디오 변환(T2V) 작업 때문이다.
- 본 논문에서는 구조 수준과 내용 수준에서 비디오의 공간적 및 시간적 요소를 분리하는 새로운 방법인 HiGen을 제안하여 성능을 향상시킨다.
- 구조 수준에서는 통합된 디노이저를 사용해 T2V 작업을 공간 추론과 시간 추론의 두 단계로 분해한다. 구체적으로, 공간 추론 동안 텍스트를 사용해 공간적으로 일관된 선행 사항을 생성한 다음, 이를 바탕으로 시간 추론 동안 일관된 움직임을 생성한다.
- 내용 수준에서는 입력 비디오의 내용에서 움직임과 외관 변화를 각각 표현하는 두 가지 미묘한 신호를 추출한다. 이 두 신호는 모델 훈련을 안내하여 내용의 유연한 변화를 가능하게 하고 시간적 안정성을 강화한다.
- 분리된 패러다임을 통해 HiGen은 해당 작업의 복잡성을 효과적으로 줄이고 의미론적 정확성과 동작 안정성을 갖춘 현실적인 비디오를 생성한다.
- 광범위한 실험을 통해 HiGen이 최신 T2V 방법보다 뛰어난 성능을 가짐을 입증한다.

### [Gen2Det: Generate to Detect](https://arxiv.org/abs/2312.04566)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/_4epd949NAWHZfLcugEUq.png)

Vote: 6

Authors: Saksham Suri, Fanyi Xiao, Animesh Sinha, Sean Chang Culatana, Raghuraman Krishnamoorthi, Chenchen Zhu, Abhinav Shrivastava

- 최근 확산 모델이 합성 이미지 품질의 향상과 생성에 있어서 더 나은 제어를 가능하게 함에 따라, 저희는 Gen2Det를 제시합니다. 이는 최신의 지상형 이미지 생성 방법을 활용하여 객체 탐지를 위한 합성 훈련 데이터를 무료로 생성하는 간단한 모듈식 파이프라인입니다.
- 기존의 작업들이 개별 객체 인스턴스를 생성하고 전경을 식별한 다음 다른 이미지에 붙이는 것이 필요했던 것과 달리, Gen2Det는 직접 장면 중심 이미지를 생성하는 것으로 간소화합니다.
- 합성 데이터와 더불어 Gen2Det는 생성된 데이터를 최적으로 활용하기 위한 일련의 기술들도 제안합니다. 이에는 이미지 수준 필터링, 인스턴스 수준 필터링 및 생성에서의 결함을 고려하기 위한 더 나은 훈련 방식이 포함됩니다.
- Gen2Det를 사용하여 다양한 설정에서의 객체 탐지 및 분할 작업에 상당한 개선을 보여줍니다. 또한 감지 방법에 구애받지 않고 성능을 향상시킵니다.
- LVIS의 긴 꼬리 탐지 설정에서, Gen2Det는 드문 카테고리의 성능을 크게 향상시킬 뿐만 아니라, 다른 카테고리의 성능도 크게 개선합니다. 예를 들어, Mask R-CNN을 사용하여 LVIS에서 실제 데이터로만 훈련했을 때와 비교하여 2.13 Box AP 및 1.84 Mask AP의 개선을 보입니다.
- COCO의 저데이터 체제 설정에서, Gen2Det는 Box AP와 Mask AP를 각각 2.27점과 1.85점 일관되게 향상시킵니다.
- 가장 일반적인 탐지 설정에서도, Gen2Det는 견고한 성능 개선을 보여주는데, 이는 COCO에서 Box AP와 Mask AP를 각각 0.45점과 0.32점 향상시킵니다.

### [Efficient Monotonic Multihead Attention](https://arxiv.org/abs/2312.04515)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xfyKBnDsKXf4OeF37TSok.png)

Vote: 5

Authors: Xutai Ma, Anna Sun, Siqi Ouyang, Hirofumi Inaguma, Paden Tomasello

- 본 논문에서는 수치적으로 안정되고 편향되지 않은 단조 정렬 추정을 가진 최첨단 동시 번역 모델인 효율적인 단조 멀티헤드 어텐션(EMMA)을 소개합니다.
- 오프라인 번역 모델로부터의 동시 세밀조정 및 단조 정렬 변동 감소와 같은 개선된 훈련 및 추론 전략을 제시하였습니다.
- 실험 결과, 제안된 모델이 스페인어와 영어 동시 음성-텍스트 번역 작업에서 최첨단 성능을 달성함을 보여줍니다.

### [NeRFiller: Completing Scenes via Generative 3D Inpainting](https://arxiv.org/abs/2312.04560)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pqfSmDfrbONsVSoxz3m-p.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pqfSmDfrbONsVSoxz3m-p.mp4" muted="false"></video></div>

Vote: 4

Authors: Ethan Weber, Aleksander Hołyński, Varun Jampani, Saurabh Saxena, Noah Snavely, Abhishek Kar, Angjoo Kanazawa

- 네RFiller는 표준 2D 시각 생성 모델을 사용하여 3D 캡처의 누락된 부분을 완성하는 생성적 3D 인페인팅 방법을 제안합니다.
- 메시 재구축 실패 또는 관찰 부족 (예: 물체의 바닥이나 접근하기 어려운 영역과 같은 접촉 영역)으로 인해 3D 장면이나 객체의 일부가 누락될 수 있습니다.
- 2D 인페인팅 확산 모델을 활용하는 이 방법은, 모델들이 2×2 그리드 형태의 이미지에서 더 3D 일관성 있는 필링 결과를 생성한다는 놀라운 특성을 발견하였습니다.
- 이러한 행동을 네 개 이상의 이미지로 일반화하는 방법을 보여주며, 이를 바탕으로 높은 일관성의 단일 3D 장면으로 인페인팅 영역을 정제하는 순환 프레임워크를 제시합니다.
- 본 연구는 배경 객체를 제거하는 것이 아니라 장면을 완성하는 것에 주력하며, 접근법은 밀착된 2D 객체 마스크나 텍스트를 필요로 하지 않습니다.
- 다양한 장면에 대해 NeRFiller를 적용한 결과, 가장 3D 일관적이며 설득력 있는 장면 완성을 생성하는 것으로 나타났습니다.
- 프로젝트 페이지는 https://ethanweber.me/nerfiller 에서 확인할 수 있습니다.

### [Scaling Laws of Synthetic Images for Model Training ... for Now](https://arxiv.org/abs/2312.04567)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/pNzeaYkVi1oK8zL4P3yGw.png)

Vote: 4

Authors: Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, Yonglong Tian

- 최근의 텍스트-이미지 모델의 주목할만한 발전으로, 대량의 큐레이팅된 데이터 수집에 대한 어려움을 극복하고 합성 이미지를 사용하여 비전 시스템을 훈련할 가능성이 열렸습니다.
- 본 논문은 최첨단 텍스트-이미지 모델이 생성한 합성 이미지의 확장 법칙을 분석하여 지도학습 모델(레이블 감독을 받는 이미지 분류기 및 언어 감독을 받는 CLIP)의 훈련에 대해 연구합니다.
- 텍스트 프롬프트, 분류기 없는 안내 스케일, 텍스트-이미지 모델 유형 등 여러 요소들이 스케일링 동작에 중대한 영향을 미친다는 것을 확인했습니다.
- 이런 요소들을 조정한 후, 합성 이미지가 CLIP 모델 훈련에서 실제 이미지보다 약간 덜 효과적이지만 비슷한 추세를 보이는 반면, 지도학습 이미지 분류기 훈련 시에는 확장성이 현저히 떨어지는 것을 관찰했습니다.
- 이미지 분류기 훈련에서의 낮은 성능은 기존 텍스트-이미지 모델이 특정 개념을 생성하는 데 있어서의 한계가 주요 원인임을 분석을 통해 지적하고 있습니다.
- 또한 본 연구는 합성 데이터를 확장하는 것이 다음과 같은 시나리오에서 특히 효과적이라고 제안합니다: (1) 지도학습 문제에 대한 실제 이미지의 공급이 제한적일 경우(예: ImageNet의 0.5백만 개 미만의 이미지), (2) 훈련 데이터와 상당히 다른 평가 데이터셋을 가질 때, 이는 분포 외 시나리오를 나타냅니다, (3) CLIP 모델 훈련과 같이 실제 이미지와 합성 데이터를 함께 사용하는 경우입니다.

### [Pearl: A Production-ready Reinforcement Learning Agent](https://arxiv.org/abs/2312.03814)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/asMSwz7WCxViySqsONVXE.png)

Vote: 4

Authors: Zheqing Zhu, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel Jiang, Yi Wan, Yonathan Efroni, Liyuan Wang, Ruiyang Xu, Hongbo Guo, Alex Nikulkov, Dmytro Korenkevych, Urun Dogan, Frank Cheng, Zheng Wu, Wanqiao Xu

- 강화 학습(RL)은 장기적인 목표를 달성하기 위한 다재다능한 프레임워크를 제공하며 다양한 실세계의 지능 시스템이 직면하는 문제들 - 지연된 보상, 부분 가시성 처리, 탐험과 활용 간의 딜레마, 오프라인 데이터를 활용한 온라인 성능 향상, 안전 제약 조건 충족 - 을 정식화할 수 있습니다.
- RL 연구 커뮤니티가 이러한 문제들을 해결하기 위해 상당한 진전을 이루었음에도 불구하고, 현재의 오픈소스 RL 라이브러리는 RL 솔루션 파이프라인의 좁은 영역에 초점을 맞추고 있어 다른 측면은 대체로 관리되지 않고 있습니다.
- 본 논문은 모듈 방식으로 이러한 도전에 적극적으로 대응하도록 특별히 설계된 생산 준비가 된 강화 학습 에이전트 소프트웨어 패키지인 Pearl을 소개합니다.
- 예비 벤치마크 결과에 대해 설명하였을 뿐만 아니라, Pearl이 생산 환경에서 사용될 준비가 되었음을 보여주는 업계 채택 사례를 강조합니다.
- Pearl은 Github에서 오픈 소스로 제공되며(https://github.com/facebookresearch/pearl), 공식 웹사이트는 https://pearlagent.github.io 에 위치하고 있습니다.

### [Generating Illustrated Instructions](https://arxiv.org/abs/2312.04552)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/_KLALB-luKVY2OBtgs5wu.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/_KLALB-luKVY2OBtgs5wu.mp4" muted="false"></video></div>

Vote: 4

Authors: Sachit Menon, Ishan Misra, Rohit Girdhar

- 이 연구에서는 사용자의 요구에 맞춘 시각적 지침을 생성하는 새로운 과제인 'Illustrated Instructions'를 도입하였습니다.
- 그 과제에 필요한 독특한 조건들을 식별하고, 생성물의 유효성, 일관성 및 효율성을 측정하기 위해 자동 및 인간 평가 척도 모음을 통해 이를 정식화했습니다.
- 대규모 언어 모델(Large Language Models, LLMs)의 힘과 강력한 텍스트-이미지 생성 확산 모델을 결합하여 입력 텍스트를 바탕으로 이러한 지침을 생성하는 단순한 접근법인 'StackedDiffusion'을 제안하였습니다.
- 결과 모델은 기준선 접근 방식과 최신 다중 모달 LLMs를 크게 능가하며, 경우에 따라 사용자는 인간이 생성한 문서보다 이 모델을 선호하기도 합니다.
- 특히, 웹상의 정적 기사들이 제공할 수 있는 것을 훨씬 뛰어넘는, 사용자의 개별 상황에 대한 맞춤형 지침을 중간 단계와 그림과 함께 제공하는 등 다양한 새롭고 흥미로운 응용 프로그램을 가능하게 합니다.

### [LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning](https://arxiv.org/abs/2312.03849)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/XhKZL1RpGXLwCY3qYA9FG.png)

Vote: 4

Authors: Bolin Lai, Xiaoliang Dai, Lawrence Chen, Guan Pang, James M. Rehg, Miao Liu

- 이 논문은 일상적인 인간 행동을 자기중심적 시점에서 지시 이미지 형태로 생성하는 새로운 문제인 자기중심적 행동 프레임 생성을 소개합니다.
- 사용자의 입력 이미지와 프롬프트 질문에 조건을 부가하여 행동 프레임을 합성하는 것이 목표이며, 기존 자기중심 데이터셋은 행동 실행을 자세히 설명하는 주석이 부족합니다.
- 확산 기반 이미지 조작 모델은 해당 자기중심 이미지 픽셀 공간 내에서 행동의 상태 변화를 제어하는데 실패합니다.
- 이를 해결하기 위해, 풍부한 행동 설명을 큐레이트하며 시각적 지시를 통해 시각적 대규모 언어 모델(VLLM)을 미세조정합니다.
- 또한, VLLM에서 추출한 이미지 및 텍스트 임베딩을 추가 조건으로 활용하여 자기중심(LEGO) 행동 프레임 생성을 학습할 것을 제안합니다.
- Ego4D와 Epic-Kitchens 등 두 자기중심 데이터셋에서 제안된 모델을 검증하였으며, 이는 정량적 및 정성적 평가에서 기존 이미지 조작 모델보다 뛰어난 개선을 보여줍니다.
- 논문은 자세한 배제 연구와 분석을 통해 제안하는 방법에 대한 통찰을 제공합니다.

### [Large Language Models for Mathematicians](https://arxiv.org/abs/2312.04556)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/1yalQhF_Dbz3q1pYeFuQc.png)

Vote: 4

Authors: Simon Frieder, Julius Berner, Philipp Petersen, Thomas Lukasiewicz

- ChatGPT와 같은 대규모 언어 모델(LLMs)은 일반 언어 이해와 고품질 텍스트 또는 컴퓨터 코드 생성 능력으로 큰 관심을 받고 있다.
- 많은 직업에서 LLMs는 작업 속도를 높이고 품질을 개선할 수 있는 소중한 도구로 여겨진다.
- 본 노트에서는 전문 수학자들이 LLMs를 활용하여 어느 정도까지 도움을 받을 수 있는지에 대해 논의한다.
- 현대의 모든 언어 모델에 사용되는 트랜스포머 모델의 수학적 설명을 우선 제공한다.
- 최근 연구를 바탕으로, 최적의 실천법과 잠재적 문제점들을 개관하고 언어 모델의 수학 능력에 대해 보고한다.
- 마지막으로, LLMs가 수학자들의 작업 방식을 어떻게 변화시킬 가능성에 대해 조명한다.

