## Daily Papers (2023-12-14)

### [SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention](https://arxiv.org/abs/2312.07987)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ZrV3Su6TGsPRLxw0ySBzO.png)

Vote: 17

Authors: Róbert Csordás, Piotr Piękos, Kazuki Irie

- 현대 트랜스포머 모델의 고비용 자기주의(self-attention) 레이어는 시퀀스 길이에 제곱으로 비례하는 메모리와 계산을 요구합니다.
- 기존의 근사화 방법들은 실제로 상당한 속도 향상을 얻지 못하고 성능이 떨어지는 문제점이 있습니다.
- 본 연구에서는 동일한 파라미터 예산을 가진 표준 트랜스포머의 언어 모델링 성능과 동등하면서 계산 및 메모리 요구 사항을 줄이고 실제 시간 속도 향상을 이루는 새로운 방법인 SwitchHead를 소개합니다.
- SwitchHead는 가치(value)와 출력(output) 투영을 위한 전문가 혼합(Mixture-of-Experts, MoE) 레이어를 사용하며, 표준 트랜스포머보다 4에서 8배 적은 수의 주의력 행렬을 필요로 합니다.
- 이 새로운 주의력 메커니즘은 MoE MLP 레이어와 결합될 수 있으며, 이를 통해 효율적인 전적으로 MoE 기반의 "SwitchHead" 트랜스포머 모델을 구현할 수 있습니다.
- 연구팀은 이 방법론에 대한 코드를 공개하였습니다.

### [Distributed Inference and Fine-tuning of Large Language Models Over The Internet](https://arxiv.org/abs/2312.08361)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/BUyZo5Zka6MRk9efoij4h.png)

Vote: 12

Authors: Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, Colin Raffel

- 대규모 언어 모델(Large language models, LLMs)은 NLP 작업에 유용하며, 50억 개 이상의 매개변수를 가진 최고의 오픈 소스 모델로 크기가 커짐에 따라 능력도 향상됩니다.
- 이러한 50B+ 모델을 사용하려면 고사양 하드웨어가 필요하지만, 대부분의 연구자에게 접근하기 어려운 문제가 있습니다.
- 본 연구에서는 LLM의 비용 효율적인 추론 및 파인튜닝 방법에 대해 조사하고 지역 및 분산 전략을 비교합니다.
- 50B+ 크기의 충분히 큰 모델은 소비자 등급 네트워크에서 지리적으로 분산된 장치에서도 효율적으로 실행될 수 있다는 것을 관찰했습니다.
- 이를 통해 연구 그룹 및 자원 봉사자의 유휴 컴퓨팅 자원을 함께 모아 LLM을 효율적으로 실행할 수 있는 가능성을 제시합니다.
- 연구진은 장치가 갑자기 연결 해제될 수 있고 하드웨어 성능이 다르며 임의로 참여하고 떠나는 장치들 사이에서 안정적으로 추론 및 파인튜닝하는 두 가지 문제를 다룹니다.
- 이러한 문제들을 해결하기 위해 특별한 내결함성 추론 알고리즘과 시스템 처리량을 최대화하기 위해 자동으로 장치를 할당하는 부하 균형 프로토콜을 개발했습니다.
- 연구진은 이 알고리즘을 통해 대화형 생성 작업에서 기존의 오프로딩보다 최대 10배 빠르게 Llama 2(70B)와 BLOOM(176B)을 인터넷에서 실행할 수 있는 분산 시스템 'Petals'을 선보입니다.
- 이 시스템의 성능은 시뮬레이션 조건과 두 대륙에 걸쳐 있는 실제 세계 설정에서 평가되었습니다.

### [CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor](https://arxiv.org/abs/2312.07661)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/OkRk-8mOj6G6Xj0a6Lt_j.png)

Vote: 10

Authors: Shuyang Sun, Runjia Li, Philip Torr, Xiuye Gu, Siyang Li

- 기존의 개방형 어휘 이미지 분할 방법들은 마스크 주석 및/또는 이미지-텍스트 데이터셋에 대한 미세조정 단계를 필요로 하지만, 그로 인해 사전 훈련된 VLM(비전과 언어 모델)의 개별 어휘 능력이 크게 감소합니다. 
- 수작업으로 마스크 레이블을 생성하는 데 드는 노력이 많기 때문에 분할 데이터셋의 카테고리 수가 제한됩니다.
- 한편, 미세조정 없이 훈련된 VLM들은 이미지에 존재하지 않는 개념에 대하여 텍스트 질의가 있을 경우 최적화되지 않은 마스크 예측을 하는 경향이 있습니다.
- 이러한 문제를 해결하기 위해, 우리는 무수한 비주얼 개념을 세분화하기 위해 훈련 노력 없이 점진적으로 무관한 텍스트를 필터링하고 마스크 품질을 개선하는 새로운 순환적 프레임워크를 소개합니다.
- 순환 유닛은 고정된 가중치를 가진 VLM을 기반으로 한 두 단계 세분화기로 구성됩니다.
- 우리의 모델은 VLM의 광범위한 어휘 공간을 유지하면서 세분화 능력을 강화합니다.
- 실험 결과에 따르면, 우리의 방법은 훈련 과정이 필요 없는 대안들뿐만 아니라 추가적인 데이터 샘플 수백만 개로 미세조정된 방법들을 능가하며, 제로샷 의미론적 이미지 및 참조 이미지 분할 작업 모두에 있어 새로운 최신 기록을 세웁니다.
- 구체적으로, 우리의 방법은 Pascal VOC, COCO Object, Pascal Context에서 각각 mIoU를 현재 기록보다 28.8, 16.0, 6.9 포인트 향상시켰습니다.

### [PromptBench: A Unified Library for Evaluation of Large Language Models](https://arxiv.org/abs/2312.07910)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/VuTuxjmIJGQHeGdLt0f__.png)

Vote: 8

Authors: Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie

- 대규모 언어 모델(LLMs)의 성능을 평가하고 잠재적 보안 위험을 완화하는 것은 매우 중요합니다.
- 본 논문에서는 LLMs를 평가하기 위한 통합 라이브러리인 PromptBench를 소개합니다.
- 이 라이브러리는 프롬프트 구성, 프롬프트 엔지니어링, 데이터셋 및 모델 로딩, 적대적 프롬프트 공격, 동적 평가 프로토콜, 분석 도구 등 연구자가 쉽게 사용하고 확장할 수 있는 몇 가지 주요 구성 요소로 이루어져 있습니다.
- PromptBench는 연구 목적을 위한 개방적이고 일반적이며 유연한 코드베이스로 설계되었으며 새로운 벤치마크의 생성, 하위 응용 프로그램의 배포 및 새로운 평가 프로토콜 설계에 대한 원래 연구를 용이하게 합니다.
- 코드는 https://github.com/microsoft/promptbench 에서 제공되며 지속적으로 지원될 예정입니다.

### [Clockwork Diffusion: Efficient Generation With Model-Step Distillation](https://arxiv.org/abs/2312.08128)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/IKMSNhHOpA4wz3BUEq7p3.png)

Vote: 7

Authors: Amirhossein Habibian, Amir Ghodrati, Noor Fathima, Guillaume Sautiere, Risheek Garrepalli, Fatih Porikli, Jens Petersen

- 이 연구는 텍스트-이미지 확산 모델의 효율성을 향상시키는 것을 목표로 합니다.
- 전통적인 확산 모델들은 생성 과정의 각 단계마다 계산 비용이 많이 드는 UNet 기반 잡음 제거 연산을 사용하지만, 이러한 연산이 최종 출력 품질에 똑같이 중요하지 않다는 점을 발견했습니다.
- 특히, 고해상도 특징 맵(feature maps)에서 동작하는 UNet 계층은 작은 변동에 민감한 반면, 저해상도 특징 맵은 이미지의 의미 구조에 영향을 주고, 종종 눈에 띄는 변화 없이 변형될 수 있습니다.
- 이 관찰에 기반하여, 저자들은 Clockwork Diffusion이라는 방법을 제안하며, 이는 이전의 잡음 제거 단계에서 계산을 주기적으로 재사용하여 하나 이상의 후속 단계에서 저해상도 특징 맵을 근사합니다.
- 텍스트-이미지 생성 및 이미지 편집 모두에 대한 여러 기준 모델에 대해, Clockwork가 현저히 줄어든 계산 복잡성으로 비교 가능하거나 개선된 지각 점수(perceptual scores)를 달성하는 것을 증명합니다.
- 예를 들어, 8 DPM++ 단계를 가진 Stable Diffusion v1.5에서 32%의 FLOP를 절약하면서도 무시할 수 있는 FID 및 CLIP 점수의 변화를 경험했습니다.

### [Foundation Models in Robotics: Applications, Challenges, and the Future](https://arxiv.org/abs/2312.07843)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/T0cnBlU3U_YLy5RyM4cXU.png)

Vote: 7

Authors: Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, Brian Ichter, Danny Driess, Jiajun Wu, Cewu Lu, Mac Schwager

- 전통적인 로봇공학 딥러닝 모델은 특정 작업을 위해 준비된 소규모 데이터셋으로 트레이닝되어, 다양한 응용 프로그램에 적용하는 데 있어 한계가 있습니다.
- 반면에 인터넷 규모 데이터로 사전 훈련된 기초 모델(foundation models)은 우수한 일반화 능력을 보이며, 때때로 훈련 데이터에 없는 문제에 대해 제로샷 해결책을 찾는 능력이 나타나기도 합니다.
- 기초 모델은 인지부터 의사결정, 제어에 이르는 로봇 자율성의 다양한 구성요소를 향상시킬 잠재력을 가지고 있습니다.
- 예컨대, 대규모 언어 모델은 코드를 생성하거나 상식적인 추론을 제공할 수 있으며, 시각-언어 모델은 개방형 어휘를 사용한 시각적 인식을 가능하게 합니다.
- 그러나 로봇 관련 트레이닝 데이터의 부족, 안전 보장 및 불확실성 측정, 실시간 실행 등에 관한 상당한 개방형 연구 과제들이 남아 있습니다.
- 본 조사에서는 로봇 문제를 해결하기 위해 사용되거나 구축된 기초 모델에 대한 최근 논문들을 공부하고, 기초 모델이 인지, 의사결정, 제어 영역에서 로봇 기능 개선에 어떻게 기여하는지 탐구합니다.
- 로봇 자율성에서 기초 모델 채택을 가로막는 도전과제들을 논의하며, 미래 발전을 위한 기회와 잠재적인 경로를 제시합니다.
- 본 논문과 관련된 GitHub 프로젝트(초안 발표. 본 작업의 질과 관련성을 보장하기 위해 지속적으로 강화하고 업데이트할 것을 약속합니다)는 다음 주소에서 찾을 수 있습니다: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models

### [Invariant Graph Transformer](https://arxiv.org/abs/2312.07859)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ZWvQdniFex__25rOWHPn3.png)

Vote: 3

Authors: Zhe Xu, Menghai Pan, Yuzhong Chen, Huiyuan Chen, Yuchen Yan, Mahashweta Das, Hanghang Tong

- 'Invariant Graph Transformer' 논문은 그래프 기계 학습에서 중요한 하위 그래프를 찾아내는 것, 즉 예측 결과를 결정짓는 그래프 근거를 찾는 과제를 다룬다.
- 기존의 그래프 데이터에 대한 개입 전략이 그래프 수준에서 개략적으로 진행된 것에 반해, 이 연구는 트랜스포머 모델의 발달에 힘입어 보다 세밀한 개입 전략을 제안한다.
- 자기주의 모듈을 기반으로 한 제안된 불변 그래프 트랜스포머(Invariant Graph Transformer, IGT)는 세밀한, 특히 노드 수준과 가상 노드 수준에서의 개입을 달성한다.
- IGT는 가정된 불변성, 즉 어떤 변경된 환경 하위 그래프에서도 그래프 근거로부터의 의미는 불변하여, 올바른 예측 결과를 보증한다.
- 실세계 데이터셋 7개를 포함한 포괄적인 실험을 통해, IGT는 13개의 기준 모델에 비해 중요한 성능 이점을 보여준다.

### [FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects](https://arxiv.org/abs/2312.08344)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/jgcaVux9oxYqBQNWFQK5B.png)

Vote: 3

Authors: Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield

- 우리는 모델 기반 및 모델 프리 설정 모두를 지원하는 6D 객체 포즈 추정 및 추적을 위한 통합 기초 모델인 FoundationPose를 제시한다.
- 제안된 접근 방식은 테스트 시 새로운 객체에 대해 CAD 모델이 제공되거나 소수의 참조 이미지가 캡처된 경우에도 미세 조정 없이 즉시 적용될 수 있다.
- 두 설정 간의 간극은 효과적인 새로운 뷰 합성을 허용하는 신경 암묵적 표현을 통해 해소되며, 이를 통해 하위 포즈 추정 모듈을 동일한 통합 프레임워크 하에서 불변하게 유지한다.
- 대규모 합성 훈련, LLM(대규모 언어 모델), 새로운 트랜스포머 기반 아키텍처 및 대조 학습 구성을 돕는 강력한 일반화성을 달성했다.
- 공개 데이터셋에서의 광범위한 평가는 복잡한 시나리오와 객체를 포함하며, 우리의 통합 접근법이 각 작업에 특화된 기존 메소드를 큰 차이로 능가함을 보여준다.
- 더욱이, 감소된 가정에도 불구하고 제한된 수준의 방법과 비교하여 비슷한 결과를 달성한다.
- 프로젝트 페이지에서 자세한 정보와 자료를 확인할 수 있다: https://nvlabs.github.io/FoundationPose/.

### [ProNeRF: Learning Efficient Projection-Aware Ray Sampling for Fine-Grained Implicit Neural Radiance Fields](https://arxiv.org/abs/2312.08136)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/msHb0Ow1mbFSoU_nInBJP.png)

Vote: 1

Authors: Juan Luis Gonzalez Bello, Minh-Quan Viet Bui, Munchurl Kim

- 최근 신경 렌더링 분야에서는 크기가 작은 암시적 모델을 사용하여 여러 뷰에서 장면의 기하학적 형태와 시점 종속적 외현을 학습할 수 있지만, 이러한 모델의 추론 속도는 느린 편이다.
- 소규모 메모리를 유지하면서 추론 속도를 높이기 위해 최근 연구들은 암시적 뉴럴 레디언스 필드의 각 레이에 대해 적은 수의 포인트를 적응적으로 샘플링하는 `샘플러` 네트워크를 적용하였다.
- 이러한 방법들은 렌더링 시간을 10배까지 줄일 수 있는 성과를 보였으나, 기존 NeRF 대비 상당한 품질 저하의 문제가 있다.
- 반면, 우리가 제안하는 ProNeRF는 메모리 크기는 NeRF와 유사하고, 속도는 HyperReel보다 빠르며, K-Planes보다 우수한 품질을 달성하는 최적의 절충안을 제공한다.
- ProNeRF는 효율적인 고밀도 입자 샘플링을 가능케 하는 새로운 투사 인식 샘플링(PAS) 네트워크와 함께 레이 탐색 및 활용을 위한 신규 훈련 전략을 갖추고 있다.
- ProNeRF는 NeRF보다 15-23배 빠른 속도와 0.65dB 높은 PSNR을 기록하며, 최고의 샘플러 기반 방법인 HyperReel보다 0.95dB 높은 PSNR을 달성하면서도 최첨단 성능을 제공한다.
- ProNeRF의 탐색 및 활용 훈련 전략은 전체 장면의 색상과 밀도 분포를 학습할 뿐만 아니라, 밀도가 가장 높은 영역에 집중된 효율적인 레이 샘플링도 학습하도록 한다.
- LLFF와 Blender와 같이 널리 사용되는 전방향 및 360도 데이터셋에 대한 실험 결과는 우리의 방법이 효과적임을 뒷받침해 준다.

