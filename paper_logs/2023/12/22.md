## Daily Papers (2023-12-22)

### [AppAgent: Multimodal Agents as Smartphone Users](https://arxiv.org/abs/2312.13771)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/JuTCKRUNmCugeqYdk7MAo.png)

Vote: 21

Authors: Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, Gang Yu, Gang Yu

- 이 논문은 LLM(Large Language Models)을 기반으로 한 새로운 멀티모달 에이전트 구조를 소개하며, 이 구조는 스마트폰 애플리케이션을 운영할 수 있도록 개발되었습니다.
- 에이전트는 사람 같은 상호 작용인 탭(tapping)과 스와이프(swiping)를 모방하는 단순화된 작업 공간을 통해 스마트폰 앱을 조작할 수 있습니다.
- 이러한 접근 방식은 시스템 백엔드 접근의 필요성을 우회하여 앱 간의 적용 가능성을 확장시킵니다.
- 에이전트의 기능 핵심은 자율적 탐험 또는 인간 시연 관찰을 통해 새로운 앱을 탐색하고 사용하는 방식으로 지식 베이스를 생성하는 혁신적인 학습 방법입니다.
- 에이전트는 사회적 미디어, 이메일, 지도, 쇼핑 그리고 복잡한 이미지 편집 도구 등 다양한 10개의 애플리케이션에서 50가지 작업을 수행하는 데 대한 방대한 테스트를 통해 실용성을 입증했습니다.
- 테스트 결과는 에이전트가 다양한 고급 작업을 능숙하게 처리할 수 있음을 확인시켜 줍니다.

### [VideoPoet: A Large Language Model for Zero-Shot Video Generation](https://arxiv.org/abs/2312.14125)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vKg_p5IwCqslxFW-lO0g0.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/vKg_p5IwCqslxFW-lO0g0.mp4" muted="false"></video></div>

Vote: 20

Authors: Dan Kondratyuk, Dan Kondratyuk, Lijun Yu, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, David Ross, Grant Schindler, +

- 'VideoPoet' 모델은 다양한 컨디셔닝 신호들을 기반으로 고품질의 비디오와 일치하는 오디오를 합성할 수 있는 언어 모델입니다.
- 이 모델은 이미지, 비디오, 텍스트, 오디오 등 다중 모달 입력을 처리하는 디코더 전용 트랜스포머 구조를 사용합니다.
- 전처리 단계에서는 자기회귀형 트랜스포머 틀 안에서 여러 가지 다중 모달 생성 목표들을 결합합니다.
- 전처리된 라지 랭귀지 모델(LLM)은 비디오 생성 작업의 범위를 위해 적응될 수 있는 기반이 됩니다.
- VideoPoet는 제로-샷 비디오 생성에서 고품질의 움직임을 생산할 수 있는 능력을 포함하여 최첨단의 성능을 보여주는 실험 결과들을 제시합니다.
- 프로젝트 페이지는 http://sites.research.google/videopoet/ 에서 확인할 수 있습니다.

### [Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis](https://arxiv.org/abs/2312.13834)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gidEDTNJ3K-oQSA95TXU8.png)

Vote: 18

Authors: Bichen Wu, Bichen Wu, Ching-Yao Chuang, Xiaoyan Wang, Xiaoyan Wang, Yichen Jia, Kapil Krishnakumar, Tong Xiao, Feng Liang, Licheng Yu, Licheng Yu, Peter Vajda, Peter Vajda

- 본 논문에서는 이미지 편집 확산 모델을 기반으로 하여 비디오 편집 응용 프로그램을 위해 강화된 간단하면서도 강력한 Fairy를 소개한다.
- Fairy의 접근법은 프레임 간의 확산 특징이 암묵적으로 전파됨으로써 시간적 일관성과 고품질 합성을 보장하는 앵커 기반 크로스-프레임 주의 메커니즘 개념을 중심으로 한다.
- 이 모델은 이전 모델들의 메모리와 처리 속도의 한계를 해결함과 동시에, 고유의 데이터 증대 전략을 통해 시간적 일관성을 향상시킨다.
- 새로운 전략은 모델이 소스 및 대상 이미지에서 어파인 변환에 등변이 되도록 한다.
- Fairy는 놀라우리만큼 효율적으로, 512x384 해상도의 120프레임 비디오(30FPS에서 4초 길이)를 단 14초만에 생성할 수 있으며, 이는 이전 연구보다 최소 44배 빠르다.
- 1000개의 생성된 샘플을 포함하는 포괄적인 사용자 연구를 통해, 우리의 접근법이 우수한 품질을 제공하며 기존의 방법들을 결정적으로 능가함을 확인하였다.

### [Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models](https://arxiv.org/abs/2312.13913)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-Fx9n5C-k4oqKxJ2DAzr-.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-Fx9n5C-k4oqKxJ2DAzr-.mp4" muted="false"></video></div>

Vote: 10

Authors: Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, BIN FU, Yong Liu, Gang Yu

- 이 논문은 텍스트 또는 이미지 입력에 따라 비텍스처 3D 메시에 대한 고해상도, 조명 없는, 다양한 2K UV 텍스처 맵을 생성할 수 있는 새로운 조대-미세 생성 프레임워크인 Paint3D를 제시합니다.
- 주된 도전 과제는 조명 정보가 포함되지 않은 고품질 텍스처를 생성하는 것으로, 이로 인해 텍스처를 현대 그래픽 파이프라인에서 재조명하거나 재편집하는 것이 가능해집니다.
- 방법론은 우선 사전 훈련된 깊이 인식 2D 확산 모델을 사용하여 뷰 조건부 이미지를 생성하고 멀티뷰 텍스처 융합을 수행하여 초기 조악한 텍스처 맵을 생성합니다.
- 그러나 2D 모델이 3D 형태를 완벽하게 표현하지 못하고 조명 효과를 비활성화시키기 때문에, 생성된 조악한 텍스처 맵에는 불완전한 영역과 조명 아티팩트가 나타납니다.
- 이를 해결하기 위해, 불완전한 영역을 정제하고 조명 아티팩트를 제거하는 데 특화된 별도의 UV 인페인팅 및 UVHD 확산 모델을 훈련시킵니다.
- 이러한 조대-미세 과정을 통해 Paint3D는 의미론적 일관성을 유지하면서도 조명이 없는 고품질 2K UV 텍스처를 생성할 수 있으며, 이는 3D 객체에 텍스처를 적용하는 최신 기술을 크게 향상시킵니다.

### [DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation](https://arxiv.org/abs/2312.13578)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/oNQ0Efp-5vYVx9bznjBe1.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/oNQ0Efp-5vYVx9bznjBe1.mp4" muted="false"></video></div>

Vote: 10

Authors: Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, Guoxian Song, You Xie, You Xie, Linjie Luo, Yapeng Tian, Xiaohu Guo, Jiashi Feng

- 기존 LSTM 네트워크가 감정 표현의 미묘한 차이를 포착하는데 한계가 있는 가운데, 단일 초상화 이미지로부터의 감정적인 말하는 얼굴 생성은 여전히 중대한 도전 과제입니다.
- 이 논문은 다양한 표현과 정확한 입 모양의 동기화를 동시에 달성하기 위해 DREAM-Talk, 즉 두 단계로 구성된 확산 기반 오디오 주도 프레임워크를 소개합니다.
- 첫 번째 단계에서는 오디오와 참조된 감정 스타일에 따라 다양하고 역동적인 감정 표현 및 머리 포즈를 생성하는 EmoDiff라는 새로운 확산 모듈을 제안합니다.
- 강한 상관관계가 있는 입 모양의 움직임과 오디오를 기반으로, 오디오 특징과 감정 스타일을 사용하여 입 모양 동기화의 정확도를 강화하고 세세한 조정을 합니다.
- 마지막으로, 비디오-비디오 렌더링 모듈을 배치하여 프록시 3D 아바타에서 표현과 입 움직임을 임의의 초상화로 전송합니다.
- 정량적 및 정성적 평가에서 DREAM-Talk은 표현력, 입 모양 동기화의 정확성, 그리고 지각적 품질 측면에서 최신 기술을 능가하는 성능을 보여줍니다.

### [Time is Encoded in the Weights of Finetuned Language Models](https://arxiv.org/abs/2312.13401)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/OcfcxivXhU_YLfFcdhYbu.png)

Vote: 9

Authors: Kai Nylund, Kai Nylund, Suchin Gururangan, Suchin Gururangan, Noah A. Smith

- 본 논문은 새로운 시기에 특화된 언어 모델을 맞춤화하기 위한 간단한 도구인 '시간 벡터'를 소개합니다.
- 시간 벡터는 단일 시간(예: 연도나 월) 데이터로 언어 모델의 파인튜닝을 완료한 후 원래 사전 훈련된 모델의 가중치를 뺌으로써 생성됩니다.
- 실험 결과, 이 벡터는 해당 시기의 텍스트에 대한 성능 향상에 기여하는 가중치 공간 내의 방향을 지정한다고 나타났습니다.
- 인접 시기에 특화된 시간 벡터들은 가중치 공간의 매니폴드에서 서로 가까이 위치하는 것으로 보입니다.
- 이 구조를 이용하여 우리는 추가 훈련 없이 중간 시기 및 미래 시기 텍스트에 대해 보다 나은 성능을 발휘하는 새로운 모델을 유도하기 위해 시간 벡터들 사이를 보간합니다.
- 연구 결과는 다양한 작업, 도메인, 모델 크기 및 시간 척도에 걸쳐 일관성이 있음을 입증하며, 파인튜닝된 모델의 가중치 공간에 시간이 인코딩되어 있음을 제안합니다.

### [HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models](https://arxiv.org/abs/2312.14091)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Kl-L22KVC_oOP6rIv1UCB.png)

Vote: 9

Authors: Hayk Manukyan, Andranik Sargsyan, Andranik Sargsyan, Barsegh Atanyan, Barsegh Atanyan, Zhangyang Wang, Shant Navasardyan, Shant Navasardyan, Humphrey Shi, Humphrey Shi

- 최근 텍스트 가이드 이미지 복원 기술은 텍스트에서 이미지로 변환하는 확산 모델의 성공으로 인해 실제적이고 시각적으로 설득력 있는 결과를 도출하였으나, 여전히 사용자 프롬프트에 더 잘 부합하고 고해상도 복원 수행 분야에서 개선의 여지가 있음.
- 본 논문에서는 프롬프트에 정확히 따르고 고해상도 이미지 복원으로 일관되게 확장할 수 있는 완전히 트레이닝이 필요 없는 접근 방식인 HD-Painter를 소개함.
- 프롬프트 정보를 통해 자기 주의(self-attention) 점수를 강화하여 텍스트 정렬 생성을 더 잘 수행하는 Prompt-Aware Introverted Attention (PAIntA) 레이어를 설계함.
- 프롬프트 일치성을 개선하기 위해, 일반 DDIM 양식에 후속 샘플링 전략을 통합하는 Reweighting Attention Score Guidance (RASG) 메커니즘을 도입함.
- HD-Painter는 복원에 특화된 슈퍼 해상도 기술을 도입함으로써, 최대 2K 해상도의 이미지에서 누락된 영역을 완성할 수 있도록 확장성을 제공함.
- 실험을 통해 HD-Painter가 기존 최신 기술보다 질적, 양적으로 우수함을 입증하며 생성 정확도도 61.4% 대 51.9%로 상당한 향상을 보임.
- 연구에서 개발한 코드는 https://github.com/Picsart-AI-Research/HD-Painter 에서 공개할 예정임.

### [DreamTuner: Single Image is Enough for Subject-Driven Generation](https://arxiv.org/abs/2312.13691)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ju6hC3HqyDnxFFtMapoyC.png)

Vote: 9

Authors: Miao Hua, Miao Hua, Jiawei Liu, Jiawei Liu, Fei Ding, Wei Liu, Jie Wu, Jie Wu, Qian He

- 확산 기반 모델은 텍스트-이미지 생성에서 인상적인 능력을 보여주었으며, 참조 이미지가 하나 또는 몇 개인 맞춤형 개념 생성을 위한 주제 중심의 생성에 대한 개인화된 응용을 기대하고 있다.
- 기존 방식들은 사전 훈련된 모델의 생성 능력을 유지하는 동시에 주제 학습에 대한 균형을 맞추는 데 실패했고, 추가 이미지 인코더를 사용하는 다른 방법들은 인코딩 압축으로 인해 주제의 중요한 세부 사항을 잃게 되었다.
- 이러한 도전 과제를 해결하기 위해, DreamTurner라는 새로운 방법을 제안하며, 이는 참조 정보를 거칠게에서 미세하게 주입하여 더 효과적으로 주제 중심 이미지 생성을 달성한다.
- DreamTurner는 조각적인 주제 정체성을 보존하기 위해 주제 인코더를 도입했고, 일반적인 주제 특징을 시각-텍스트 교차 주의 전에 주의 계층을 통해 도입한다.
- 사전 훈련된 텍스트-이미지 모델 내의 자기 주의 계층을 대상 주제의 세부 사항을 세밀하게 다듬기 위해 자기 주제 주의로 수정한다.
- 생성된 이미지는 자기 주제 주의에서 참조 이미지와 자체의 상세한 특징을 묻는다.
- 자기 주제 주의는 사용자 지정 주제의 상세한 특징을 유지하기 위한 효과적이고 우아하며 훈련이 필요 없는 방법이며, 추론 중에 플러그 앤 플레이 솔루션으로 작동할 수 있다.
- 추가적인 주제 중심의 미세 조정으로 DreamTurner는 주제 중심 이미지 생성에서 주목할 만한 성능을 달성하며, 이는 텍스트나 포즈와 같은 다른 조건에 의해 제어될 수 있다.
- 이 프로젝트에 대한 자세한 내용은 https://dreamtuner-diffusion.github.io/에서 확인할 수 있다.

### [PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models](https://arxiv.org/abs/2312.13964)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/oOJBoFjMajV9_c1dfSypr.png)

Vote: 7

Authors: Yiming Zhang, Zhening Xing, Zhening Xing, Yanhong Zeng, Yanhong Zeng, Youqing Fang, Kai Chen

- 최근 개인화된 텍스트-이미지(T2I) 모델의 발전은 비전문가들도 독특한 스타일의 놀라운 이미지를 생성할 수 있게 하여 콘텐츠 제작 분야에 혁신을 가져왔습니다.
- 그러나 이러한 개인화된 이미지에 사실적인 움직임을 텍스트로 추가하는 것은 독특한 스타일을 유지하는 동시에 고해상도 디테일과 텍스트로 움직임을 제어하는 데 상당한 도전이 됩니다.
- 본 논문에서는 이러한 조건 이미지에 부합하고, 텍스트에 의한 움직임 제어능력이 뛰어나며, 구체적인 튜닝 없이 다양한 개인화된 T2I 모델과 호환되는 개인 맞춤형 이미지 애니메이터인 PIA를 제안합니다.
- PIA는 기존 T2I 모델에 잘 훈련된 시간적 정렬 레이어를 더하여, 어떤 개인화된 T2I 모델도 이미지 애니메이션 모델로 원활하게 변환할 수 있습니다.
- PIA의 핵심 구성 요소 중 하나는 조건 모듈로, 조건 프레임과 프레임 간 친화도를 입력으로 활용하여 라텐트 공간에서 개별 프레임 합성을 위한 친화도 힌트에 의해 안내되는 외형 정보를 전송합니다.
- 이러한 설계는 움직임과 관련된 가이드와의 정렬에 더 집중할 수 있게 하여, 움직임과 관련된 도전 과제를 완화시킵니다.

### [TinySAM: Pushing the Envelope for Efficient Segment Anything Model](https://arxiv.org/abs/2312.13789)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/W-ZvzDmhfH0zEWew92VSq.png)

Vote: 7

Authors: Han Shu, Wenshuo Li, Yehui Tang, Yehui Tang, Yiman Zhang, Yiman Zhang, Yihao Chen, Houqiang Li, Yunhe Wang, Xinghao Chen, Xinghao Chen

- 최근 컴퓨터 비전 분야에서 강력한 분할 능력을 보여준 세그먼트 에니싱 모델(SAM)에 대한 주목이 이루어졌으며, 예비 학습된 SAM을 기반으로 다양한 응용 프로그램을 개발하는 후속 작업들이 인상적인 성능을 달성하였습니다.
- 그러나 SAM은 무거운 구조를 가지며 많은 계산 능력을 요구하므로, 계산 제약이 있는 엣지 디바이스에서의 SAM의 추가적인 응용을 방해합니다.
- 이를 해결하기 위해, 본 논문에서는 강력한 제로샷(zero-shot) 성능을 유지하면서 효율적인 세그먼트 에니싱 모델(TinySAM)을 얻기 위한 프레임워크를 제안합니다.
- 먼저, 온라인 하드 프롬프트 샘플링 전략을 사용한 전체 단계 지식 전수 방법을 제시하여 경량화된 학생 모델을 얻습니다.
- 본 연구는 프럼프트 가능한 세그먼트 작업에 후 훈련 양자화(post-training quantization)를 적용하여 계산 비용을 더욱 줄입니다.
- 또한, 거의 성능 저하 없이 에브리싱(all-thing) 추론을 2배 가속화하는 계층적인 세그먼팅 에브리싱 전략을 제안합니다.
- 제안된 모든 방법을 통해 TinySAM은 계산 비용을 크게 줄이면서 효율적인 세그먼트 에니싱 작업을 위한 새로운 가능성을 제시합니다.
- 다양한 제로샷 전송 작업에서 실시한 광범위한 실험을 통해 TinySAM이 경쟁 방법들에 비해 상당히 우수한 성능을 보여주는 것으로 나타났습니다.
- 사전 훈련된 모델과 코드는 https://github.com/xinghaochen/TinySAM 및 https://gitee.com/mindspore/models/tree/master/research/cv/TinySAM에서 사용할 수 있습니다.

### [Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models](https://arxiv.org/abs/2312.13763)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8sABWVfhmjU8RPxl2YIZz.png)

Vote: 7

Authors: Huan Ling, Huan Ling, Seung Wook Kim, Seung Wook Kim, Antonio Torralba, Sanja Fidler, Karsten Kreis

- 이 연구에서는 동적인 애니메이션된 3D 객체를 텍스트로부터 4D 설정으로 합성하기 위해 점수 증류 방법을 활용하고, 추가적인 시간 차원을 통합합니다.
- 이전 연구와 달리, 복합적인 생성 기반 접근방식을 추구하고, 텍스트-이미지, 텍스트-비디오 및 3D 인지 멀티뷰 확산 모델을 결합하여 4D 객체 최적화 동안 피드백을 제공합니다.
- 우리의 방법인 'Align Your Gaussians' (AYG)는 동적인 3D 가우스 스플래팅을 변형 필드와 결합한 4D 표현을 활용합니다.
- AYG의 중요한 부분은 이동하는 3D 가우스 분포를 정규화하여 최적화를 안정화시키고 동작을 유도하는 새로운 방법입니다.
- 또한, 모션 증폭 메커니즘과 더 긴 생성을 위해 여러 4D 시퀀스를 생성하고 결합하는 새로운 자동 회귀 합성 방식을 제안합니다.
- 이 기술들은 생생한 동적 장면을 합성할 수 있도록 하며, 이전 작업들을 질적으로나 양적으로 뛰어넘고, 최신 텍스트-투-4D 성능을 달성합니다.
- 가우스 4D 표현 덕분에 다양한 4D 애니메이션을 원활하게 결합할 수 있습니다.
- AYG는 애니메이션, 시뮬레이션 및 디지털 콘텐츠 생성 뿐만 아니라 합성 데이터 생성을 위한 유망한 길을 열었습니다.

### [DyBluRF: Dynamic Deblurring Neural Radiance Fields for Blurry Monocular Video](https://arxiv.org/abs/2312.13528)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/OkpWCf9OwRLMoP3_YKvL8.png)

Vote: 6

Authors: Minh-Quan Viet Bui, Minh-Quan Viet Bui, Jongmin Park, Jihyong Oh, Jihyong Oh, Munchurl Kim

- 동적 3D 장면을 재구성하고 주어진 흐린 프레임에서 추출된 부정확한 카메라 포즈 정보와 싸우기 위해 상호 맞춤식 레이 개선(Interleave Ray Refinement, IRR) 단계와 움직임 분해 기반 디블러링(Motion Decomposition-based Deblurring, MDD) 단계로 구성된 새로운 동적 디블러링 신경 복사장(NeRF) 프레임워크인 DyBluRF가 제안되었습니다.
- DyBluRF는 흐릿한 단안 비디오의 새로운 시점을 합성하기 위해 처음으로 이 문제를 다루고 처리합니다.
- IRR 단계는 동적 3D 장면 재구성과 부정확한 카메라 포즈 정보의 정제를 공동으로 수행합니다.
- MDD 단계는 글로벌 카메라 이동과 로컬 객체 이동 요소로 흐릿한 레이를 분해하는 증분 잠재 선명 레이 예측(ILSP) 접근법을 새롭게 도입합니다.
- 벌써 소개된 가장 최신 기법들을 질적으로 그리고 양적으로 뛰어넘는 것을 종합적인 실험 결과들이 입증하였습니다.
- 프로젝트 페이지는 소스 코드와 사전 훈련된 모델을 포함하여 https://kaist-viclab.github.io/dyblurf-site/ 에서 공개되어 있습니다.

### [ShowRoom3D: Text to High-Quality 3D Room Generation Using 3D Priors](https://arxiv.org/abs/2312.13324)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/nzs-6aRaIyiuEesXgVw_u.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/nzs-6aRaIyiuEesXgVw_u.mp4" muted="false"></video></div>

Vote: 6

Authors: Weijia Mao, Weijia Mao, Yan-Pei Cao, Yan-Pei Cao, Jia-Wei Liu, Jia-Wei Liu, Zhongcong Xu, Zhongcong Xu, Mike Zheng Shou, Mike Zheng Shou

- ShowRoom3D라는 새로운 접근법을 통해 텍스트로부터 고퀄리티의 3D 방 스케일 장면을 생성하는 방법을 소개합니다.
- 기존 방법들이 2D 확산(diffusion) 기반 모델을 사용하여 실망스런 퀄리티의 3D 방 장면을 생성한 것과 달리, 3D 확산 기반 모델인 MVDiffusion을 사용하여 향상된 결과를 얻을 수 있음을 밝혔습니다.
- 첫째, 네트워크 레디언스 필드(NeRF)를 최적화하기 위해 점진적 시점 선택 프로세스를 제안하여 카메라 샘플링 범위를 단계적으로 확장하는 새로운 3단계 훈련 과정을 도입합니다.
- 둘째, MVDiffusion이 정확한 뷰 지도를 제공할 수 있도록 두 번째 단계에서 포즈 변환 방법을 제안합니다.
- ShowRoom3D는 강화된 구조적 무결성, 어떤 시점에서도 더욱 명확한 시각 효과, 반복되는 내용 감소, 그리고 다양한 관점에 대한 높은 일관성을 특징으로 하는 방 생성을 가능하게 합니다.
- 광범위한 실험을 통해 우리의 방법이 사용자 연구 측면에서 기존의 최신 기법들을 큰 차이로 능가함을 보여줍니다.

### [Neural feels with neural fields: Visuo-tactile perception for in-hand manipulation](https://arxiv.org/abs/2312.13469)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0lLpEJOpDcix2LF-ZBa9_.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0lLpEJOpDcix2LF-ZBa9_.mp4" muted="false"></video></div>

Vote: 5

Authors: Sudharshan Suresh, Sudharshan Suresh, Haozhi Qi, Tingfan Wu, Taosha Fan, Luis Pineda, Mike Lambeta, Jitendra Malik, Jitendra Malik, Mrinal Kalakrishnan, Mrinal Kalakrishnan, Roberto Calandra, Michael Kaess, Joseph Ortiz, Mustafa Mukadam

- 로봇이 손에서 조작하는 동안 새로운 객체의 위치와 모양을 추정하기 위해, 이 논문은 시각과 촉각을 결합하여 이러한 공간 인식을 추론하는 방법을 다룬다.
- 다중 손가락 핸드에서 시각과 촉감 센싱을 사용하여 손에서 조작하는 동안 객체의 위치와 형태를 추정하는, NeuralFeels라는 방법을 제안한다.
- NeuralFeels는 온라인으로 신경 필드를 학습하여 객체 지오메트리를 인코딩하고, 위치 그래프 문제를 최적화하여 이를 동시에 추적한다.
- 시뮬레이션과 실제 세계에서 다양한 객체와 상호작용하며, 고유 감각 중심 정책을 사용하여 다중 모달 손에서 인식에 대해 연구한다.
- 실험 결과는 최종 재건 F-score가 81%이고 평균 위치 편차가 4.7mm로, 알려진 CAD 모델을 사용할 경우 2.3mm로 줄어들었다는 것을 보여준다.
- 심한 시각적 가림 현상 하에서 시각-단독 방법들과 비교하여 최대 94%의 추적 향상을 달성할 수 있음을 알 수 있었다.
- 촉각은 적어도 시각적 추정을 정제하고, 최선의 경우에는 시각적 추정을 명확하게 할 수 있다는 결과를 입증한다.
- 이 분야의 벤치마킹을 향한 발걸음으로써, 70개의 실험으로 구성된 평가 데이터셋인 FeelSight를 공개한다.
- 다중 모달 센싱으로 구동되는 신경적 표현은 로봇 민첩성을 발전시키는 지각의 기반이 될 수 있다는 것을 보여준다.
- 추가 정보 및 비디오는 프로젝트 홈페이지 (https://suddhu.github.io/neural-feels/)에서 확인할 수 있다.

### [Unlocking Pre-trained Image Backbones for Semantic Image Synthesis](https://arxiv.org/abs/2312.13314)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kMBtQKlkRl19oxnbsetrz.png)

Vote: 4

Authors: Tariq Berrada, Tariq Berrada, Jakob Verbeek, Camille Couprie, Karteek Alahari

- 사용자 제공의 의미론적 레이블 맵에서 이미지를 생성하는 의미론적 이미지 합성은 생성된 이미지의 내용과 공간적 구성을 조절할 수 있게 함으로써 중요한 조건부 이미지 생성 과제입니다.
- 확산 모델은 생성 이미지 모델링에서 최첨단 기술을 이끌고 있지만, 그들의 추론 과정이 반복적인 성질 때문에 계산 비용이 많이 들어갑니다.
- GAN과 같은 다른 접근법들은 단일 전진 패스만으로 생성이 가능해 효율적이지만, 크고 다양한 데이터셋에서 이미지 품질이 떨어지는 경향이 있습니다.
- 이 연구에서는 이미지 분류와 같은 작업에 대해 사전 학습된 특징 백본 네트워크를 활용하여 매우 현실적인 이미지를 생성하는 새로운 클래스의 GAN 판별기를 제안합니다.
- 또한, 더 나은 문맥 모델링을 가지고 있는 새로운 생성기 구조를 소개하며 크로스 어텐션을 사용하여 잠재 변수에 노이즈를 주입함으로써 생성된 이미지의 다양성을 증가시킵니다.
- 우리가 DP-SIMS라고 부르는 모델은 ADE-20K, COCO-Stuff, 및 Cityscapes에서 입력 레이블 맵과의 일관성 면에서 높은 이미지 품질을 달성하며, 최근의 확산 모델들을 뛰어넘는 성과를 보이며, 두 자리 숫자의 계산량 감소로 추론이 가능합니다.

### [Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning](https://arxiv.org/abs/2312.13980)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xsDqo0qtlwYkY6XSGFd-Z.png)

Vote: 4

Authors: Desai Xie, Desai Xie, Jiahao Li, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, Zhixin Shu, Yi Zhou, Sai Bi, Sören Pirk, Arie E. Kaufman

- 본 논문에서는 텍스트-이미지 확산 모델을 미세 조정하여 다각도에서 이미지를 생성한 후 NeRF 복원 기술을 사용하는 3차원 텍스트 생성 작업의 최근 발전을 논의합니다.
- 기존의 지도 학습으로 미세 조정된(SFT) 확산 모델은 여러 관점의 불일치와 결과적인 NeRF 아티팩트에서 여전히 문제를 겪고 있습니다.
- SFT를 더 긴 시간 동안 학습시키면 일관성은 향상되지만, 이는 다양성과 리얼리스틱한 세부 사항을 줄이는 분포 이동(distribution shift)을 유발합니다.
- 논문은 SFT가 LLM 정렬 파이프라인의 지시 미세 조정 단계와 유사하며, 강화 학습을 통한 미세 조정(RLFT) 방법으로부터 혜택을 받을 수 있다고 주장합니다.
- Carve3D는 다중 관점 확산 모델의 일관성을 향상하기 위해 RLFT 방법과 다중 관점 복원 일관성(Multi-view Reconstruction Consistency, MRC) 지표를 결합한 새로운 방법입니다.
- MRC를 계산하기 위해 집합 내의 다중 관점 이미지를 동일한 관점에서 복원된 NeRF의 렌더링과 비교합니다.
- 저자들은 제어된 불일치 수준에서 MRC의 견고성을 광범위한 실험을 통해 검증하였습니다.
- 저자들은 교육 과정을 안정화하고, 분포 이동을 줄이며, 확장 법칙을 식별하기 위해 기본 RLFT 알고리즘을 향상시켰습니다.
- Carve3D는 질적 및 양적 실험과 사용자 연구를 통해 향상된 다중 관점 일관성, 더 우수한 NeRF 복원 품질과 더 긴 SFT에 비해 최소한의 분포 이동을 보여주었습니다.
- 프로젝트 웹페이지 주소는 https://desaixie.github.io/carve-3d 입니다.

### [HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs](https://arxiv.org/abs/2312.14140)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/W4vhq7KYGTNl7M5IcO__g.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/W4vhq7KYGTNl7M5IcO__g.mp4" muted="false"></video></div>

Vote: 3

Authors: Artem Sevastopolsky, Artem Sevastopolsky, Philip-William Grassal, Simon Giebenhain, Simon Giebenhain, ShahRukh Athar, ShahRukh Athar, Luisa Verdoliva, Matthias Niessner

- 인간 머리 모델링의 최신 발전은 신경 표현을 통해 있는 그대로의 3D 머리 모델을 생성할 수 있도록 해줍니다.
- 그러나, 고도로 완성된 머리 모델을 명확한 애니메이션 제어와 함께 구축하는 것은 여전히 문제점을 내포하고 있습니다.
- 특히, 부분적 관찰(예: 깊이 센서에서 온 데이터)을 기반으로 머리 기하학을 완성하는 과정에서 세부사항을 유지하는 것이 현존하는 방법들로는 어려움을 겪습니다.
- 이 연구에서는 높은 세부사항을 동시에 보존하면서 명시적 애니메이션을 허용하는 세밀한 3D 머리 메쉬를 위한 생성 모델을 제안합니다.
- 제안된 방법은 두 단계로 훈련됩니다: 첫째, 정확한 3D 머리 스캔 데이터셋인 NPHM을 통해 각 메쉬에 대해 정점 변위와 함께 파라메트릭 머리 모델을 등록합니다.
- 추정된 변위는 수작업으로 정리된 UV 레이아웃에 저장됩니다.
- 둘째, 변위 UV 맵을 일반화시키기 위해 StyleGAN 모델을 훈련합니다.
- 파라메트릭 모델의 분해와 고품질 정점 변위는 모델의 애니메이션과 의미 있는 수정을 가능하게 합니다.
- 무조건적 생성 및 전체 또는 부분 관측에 대한 적합성을 과시하는 결과를 보여줍니다.
- 프로젝트 페이지는 https://seva100.github.io/headcraft/ 에서 확인할 수 있습니다.

