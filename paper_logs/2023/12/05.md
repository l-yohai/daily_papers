## Daily Papers (2023-12-05)

### [Magicoder: Source Code Is All You Need](https://arxiv.org/abs/2312.02120)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-m7NQI--aISLeAzdzOt8h.png)

Authors: Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang

- 'Magicoder'는 75,000개의 합성 지시 데이터를 사용하여 교육된 최대 7B 매개변수를 가진 개방 소스 대규모 언어 모델(LLMs) 시리즈로, 코드 모델링의 성능 격차를 크게 줄였습니다.
- 이 모델들은 공개 소스 코드 스니펫을 사용하는 새로운 방식인 'OSS-Instruct'를 통해 풍부한 데이터로부터 더 다양하고 현실적이며 제어 가능한 높은 품질의 코드 생성 데이터를 만듭니다.
- 'Evol-Instruct'와 같은 다른 데이터 생성 방법과의 직교성을 통해 'MagicoderS'라는 향상된 모델을 구축할 수 있습니다.
- Magicoder 및 MagicoderS는 Python 텍스트-코드 생성, 다국어 코딩, 데이터 과학 프로그램 완성 등 다양한 코딩 벤치마크에서 크기가 비슷하거나 더 큰 최신 코드 모델보다 월등한 성과를 보였습니다.
- 특히, 'CodeLlama'에 기반한 'MagicoderS-CL-7B'는 'HumanEval+'에서 성능 평가에서 'ChatGPT'보다 높은 점수를 달성했습니다(66.5 대 65.9 in pass@1).
- 전체적으로, 'OSS-Instruct'는 풍부한 공개 소스 참고 자료를 사용하는 저편향 및 고품질 지시 튜닝을 위한 새로운 방향을 제시합니다.

### [VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models](https://arxiv.org/abs/2312.00845)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/1KzMMuvCRLqHrRcChoDfk.png)

Authors: Hyeonho Jeong, Geon Yeong Park, Jong Chul Ye

- 텍스트 기반 동영상 합성 모델이 동영상 생성 분야에서 큰 진전을 이루었음에도 불구하고 특정 동작을 정확히 재현하고 다양한 시각적 변형을 창출하는 것에 어려움이 있다.
- 이러한 문제를 해결하기 위해 본 논문에서는 'Video Motion Customization (VMC)' 프레임워크를 제시하여, 비디오 확산 모델 내의 시간적 주의력 층을 조정하는 새로운 일회성 튜닝 방법을 개발하였다.
- VMC는 연속적인 프레임 사이의 잔차 벡터를 이용하여 동작을 추출하는 새로운 목표를 도입하여, 저주파 동작의 궤적을 보존하고 이미지 공간에서 관련 없는 고주파 잡음을 줄인다.
- 제안된 방법은 다양한 실제 동작과 맥락에서 최신 동영상 생성 모델에 대해 검증되었다.
- 관련 코드, 데이터 및 프로젝트 데모는 https://video-motion-customization.github.io 에서 확인할 수 있다.

### [The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning](https://arxiv.org/abs/2312.01552)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kg4oT5scURsTXbUOY3zg-.png)

Authors: Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, Yejin Choi

- 대규모 언어 모델(LLMs)의 조정 과정에는 일반적으로 감독 학습을 통한 지시 학습(Supervised Fine-Tuning, SFT)과 인간 피드백으로부터 강화 학습(Reinforcement Learning from Human Feedback, RLHF)을 사용한 선호도 조정이 포함된다.
- 최근 연구인 LIMA는 단지 1K 예시를 사용한 SFT만으로도 상당한 조정 성능을 달성할 수 있음을 보여, 조정 과정의 효과가 "피상적일" 수 있음을 제안한다.
- 연구진은 기본 LLM과 조정된 LLM 사이의 토큰 분포 변화를 분석하고, 대다수의 토큰 위치에서 거의 동일하게 해독하는 것을 발견하여 LIMA에 의해 제안된 피상적 조정 가설을 강하게 지지한다.
- 이러한 발견을 바탕으로, SFT나 RLHF 없이도 기본 LLM을 얼마나 효과적으로 조정할 수 있는지에 대한 연구 질문을 던진다.
- 연구진은 간단하고 조정이 필요 없는 조정 방법인 URIAL을 소개하며, 이는 단 3개의 일정한 스타일 예시와 시스템 프롬프트를 통해 기본 LLM만으로 효과적인 조정을 달성한다.
- JUST-EVAL-INSTRUCT라는 다양한 예시 집합에 대한 세밀하고 해석 가능한 평가를 수행하고, URIAL을 사용한 기본 LLM이 SFT나 SFT+RLHF로 조정된 LLM의 성능을 맞추거나 초과할 수 있다는 결과를 보여준다.
- 전략적 프롬프팅과 문맥 내 학습(ICL)을 통해 조정이 필요 없는 방법과 기반의 조정 방법 간의 격차를 상당히 줄일 수 있다는 것을 보여준다.
- URIAL과 관련된 결과는 LLM 연구의 미래에 있어 조정의 깊은 분석 및 이론적 이해가 중요함을 제시한다.

### [VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence](https://arxiv.org/abs/2312.02087)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/PahRPBsgHAbIPAs7E8F-u.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/PahRPBsgHAbIPAs7E8F-u.mp4" muted="false"></video></div>

Authors: Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, Kevin Tang

- 현재의 확산 기반 비디오 편집 기술은 주로 밀도 높은 대응 관계를 사용하여 시간적 일관성과 운동 정렬을 보장하며 구조를 보존하는 편집에 초점을 맞추고 있습니다.
- 그러나 대상 편집이 형태 변화를 포함할 경우 이러한 접근 방식은 종종 효과가 없습니다.
- 이러한 문제를 해결하기 위해 본 연구에서는 원본 비디오의 주요 주체를 다른 정체성과 형태를 가진 대상 주체로 교체하는 맞춤형 비디오 주체 교체에 초점을 맞춥니다.
- 본 논문에서는 주체의 움직임 궤적을 맞추고 형태를 수정하는 데 필요한 의미 있는 지점들이 몇 개 되지 않는다는 관찰에 기초하여 기존의 밀도 높은 대응 관계에 의존하는 방법 대신 의미 있는 지점 대응을 활용하는 VideoSwap 프레임워크를 소개합니다.
- 사용자가 의미 지점 대응을 처리하기 위해 지점들을 제거하거나 끌어다 놓는 등 사용자 지점 상호작용을 도입하였습니다.
- 광범위한 실험을 통해 다양한 실세계 비디오에서 최신 비디오 주체 교체 결과를 시연합니다.

### [DeepCache: Accelerating Diffusion Models for Free](https://arxiv.org/abs/2312.00858)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/A3C7QqlJUqYhP0i848_QK.png)

Authors: Xinyin Ma, Gongfan Fang, Xinchao Wang

- 확산 모델은 뛰어난 생성 능력으로 이미지 합성 분야에서 주목을 받고 있지만, 연속적인 잡음 제거 과정과 큰 모델 크기로 인해 상당한 계산 비용이 발생합니다.
- 본 논문에서는 모델 아키텍처의 관점에서 확산 모델을 가속화하는 새로운 훈련 없는 방식인 DeepCache를 소개합니다.
- DeepCache는 모델의 연속적인 잡음 제거 단계에서 관찰되는 시간적 중복성을 이용하여, 인접하는 잡음 제거 단계간의 특징을 캐시하고 검색함으로써 불필요한 계산을 줄입니다.
- U-Net의 특성을 활용하여 고위 특징을 재사용하는 동시에 저렴한 비용으로 저위 특징을 최신화합니다.
- 이 방식은 Stable Diffusion v1.5에 대해 2.3배의 속도 향상과 CLIP 점수의 0.05 감소, LDM-4-G에 대해서는 4.1배 속도 향상과 ImageNet에서 FID 점수의 0.22 감소를 가능하게 합니다.
- 실험 결과에 따르면, DeepCache는 재훈련을 필요로 하는 기존의 가지치기 및 증류 방법들보다 우수하며, 현재의 샘플링 기술과도 호환됩니다.
- 같은 처리량에서 DeepCache는 DDIM이나 PLMS와 비교하여 비슷하거나 약간 개선된 결과를 도출하며, 관련 코드는 https://github.com/horseee/DeepCache에서 확인 가능합니다.

### [Segment and Caption Anything](https://arxiv.org/abs/2312.00869)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/uVoIhAqgvhaXDcF3gKB-L.png)

Authors: Xiaoke Huang, Jianfeng Wang, Yansong Tang, Zheng Zhang, Han Hu, Jiwen Lu, Lijuan Wang, Zicheng Liu

- 본 연구에서는 Segment Anything Model(SAM)에 효율적으로 지역 캡션 생성 기능을 제공하는 방법을 제안합니다.
- SAM은 모든 것을 세분화할 수 있는 강력한 일반화 능력을 보이지만, 의미적 이해 측면에서는 부족합니다.
- 경량형 쿼리 기반 기능 믹서를 도입하여 지역별 특성을 언어 모델의 임베딩 공간과 정렬시켜 추후 캡션 생성을 위한 기반을 마련합니다.
- 훈련 가능한 매개변수의 수가 적고(일반적으로 수천만 단위), 계산 비용, 메모리 사용량 및 통신 대역폭이 적게 들어 훈련이 빠르고 확장 가능합니다.
- 지역 캡션 데이터의 부족 문제를 해결하기 위해, 먼저 객체 탐지 및 세분화 작업에 대해 모델을 사전 훈련시킬 것을 제안합니다.
- 이러한 사전 훈련 단계를 약한 감독 사전 훈련이라고 합니다. 사전 훈련 데이터는 전체 문장 설명 대신 범주 이름만을 포함합니다.
- 약한 감독 사전 훈련을 통해 공개적으로 이용 가능한 많은 객체 탐지 및 세분화 데이터 세트를 활용할 수 있습니다.
- 여러 실험을 통해 우리의 방법의 우수성을 입증하고 각 설계 선택을 검증합니다.
- 이 연구는 지역 캡션 데이터를 확장하는 방향으로 나아가는 발판을 마련하며, SAM에 지역 의미론을 효율적으로 증가시킬 방법을 탐색하는 데에 도움이 됩니다.
- 프로젝트 페이지와 관련 코드는 다음의 링크를 통해 접근할 수 있습니다: https://xk-huang.github.io/segment-caption-anything/.

### [GIVT: Generative Infinite-Vocabulary Transformers](https://arxiv.org/abs/2312.02116)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/G_9ZHDe9BdHde3XnD3dpn.png)

Authors: Michael Tschannen, Cian Eastwood, Fabian Mentzer

- 본 논문에서는 유한한 단어 집합에서 뽑은 이산 토큰 대신 실수 값으로 구성된 벡터 시퀀스를 생성하는 생성적 무한 어휘 변환기(GIVT)를 소개한다.
- 디코더 전용 변환기에 두 가지 놀라울 정도로 단순한 수정을 제안하는데, 첫째로 입력에서 유한 어휘 조회 테이블을 입력 벡터의 선형 투영으로 대체하고, 둘째로 출력에서 범주형 분포로 매핑되는 로짓 예측을 다변수 가우시안 혼합 모델의 매개변수로 대체한다.
- VQ-GAN과 MaskGIT과 같은 이미지 생성 패러다임에 영감을 받아 변환기들이 VQ-VAE의 이산 잠재 시퀀스를 모델링하는 대신, GIVT는 VAE의 양자화되지 않은 실수 값을 가진 잠재 시퀀스를 모델링하는데 사용된다.
- 이터러티브 마스킹 모델링을 사용한 클래스 조건부 이미지 생성에 GIVT를 적용할 때, MaskGIT와 경쟁력 있는 결과를 보여주며, 원인 모델링에 사용할 때는 VQ-GAN과 MaskGIT보다 우수한 성능을 보인다.
- 마지막으로, VAE 기반 변형된 UViM 프레임워크를 활용한 파노프틱 분할과 깊이 추정 분야에서도 경쟁력 있는 결과를 얻어내며 이미지 생성 외부 영역에도 접근 방식을 적용한다.

### [Segment Any 3D Gaussians](https://arxiv.org/abs/2312.00860)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/y-qGKx5OOM-XDqPOCLchM.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/y-qGKx5OOM-XDqPOCLchM.mp4" muted="false"></video></div>

Authors: Jiazhong Cen, Jiemin Fang, Chen Yang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian

- 3D 방사장에서의 상호 작용하는 세분화 과제는 3D 장면 이해와 조작에서 중요하지만 현재 기존 방법들은 세밀한 다중 세분화를 달성하거나 실시간 상호 작용을 제한하는 상당한 계산 부담에 직면해 있습니다.
- 이 논문에서는 2D 세분화 기본 모델과 최근 방사 필드의 혁신적인 돌파구인 3D Gaussian Splatting(3DGS)를 원활하게 결합하는 새로운 3D 상호 작용 세분화 접근 방법인 Segment Any 3D GAussians(SAGA)를 제시합니다.
- SAGA는 세분화 기본 모델에 의해 생성된 다중 세분화 2D 결과를 잘 설계된 대조적 학습을 통해 3D 가우시안 점 특징에 효율적으로 통합합니다.
- 기존 벤치마크에서의 평가는 SAGA가 최첨단 방법들과 경쟁력 있는 성능을 달성할 수 있음을 보여줍니다.
- SAGA는 다중 세분화를 달성하고 점, 낙서, 2D 마스크를 포함한 다양한 프롬프트에 적응할 수 있습니다.
- 특히, SAGA는 3D 세분화를 밀리초 내에 완료하여 이전 SOTA에 비해 거의 1000배의 가속을 달성합니다.
- 프로젝트 페이지는 https://jumpat.github.io/SAGA 에서 확인할 수 있습니다.

### [Object Recognition as Next Token Prediction](https://arxiv.org/abs/2312.02142)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/_HhUDxs5c9K0X9WYp9CtU.png)

Authors: Kaiyu Yue, Bor-Chun Chen, Jonas Geiping, Hengduo Li, Tom Goldstein, Ser-Nam Lim

- 이 연구에서는 객체 인식을 다음 토큰 예측 문제로 접근하는 방법을 제시합니다.
- 이미지 임베딩에서 텍스트 토큰을 자동 회귀적으로 예측하여 레이블을 형성하는 언어 디코더를 적용합니다.
- 자동 회귀 예측 과정을 기반으로 비인과적(non-causal) 주의 마스크(attention mask)를 디코더에 맞춤화하여, 서로 다른 레이블의 토큰들이 독립적이라는 점을 모델링하고, 이미지 토큰들을 접두사로 취급하는 두 가지 주요 특징을 포함합니다.
- 이 마스킹 메커니즘은 추론 중에 여러 레이블의 토큰들을 동시에 병렬로 샘플링하고 생성된 레이블을 그들의 확률에 따라 순위를 매기는 효율적인 방법인 '원샷 샘플링'을 영감합니다.
- 효율성을 더욱 향상시키기 위해, 사전 훈련된 언어 모델의 중간 블록을 단순히 버리는 간단한 전략을 통해 컴팩트한 디코더를 구축하는 방안을 제안합니다.
- 제안된 접근 방식은 전체 모델의 성능에 필적하면서 훨씬 더 효율적인 디코더를 만들어냅니다.
- 코드는 https://github.com/kaiyuyue/nxtp 에서 확인할 수 있습니다.

### [RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback](https://arxiv.org/abs/2312.00849)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/v9NxmHM0C2OKDAYbHJRZ5.png)

Authors: Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, Tat-Seng Chua

- 다중 모달 대규모 언어 모델(MLLMs)은 멀티모달 이해, 추론, 상호작용에서 뛰어난 기능을 보여주지만, 관련 이미지에 사실상 근거하지 않는 텍스트를 생성하는 환각 문제로 고통받고 있습니다.
- 이러한 환각 문제는 기존 MLLMs의 신뢰성을 감소시켜 실제 세계(특히 고위험) 응용 프로그램에서 비현실적으로 만듭니다.
- 이 도전에 대응하기 위해, 우리는 인간의 세밀한 수정 피드백에서 행동 정렬을 통해 MLLM의 신뢰성을 향상시키는 RLHF-V를 제시합니다.
- 구체적으로, RLHF-V는 환각에 대한 세그먼트 수준의 수정 형태로 인간의 선호도를 수집하고 인간의 피드백에 대한 밀도 높은 직접 우선 순위 최적화를 수행합니다.
- 자동 및 인간 평가에서 다섯 가지 벤치마크에 대한 포괄적 실험은 RLHF-V가 유망한 데이터 및 계산 효율성을 가지면서 더 신뢰할 수 있는 MLLM 행동을 가능하게 할 수 있음을 보여줍니다.
- 주목할 점은, RLHF-V는 1.4k개의 주석이 달린 데이터 샘플을 사용하여 기본 MLLM의 환각 비율을 34.8%나 대폭 줄였으며, 10k개의 주석이 달린 데이터로 훈련된 동시대 LLaVA-RLHF보다 뛰어납니다.
- 최종 모델은 개방 소스 MLLMs 중 신뢰도에서 최고 수준의 성능을 달성하며, 과잉 일반화로 인해 발생하는 환각을 방지하는 것에서 GPT-4V보다 더 나은 견고성을 보입니다.
- 우리는 코드, 모델 및 데이터를 https://github.com/RLHF-V/RLHF-V 에서 공개합니다.

### [Fast View Synthesis of Casual Videos](https://arxiv.org/abs/2312.02135)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/bOp8LHXiKmR4XSuSs6OBl.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/bOp8LHXiKmR4XSuSs6OBl.mp4" muted="false"></video></div>

Authors: Yao-Chih Lee, Zhoutong Zhang, Kevin Blackburn-Matzen, Simon Niklaus, Jianming Zhang, Jia-Bin Huang, Feng Liu

- 야외 비디오에서의 새로운 시점 합성은 장면 동적 및 시차 부족과 같은 도전 때문에 어렵다.
- 기존 방법은 암시적인 신경 복사장을 사용하여 유망한 결과를 보였으나, 훈련과 렌더링이 느리다.
- 본 논문은 단안 비디오에서 효율적으로 고화질 새 시점의 영상을 합성하기 위해 명시적 비디오 표현으로 접근한다.
- 정적 및 동적 비디오 콘텐츠를 분리하여 처리하고, 확장된 평면 기반 장면 표현을 사용하여 시간적으로 일관된 새 비디오를 합성한다.
- 평면 기반 장면 표현은 구면 조화 및 변위 맵과 결합되어 시점 의존적 효과와 비평면 복잡한 표면 기하학을 모델링한다.
- 효율성을 위해 동적 콘텐츠는 프레임별 포인트 클라우드로 표현되며, 운동으로 인해 경미한 시간적 불일치는 시각적으로 가려진다.
- 이러한 하이브리드 비디오 표현을 빠르게 추정하고 실시간으로 새로운 시점을 렌더링할 수 있는 방법을 개발한다.
- 실험 결과, 본 논문의 방법은 야외 비디오에서 고화질의 새로운 시점을 현존 최고 수준 방법과 유사한 품질로 렌더링하며, 훈련 속도는 100배 빠르고 실시간 렌더링이 가능하다.

### [Nash Learning from Human Feedback](https://arxiv.org/abs/2312.00886)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/EEa8IaSPKiMsWf3tNFvgz.png)

Authors: Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mésnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, Bilal Piot

- 인간 피드백을 통한 강화 학습(RLHF)은 대규모 언어 모델(LLMs)을 인간의 선호도와 일치시키는 주요 패러다임으로 떠오르고 있습니다.
- 전통적인 RLHF는 훈련된 LLM에 의해 생성된 텍스트 쌍 사이의 인간의 선호도를 반영한 보상 모델을 초기에 학습하는 과정을 포함합니다.
- 그 후, LLM의 정책이 보상 모델을 최대화하는 방향으로 강화 학습 알고리즘을 통해 미세 조정됩니다.
- 하지만 현재의 보상 모델은 인간 선호도의 풍부함과 샘플링 분포에 따른 의존성을 충분히 나타내지 못하는 한계가 있습니다.
- 본 연구에서는, 주어진 프롬프트에 대한 두 입력에 조건을 둔 선호 모델을 먼저 학습하는 것을 포함하는, LLMs을 미세 조정하기 위한 대안적 파이프라인을 소개합니다.
- 이 접근법은 모든 경쟁 정책보다 선호되는 반응을 일관되게 생성하는 정책을 추구함으로써, 이 선호 모델의 나쉬 균형을 정의하는 '인간 피드백으로부터의 나쉬 학습(NLHF)'으로 명명됩니다.
- 미러 디센트 원리에 기반한 새로운 알고리즘 해결책인 Nash-MD를 제시하며, 이는 정규화된 나쉬 균형에 수렴하는 정책 시퀀스를 생산합니다.
- 또한, 정책의 파라메트릭 표현을 탐구하고 심층 학습 구조를 위한 경사 하강 알고리즘을 도입합니다.
- 본 접근법의 효과를 입증하기 위해, 텍스트 요약 작업을 위한 LLM의 미세 조정과 관련한 실험 결과를 제시합니다.
- NLHF는 선호 학습과 정책 최적화 분야를 진전시킬 수 있는 매력적인 방법으로, LLMs를 인간의 선호도와 일치시키는 잠재력을 가지고 있다고 믿습니다.

### [DiffiT: Diffusion Vision Transformers for Image Generation](https://arxiv.org/abs/2312.02139)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/DhbNL5U54i7u7x0vxoWbU.png)

Authors: Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, Arash Vahdat

- 확산 모델은 강력한 표현력과 높은 샘플 품질로 인해 다양한 분야에 새로운 응용 프로그램과 사용 사례를 가능하게 했습니다.
- 샘플 생성을 위해 이 모델들은 이미지를 반복적으로 잡음 제거하는 역할을 하는 디노이징(neural network)에 의존합니다.
- 본 연구에서는 디퓨전 기반 생성 학습에서 비전 트랜스포머의 효과성을 탐구하였습니다.
- 연구팀은 U자형 인코더와 디코더를 포함하는 하이브리드 계층 구조로 구성된 새로운 모델인 Diffusion Vision Transformers (DiffiT)를 제안합니다.
- 새로운 시간-의존적 자기 주의 모듈을 도입하여 주의 계층이 효율적으로 잡음 제거 과정의 다른 단계에서 행동을 적응시킬 수 있도록 하였습니다.
- 고해상도 이미지 생성을 위해 제안된 자기 주의 계층을 갖는 트랜스포머 모델인 잠재 DiffiT 또한 소개합니다.
- DiffiT는 고품질 이미지를 생성하는 데 놀라울 정도로 효과적이며, 다양한 클래스 조건부 및 비조건부 합성 작업에서 최신 기술(SOTA) 기준을 달성했습니다.
- 잠재 공간에서 DiffiT는 ImageNet-256 데이터셋에서 새로운 최상의 SOTA FID 점수 1.73을 달성했습니다.
- 연구 결과에 대한 코드는 https://github.com/NVlabs/DiffiT 링크를 통해 공유되었습니다.

### [Style Aligned Image Generation via Shared Attention](https://arxiv.org/abs/2312.02133)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/q5Nmf36N7XauY7PL3a9jB.png)

Authors: Amir Hertz, Andrey Voynov, Shlomi Fruchter, Daniel Cohen-Or

- 대규모의 텍스트-이미지(Text-to-Image, T2I) 모델들이 창작 분야에서 빠르게 도입되며, 텍스트 프롬프트로부터 시각적으로 매력적인 이미지를 생성하고 있습니다.
- 그러나, 이러한 모델들을 제어하여 일관된 스타일을 유지하는 것은 여전히 도전적이며, 기존 방법들은 콘텐츠와 스타일을 분리하기 위해 미세조정과 수동 개입을 필요로 합니다.
- 본 논문에서는 일련의 생성된 이미지 사이에 스타일 정렬을 확립하도록 고안된 새로운 기술인 StyleAligned를 소개합니다.
- StyleAligned 방법은 확산 과정 동안 최소한의 ‘주의 공유(attention sharing)’를 사용하여 T2I 모델 내에서 이미지 간의 스타일 일관성을 유지합니다.
- 이 접근법은 간단한 역전 연산을 통해 참조 스타일을 사용하여 스타일이 일관된 이미지를 생성할 수 있게 합니다.
- 다양한 스타일과 텍스트 프롬프트에 걸쳐 실시된 방법 평가는 높은 품질의 합성과 충실성을 보여주며, 다양한 입력에서 일관된 스타일을 달성하는 데 있어 이 기술의 효과성을 강조합니다.

### [SANeRF-HQ: Segment Anything for NeRF in High Quality](https://arxiv.org/abs/2312.01531)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/m_EQLfh0E0ctOCzvE53-Q.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/m_EQLfh0E0ctOCzvE53-Q.mp4" muted="false"></video></div>

Authors: Yichen Liu, Benran Hu, Chi-Keung Tang, Yu-Wing Tai

- 최근 ‘Segment Anything Model (SAM)’은 제로샷 세그멘테이션의 뛰어난 능력을 보이는 한편, ‘NeRF(Neural Radiance Fields)’는 새로운 시점 합성 이상의 다양한 3차원 문제들에 인기를 얻고 있는 방법론입니다.
- 복잡한 상황에서 객체를 정확하고 일관되게 세분화하는 문제에 직면하여, 우리는 주어진 장면에서 어떤 객체든 고품질로 3D 세분화를 달성하는 ‘SANeRF-HQ(Segment Anything for NeRF in High Quality)’를 제안합니다.
- SANeRF-HQ는 사용자가 제공한 프롬프트에 의해 안내되는 오픈-월드 객체 세그멘테이션을 위해 SAM을 사용하고 다양한 관점에서 정보를 집계하기 위해 NeRF를 활용합니다.
- 집계하는 동안 세그멘테이션 경계의 정확성을 높이기 위하여 밀도 필드와 RGB 유사성을 사용합니다.
- 우리는 고품질의 기준진실(ground-truth)을 갖추고 있거나 수동으로 주석이 달린 여러 NeRF 데이터 세트에서 정량적으로 우리의 방법을 평가하여 세그멘테이션 정확도를 강조합니다.
- SANeRF-HQ는 이전의 최첨단 방법보다 NeRF 객체 세그멘테이션에서 상당한 품질 향상을 보여주며, 객체 지역화에 대한 더 높은 유연성을 제공하고, 다중 뷰에 걸쳐 더 일관된 객체 세그멘테이션을 가능하게 합니다.
- 추가적인 정보는 https://lyclyc52.github.io/SANeRF-HQ/ 에서 찾아볼 수 있습니다.

### [VideoRF: Rendering Dynamic Radiance Fields as 2D Feature Video Streams](https://arxiv.org/abs/2312.01407)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/98ZAZyaKihGgUMesFXzdh.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/98ZAZyaKihGgUMesFXzdh.mp4" muted="false"></video></div>

Authors: Liao Wang, Kaixin Yao, Chengcheng Guo, Zhirui Zhang, Qiang Hu, Jingyi Yu, Lan Xu, Minye Wu

- 신경 방사선 필드(NeRFs)는 정지된 장면을 사실적으로 렌더링하는 데에 탁월하지만, 동적이고 오래 지속되는 방사선 필드를 휴대용 장치에서 렌더링하기 위한 데이터 저장 및 계산 제약으로 인해 어려움이 있습니다.
- 'VideoRF'는 모바일 플랫폼에서 동적 방사선 필드의 실시간 스트리밍과 렌더링을 가능하게 하는 첫 번째 접근 방식으로, 2D 특징 이미지 스트림이 4D 방사선 필드를 일체화하여 표현합니다.
- 2D 도메인에 직접 적용된 맞춤형 훈련 방식을 도입해, 특징 이미지 스트림의 시간적 및 공간적 중복을 조성합니다.
- 중복을 활용함으로써, 특징 이미지 스트림은 2D 비디오 코덱에 의해 효과적으로 압축되며, 이를 통해 실시간 디코딩을 실현하는 비디오 하드웨어 가속기의 장점을 활용할 수 있습니다.
- 또한, 특징 이미지 스트림을 기반으로, 방사도 속성을 효율적으로 쿼리하는 특별한 공간 매핑이 있는 VideoRF용 새로운 렌더링 파이프라인을 제안합니다.
- 지연된 쉐이딩 모델과 함께, VideoRF는 효율성 덕분에 모바일 장치에서 실시간 렌더링을 수행할 수 있으며, 다양한 장치에서 온라인 스트리밍 및 동적 장면 렌더링을 가능하게 하는 실시간 상호 작용 플레이어를 개발했습니다.

### [GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis](https://arxiv.org/abs/2312.02155)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wHj8hN2pSLBJpRWXheOOM.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wHj8hN2pSLBJpRWXheOOM.mp4" muted="false"></video></div>

Authors: Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu

- 'GPS-Gaussian'이라는 새로운 접근법을 제안하여 캐릭터의 실시간으로 새로운 시점을 합성할 수 있게 하며, 이를 통해 희소한 카메라 설정에서도 2K 해상도 렌더링이 가능합니다.
- 기존의 가우시안 스플래팅 혹은 신경 망 암시적 렌더링 방법과 달리, 대상별 최적화가 필요 없으며, 소스 뷰에 정의된 가우시안 파라미터 맵을 도입해 즉시 새로운 시점을 합성할 수 있는 가우시안 스플래팅 속성을 직접 회귀합니다.
- 대량의 인간 스캔 데이터에 대한 가우시안 파라미터 회귀 모듈을 깊이 추정 모듈과 함께 훈련하여 2D 파라미터 맵을 3D 공간으로 끌어올리며, 제안한 프레임워크는 완전히 미분 가능합니다.
- 실험 결과에서 본 방법은 현재 최고의 기법들을 뛰어넘으면서도 렌더링 속도가 매우 빠르다는 것을 입증함으로써, 여러 데이터셋에서 우수한 성능을 보여줍니다.

### [Generative Powers of Ten](https://arxiv.org/abs/2312.02149)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SahTp-Q9XHf70R3NapR6C.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SahTp-Q9XHf70R3NapR6C.mp4" muted="false"></video></div>

Authors: Xiaojuan Wang, Janne Kontkanen, Brian Curless, Steve Seitz, Ira Kemelmacher, Ben Mildenhall, Pratul Srinivasan, Dor Verbin, Aleksander Holynski

- 본 논문에서는 광각의 자연 풍경으로부터 나무 가지 위에 앉은 곤충의 크로즈업 장면에 이르는 극단적인 의미론적 확대를 가능하게 하는 여러 이미지 스케일에 걸쳐 일관된 컨텐츠를 생성하는 텍스트-이미지 모델을 제안합니다.
- 다양한 스케일에 걸친 일관성을 유지하면서 각각의 샘플링 과정의 독립성을 보존하는 공동 멀티스케일 확산 샘플링 접근법을 통해 이를 달성합니다.
- 각 생성된 스케일은 다른 텍스트 프롬프트에 의해 안내되며, 우리의 방법은 기존의 슈퍼 해상도 기법들이 매우 다른 스케일에서 새로운 문맥적 구조를 생성하는 데 어려움을 겪는 것보다 더 깊은 수준의 줌을 가능하게 합니다.
- 이미지 슈퍼 해상도 및 아웃페인팅과 같은 대안적 기술과의 질적 비교를 통해, 저희 방법이 여러 스케일에 걸친 일관된 컨텐츠 생성에 있어 가장 효과적임을 보여줍니다.

### [Rejuvenating image-GPT as Strong Visual Representation Learners](https://arxiv.org/abs/2312.02147)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/A3jcR_RFvhJ1119SVv93n.png)

Authors: Sucheng Ren, Zeyu Wang, Hongru Zhu, Junfei Xiao, Alan Yuille, Cihang Xie

- 이 논문은 시각적 표현 학습을 위해 다음 픽셀을 예측하는 자동회귀 전처리 방식을 도입한 선도적 연구인 image-GPT(iGPT)를 향상시켰습니다.
- 첫 번째 중요한 변화는 원시 픽셀에서 의미론적 토큰으로 예측 대상을 변경하여, 시각 콘텐츠의 보다 상위 수준의 이해를 가능하게 했습니다.
- 두 번째 개선점은 다음 토큰뿐만 아니라 보이는 토큰도 예측하도록 모델을 보완하는 것으로, 자동회귀 모델링을 보완했습니다.
- 이러한 접근 방식을 디스크리미네이티브하게 훈련된 모델들, 예를 들어 CLIP에 의해 인코딩된 의미론적 토큰과 결합하면 특히 효과적입니다.
- D-iGPT라고 하는 이 새로운 접근법은 광범위한 실험을 통해 강력한 시각적 표현 학습자로서의 우수성을 입증하였으며, 주목할 만한 성과로는 공개 데이터셋만으로 ImageNet-1K 데이터셋에서 뛰어난 성능을 달성했습니다.
- 바닐라 ViT-Large 모델을 사용하여 D-iGPT는 89.5%의 top-1 정확도를 기록했습니다.
- 이 모델은 다운스트림 작업에서의 강력한 일반화 능력과, 분포 외 샘플에서의 견고함을 보여주었습니다.
- 관련 코드는 https://github.com/OliverRensu/D-iGPT 에서 제공됩니다.

### [Using Large Language Models to Accelerate Communication for Users with Severe Motor Impairments](https://arxiv.org/abs/2312.01532)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xEgNCjP26quCtrWpWqsCA.png)

Authors: Shanqing Cai, Subhashini Venugopalan, Katie Seaver, Xiang Xiao, Katrin Tomanek, Sri Jalasutram, Meredith Ringel Morris, Shaun Kane, Ajit Narayanan, Robert L. MacDonald, Emily Kornman, Daniel Vance, Blair Casey, Steve M. Gleason, Philip Q. Nelson, Michael P. Brenner

- 중증의 운동 장애가 있는 사용자를 위한 텍스트 입력 속도 향상을 위한 연구가 지속적으로 이루어져 왔습니다.
- 향상된 텍스트 입력을 위한 전략과 사용자 인터페이스를 재고할 새로운 기회를 자연어 처리의 신경망 발전이 제공하고 있습니다.
- 연구진은 'SpeakFaster'라는 시스템을 개발했으며, 이는 대규모 언어 모델(Large Language Models, LLMs)과 함께 고안된 사용자 인터페이스를 포함하여, 전통적인 예측 키보드보다 57% 더 많은 운동 동작을 절약할 수 있게 해줍니다.
- 비장애인 참가자 19명이 모바일 기기에서 수동으로 타이핑하는 파일럿 연구에서, 오프라인 시뮬레이션과 일치하는 운동 절약 효과를 보였으며, 전반적인 타이핑 속도에는 작은 영향만 미쳤습니다.
- 근위축성 측색경화증(ALS)을 앓고 있는 두 명의 눈동자 추적 타이핑 사용자에 대한 실험실 및 현장 테스트에서는 기존 방식보다 29-60% 빠른 텍스트 입력 속도를 보였습니다.
- 이는 맥락 인식 LLMs로부터의 구문 및 단어 예측을 통해 많은 키스트로크를 절약할 수 있었기 때문입니다.
- 이 연구 결과는 운동 장애가 있는 사용자를 위한 텍스트 기반 커뮤니케이션을 크게 가속화하기 위한 추가적인 탐색을 위한 강력한 기초를 제공하고, LLMs를 텍스트 기반 사용자 인터페이스에 적용하는 방향을 보여줍니다.

### [TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents](https://arxiv.org/abs/2312.01279)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6TVVPQk6rBfr_lqqHbtTU.png)

Authors: James Enouen, Hootan Nakhost, Sayna Ebrahimi, Sercan O Arik, Yan Liu, Tomas Pfister

- 큰 언어 모델들(Large language models, LLMs)은 점점 정확한 응답과 일관된 추론 능력으로 실제 응용 프로그램에서의 관심이 커지고 있으며, 복잡한 추론 과정을 사용하는 '블랙 박스'로서 생성된 내용에 대한 신뢰할 수 있고 확장 가능한 설명의 수요도 증가할 것으로 예상됩니다.
- 지난 십년 간 신경망 모델의 해석 가능성에 중요한 발전이 있었으나, 특히 샤플리 값과 같은 사후 해석 방법이 딥러닝 모델 해석에 효과적임을 입증하고 있음에도 불구하고, 수천 개의 토큰이 포함된 긴 입력 컨텍스트와 자동 회귀적으로 생성된 출력 시퀀스를 다룰 때 샤플리 값의 확장에는 여전히 주요 과제가 있습니다.
- 본 논문에서는 언어 모델 특정 기술을 통합한 효율적인 사후 설명 방법인 TextGenSHAP을 소개하며, 이 방법이 기존의 샤플리 값 계산 방식에 비해 속도 면에서 현저한 향상을 보여주며, 토큰 수준 설명의 경우 몇 시간에서 몇 분으로, 문서 수준 설명의 경우 몇 초로 처리 시간을 단축시킵니다.
- 또한 본 논문에서는 실시간 샤플리 값을 두 가지 중요한 시나리오에서 활용하는 방법을 보여줍니다: 길이가 긴 문서에 있는 질문 응답을 더 잘 이해하기 위해 중요한 단어와 문장을 찾아내는 것, 그리고 선별된 문장들의 정확성을 향상시켜 최종 응답의 향상을 통해 기존의 문서 검색 시스템을 개선하는 것입니다.

### [Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models](https://arxiv.org/abs/2312.01409)

[Watch Video]https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/mkfeU2AJOKh-gTWQvbrC5.mp4
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/mkfeU2AJOKh-gTWQvbrC5.mp4" muted="false"></video></div>

Authors: Shengqu Cai, Duygu Ceylan, Matheus Gadelha, Chun-Hao Paul Huang, Tuanfeng Yang Wang, Gordon Wetzstein

- 컴퓨터 생성 비디오의 번거로운 수작업 과정을 자동화하기 위해, 기존의 3차원(3D) 콘텐츠 생성 도구의 제어 가능성과 신흥 확산 모델의 표현력 및 편집 가능성을 결합한 새로운 접근 방식을 제시합니다.
- 이 방법은 동적인 3D 메쉬가 저해상도로 렌더링된 애니메이션을 입력으로 받아, 해당 메쉬로부터 얻은 실제 대응 정보를 사전 훈련된 텍스트 대 이미지 생성 모델의 여러 단계에 적용함으로써 고품질이고 시간적으로 일관된 프레임을 출력합니다.
- 제안된 접근법은 리깅된 자산을 애니메이션하는 것이나 카메라 경로를 변경하는 것 등, 다양한 방식으로 움직임을 구현할 수 있는 예시를 통해 시연됩니다.
- 비디오 확산 모델이 아직 완벽하게 제어가 어렵기 때문에, 사용자의 창의성을 적용하기보다는 확대하는 데 제한이 있었는데, 이 문제를 해결하기 위한 연구입니다.

### [Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training](https://arxiv.org/abs/2312.01663)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xOyN6ZHeXEAMBGJfs5Efr.png)

Authors: Runze He, Shaofei Huang, Xuecheng Nie, Tianrui Hui, Luoqi Liu, Jiao Dai, Jizhong Han, Guanbin Li, Si Liu

- 본 논문에서는 텍스트 설명 또는 참조 이미지를 편집 프롬프트로 사용하여 적응형 출처 주도 3D 장면 편집 작업을 위한 CustomNeRF 모델을 제안한다.
- 전경 영역만 정확하게 편집하고 단일 시점 참조 이미지를 사용할 때 다양한 시점 간의 일관성을 유지하는 것은 쉽지 않은 두 가지 주요 문제에 직면한다.
- 첫 번째 문제를 해결하기 위해, 전경 영역 편집과 전체 이미지 편집 사이를 번갈아 가며 전경 조작에 초점을 맞추면서 배경을 유지하는 Local-Global Iterative Editing (LGIE) 훈련 방식을 제안한다.
- 두 번째 문제를 해결하기 위해, 이미지 기반 편집에서 다양한 시점 간의 불일치 문제를 완화하기 위해 생성 모델 내의 클래스 선행 정보를 활용하는 클래스 가이드 정규화를 설계한다.
- 광범위한 실험을 통해 CustomNeRF는 텍스트 및 이미지 기반 설정 모두에서 다양한 실제 장면에 대한 정확한 편집 결과를 생성함을 보여준다.

