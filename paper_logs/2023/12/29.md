## Daily Papers (2023-12-29)

### [TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones](https://arxiv.org/abs/2312.16862)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/cnYG14kZlMu8zhuzNA1jo.png)

Vote: 16

Authors: Zhengqing Yuan, Zhengqing Yuan, Zhaoxu Li, Lichao Sun

- GPT-4V와 같은 멀티모달 대규모 언어 모델(MLLM)은 언어와 시각 요소를 통합하는 데 중요한 발전을 이루었으나, 이러한 모델들은 대체로 소스가 공개되지 않고 상당한 계산 능력을 요구합니다.
- 공개된 MLLM 예로는 LLaVA 및 MiniGPT-4가 있으며, 이러한 모델들도 나름의 성공을 달성했지만 계산 효율성 문제는 여전히 해결되지 않았습니다.
- 이러한 문제를 해결하기 위해, TinyGPT-V는 일반적인 컴퓨팅 환경에서도 우수한 성능을 제공하는 새로운 모델을 소개하며, 교육용으로는 24G GPU가, 추론용으로는 8G GPU 또는 CPU가 필요합니다.
- TinyGPT-V는 효과적인 언어 백본과 BLIP-2 또는 CLIP에서 사전 훈련된 시각 모듈을 결합하여 구축되었으며, 2.8B 매개변수를 가지고 있습니다.
- 이 모델은 고유한 양자화 과정을 통해 8G 장치에서 로컬 배치 및 추론 작업에 적합하며, 다양한 실제 시나리오에서 사용할 수 있도록 확장되었습니다.
- 이 연구는 저비용, 효율성, 그리고 높은 성능을 제공하는 MLLM을 설계하고, 그 적용 가능성을 넓히기 위한 새로운 패러다임을 제안합니다.
- 해당 연구의 코드와 훈련 가중치는 GitHub과 Huggingface에 각각 공개되어 있습니다(https://github.com/DLYuanGod/TinyGPT-V 및 https://huggingface.co/Tyrannosaurus/TinyGPT-V).

### [MobileVLM : A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices](https://arxiv.org/abs/2312.16886)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/fMS0DdmeK55Z4fm5yOz2i.png)

Vote: 13

Authors: Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, Chunhua Shen

- MobileVLM은 모바일 기기에서 작동을 목표로 하는 유능한 다중모달 비전 언어 모델(MMVLM)을 소개합니다.
- 이 모델은 모바일 지향적인 다양한 아키텍처 설계와 기술의 결합으로, 처음부터 훈련된 1.4B 및 2.7B 매개변수 규모의 언어 모델들을 포함합니다.
- 또한 CLIP 방식으로 사전 훈련된 다중모달 비전 모델과 효율적인 프로젝터를 통한 교차 모달리티 상호작용이 특징입니다.
- MobileVLM은 여러 일반적인 VLM 벤치마크에서 평가되며, 훨씬 더 큰 모델들과 견줄 만한 성능을 보여줍니다.
- 더욱 중요한 것은, Qualcomm Snapdragon 888 CPU와 NVIDIA Jeston Orin GPU에서의 추론 속도를 측정했을 때 각각 초당 21.5 토큰, 65.3 토큰이라는 업계 최고의 성능을 달성했다는 점입니다.
- 이 모델의 코드는 https://github.com/Meituan-AutoML/MobileVLM에서 공개될 예정입니다.

### [Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action](https://arxiv.org/abs/2312.17172)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3Fk92GjOlQB5Gb5mIXPtC.png)

Vote: 10

Authors: Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, Aniruddha Kembhavi

- Unified-IO 2는 이미지, 텍스트, 오디오, 그리고 액션을 이해하고 생성할 수 있는 최초의 자동 회귀형 다중 모달 모델을 제안합니다.
- 이 모델은 이미지, 텍스트, 오디오, 액션, 경계 상자 등 다양한 모달리티의 입력과 출력을 공유된 의미 공간으로 토크나이즈 한 후 단일 인코더-디코더 트랜스포머 모델로 처리합니다.
- 다양한 모달리티로 트레이닝 하는 것이 어려움을 극복하고자, 모델 트레이닝을 안정화시킬 수 있는 다양한 구조적 개선 방안을 제안합니다.
- Unified-IO 2는 다양한 소스에서 대규모 다중 모달 전처리 말뭉치를 바탕으로 처음부터 트레이닝하며, 다중 모달 제거자의 목적을 달성하는 믹스 모델을 사용합니다.
- 다중 모달 지시 사항을 따르는 등의 다양한 기술을 학습하기 위해 120개의 데이터셋으로 구성하고 프롬프트 및 증강을 통해 파인튜닝합니다.
- Unified-IO 2는 단일 통합 모델을 사용하여 GRIT 벤치마크에서 최첨단 성능을 달성할 뿐만 아니라 이미지 생성 및 이해, 자연어 이해, 비디오 및 오디오 이해, 로봇 조작 등 35개 이상의 벤치마크에서 강력한 결과를 보여줍니다.
- 연구 커뮤니티를 위해 모든 모델을 공개합니다.

### [Unsupervised Universal Image Segmentation](https://arxiv.org/abs/2312.17243)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/HxU8Swn_N4SL_cZlfE_WK.png)

Vote: 9

Authors: Dantong Niu, Xudong Wang, Xinyang Han, Long Lian, Roei Herzig, Trevor Darrell

- 이 논문은 기존의 수작업으로 주석된 세그멘테이션 마스크가 필요 없는 비지도 이미지 세그멘테이션 방법을 제안합니다.
- 지금까지의 모델들은 의미론적 세그멘테이션과 클래스 무관 인스턴스 세그멘테이션을 별도로 다루었으며, 팬옵틱 세그멘테이션을 처리하지 못했습니다.
- 본 논문에서는 인스턴스, 의미론적, 팬옵틱 세그멘테이션의 다양한 이미지 세그멘테이션 작업을 수행할 수 있는 비지도 범용 세그멘테이션 모델(U2Seg)을 제안합니다.
- U2Seg는 자기 지도형 모델을 활용하여 각 클러스터를 통해 픽셀의 다른 의미론적 및/또는 인스턴스 멤버십을 나타내는 가짜 의미론적 레이블을 생성합니다.
- 가짜 의미론적 레이블을 사용하여 모델을 자가 훈련시킴으로써, 특정 작업에 맞춰진 방법들에 비해 상당한 성능 향상을 달성했습니다.
- 또한, 이전에 탐구되지 않았던 비지도 팬옵틱 세그멘테이션에 대한 새로운 기준선을 설정합니다.
- U2Seg는 소수샷 세그멘테이션을 위한 강력한 사전 훈련된 모델로, 적은 데이터(예: COCO 레이블의 1%만)에서 훈련했을 때 CutLER보다 AP^{mask}에서 +5.0의 향상을 보여줍니다.
- 이 간단하면서도 효과적인 방법이 비지도 범용 이미지 세그멘테이션에 대한 더 많은 연구를 촉진하기를 바랍니다.

### [DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision](https://arxiv.org/abs/2312.16256)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/JK3oEh-KqgDFbWQhNvP3v.png)

Vote: 9

Authors: Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, Xuanmao Li, Xingpeng Sun, Rohan Ashok, Aniruddha Mukherjee, Hao Kang, Xiangrui Kong, Gang Hua, Tianti Zhang, Bedrich Benes, Aniket Bera

- 딥러닝 기반 3D 비전의 상당한 진보가 있었으나, 기존의 장면 레벨 데이터셋은 합성 환경이나 제한된 실세계 장면에 국한되어 현재 방법론의 포괄적 벤치마크를 방해하고 향후 3D 분석 연구를 제한한다.
- 이러한 중요한 격차를 메우기 위해, 'DL3DV-10K'라는 대규모 장면 데이터셋을 제시하였으며, 여기에는 다양한 반사, 투명도 및 조명 수준을 갖춘 경계가 있는/없는 장면에서 포착한 51.2백만 프레임이 포함된 10,510개의 비디오가 있다.
- 이 데이터셋은 65종의 관심 지점(Point-of-Interest, POI) 위치에서 촬영되었으며, 최근의 NVS(새로운 뷰 합성) 방법론에 대한 종합적 벤치마크를 실시하여 미래 연구에 대한 유익한 통찰을 제공한다.
- DL3DV-10K를 사용한 NeRF(신경 복사체 분야) 범용 학습에 대한 예비 연구에서 고무적인 결과를 얻어, 대규모 장면 레벨 데이터셋이 3D 표현 학습을 위한 기초 모델을 향한 길을 마련하는 데 필수적임을 나타낸다.
- DL3DV-10K 데이터셋, 벤치마크 결과 및 모델은 https://dl3dv-10k.github.io/DL3DV-10K/에서 공개적으로 접근 가능하다.

### [Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math](https://arxiv.org/abs/2312.17120)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/x-ODLJJrsprV6XCO-qx_2.png)

Vote: 9

Authors: Zengzhi Wang, Zengzhi Wang, Rui Xia, Pengfei Liu

- 본 논문은 약 95억 개의 토큰으로 구성된 다양하고 고품질의 수학 중심 데이터셋인 MathPile을 소개한다.
- "더 적은 것이 더 많은 것"이라는 원칙을 따라 데이터의 양보다 질을 중시하는 전략을 유지하면서 사전 훈련 단계에서도 우수한 데이터 품질을 확보했다.
- 복잡한 전처리, 사전 필터링, 언어 식별, 정제, 필터링 및 중복 제거를 포함하는 철저한 데이터 수집 및 처리 작업을 수행했다.
- 또한, 하류 벤치마크 테스트 세트에서 데이터 오염 감지를 실행하여 중복을 제거했다.
- 연구자들은 언어 모델의 수학적 추론 능력을 강화하기 위해 MathPile을 사용할 수 있기를 기대하며, 이 분야의 미래 발전을 도울 수 있도록 MathPile의 다양한 버전과 처리에 사용된 스크립트를 오픈 소스로 공개할 계획이다.

### [DreamGaussian4D: Generative 4D Gaussian Splatting](https://arxiv.org/abs/2312.17142)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/O0Cldl1HJhjpawKNyZPwK.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/O0Cldl1HJhjpawKNyZPwK.mp4" muted="false"></video></div>

Vote: 8

Authors: Jiawei Ren, Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, Ziwei Liu

- 최근 4D 컨텐츠 생성 분야에서 눈에 띄는 발전이 이루어졌으나 기존 방법들은 긴 최적화 시간, 동작 조절 능력 부족, 낮은 디테일 수준 등의 문제점을 갖고 있습니다.
- 본 논문에서는 DreamGaussian4D라는 효율적인 4D 생성 프레임워크를 소개하며, 이는 4D 가우시안 스플래팅 표현을 기반으로 구축됩니다.
- 우리의 주요 통찰은 가우시안 스플래팅의 공간 변환을 명시적으로 모델링하는 것이 암묵적 표현에 비해 4D 생성 설정에 더 적합하다는 점입니다.
- DreamGaussian4D는 최적화 시간을 몇 시간에서 몇 분으로 단축시키고, 생성된 3D 동작의 유연한 제어를 가능하게 하며, 효율적으로 렌더링할 수 있는 애니메이션 메시를 생성합니다.

### [City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web](https://arxiv.org/abs/2312.16457)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/CM8xP4tRQmFOqY0bF_v0I.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/CM8xP4tRQmFOqY0bF_v0I.mp4" muted="false"></video></div>

Vote: 6

Authors: Kaiwen Song, Juyong Zhang

- NeRF는 다양한 환경에 걸쳐 정교한 세부 사항을 포착하여 3D 장면 재구성을 크게 발전시켜 왔으나, 대규모 장면에 적용할 경우 실시간 체험을 제공하는데 어려움을 겪는다.
- 본 논문에서는 'City-on-Web'을 제안하여 전체 장면을 관리 가능한 블록으로 분할하고 각 블록에 대한 세부 수준을 지정하여 높은 충실도와 효율적인 메모리 관리 및 빠른 렌더링을 보장한다.
- 특히, 웹상에서의 최종 렌더링 결과가 훈련 시와 일관되도록 훈련 및 추론 과정을 세심하게 설계하여 자원이 제한된 환경에서 대규모 장면의 실시간 렌더링을 처음으로 달성했다.
- 이러한 새로운 표현 방식과 엄선된 훈련/추론 과정 덕분에 웹 플랫폼에서 대규모 장면을 실시간으로 렌더링할 수 있게 되었으며, RTX 3060 GPU를 사용하여 1080P 해상도에서 초당 32프레임을 달성하면서도 최신 기술 수준의 품질과 매우 근접한 결과를 제공한다.
- 프로젝트 페이지는 https://ustc3dv.github.io/City-on-Web/ 에서 확인할 수 있다.

### [I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models](https://arxiv.org/abs/2312.16693)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/XaWTqlx6r3Npdrc_A0SiX.png)

Vote: 6

Authors: Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Chongyang Ma, Weiming Hu, Zhengjun Zha, Haibin Huang, Pengfei Wan, Di Zhang

- 본 논문은 고정된 이미지를 동적이고 생생한 비디오 시퀀스로 변환하는 이미지-투-비디오(I2V)의 복잡한 문제에 대해 다루며, 기존 이미지 정보를 유지하는 동시에 비디오 확산 모델에 초점을 두고 있습니다.
- 기존 방법은 이미지를 확산 과정에 통째로 통합하거나 사전 학습된 인코더를 사용하여 cross attention을 구현하는 것이 일반적이지만, 이는 텍스트-투-이미지(T2I) 모델의 근본적인 가중치를 변경해야 하며 이에 따른 재사용성 제한이 있습니다.
- 저희는 T2I 모델과 그 모션 모듈의 구조적 무결성을 유지하면서 제한을 극복할 수 있는 새로운 솔루션인 I2V-Adapter를 소개합니다.
- I2V-Adapter는 가벼운 어댑터 모듈을 활용하여 입력 이미지와 병렬로 노이즈가 있는 비디오 프레임을 처리하는 방식으로 작동하며, T2I 모델의 구조 변화 없이 공간적인 세부 정보를 보존합니다.
- 또한, I2V-Adapter는 전통적인 모델들에 비해 훨씬 적은 파라미터가 필요하며, 기존의 커뮤니티 중심의 T2I 모델 및 제어 도구와 호환성을 지닙니다.
- 실험 결과, I2V-Adapter는 높은 품질의 비디오 출력을 생성할 수 있으며, 이러한 성능, 다양성, 그리고 훈련 가능한 파라미터의 필요성을 줄인 점은 AI 기반 비디오 생성 분야, 특히 창의적 응용 프로그램 측면에서의 중요한 진보를 나타냅니다.

### [Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis](https://arxiv.org/abs/2312.16812)

[Watch Video](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/2KVqAH5NRngctsRcb8C9G.mp4)
<div><video controls src="https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/2KVqAH5NRngctsRcb8C9G.mp4" muted="false"></video></div>

Vote: 5

Authors: Zhan Li, Zhang Chen, Zhong Li, Yi Xu

- 동적 장면의 새로운 시점 합성은 매혹적이지만 도전적인 문제입니다.
- 고해상도 사실적 결과, 실시간 렌더링, 그리고 컴팩트한 저장을 동시에 달성하는 것은 여전히 어려운 과제입니다.
- 이러한 도전에 대응하기 위해, 우리는 시공간 가우시안 특징 스플래팅이라는 새로운 동적 장면 표현 방식을 제안합니다.
- 우리는 3D 가우시안을 강화하여 시간적 불투명도와 매개변수적인 운동/회전을 결합한 표현력 있는 시공간 가우시안을 도입했습니다.
- 이는 정적, 동적 및 일시적 컨텐츠를 하나의 장면 내에서 포착할 수 있도록 합니다.
- 두 번째로, 구형 조화를 신경망 특징으로 대체하는 스플래트된 특징 렌더링을 소개합니다.
- 이러한 특징들은 시야 및 시간 의존적 외관을 모델링하는 동시에 작은 크기를 유지할 수 있도록 합니다.
- 마지막으로, 우리는 훈련 오류와 거친 깊이의 안내를 활용하여 기존 파이프라인으로 수렴하기 어려운 영역에서 새로운 가우시안을 샘플링합니다.
- 여러 실세계 데이터셋에서의 실험을 통해, 우리의 방법이 최신 렌더링 품질과 속도에서 최고의 성능을 달성함을 보여줍니다.
- 우리의 라이트 버전 모델은 Nvidia RTX 4090 GPU에서 8K 해상도로 60 FPS에서 렌더링을 할 수 있습니다.

### [The LLM Surgeon](https://arxiv.org/abs/2312.17244)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/HAxZCWnzVLCT7TBgSb6wV.png)

Vote: 5

Authors: Tycho F. A. van der Ouderaa, Markus Nagel, Mart van Baalen, Yuki M. Asano, Tijmen Blankevoort

- 최신 언어 모델은 점점 더 대규모의 텍스트 데이터에서 상위 성능을 달성하기 위해 규모가 확장되고 있지만, 트랜스포머 아키텍처의 크기 때문에 여러 제약 조건 내에서 모델을 배포하기 어려운 문제가 있다.
- 이 연구에서는 작은 모델을 처음부터 훈련시키는 대안으로 기존에 사전 훈련된 모델의 데이터 기반 압축을 탐구한다.
- 연구팀은 크로네커 곱으로 근사된 대상 손실 환경의 스케일링을 통해 구조물의 동적 할당 및 제거에 따라 남은 가중치의 업데이트를 계산한다.
- 제시된 일반 프레임워크는 비구조적, 반구조적, 구조적 가지치기(pruning)를 포함하며, 가중치 간의 상관 관계를 더 많이 포착하면서도 계산 효율성을 유지하도록 가중치 업데이트를 개선한다.
- 실험적으로, 이 방법은 OPT 모델 범위와 Llamav2-7B의 행과 열을 20%-30% 가지치기하여 성능 손실을 거의 없이 달성하고, 대용량 언어 모델의 비구조적 및 반구조적 가지치기에서 최신 결과를 보여준다.

### [SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation](https://arxiv.org/abs/2312.16272)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/28foWMttWWeRjquvVyc-U.png)

Vote: 4

Authors: Yuxuan Zhang, Jiaming Liu, Yiren Song, Rui Wang, Hao Tang, Jinpeng Yu, Huaxia Li, Xu Tang, Yao Hu, Han Pan, Zhongliang Jing

- 최근 주제 중심의 이미지 생성에서 제로샷 생성이 가능해졌지만, 중요한 주제 표현을 정확하게 선택하고 집중하는 것은 여전히 어려운 과제로 남아있다.
- 이 문제를 해결하기 위해, 단일 또는 다중 참조 이미지에서 모든 주제를 선택적으로 캡처하는 것을 목표로 하는 새로운 아키텍처인 SSR-Encoder를 소개한다.
- SSR-Encoder는 텍스트 및 마스크를 포함한 다양한 쿼리 모달리티에 응답하며, 테스트 시간의 미세 조정(fine-tuning)을 필요로 하지 않는다.
- 토큰 대 이미지 패치 정렬(Token-to-Patch Aligner)과 세부사항 보존 주제 인코더(Detail-Preserving Subject Encoder)를 결합하여 이미지 패치와 쿼리 입력을 정렬하고 주제의 미세 특징을 추출 및 보존한다.
- 이렇게 생성된 주제 임베딩은 원래의 텍스트 임베딩과 함께 이미지 생성 과정을 조건짓는 데 사용된다.
- SSR-Encoder는 다양한 맞춤 모델과 제어 모듈에 적응할 수 있는 일반성과 효율성을 특징으로 한다.
- 임베딩 일관성 정규화 손실(Embedding Consistency Regularization Loss)에 의해 향상된 훈련으로, 다양한 환경에서 고품질 이미지 생성의 효과성을 입증한다.
- 프로젝트 페이지(https://ssr-encoder.github.io)에서 이러한 방법의 광범위한 응용 가능성을 나타내는 결과를 확인할 수 있다.

### [Hyper-VolTran: Fast and Generalizable One-Shot Image to 3D Object Structure via HyperNetworks](https://arxiv.org/abs/2312.16218)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/maqGtaamm6arax9kTD0jy.png)

Vote: 4

Authors: Christian Simon, Sen He, Juan-Manuel Perez-Rua, Frost Xu, Amine Benhalloum, Tao Xiang

- 'Hyper-VolTran'은 단일 뷰에서 이미지를 3D 구조로 변환하는 어려운 문제를 해결하기 위해 새로운 신경 렌더링 기술을 소개합니다.
- 이 방법은 부호 거리 함수(Signed Distance Function, SDF)를 표면 표현으로 사용하며, 기하학적 볼륨 및 HyperNetworks를 통해 일반화 가능한 선행 지식을 통합합니다.
- 복수의 시점에서 생성된 입력을 바탕으로 신경 인코딩 볼륨을 구성하며, 테스트 시 입력 이미지에 의존하여 새로운 장면에 모델을 적응시킬 수 있도록 SDF 네트워크의 가중치를 조정합니다.
- 생성된 시점에서 파생된 아티팩트를 완화하기 위해, 각각의 시점을 별도로 처리하는 대신 이미지 특징들을 개선하기 위해 볼륨 변환 모듈을 도입합니다.
- 이러한 방식으로 'Hyper-VolTran'은 장면별 최적화의 병목 현상을 피하면서도 다양한 관점에서 생성된 이미지 간의 일관성을 유지합니다.
- 실험을 통해 제안된 방법이 일관된 결과를 제공하고 빠른 생성이 가능함을 보여줍니다.

### [InsActor: Instruction-driven Physics-based Characters](https://arxiv.org/abs/2312.17135)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/NgABox79l28N9ovljE08b.qt)

Vote: 4

Authors: Jiawei Ren, Jiawei Ren, Mingyuan Zhang, Cunjun Yu, Xiao Ma, Liang Pan, Ziwei Liu

- 물리 기반 캐릭터들의 직관적 조작을 가능하게 하는 애니메이션 생성은 오랫동안 원하는 작업이었으나, 물리적 환경의 복잡성과 인간 언어의 풍부함 때문에 높은 수준의 인간 지시를 반영하는 물리 시뮬레이션 애니메이션을 생성하는 것은 어려운 문제입니다.
- 본 논문에서는 InsActor라는 원칙 기반 생성 프레임워크를 제시하며, 이는 최근 확산 기반 인간 동작 모델의 발전을 활용하여 물리 기반 캐릭터들의 지시 기반 애니메이션을 생산합니다.
- 프레임워크는 높은 수준의 인간 지시와 캐릭터 동작 사이의 복잡한 관계를 포착할 수 있도록 유연한 조건을 가진 동작 계획을 위한 확산 정책을 사용합니다.
- InsActor는 무효 상태와 실행 불가능한 상태 전이를 극복하기 위해 저수준 기술을 발견하고 계획을 간결한 잠재 공간 안의 잠재 기술 시퀀스에 매핑합니다.
- 광범위한 실험을 통해 InsActor는 지시 기반 동작 생성 및 지시 기반 웨이포인트 향유 등 다양한 작업에서 최신 결과를 달성함을 입증합니다.
- 특히, InsActor의 고수준 인간 지시를 사용하여 물리적으로 시뮬레이션된 애니메이션을 생성하는 능력은 풍부한 지시 세트를 가진 장기적 과제를 수행하는 데 있어 중요한 도구로서 가치가 있습니다.

### [DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaption by Combining 3D GANs and Diffusion Priors](https://arxiv.org/abs/2312.16837)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/GgjhItGoQEsEAodmplzjk.png)

Vote: 4

Authors: Biwen Lei, Kai Yu, Mengyang Feng, Miaomiao Cui, Xuansong Xie

- 본 논문에서는 텍스트 가이드 3D 도메인 적응 및 생성을 위한 새로운 프레임워크인 DiffusionGAN3D를 제안하여 3D GANs와 확산 사전을 결합하여 품질과 효율성을 향상시킨다.
- 사전 훈련된 3D 생성 모델(예: EG3D)과 텍스트-이미지 확산 모델을 통합해 안정적이고 고품질의 아바타 생성을 위한 강력한 기초를 제공한다.
- 확산 모델은 강력한 사전 정보를 제공하고 3D 생성기를 유익한 방향으로 미세 조정하여 텍스트 가이드 도메인 적응을 유연하고 효율적으로 달성한다.
- 도메인 적응의 다양성과 텍스트-아바타 생성 능력을 강화하기 위해 상대 거리 손실과 케이스별 학습 가능한 트라이플레인을 도입한다.
- 이 외에도, 도메인 적응과 텍스트-아바타 작업 모두에서 질감 품질을 개선하기 위해 점진적 질감 세련 모듈을 설계한다.
- 다양한 실험을 통해 제안된 프레임워크는 도메인 적응 및 텍스트-아바타 작업 모두에서 우수한 결과를 달성하며, 생성 품질과 효율성 측면에서 기존 방법들을 뛰어넘는 것으로 나타난다.
- 프로젝트 홈페이지는 해당 링크(https://younglbw.github.io/DiffusionGAN3D-homepage/)에서 제공된다.

### [PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with Time-Decoupled Training and Reusable Coop-Diffusion](https://arxiv.org/abs/2312.16486)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/m5oY-fpg8xbndEo1bZ2lr.png)

Vote: 4

Authors: Guansong Lu, Yuanfan Guo, Jianhua Han, Minzhe Niu, Yihan Zeng, Songcen Xu, Zeyi Huang, Zhao Zhong, Wei Zhang, Hang Xu

- 현재 대규모 확산 모델은 텍스트, 인간의 자세, 가장자리 같은 다양한 신호를 해석할 수 있는 조건부 이미지 합성에서 큰 발전을 나타내지만, 상당한 계산 자원과 광범위한 데이터 수집에 대한 의존성은 여전히 제약 요소로 남아있다.
- 기존 확산 모델들을 통합하는 것은 상호 호환되지 않는 이미지 해상도와 잠재 공간 임베딩 구조로 인해 과제가 되어왔으며, 이것은 그들의 공동 사용을 방해한다.
- 이러한 제약들을 해결하기 위해, "PanGu-Draw"라는 새로운 잠재 확산 모델은 다양한 제어 신호를 유연하게 수용하면서 자원 효율적인 텍스트-이미지 합성을 위해 고안되었다.
- 첫 번째로, 구조와 질감 생성기로 모놀리식 텍스트-이미지 모델을 분할하는 자원 효율적인 '타임-디커플링 훈련 전략'을 제안한다.
- 각 생성기는 데이터 이용과 계산 효율성을 극대화하는 방식으로 훈련되어 데이터 준비를 48% 줄이고 훈련 자원을 51% 절감한다.
- 두 번째로, "쿱-확산"이라는 알고리즘은 다양한 잠재 공간과 미리 정의된 해상도를 가진 다양한 사전 훈련된 확산 모델들의 협력적 사용을 가능하게 한다.
- 이것은 추가 데이터나 재훈련 없이 임의의 해상도에서 다중 제어 이미지 합성을 가능하게 한다.
- PanGu-Draw의 경험적 검증은 텍스트-이미지 및 다중 제어 이미지 생성에서 그 탁월한 능력을 보여줌으로써, 향후 모델 훈련의 효과성과 생성 다양성에 대한 유망한 방향을 암시한다.
- 가장 큰 5B T2I PanGu-Draw 모델은 Ascend 플랫폼에서 공개되며, 프로젝트 페이지는 https://pangu-draw.github.io 에서 확인할 수 있다.

### [Compact Neural Graphics Primitives with Learned Hash Probing](https://arxiv.org/abs/2312.17241)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/6jvxCJOCoggQFY09HOBik.png)

Vote: 3

Authors: Towaki Takikawa, Thomas Müller, Merlin Nimier-David, Alex Evans, Sanja Fidler, Alec Jacobson, Alexander Keller

- 뉴럴 네트워크가 학습 가능한 특징들을 그리드에 배열하는 공간 데이터 구조로 보강될 때, 뉴럴 그래픽 프리미티브(neural graphics primitives)의 속도와 품질이 향상된다.
- 기존의 피쳐 그리드는 대용량 메모리를 필요로 하거나(tree와 해시 테이블 같은) 느린 성능(인덱스 학습 및 벡터 양자화)을 갖는 문제점을 가지고 있다.
- 본 논문에서는 학습된 프로브 기능을 갖는 해시 테이블이 이러한 단점을 해결하여, 크기와 속도 면에서 유리한 조합을 이룬다는 것을 보여준다.
- 추론(inference)는 같은 품질의 프로브되지 않은 해시 테이블보다 빠르며, 학습 시간은 기존 인덱스 학습 방식에 비해 1.2-2.6배만 느리다.
- 이러한 구조는 모든 피쳐 그리드를 공통 프레임워크 안에 놓고, 각각의 그리드를 피처 벡터의 테이블로 인덱싱하는 조회 기능으로 캐스팅함으로써 도달하게 되었다.
- 이 프레임워크에서는 기존 데이터 구조의 조회 함수가 간단한 산술적 조합을 통해 합쳐져 압축과 속도 면에서 파레토 최적(Pareto optimal)을 달성하게 된다.

### [Restoration by Generation with Constrained Priors](https://arxiv.org/abs/2312.17161)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/73dR5DcI2lkBZMmTnsFHj.png)

Vote: 2

Authors: Zheng Ding, Xuaner Zhang, Zhuowen Tu, Zhihao Xia

- 이 연구에서는 사전에 훈련된 디노이징 확산 모델을 입력 이미지에 잡음을 추가하고 다시 정화하는 방식으로 이미지 복원 작업에 적용하는 방법을 제안합니다.
- 제안된 방법은 생성 모델의 공간을 제한해야 한다는 관찰에 기반하며, 입력 이미지의 특성을 포착하는 앵커 이미지 세트를 사용하여 생성 모델을 미세조정함으로써 이 제약을 부과합니다.
- 생성 공간을 제약한 상태로 생성을 위해 사용되는 샘플링 전략을 이미지 복원에 활용하여 신원 보존 및 이미지 품질 면에서 이전 방법들에 비해 우수한 성능을 보인다는 것을 평가했습니다.
- 또한, 개인 앨범을 앵커 이미지로 사용하는 개인화된 복원에 대한 중요하고 실용적인 응용을 보여주며, 이전 작업들이 할 수 없었던 고주파 세부 사항을 정확하게 보존하는 결과를 생성할 수 있습니다.
- 해당 연구 프로젝트의 웹페이지 주소는 https://gen2res.github.io 입니다.

### [Prompt Expansion for Adaptive Text-to-Image Generation](https://arxiv.org/abs/2312.16720)

![](https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/oSgvAwHygcjq_TtdKF2xw.png)

Vote: 2

Authors: Siddhartha Datta, Alexander Ku, Deepak Ramachandran, Peter Anderson

- 텍스트-이미지 생성 모델은 강력하지만 사용하기 어려운데, 이는 사용자들이 더 나은 이미지를 얻기 위해서 특정 프롬프트를 설정해야 하며, 그 이미지들이 반복적일 수 있기 때문입니다.
- 이 논문은 Prompt Expansion 프레임워크를 제안하며, 이는 사용자가 적은 노력으로 고품질이고 다양한 이미지를 생성할 수 있도록 돕습니다.
- Prompt Expansion 모델은 텍스트 쿼리를 입력으로 받아, 텍스트-이미지 모델에 전달될 때 매력적인 이미지의 다양성을 넓히는 것으로 최적화된 확장된 텍스트 프롬프트 세트를 출력합니다.
- 인간 평가 연구를 실시한 결과, Prompt Expansion을 통해 생성된 이미지가 기반 방법들로 생성된 이미지보다 미적으로 더 매력적이고 다양하다는 것을 밝혔습니다.
- 전반적으로, 이 논문은 텍스트-이미지 생성 경험을 향상시키기 위한 새롭고 효과적인 접근법을 제시합니다.

